{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Digital Garden","text":"<p>A living knowledge base for distributed systems, storage architecture, and infrastructure engineering</p>"},{"location":"#latest-posts","title":"\ud83d\udcdd Latest Posts","text":"<p>\u27a1\ufe0f View All Posts</p>"},{"location":"#kubectl-essentials-your-kubernetes-swiss-army-knife","title":"kubectl Essentials: Your Kubernetes Swiss Army Knife","text":"<p>Kubernetes \u00b7 13 min read</p> <p>Master kubectl commands, output formats, and productivity patterns essential for CKA exam success. Learn imperative vs declarative approaches, JSONPath queries, context management, and time-saving aliases for Kubernetes cluster management.</p> <p>Topics: kubernetes, k8s, cka-prep, kubectl</p> <p>Read more \u2192</p>"},{"location":"#setting-up-your-kubernetes-lab-environment","title":"Setting Up Your Kubernetes Lab Environment","text":"<p>Kubernetes \u00b7 Infrastructure \u00b7 12 min read</p> <p>Complete guide to setting up Kubernetes lab environments for CKA preparation. Covers kubeadm cluster setup, Minikube for local development, kind for testing, kubectl installation, and kubeconfig management with practical examples.</p> <p>Topics: kubernetes, k8s, cka-prep, kubeadm, kubectl, minikube</p> <p>Read more \u2192</p>"},{"location":"#kubernetes-architecture-fundamentals","title":"Kubernetes Architecture Fundamentals","text":"<p>Kubernetes \u00b7 Architecture \u00b7 15 min read</p> <p>Deep dive into Kubernetes cluster architecture, control plane components, and the distributed systems design that powers container orchestration at scale. Essential foundations for CKA certification with comprehensive diagrams, kubectl commands, and hands-on practice exercises.</p> <p>Topics: kubernetes, k8s, cka-prep, architecture, control-plane, kubectl</p> <p>Read more \u2192</p>"},{"location":"#high-performance-pnfs-v42-distributed-storage-architecture","title":"High-Performance pNFS v4.2 Distributed Storage Architecture","text":"<p>Storage \u00b7 Architecture \u00b7 12 min read</p> <p>A deep dive into building a clustered, high-availability parallel NFS storage system with load-balanced metadata servers, NVMe-backed storage nodes, and low-latency interconnects. Features production-grade architecture with InfiniBand/RoCE fabrics, active-active MDS clustering, and parallel data paths achieving 28 GB/s aggregate throughput.</p> <p>Topics: pNFS v4.2, distributed storage, NVMe, InfiniBand, RoCE, high availability, load balancing, metadata clustering</p> <p>Read more \u2192</p>"},{"location":"#browse-by-theme","title":"\ud83d\uddc2\ufe0f Browse by Theme","text":"<ul> <li> <p> Kubernetes CKA Mastery</p> <p>Complete CKA certification prep with 22 comprehensive posts</p> <p>Explore Kubernetes \u2192</p> </li> <li> <p> AI &amp; Automation</p> <p>Agents, MCP servers, tool orchestration, LLM workflows</p> <p>Explore AI \u2192</p> </li> <li> <p> Cloud Infrastructure</p> <p>GCP, Azure, AWS, multi-cloud architectures</p> <p>Explore Cloud \u2192</p> </li> <li> <p> Infrastructure as Code</p> <p>Terraform, Ansible, GitOps, automation</p> <p>Explore IaC \u2192</p> </li> <li> <p> On-Premise Systems</p> <p>Bare metal, datacenter, networking, storage</p> <p>Explore On-Prem \u2192</p> </li> <li> <p> Cybersecurity</p> <p>Security architecture, hardening, compliance</p> <p>Explore Security \u2192</p> </li> <li> <p> Storage &amp; Networking</p> <p>File systems, protocols, high-performance I/O</p> <p>Explore Storage \u2192</p> </li> </ul>"},{"location":"#additional-resources","title":"\ud83e\udded Additional Resources","text":"<ul> <li>Knowledge Base: Curated reference material and evergreen documentation</li> <li>Principles: Engineering principles and design patterns</li> <li>Journal: Progress logs and learning notes</li> <li>Tags: Browse all content by tag</li> </ul> <p>This garden grows continuously \u00b7 Follow on GitHub \u00b7 RSS Feed</p>"},{"location":"about/","title":"About Me","text":"Alfredo Jimenez Roa Cano <p>DevOps Engineer &amp; Infrastructure Specialist</p> <p>\ud83d\udccd Bogot\u00e1, Colombia</p>"},{"location":"about/#hey-there","title":"Hey there! \ud83d\udc4b","text":"<p>I'm a DevOps engineer passionate about building reliable, scalable infrastructure that just works. With over a decade in IT, I've gone from troubleshooting servers in data centers to orchestrating containerized workloads across cloud platforms.</p> <p>These days, you'll find me deep in Kubernetes manifests, automating everything I can, and exploring the fascinating world of distributed systems. I believe in infrastructure as code, comprehensive observability, and the kind of automation that lets you sleep soundly at night.</p> <p>When I'm not deploying to production or diving into storage architectures, I'm probably here in my digital garden\u2014documenting what I learn, preparing for certifications (currently tackling the CKA), and sharing insights from the trenches of modern infrastructure engineering.</p>"},{"location":"about/#what-i-do","title":"What I Do","text":"<ul> <li> <p> Container Orchestration</p> <p>Kubernetes architecture, deployment strategies, service mesh, and production-grade cluster management. Currently leveling up with CKA certification prep.</p> </li> <li> <p> Cloud Infrastructure</p> <p>AWS and GCP cloud platforms, multi-cloud strategies, cloud-native architectures, and cost optimization.</p> </li> <li> <p> Infrastructure as Code</p> <p>Terraform, Ansible, GitOps workflows, and declarative infrastructure management. Everything versioned, everything automated.</p> </li> <li> <p> CI/CD &amp; Automation</p> <p>Build pipelines, deployment automation, testing strategies, and continuous delivery workflows that developers actually enjoy using.</p> </li> <li> <p> Distributed Storage</p> <p>Parallel file systems (pNFS), high-performance storage architectures, and data management at scale. Fascinated by how data moves and persists.</p> </li> <li> <p> Security &amp; Compliance</p> <p>Security best practices, RBAC, secrets management, network policies, and building systems that are secure by default.</p> </li> </ul>"},{"location":"about/#current-focus","title":"Current Focus","text":"<p> Learning: Deep-diving into Kubernetes for the CKA exam\u2014covering everything from cluster architecture to advanced troubleshooting. Check out my Kubernetes CKA series for detailed notes and labs.</p> <p> Building: Growing this digital garden as a living knowledge base. It's part technical documentation, part learning journal, part reference library.</p> <p> Exploring: Distributed systems patterns, storage architectures (especially pNFS and high-performance storage), and the intersection of networking and containerization.</p>"},{"location":"about/#technical-toolkit","title":"Technical Toolkit","text":"<p>Languages &amp; Scripting: Python, Bash, YAML (does it count? \ud83d\ude04), HCL</p> <p>Platforms: Kubernetes, Docker, AWS, GCP, Linux (CentOS, Ubuntu, RHEL)</p> <p>IaC &amp; Config Management: Terraform, Ansible, Helm, Kustomize</p> <p>CI/CD: GitHub Actions, GitLab CI, Jenkins</p> <p>Monitoring &amp; Observability: Prometheus, Grafana, ELK Stack</p> <p>Storage &amp; Networking: NFS, pNFS, InfiniBand, RoCE, CNI plugins</p> <p>Languages: Spanish (native), English (fluent)</p>"},{"location":"about/#lets-connect","title":"Let's Connect","text":"<p>This digital garden is my way of learning in public and sharing what I discover along the way. If you're exploring similar topics, found something useful here, or just want to chat about Kubernetes, infrastructure, or distributed systems\u2014I'd love to hear from you.</p> <p> GitHub: github.com/alfredojrc</p> <p> Email: Feel free to reach out via the repo's contact info</p> <p>This garden is always growing. Come back often to see what's new! \ud83c\udf31</p>"},{"location":"blog/","title":"Blog","text":"<p>Technical deep-dives into distributed systems, storage architecture, and infrastructure engineering.</p>"},{"location":"blog/#navigate","title":"Navigate","text":"<ul> <li>Browse by Tags for topic-based exploration</li> <li>View the Archive for chronological browsing</li> <li>Filter by Categories below</li> </ul> <p>All posts include estimated read times and are optimized for both technical depth and practical application.</p>"},{"location":"blog/tags/","title":"Tag Index","text":"<p>Browse all posts by tag. Tags provide fine-grained topic classification across multiple dimensions.</p>"},{"location":"blog/tags/#tag:infiniband","title":"InfiniBand","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:nvme","title":"NVMe","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:roce","title":"RoCE","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:affinity","title":"affinity","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:architecture","title":"architecture","text":"<ul> <li>            Custom Resources and Operators          </li> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:autoscaling","title":"autoscaling","text":"<ul> <li>            Kubernetes Monitoring, Metrics, and Resource Management          </li> </ul>"},{"location":"blog/tags/#tag:cka-prep","title":"cka-prep","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> <li>            ConfigMaps, Secrets, and Volume Mounts          </li> <li>            CoreDNS and Service Discovery Deep Dive          </li> <li>            Custom Resources and Operators          </li> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> <li>            Helm: Kubernetes Package Manager          </li> <li>            Ingress and Gateway API: Modern Traffic Management          </li> <li>            Kubernetes Application Troubleshooting and Log Analysis          </li> <li>            Kubernetes Architecture Fundamentals          </li> <li>            Kubernetes Monitoring, Metrics, and Resource Management          </li> <li>            Kubernetes Namespaces and Resource Quotas          </li> <li>            Kubernetes Services: Exposing Your Applications          </li> <li>            Kustomize: Template-Free Kubernetes Configuration Management          </li> <li>            Network Policies: Securing Pod Communication          </li> <li>            Persistent Volumes and Claims: Stateful Storage          </li> <li>            Pods: The Atomic Unit of Kubernetes          </li> <li>            RBAC: Role-Based Access Control          </li> <li>            Security Contexts and Pod Security Standards          </li> <li>            Setting Up Your Kubernetes Lab Environment          </li> <li>            Troubleshooting Kubernetes Clusters, Nodes, and Components          </li> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:cluster-health","title":"cluster-health","text":"<ul> <li>            Troubleshooting Kubernetes Clusters, Nodes, and Components          </li> </ul>"},{"location":"blog/tags/#tag:clustering","title":"clustering","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:command-line","title":"command-line","text":"<ul> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:configmaps","title":"configmaps","text":"<ul> <li>            ConfigMaps, Secrets, and Volume Mounts          </li> </ul>"},{"location":"blog/tags/#tag:configuration","title":"configuration","text":"<ul> <li>            ConfigMaps, Secrets, and Volume Mounts          </li> <li>            Kustomize: Template-Free Kubernetes Configuration Management          </li> </ul>"},{"location":"blog/tags/#tag:containers","title":"containers","text":"<ul> <li>            Pods: The Atomic Unit of Kubernetes          </li> </ul>"},{"location":"blog/tags/#tag:control-plane","title":"control-plane","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:coredns","title":"coredns","text":"<ul> <li>            CoreDNS and Service Discovery Deep Dive          </li> </ul>"},{"location":"blog/tags/#tag:debugging","title":"debugging","text":"<ul> <li>            Kubernetes Application Troubleshooting and Log Analysis          </li> <li>            Troubleshooting Kubernetes Clusters, Nodes, and Components          </li> </ul>"},{"location":"blog/tags/#tag:deployments","title":"deployments","text":"<ul> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> </ul>"},{"location":"blog/tags/#tag:distributed-storage","title":"distributed-storage","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:dns","title":"dns","text":"<ul> <li>            CoreDNS and Service Discovery Deep Dive          </li> </ul>"},{"location":"blog/tags/#tag:endpoints","title":"endpoints","text":"<ul> <li>            Kubernetes Services: Exposing Your Applications          </li> </ul>"},{"location":"blog/tags/#tag:gateway-api","title":"gateway-api","text":"<ul> <li>            Ingress and Gateway API: Modern Traffic Management          </li> </ul>"},{"location":"blog/tags/#tag:gitops","title":"gitops","text":"<ul> <li>            Kustomize: Template-Free Kubernetes Configuration Management          </li> </ul>"},{"location":"blog/tags/#tag:helm","title":"helm","text":"<ul> <li>            Helm: Kubernetes Package Manager          </li> </ul>"},{"location":"blog/tags/#tag:high-availability","title":"high-availability","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:ingress","title":"ingress","text":"<ul> <li>            Ingress and Gateway API: Modern Traffic Management          </li> </ul>"},{"location":"blog/tags/#tag:isolation","title":"isolation","text":"<ul> <li>            Network Policies: Securing Pod Communication          </li> </ul>"},{"location":"blog/tags/#tag:k8s","title":"k8s","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> <li>            ConfigMaps, Secrets, and Volume Mounts          </li> <li>            CoreDNS and Service Discovery Deep Dive          </li> <li>            Custom Resources and Operators          </li> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> <li>            Helm: Kubernetes Package Manager          </li> <li>            Ingress and Gateway API: Modern Traffic Management          </li> <li>            Kubernetes Application Troubleshooting and Log Analysis          </li> <li>            Kubernetes Architecture Fundamentals          </li> <li>            Kubernetes Monitoring, Metrics, and Resource Management          </li> <li>            Kubernetes Namespaces and Resource Quotas          </li> <li>            Kubernetes Services: Exposing Your Applications          </li> <li>            Kustomize: Template-Free Kubernetes Configuration Management          </li> <li>            Network Policies: Securing Pod Communication          </li> <li>            Persistent Volumes and Claims: Stateful Storage          </li> <li>            Pods: The Atomic Unit of Kubernetes          </li> <li>            RBAC: Role-Based Access Control          </li> <li>            Security Contexts and Pod Security Standards          </li> <li>            Setting Up Your Kubernetes Lab Environment          </li> <li>            Troubleshooting Kubernetes Clusters, Nodes, and Components          </li> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:kubeadm","title":"kubeadm","text":"<ul> <li>            Setting Up Your Kubernetes Lab Environment          </li> </ul>"},{"location":"blog/tags/#tag:kubectl","title":"kubectl","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> <li>            Setting Up Your Kubernetes Lab Environment          </li> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:kubernetes","title":"kubernetes","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> <li>            ConfigMaps, Secrets, and Volume Mounts          </li> <li>            CoreDNS and Service Discovery Deep Dive          </li> <li>            Custom Resources and Operators          </li> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> <li>            Helm: Kubernetes Package Manager          </li> <li>            Ingress and Gateway API: Modern Traffic Management          </li> <li>            Kubernetes Application Troubleshooting and Log Analysis          </li> <li>            Kubernetes Architecture Fundamentals          </li> <li>            Kubernetes Monitoring, Metrics, and Resource Management          </li> <li>            Kubernetes Namespaces and Resource Quotas          </li> <li>            Kubernetes Services: Exposing Your Applications          </li> <li>            Kustomize: Template-Free Kubernetes Configuration Management          </li> <li>            Network Policies: Securing Pod Communication          </li> <li>            Persistent Volumes and Claims: Stateful Storage          </li> <li>            Pods: The Atomic Unit of Kubernetes          </li> <li>            RBAC: Role-Based Access Control          </li> <li>            Security Contexts and Pod Security Standards          </li> <li>            Setting Up Your Kubernetes Lab Environment          </li> <li>            Troubleshooting Kubernetes Clusters, Nodes, and Components          </li> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:kustomize","title":"kustomize","text":"<ul> <li>            Kustomize: Template-Free Kubernetes Configuration Management          </li> </ul>"},{"location":"blog/tags/#tag:labels","title":"labels","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag:load-balancing","title":"load-balancing","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:logs","title":"logs","text":"<ul> <li>            Kubernetes Application Troubleshooting and Log Analysis          </li> </ul>"},{"location":"blog/tags/#tag:manifests","title":"manifests","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag:metadata","title":"metadata","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:metrics","title":"metrics","text":"<ul> <li>            Kubernetes Monitoring, Metrics, and Resource Management          </li> </ul>"},{"location":"blog/tags/#tag:minikube","title":"minikube","text":"<ul> <li>            Setting Up Your Kubernetes Lab Environment          </li> </ul>"},{"location":"blog/tags/#tag:monitoring","title":"monitoring","text":"<ul> <li>            Kubernetes Monitoring, Metrics, and Resource Management          </li> </ul>"},{"location":"blog/tags/#tag:namespaces","title":"namespaces","text":"<ul> <li>            Kubernetes Namespaces and Resource Quotas          </li> </ul>"},{"location":"blog/tags/#tag:network-policies","title":"network-policies","text":"<ul> <li>            Network Policies: Securing Pod Communication          </li> </ul>"},{"location":"blog/tags/#tag:networking","title":"networking","text":"<ul> <li>            CoreDNS and Service Discovery Deep Dive          </li> <li>            Ingress and Gateway API: Modern Traffic Management          </li> <li>            Kubernetes Services: Exposing Your Applications          </li> <li>            Network Policies: Securing Pod Communication          </li> </ul>"},{"location":"blog/tags/#tag:node-selector","title":"node-selector","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:objects","title":"objects","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag:pnfs","title":"pNFS","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:parallel-io","title":"parallel-io","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:persistent-volumes","title":"persistent-volumes","text":"<ul> <li>            Persistent Volumes and Claims: Stateful Storage          </li> </ul>"},{"location":"blog/tags/#tag:pods","title":"pods","text":"<ul> <li>            Pods: The Atomic Unit of Kubernetes          </li> </ul>"},{"location":"blog/tags/#tag:quotas","title":"quotas","text":"<ul> <li>            Kubernetes Namespaces and Resource Quotas          </li> </ul>"},{"location":"blog/tags/#tag:rbac","title":"rbac","text":"<ul> <li>            RBAC: Role-Based Access Control          </li> </ul>"},{"location":"blog/tags/#tag:replicasets","title":"replicasets","text":"<ul> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> </ul>"},{"location":"blog/tags/#tag:resource-management","title":"resource-management","text":"<ul> <li>            Kubernetes Namespaces and Resource Quotas          </li> </ul>"},{"location":"blog/tags/#tag:rolling-updates","title":"rolling-updates","text":"<ul> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> </ul>"},{"location":"blog/tags/#tag:scheduling","title":"scheduling","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:secrets","title":"secrets","text":"<ul> <li>            ConfigMaps, Secrets, and Volume Mounts          </li> </ul>"},{"location":"blog/tags/#tag:security","title":"security","text":"<ul> <li>            Network Policies: Securing Pod Communication          </li> <li>            RBAC: Role-Based Access Control          </li> <li>            Security Contexts and Pod Security Standards          </li> </ul>"},{"location":"blog/tags/#tag:security-contexts","title":"security-contexts","text":"<ul> <li>            Security Contexts and Pod Security Standards          </li> </ul>"},{"location":"blog/tags/#tag:selectors","title":"selectors","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag:service-discovery","title":"service-discovery","text":"<ul> <li>            CoreDNS and Service Discovery Deep Dive          </li> <li>            Kubernetes Services: Exposing Your Applications          </li> </ul>"},{"location":"blog/tags/#tag:services","title":"services","text":"<ul> <li>            Kubernetes Services: Exposing Your Applications          </li> </ul>"},{"location":"blog/tags/#tag:stateful","title":"stateful","text":"<ul> <li>            Persistent Volumes and Claims: Stateful Storage          </li> </ul>"},{"location":"blog/tags/#tag:storage","title":"storage","text":"<ul> <li>            Persistent Volumes and Claims: Stateful Storage          </li> </ul>"},{"location":"blog/tags/#tag:taints","title":"taints","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:tolerations","title":"tolerations","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:traffic-management","title":"traffic-management","text":"<ul> <li>            Ingress and Gateway API: Modern Traffic Management          </li> </ul>"},{"location":"blog/tags/#tag:troubleshooting","title":"troubleshooting","text":"<ul> <li>            Kubernetes Application Troubleshooting and Log Analysis          </li> <li>            Troubleshooting Kubernetes Clusters, Nodes, and Components          </li> </ul>"},{"location":"blog/tags/#tag:workloads","title":"workloads","text":"<ul> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> <li>            Pods: The Atomic Unit of Kubernetes          </li> </ul>"},{"location":"blog/tags/#tag:yaml","title":"yaml","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag-descriptions","title":"Tag Descriptions","text":"<ul> <li>pNFS: Parallel NFS protocol and implementations</li> <li>distributed-storage: Multi-node storage architectures</li> <li>NVMe: Non-Volatile Memory Express technologies</li> <li>high-availability: HA clustering and failover systems</li> <li>load-balancing: Traffic distribution and request routing</li> <li>metadata: Metadata server design and optimization</li> <li>clustering: Cluster management and coordination</li> <li>InfiniBand: InfiniBand networking and RDMA</li> <li>RoCE: RDMA over Converged Ethernet</li> <li>parallel-io: Parallel I/O patterns and optimization</li> <li>file-systems: File system design and internals</li> <li>linux: Linux kernel and system programming</li> <li>performance-tuning: System optimization techniques</li> <li>scalability: Scaling strategies and patterns</li> </ul>"},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/","title":"Kubernetes Application Troubleshooting and Log Analysis","text":"<p>Master pod-level debugging, container log analysis, kubectl exec techniques, and ephemeral containers - essential skills for resolving application issues in the CKA exam's highest-weighted troubleshooting domain.</p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#overview","title":"Overview","text":"<p>Application troubleshooting accounts for a significant portion of the 30% Troubleshooting domain in the CKA exam. While cluster-level issues affect infrastructure, application-level problems are where developers and operators spend most of their time. This guide covers systematic approaches to diagnosing and resolving pod and container issues.</p> <pre><code>graph TD\n    A[Application Issue] --&gt; B{Pod Status?}\n\n    B --&gt;|Pending| C[Scheduling Issue]\n    B --&gt;|Running| D[Application Error]\n    B --&gt;|CrashLoopBackOff| E[Startup Failure]\n    B --&gt;|ImagePullBackOff| F[Image Problem]\n    B --&gt;|Error/Failed| G[Execution Failed]\n\n    C --&gt; H[Check Events&lt;br/&gt;Describe Pod]\n    D --&gt; I[Check Logs&lt;br/&gt;Exec into Container]\n    E --&gt; J[Previous Logs&lt;br/&gt;Init Containers]\n    F --&gt; K[Verify Image&lt;br/&gt;Check Registry]\n    G --&gt; L[Pod Events&lt;br/&gt;Exit Code]\n\n    style B fill:#ff9999\n    style C fill:#99ccff\n    style D fill:#ffcc99\n    style E fill:#cc99ff\n    style F fill:#99ff99\n    style G fill:#ffdddd</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#pod-lifecycle-and-common-states","title":"Pod Lifecycle and Common States","text":"<pre><code>graph LR\n    A[Pod Created] --&gt; B[Pending]\n    B --&gt; C[ContainerCreating]\n    C --&gt; D[Running]\n\n    D --&gt; E[Succeeded]\n    D --&gt; F[Failed]\n    D --&gt; G[CrashLoopBackOff]\n\n    B -.-&gt; H[ImagePullBackOff]\n    B -.-&gt; I[SchedulingFailed]\n\n    style D fill:#99ff99\n    style E fill:#99ccff\n    style F fill:#ff9999\n    style G fill:#ffcc99\n    style H fill:#cc99ff\n    style I fill:#ffdddd</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#pod-phase-states","title":"Pod Phase States","text":"Phase Description Common Causes Pending Pod accepted but not running yet Scheduling constraints, resource unavailable, image pull pending ContainerCreating Containers being created Normal transition state, pulling images Running Pod bound to node, \u22651 container running Normal operation Succeeded All containers terminated successfully Completed jobs/batch tasks Failed All containers terminated, \u22651 failed Application error, exit code \u2260 0 Unknown Pod state cannot be obtained Node communication failure","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#container-state-reasons-status-field","title":"Container State Reasons (Status field)","text":"Reason Description Resolution CrashLoopBackOff Container crashing repeatedly Check logs, verify startup command, fix application bug ImagePullBackOff Cannot pull container image Verify image name, check registry auth, fix imagePullSecrets CreateContainerConfigError Container config invalid Check volumeMounts, secrets, configMaps exist InvalidImageName Image name format incorrect Fix image: tag format ErrImagePull Initial image pull failed Check network, registry availability","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#pod-status-investigation","title":"Pod Status Investigation","text":"","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#basic-pod-status-commands","title":"Basic Pod Status Commands","text":"<pre><code># List pods with status\nkubectl get pods\nkubectl get pods -A  # All namespaces\nkubectl get pods -o wide  # Include node and IP\n\n# Watch pod status in real-time\nkubectl get pods -w\n\n# Filter by status\nkubectl get pods --field-selector=status.phase=Pending\nkubectl get pods --field-selector=status.phase=Failed\nkubectl get pods -A | grep -v Running\n</code></pre> <p>Example Output: <pre><code>NAME                    READY   STATUS             RESTARTS   AGE\nnginx-deployment-abc    1/1     Running            0          5m\nmyapp-xyz               0/1     CrashLoopBackOff   5          2m\nfrontend-123            0/1     ImagePullBackOff   0          1m\nbackend-456             0/1     CreateContainerConfigError  0  30s\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#detailed-pod-inspection","title":"Detailed Pod Inspection","text":"<pre><code># Detailed pod information\nkubectl describe pod &lt;pod-name&gt;\nkubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n\n# Get full pod YAML\nkubectl get pod &lt;pod-name&gt; -o yaml\nkubectl get pod &lt;pod-name&gt; -o json | jq .status\n\n# Check pod events\nkubectl get events --field-selector involvedObject.name=&lt;pod-name&gt;\nkubectl get events -A --sort-by='.lastTimestamp'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#container-logs-analysis","title":"Container Logs Analysis","text":"<pre><code>sequenceDiagram\n    participant App as Application\n    participant Con as Container\n    participant KB as Kubelet\n    participant Logs as Log Storage\n\n    App-&gt;&gt;Con: Write to stdout/stderr\n    Con-&gt;&gt;KB: Forward logs\n    KB-&gt;&gt;Logs: Store logs (/var/log/pods/)\n\n    Note over Logs: kubectl logs reads from here\n\n    User-&gt;&gt;KB: kubectl logs &lt;pod&gt;\n    KB-&gt;&gt;Logs: Retrieve logs\n    Logs-&gt;&gt;User: Return log output</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#kubectl-logs-command","title":"kubectl logs Command","text":"<p>Basic Log Retrieval: <pre><code># Get logs from a pod\nkubectl logs &lt;pod-name&gt;\n\n# Get logs from specific container (multi-container pod)\nkubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;\n\n# Get previous container logs (if container crashed)\nkubectl logs &lt;pod-name&gt; --previous\nkubectl logs &lt;pod-name&gt; -c &lt;container-name&gt; --previous\n\n# Follow logs in real-time (like tail -f)\nkubectl logs -f &lt;pod-name&gt;\nkubectl logs -f &lt;pod-name&gt; -c &lt;container-name&gt;\n\n# Get logs from all containers in pod\nkubectl logs &lt;pod-name&gt; --all-containers=true\n\n# Get logs with timestamps\nkubectl logs &lt;pod-name&gt; --timestamps\n\n# Get recent logs (last N lines)\nkubectl logs &lt;pod-name&gt; --tail=50\n\n# Get logs since specific time\nkubectl logs &lt;pod-name&gt; --since=10m\nkubectl logs &lt;pod-name&gt; --since=1h\nkubectl logs &lt;pod-name&gt; --since-time=2025-11-11T10:00:00Z\n\n# Limit log bytes returned\nkubectl logs &lt;pod-name&gt; --limit-bytes=1024\n</code></pre></p> <p>Log Filtering and Analysis: <pre><code># Filter logs for errors\nkubectl logs &lt;pod-name&gt; | grep -i error\nkubectl logs &lt;pod-name&gt; | grep -E \"error|exception|fail\"\n\n# Count error occurrences\nkubectl logs &lt;pod-name&gt; | grep -c error\n\n# Get logs from multiple pods (by label)\nkubectl logs -l app=myapp\nkubectl logs -l app=myapp --all-containers=true\n\n# Export logs to file\nkubectl logs &lt;pod-name&gt; &gt; /tmp/pod-logs.txt\nkubectl logs &lt;pod-name&gt; --previous &gt; /tmp/crash-logs.txt\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#common-log-patterns","title":"Common Log Patterns","text":"<p>Startup Issues: <pre><code>Failed to load configuration file: /etc/app/config.yaml\nError: listen tcp :8080: bind: address already in use\npanic: runtime error: invalid memory address\n</code></pre></p> <p>Connection Issues: <pre><code>dial tcp 10.96.0.1:443: i/o timeout\nFailed to connect to database: connection refused\nCannot resolve host: mysql-service\n</code></pre></p> <p>Permission Issues: <pre><code>mkdir /data: permission denied\nError: cannot open file /var/log/app.log: Permission denied\nchown: changing ownership of '/data': Operation not permitted\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#interactive-container-debugging-with-kubectl-exec","title":"Interactive Container Debugging with kubectl exec","text":"<pre><code># Execute command in container\nkubectl exec &lt;pod-name&gt; -- &lt;command&gt;\nkubectl exec &lt;pod-name&gt; -c &lt;container-name&gt; -- &lt;command&gt;\n\n# Interactive shell access\nkubectl exec -it &lt;pod-name&gt; -- /bin/sh\nkubectl exec -it &lt;pod-name&gt; -- /bin/bash\nkubectl exec -it &lt;pod-name&gt; -c &lt;container-name&gt; -- sh\n\n# Common debugging commands\nkubectl exec &lt;pod-name&gt; -- ps aux\nkubectl exec &lt;pod-name&gt; -- ls -la /app\nkubectl exec &lt;pod-name&gt; -- cat /etc/resolv.conf\nkubectl exec &lt;pod-name&gt; -- env\nkubectl exec &lt;pod-name&gt; -- df -h\nkubectl exec &lt;pod-name&gt; -- netstat -tulpn\n\n# Test network connectivity\nkubectl exec &lt;pod-name&gt; -- ping -c 3 google.com\nkubectl exec &lt;pod-name&gt; -- curl http://service-name:8080\nkubectl exec &lt;pod-name&gt; -- nslookup kubernetes.default\n\n# Check running processes\nkubectl exec &lt;pod-name&gt; -- ps aux | grep java\nkubectl exec &lt;pod-name&gt; -- top -bn1 | head -20\n</code></pre> <p>Interactive Debugging Session Example: <pre><code># Start interactive shell\nkubectl exec -it myapp-xyz -- /bin/sh\n\n# Inside container:\n/ # ps aux                        # Check processes\n/ # ls -la /app                   # List app files\n/ # cat /app/config.yaml          # Check config\n/ # curl localhost:8080/health    # Test endpoint\n/ # tail -f /var/log/app.log      # View logs\n/ # exit                           # Exit shell\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#ephemeral-containers-kubernetes-v125","title":"Ephemeral Containers (Kubernetes v1.25+)","text":"<p>Ephemeral containers are temporary debug containers added to running pods - perfect for distroless images or crashed containers without shells.</p> <pre><code>graph TD\n    A[Running Pod] --&gt; B{Has Shell?}\n    B --&gt;|Yes| C[kubectl exec]\n    B --&gt;|No| D[Ephemeral Container]\n\n    B2[Container Crashed?] --&gt; E{Previous Logs Available?}\n    E --&gt;|Yes| F[kubectl logs --previous]\n    E --&gt;|No| G[Ephemeral Container]\n\n    D --&gt; H[kubectl debug with --image]\n    G --&gt; H\n\n    H --&gt; I[Debugging Tools Available]\n    I --&gt; J[Interactive Shell]\n    I --&gt; K[Process Inspection]\n    I --&gt; L[Network Debugging]\n\n    style A fill:#99ccff\n    style D fill:#ff9999\n    style G fill:#ff9999\n    style H fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#using-kubectl-debug-with-ephemeral-containers","title":"Using kubectl debug with Ephemeral Containers","text":"<p>Basic Ephemeral Container Usage: <pre><code># Add ephemeral debug container to running pod\nkubectl debug -it &lt;pod-name&gt; --image=busybox:1.28 --target=&lt;container-name&gt;\n\n# Common debug images\nkubectl debug -it &lt;pod-name&gt; --image=busybox --target=&lt;container-name&gt;\nkubectl debug -it &lt;pod-name&gt; --image=alpine --target=&lt;container-name&gt;\nkubectl debug -it &lt;pod-name&gt; --image=ubuntu --target=&lt;container-name&gt;\n\n# Debug container without target (separate namespaces)\nkubectl debug -it &lt;pod-name&gt; --image=busybox\n\n# Create debug container with custom name\nkubectl debug -it &lt;pod-name&gt; --image=busybox --container=debugger\n\n# List ephemeral containers in pod\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.ephemeralContainers}'\nkubectl describe pod &lt;pod-name&gt; | grep -A 10 \"Ephemeral Containers\"\n</code></pre></p> <p>Example: Debug Distroless Container: <pre><code># Create minimal pod (pause container has no shell)\nkubectl run ephemeral-demo --image=registry.k8s.io/pause:3.1 --restart=Never\n\n# Try to exec (will fail)\nkubectl exec -it ephemeral-demo -- sh\n# Error: OCI runtime exec failed: exec: \"sh\": executable file not found in $PATH\n\n# Add ephemeral debug container\nkubectl debug -it ephemeral-demo --image=busybox:1.28 --target=ephemeral-demo\n\n# Now you have a shell with debugging tools\n/ # ps aux  # See processes from target container\n/ # ls -la /proc/1/root/  # Access target container filesystem\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#debug-by-copying-pod","title":"Debug by Copying Pod","text":"<p>Create Pod Copy with Modifications: <pre><code># Copy pod and change command\nkubectl debug &lt;pod-name&gt; -it --copy-to=&lt;new-pod-name&gt; --container=&lt;container-name&gt; -- sh\n\n# Copy pod and change image\nkubectl debug &lt;pod-name&gt; --copy-to=&lt;new-pod-name&gt; --set-image=*=ubuntu\n\n# Copy pod with specific image per container\nkubectl debug &lt;pod-name&gt; --copy-to=&lt;new-pod-name&gt; --set-image=app=myapp:debug,sidecar=sidecar:debug\n</code></pre></p> <p>Example: Debug CrashLoopBackOff: <pre><code># Pod crashing immediately\nkubectl run myapp --image=busybox:1.28 -- false\n\n# Check status\nkubectl get pod myapp\n# NAME    READY   STATUS             RESTARTS   AGE\n# myapp   0/1     CrashLoopBackOff   3          1m\n\n# Create debug copy with shell\nkubectl debug myapp -it --copy-to=myapp-debug --container=myapp -- sh\n\n# Now in debug pod with working shell\n/ # ls -la\n/ # env\n/ # ps aux\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#crashloopbackoff","title":"CrashLoopBackOff","text":"<p>CrashLoopBackOff means container starts, crashes, Kubernetes restarts it, crashes again - exponential backoff.</p> <pre><code>graph LR\n    A[Container Starts] --&gt; B[Crashes]\n    B --&gt; C[Wait 0s]\n    C --&gt; D[Restart 1]\n    D --&gt; B2[Crashes]\n    B2 --&gt; C2[Wait 10s]\n    C2 --&gt; D2[Restart 2]\n    D2 --&gt; B3[Crashes]\n    B3 --&gt; C3[Wait 20s]\n    C3 --&gt; D3[Restart 3]\n    D3 --&gt; B4[Crashes]\n    B4 --&gt; C4[Wait 40s]\n    C4 --&gt; D4[...]\n\n    style B fill:#ff9999\n    style B2 fill:#ff9999\n    style B3 fill:#ff9999\n    style B4 fill:#ff9999</code></pre> <p>Troubleshooting Steps: <pre><code># 1. Check current pod status\nkubectl get pod &lt;pod-name&gt;\nkubectl describe pod &lt;pod-name&gt;\n\n# 2. Check logs from crashed container\nkubectl logs &lt;pod-name&gt; --previous\n\n# 3. Check for common issues in describe output\nkubectl describe pod &lt;pod-name&gt; | grep -A 10 \"Events:\"\n\n# 4. Check container exit code\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.status.containerStatuses[0].lastState.terminated}'\n\n# 5. Verify container command and args\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.containers[0].command}'\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.containers[0].args}'\n</code></pre></p> <p>Common Causes:</p> Exit Code Meaning Common Cause 0 Success Normal termination (check if container should keep running) 1 Application Error Bug in code, unhandled exception 2 Misuse of Shell Builtin Command syntax error 126 Command Cannot Execute Permission problem, not executable 127 Command Not Found Typo in command, binary not in PATH 137 SIGKILL OOMKilled (out of memory), forceful termination 139 SIGSEGV Segmentation fault (memory access violation) 143 SIGTERM Graceful termination requested <p>Example: Fix CrashLoopBackOff: <pre><code># Pod crashing with exit code 127\nkubectl logs myapp --previous\n# Output: /bin/myapp: not found\n\n# Check container definition\nkubectl get pod myapp -o yaml | grep -A 5 \"command:\"\n# command:\n# - /bin/myapp  # Wrong path\n\n# Fix: Update deployment with correct path\nkubectl edit deployment myapp\n# Change command: [\"/bin/myapp\"] to command: [\"/usr/local/bin/myapp\"]\n\n# Verify fix\nkubectl get pods\n# myapp-new-xyz   1/1     Running   0   10s\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#imagepullbackoff","title":"ImagePullBackOff","text":"<p>ImagePullBackOff occurs when Kubernetes cannot pull the container image.</p> <p>Troubleshooting Steps: <pre><code># 1. Check pod events\nkubectl describe pod &lt;pod-name&gt; | grep -A 5 \"Events:\"\n# Events:\n#   Failed    ErrImagePull        Failed to pull image \"myapp:v2\": rpc error: code = Unknown desc = Error response from daemon: pull access denied\n\n# 2. Verify image name and tag\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.containers[0].image}'\n\n# 3. Check imagePullSecrets\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.imagePullSecrets}'\n\n# 4. Test image pull manually (on node)\nssh &lt;node&gt;\nsudo crictl pull &lt;image-name&gt;\n\n# 5. Check secret exists\nkubectl get secrets\nkubectl describe secret &lt;image-pull-secret&gt;\n</code></pre></p> <p>Common Causes: - Image doesn't exist: Typo in image name or tag - Registry authentication: Missing or incorrect imagePullSecret - Private registry: Need to create imagePullSecret - Network issue: Registry unreachable from cluster - Rate limiting: Docker Hub rate limits exceeded</p> <p>Fix: Create imagePullSecret: <pre><code># Create Docker registry secret\nkubectl create secret docker-registry regcred \\\n  --docker-server=&lt;registry-url&gt; \\\n  --docker-username=&lt;username&gt; \\\n  --docker-password=&lt;password&gt; \\\n  --docker-email=&lt;email&gt;\n\n# Add to pod spec\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\nspec:\n  containers:\n  - name: app\n    image: private-registry.com/myapp:v1\n  imagePullSecrets:\n  - name: regcred\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#createcontainerconfigerror","title":"CreateContainerConfigError","text":"<p>CreateContainerConfigError indicates invalid container configuration.</p> <p>Common Causes: <pre><code># 1. Check for missing ConfigMaps\nkubectl describe pod &lt;pod-name&gt; | grep -i configmap\n# configmap \"app-config\" not found\n\n# Verify ConfigMap exists\nkubectl get configmaps\nkubectl get configmap app-config\n\n# 2. Check for missing Secrets\nkubectl describe pod &lt;pod-name&gt; | grep -i secret\n# secret \"db-password\" not found\n\n# Verify Secret exists\nkubectl get secrets\nkubectl get secret db-password\n\n# 3. Check volume mounts\nkubectl get pod &lt;pod-name&gt; -o yaml | grep -A 10 volumeMounts\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#event-analysis","title":"Event Analysis","text":"<p>Events provide timeline of pod lifecycle actions - critical for troubleshooting.</p> <pre><code># Get events for specific pod\nkubectl get events --field-selector involvedObject.name=&lt;pod-name&gt;\n\n# Get events for namespace\nkubectl get events -n &lt;namespace&gt;\n\n# Get all events sorted by time\nkubectl get events -A --sort-by='.lastTimestamp'\n\n# Watch events in real-time\nkubectl get events -w\n\n# Filter events by type\nkubectl get events --field-selector type=Warning\nkubectl get events --field-selector type=Normal\n\n# Get events in wide format\nkubectl get events -o wide\n</code></pre> <p>Example Event Output: <pre><code>LAST SEEN   TYPE      REASON              OBJECT              MESSAGE\n2m          Warning   Failed              pod/myapp-xyz       Error: ImagePullBackOff\n2m          Normal    Pulling             pod/myapp-xyz       Pulling image \"myapp:v2\"\n3m          Normal    Scheduled           pod/myapp-xyz       Successfully assigned default/myapp-xyz to node-1\n5m          Warning   FailedScheduling    pod/myapp-xyz       0/3 nodes available: insufficient memory\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#init-container-troubleshooting","title":"Init Container Troubleshooting","text":"<p>Init containers run before app containers - often used for setup tasks.</p> <pre><code>sequenceDiagram\n    participant P as Pod\n    participant I1 as Init Container 1\n    participant I2 as Init Container 2\n    participant A as App Container\n\n    P-&gt;&gt;I1: Start\n    I1-&gt;&gt;I1: Run to completion\n    I1--&gt;&gt;P: Exit 0 (success)\n\n    P-&gt;&gt;I2: Start\n    I2-&gt;&gt;I2: Run to completion\n    I2--xP: Exit 1 (failure)\n\n    Note over P,A: App container never starts\n\n    P-&gt;&gt;P: Status: Init:CrashLoopBackOff</code></pre> <p>Debug Init Containers: <pre><code># Check init container status\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.status.initContainerStatuses}'\n\n# Get init container logs\nkubectl logs &lt;pod-name&gt; -c &lt;init-container-name&gt;\nkubectl logs &lt;pod-name&gt; -c &lt;init-container-name&gt; --previous\n\n# Describe pod to see init container events\nkubectl describe pod &lt;pod-name&gt;\n# Init Containers:\n#   init-db:\n#     State:      Terminated\n#       Reason:   Error\n#       Exit Code:  1\n</code></pre></p> <p>Example Init Container Failure: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\nspec:\n  initContainers:\n  - name: init-db\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']\n  containers:\n  - name: app\n    image: myapp:v1\n</code></pre></p> <pre><code># Check status\nkubectl get pod myapp\n# NAME    READY   STATUS     RESTARTS   AGE\n# myapp   0/1     Init:0/1   0          1m\n\n# Check init logs\nkubectl logs myapp -c init-db\n# waiting for mydb\n# waiting for mydb\n# Server:    10.96.0.10\n# Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n# nslookup: can't resolve 'mydb'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#multi-container-pod-troubleshooting","title":"Multi-Container Pod Troubleshooting","text":"<p>Multi-container pods share network and storage - common patterns: sidecar, adapter, ambassador.</p> <pre><code># List all containers in pod\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.containers[*].name}'\n\n# Check status of each container\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.status.containerStatuses[*].name}'\n\n# Get logs from specific container\nkubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;\n\n# Exec into specific container\nkubectl exec -it &lt;pod-name&gt; -c &lt;container-name&gt; -- sh\n\n# Check which container is failing\nkubectl get pod &lt;pod-name&gt; -o json | jq '.status.containerStatuses[] | select(.ready==false)'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#cka-exam-practice-exercises","title":"CKA Exam Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#exercise-1-debug-crashloopbackoff-pod","title":"Exercise 1: Debug CrashLoopBackOff Pod","text":"<p>Scenario: Pod <code>webapp-abc</code> is in CrashLoopBackOff state. Investigate and fix the issue.</p> Solution <pre><code># 1. Check pod status\nkubectl get pod webapp-abc\n# NAME         READY   STATUS             RESTARTS   AGE\n# webapp-abc   0/1     CrashLoopBackOff   5          2m\n\n# 2. Describe pod\nkubectl describe pod webapp-abc\n# Last State:     Terminated\n#   Reason:       Error\n#   Exit Code:    127\n\n# 3. Check logs from crashed container\nkubectl logs webapp-abc --previous\n# /bin/webapp: not found\n\n# 4. Check container command\nkubectl get pod webapp-abc -o yaml | grep -A 5 command:\n# command:\n# - /bin/webapp\n\n# 5. Create debug copy to investigate\nkubectl debug webapp-abc -it --copy-to=webapp-debug --container=webapp -- sh\n\n# Inside debug container\n/ # ls /usr/local/bin/\n# webapp  # Binary is here, not in /bin/\n\n/ # exit\n\n# 6. Fix the deployment\nkubectl get deployment webapp -o yaml &gt; webapp-deploy.yaml\n\n# Edit webapp-deploy.yaml:\n# Change: command: [\"/bin/webapp\"]\n# To: command: [\"/usr/local/bin/webapp\"]\n\nkubectl apply -f webapp-deploy.yaml\n\n# 7. Delete old pod (deployment will recreate)\nkubectl delete pod webapp-abc\n\n# 8. Verify fix\nkubectl get pods | grep webapp\n# webapp-xyz   1/1     Running   0   10s\n\nkubectl logs webapp-xyz\n# Server started on port 8080\n</code></pre>  **Key Takeaways**: - Exit code 127 = command not found - Use `--previous` flag for crashed container logs - `kubectl debug --copy-to` allows interactive investigation - Fix deployment, not individual pod","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#exercise-2-troubleshoot-imagepullbackoff","title":"Exercise 2: Troubleshoot ImagePullBackOff","text":"<p>Scenario: Pod <code>frontend-123</code> cannot pull image from private registry. Fix the image pull issue.</p> Solution <pre><code># 1. Check pod status\nkubectl get pod frontend-123\n# NAME           READY   STATUS             RESTARTS   AGE\n# frontend-123   0/1     ImagePullBackOff   0          1m\n\n# 2. Describe pod to see error\nkubectl describe pod frontend-123\n# Events:\n#   Failed     ErrImagePull    Failed to pull image \"private-registry.com/frontend:v2\"\n#   Failed     ImagePullBackOff  Back-off pulling image\n\n# 3. Check current imagePullSecrets\nkubectl get pod frontend-123 -o jsonpath='{.spec.imagePullSecrets}'\n# []  (empty - no secret configured)\n\n# 4. Check if image pull secret exists\nkubectl get secrets | grep registry\n# (no results)\n\n# 5. Create docker registry secret\nkubectl create secret docker-registry private-registry-cred \\\n  --docker-server=private-registry.com \\\n  --docker-username=myuser \\\n  --docker-password=mypassword \\\n  --docker-email=user@example.com\n\n# Verify secret created\nkubectl get secret private-registry-cred\n# NAME                      TYPE                             DATA   AGE\n# private-registry-cred     kubernetes.io/dockerconfigjson   1      5s\n\n# 6. Patch deployment to add imagePullSecrets\nkubectl get deployment frontend -o yaml &gt; frontend-deploy.yaml\n\n# Add to pod spec:\n#   imagePullSecrets:\n#   - name: private-registry-cred\n\nkubectl apply -f frontend-deploy.yaml\n\n# 7. Delete old pod (deployment will recreate with secret)\nkubectl delete pod frontend-123\n\n# 8. Verify new pod pulls image successfully\nkubectl get pods | grep frontend\n# frontend-xyz   1/1     Running   0   20s\n\nkubectl describe pod frontend-xyz | grep -A 5 \"Events:\"\n# Events:\n#   Normal  Pulling    Successfully pulled image \"private-registry.com/frontend:v2\"\n#   Normal  Started    Started container frontend\n</code></pre>  **Key Takeaways**: - ImagePullBackOff usually means authentication issue - Create secret: `kubectl create secret docker-registry` - Add `imagePullSecrets` to pod spec - Apply changes to deployment, not pod directly","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#exercise-3-debug-init-container-failure","title":"Exercise 3: Debug Init Container Failure","text":"<p>Scenario: Pod <code>myapp-xyz</code> stuck in Init:CrashLoopBackOff. The init container is failing. Investigate and fix.</p> Solution <pre><code># 1. Check pod status\nkubectl get pod myapp-xyz\n# NAME        READY   STATUS                  RESTARTS   AGE\n# myapp-xyz   0/1     Init:CrashLoopBackOff   3          2m\n\n# 2. Describe pod\nkubectl describe pod myapp-xyz\n# Init Containers:\n#   init-db:\n#     State:      Waiting\n#       Reason:   CrashLoopBackOff\n#     Last State: Terminated\n#       Reason:   Error\n#       Exit Code:    1\n\n# 3. List init containers\nkubectl get pod myapp-xyz -o jsonpath='{.spec.initContainers[*].name}'\n# init-db\n\n# 4. Check init container logs\nkubectl logs myapp-xyz -c init-db\n# waiting for mydb-service\n# nslookup: can't resolve 'mydb-service'\n# waiting for mydb-service\n# nslookup: can't resolve 'mydb-service'\n\n# 5. Check if service exists\nkubectl get svc | grep mydb\n# (no results - service doesn't exist!)\n\n# 6. Check deployment YAML\nkubectl get deployment myapp -o yaml | grep -A 10 initContainers:\n# initContainers:\n# - name: init-db\n#   image: busybox:1.28\n#   command: ['sh', '-c', 'until nslookup mydb-service; do echo waiting for mydb-service; sleep 2; done;']\n\n# 7. Verify database service name\nkubectl get svc -A | grep -i db\n# database    mysql-db   ClusterIP   10.96.5.10   &lt;none&gt;   3306/TCP   5m\n\n# Service is named \"mysql-db\", not \"mydb-service\"!\n\n# 8. Fix deployment\nkubectl get deployment myapp -o yaml &gt; myapp-deploy.yaml\n\n# Edit myapp-deploy.yaml:\n# Change: nslookup mydb-service\n# To: nslookup mysql-db\n\nkubectl apply -f myapp-deploy.yaml\n\n# 9. Delete old pod\nkubectl delete pod myapp-xyz\n\n# 10. Verify new pod starts successfully\nkubectl get pods | grep myapp\n# myapp-abc   1/1     Running   0   15s\n\nkubectl logs myapp-abc -c init-db\n# waiting for mysql-db\n# Server:    10.96.0.10\n# Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n# Name:      mysql-db\n# Address 1: 10.96.5.10 mysql-db.database.svc.cluster.local\n# (init container succeeded!)\n</code></pre>  **Key Takeaways**: - Use `-c ` to view init logs - Init containers must succeed before app containers start - Verify service names match what init containers expect - `nslookup` or `curl` are common init container checks","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#exercise-4-use-ephemeral-containers-for-debugging","title":"Exercise 4: Use Ephemeral Containers for Debugging","text":"<p>Scenario: Pod <code>distroless-app</code> is running a distroless image with no shell. Debug the application using ephemeral containers.</p> Solution <pre><code># 1. Check pod status\nkubectl get pod distroless-app\n# NAME             READY   STATUS    RESTARTS   AGE\n# distroless-app   1/1     Running   0          5m\n\n# 2. Try to exec (will fail - no shell)\nkubectl exec -it distroless-app -- sh\n# OCI runtime exec failed: exec: \"sh\": executable file not found in $PATH\n\n# 3. Add ephemeral debug container\nkubectl debug -it distroless-app --image=busybox:1.28 --target=distroless-app\n\n# Output:\n# Targeting container \"distroless-app\". If you don't see processes from this container it may be because the container runtime doesn't support this feature.\n# Defaulting debug container name to debugger-xyz.\n# If you don't see a command prompt, try pressing enter.\n\n# 4. Now in ephemeral container with busybox tools\n/ # ps aux\n# PID   USER     TIME  COMMAND\n#     1 root      0:00 /app/distroless-binary\n#    15 root      0:00 sh\n\n# 5. Inspect application files (target container's filesystem)\n/ # ls -la /proc/1/root/\n# (shows files from distroless-app container)\n\n/ # ls -la /proc/1/root/app/\n# total 12345\n# -rwxr-xr-x    1 root     root      12345678 Nov 11 12:00 distroless-binary\n# -rw-r--r--    1 root     root           256 Nov 11 12:00 config.yaml\n\n# 6. Check network connectivity\n/ # netstat -tulpn\n/ # ping -c 3 google.com\n\n# 7. View environment variables from target container\n/ # cat /proc/1/environ | tr '\\0' '\\n'\n# PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n# APP_PORT=8080\n# DB_HOST=mysql-db\n\n# 8. Test application endpoint\n/ # wget -qO- http://localhost:8080/health\n# {\"status\":\"healthy\"}\n\n# 9. Exit ephemeral container\n/ # exit\n\n# 10. Verify ephemeral container was added\nkubectl get pod distroless-app -o jsonpath='{.spec.ephemeralContainers[*].name}'\n# debugger-xyz\n\nkubectl describe pod distroless-app | grep -A 10 \"Ephemeral Containers:\"\n# Ephemeral Containers:\n#   debugger-xyz:\n#     Container ID:  containerd://abc123...\n#     Image:         busybox:1.28\n#     State:         Terminated\n</code></pre>  **Key Takeaways**: - Ephemeral containers solve \"no shell\" problem in minimal images - `--target=` shares process namespace (see target's processes) - Access target's filesystem via `/proc/1/root/` - Ephemeral containers persist until pod deletion","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#exercise-5-multi-container-pod-troubleshooting","title":"Exercise 5: Multi-Container Pod Troubleshooting","text":"<p>Scenario: Pod <code>app-with-sidecar</code> has 2 containers: <code>app</code> (main) and <code>log-shipper</code> (sidecar). Pod shows READY 1/2. Determine which container is failing and fix it.</p> Solution <pre><code># 1. Check pod status\nkubectl get pod app-with-sidecar\n# NAME               READY   STATUS    RESTARTS   AGE\n# app-with-sidecar   1/2     Running   0          2m\n\n# READY 1/2 means one container is not ready\n\n# 2. Describe pod to identify failing container\nkubectl describe pod app-with-sidecar\n# Containers:\n#   app:\n#     State:          Running\n#     Ready:          True\n#   log-shipper:\n#     State:          Running\n#     Ready:          False\n#     Reason:         Unhealthy\n\n# log-shipper is not ready!\n\n# 3. Check log-shipper logs\nkubectl logs app-with-sidecar -c log-shipper\n# Error: cannot open file /var/log/app/application.log: No such file or directory\n# Retrying in 5 seconds...\n\n# 4. Check shared volume mounts\nkubectl get pod app-with-sidecar -o yaml | grep -A 20 volumeMounts:\n# app container:\n#   volumeMounts:\n#   - mountPath: /var/log/myapp\n#     name: log-volume\n#\n# log-shipper container:\n#   volumeMounts:\n#   - mountPath: /var/log/app\n#     name: log-volume\n\n# Issue: app writes to /var/log/myapp but log-shipper reads from /var/log/app\n\n# 5. Check readiness probe\nkubectl get pod app-with-sidecar -o yaml | grep -A 5 \"readinessProbe:\"\n#     readinessProbe:\n#       exec:\n#         command:\n#         - cat\n#         - /var/log/app/application.log\n\n# Readiness probe fails because file doesn't exist at expected path\n\n# 6. Fix deployment - align volume mount paths\nkubectl get deployment app-with-sidecar -o yaml &gt; app-deploy.yaml\n\n# Edit app-deploy.yaml:\n# Change log-shipper volumeMount:\n#   - mountPath: /var/log/app  # Old\n#   - mountPath: /var/log/myapp  # New (match app container)\n\nkubectl apply -f app-deploy.yaml\n\n# 7. Delete old pod\nkubectl delete pod app-with-sidecar\n\n# 8. Verify new pod has both containers ready\nkubectl get pods | grep app-with-sidecar\n# app-with-sidecar   2/2     Running   0   20s\n\n# 9. Verify log-shipper is working\nkubectl logs app-with-sidecar -c log-shipper\n# Shipping logs from /var/log/myapp/application.log\n# Successfully connected to log aggregator\n\n# 10. Verify both containers are ready\nkubectl get pod app-with-sidecar -o jsonpath='{range .status.containerStatuses[*]}{.name}{\"\\t\"}{.ready}{\"\\n\"}{end}'\n# app            true\n# log-shipper    true\n</code></pre>  **Key Takeaways**: - READY x/y indicates container readiness (x ready out of y total) - Use `-c ` to target specific container - Volume mount paths must align between containers sharing volumes - Readiness probes determine READY status","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#quick-reference-commands","title":"Quick Reference Commands","text":"","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#log-analysis","title":"Log Analysis","text":"<pre><code>kubectl logs &lt;pod&gt; [-c &lt;container&gt;]         # Get logs\nkubectl logs &lt;pod&gt; --previous               # Previous crashed logs\nkubectl logs &lt;pod&gt; -f                       # Follow logs\nkubectl logs &lt;pod&gt; --tail=50                # Last 50 lines\nkubectl logs &lt;pod&gt; --since=10m              # Logs from last 10 min\nkubectl logs &lt;pod&gt; --timestamps             # Include timestamps\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#interactive-debugging","title":"Interactive Debugging","text":"<pre><code>kubectl exec &lt;pod&gt; -- &lt;command&gt;             # Execute command\nkubectl exec -it &lt;pod&gt; -- sh                # Interactive shell\nkubectl debug &lt;pod&gt; --image=busybox --target=&lt;container&gt;  # Ephemeral container\nkubectl debug &lt;pod&gt; --copy-to=&lt;new&gt; --set-image=*=debug   # Copy with debug image\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#status-investigation","title":"Status Investigation","text":"<pre><code>kubectl get pods                            # List pod status\nkubectl describe pod &lt;pod&gt;                  # Detailed pod info\nkubectl get events --field-selector involvedObject.name=&lt;pod&gt;  # Pod events\nkubectl get pod &lt;pod&gt; -o yaml               # Full pod YAML\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#related-resources","title":"Related Resources","text":"<ul> <li>Troubleshooting Clusters, Nodes, and Components - Cluster-level debugging</li> <li>Pods: The Atomic Unit - Understanding pod lifecycle</li> <li>kubectl Essentials - Master debugging commands</li> </ul>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/application-troubleshooting-log-analysis/#summary","title":"Summary","text":"<p>Application troubleshooting is systematic and methodical - always start with pod status, check events, analyze logs, then move to interactive debugging if needed. Master these techniques to confidently tackle pod-level issues in the CKA exam's Troubleshooting domain (30%).</p> <p>Key Takeaways: - \u2705 Pod Status \u2192 Start with <code>kubectl get pods</code>, then <code>kubectl describe pod</code> - \u2705 Logs \u2192 Use <code>kubectl logs</code>, add <code>--previous</code> for crashed containers - \u2705 Events \u2192 Check <code>kubectl get events</code> for timeline of issues - \u2705 Interactive Debug \u2192 <code>kubectl exec</code> for shell access, <code>kubectl debug</code> for ephemeral containers - \u2705 Common Issues \u2192 CrashLoopBackOff (check exit codes), ImagePullBackOff (check secrets), Init failures (check logs)</p> <p>Next: Monitoring, Metrics, and Resource Management - Metrics Server, HPA, VPA, and observability</p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","logs","debugging"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/","title":"Troubleshooting Kubernetes Clusters, Nodes, and Components","text":"<p>Master systematic cluster troubleshooting for node issues, control plane debugging, certificate problems, and etcd health checks - essential skills for the CKA exam's highest-weighted domain.</p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#overview","title":"Overview","text":"<p>Troubleshooting is the highest-weighted CKA exam domain at 30% - mastering systematic cluster debugging is essential for certification and real-world cluster operations. This guide covers methodical approaches to diagnosing and resolving cluster-level issues.</p> <pre><code>graph TD\n    A[Cluster Issue Detected] --&gt; B{What's Failing?}\n    B --&gt;|Nodes| C[Node Troubleshooting]\n    B --&gt;|Control Plane| D[Component Debugging]\n    B --&gt;|etcd| E[Data Store Issues]\n    B --&gt;|Certificates| F[Certificate Problems]\n\n    C --&gt; G[Check Node Status]\n    C --&gt; H[Inspect Kubelet]\n    C --&gt; I[Verify Runtime]\n\n    D --&gt; J[API Server Logs]\n    D --&gt; K[Scheduler/Controller]\n    D --&gt; L[Component Health]\n\n    E --&gt; M[etcd Health Check]\n    E --&gt; N[Cluster Membership]\n    E --&gt; O[Data Integrity]\n\n    F --&gt; P[Certificate Expiry]\n    F --&gt; Q[CA Validation]\n    F --&gt; R[Cert Regeneration]\n\n    style B fill:#ff9999\n    style C fill:#99ccff\n    style D fill:#ffcc99\n    style E fill:#cc99ff\n    style F fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#node-troubleshooting","title":"Node Troubleshooting","text":"","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#node-status-investigation","title":"Node Status Investigation","text":"<p>Common Node States: - Ready: Node is healthy and accepting pods - NotReady: Node cannot accept pods (kubelet issues, network problems, resource exhaustion) - Unknown: Lost communication with node (&gt;40s heartbeat timeout)</p> <pre><code>graph LR\n    A[kubectl get nodes] --&gt; B{Node Status?}\n    B --&gt;|Ready| C[\u2705 Healthy]\n    B --&gt;|NotReady| D[Check Conditions]\n    B --&gt;|Unknown| E[Network/Kubelet Issue]\n\n    D --&gt; F[kubectl describe node]\n    E --&gt; F\n\n    F --&gt; G{Condition Type?}\n    G --&gt;|MemoryPressure| H[Memory Exhausted]\n    G --&gt;|DiskPressure| I[Disk Full]\n    G --&gt;|PIDPressure| J[Too Many Processes]\n    G --&gt;|NetworkUnavailable| K[CNI Problem]\n\n    style B fill:#ff9999\n    style C fill:#99ff99\n    style G fill:#ffcc99</code></pre> <p>Diagnostic Commands: <pre><code># List nodes with status\nkubectl get nodes\nkubectl get nodes -o wide  # Include IP, OS, runtime info\n\n# Detailed node inspection\nkubectl describe node &lt;node-name&gt;\n\n# Check node conditions\nkubectl get nodes -o json | jq '.items[].status.conditions'\n\n# Node resource usage\nkubectl top node &lt;node-name&gt;  # Requires metrics-server\n</code></pre></p> <p>Example NotReady Node Output: <pre><code>NAME           STATUS     ROLES    AGE   VERSION\nkube-worker-1  NotReady   &lt;none&gt;   1h    v1.30.0\n</code></pre></p> <p>Detailed Node Description (excerpt): <pre><code>Conditions:\n  Type                 Status    Reason                Message\n  ----                 ------    ------                -------\n  MemoryPressure       Unknown   NodeStatusUnknown     Kubelet stopped posting status\n  DiskPressure         Unknown   NodeStatusUnknown     Kubelet stopped posting status\n  PIDPressure          Unknown   NodeStatusUnknown     Kubelet stopped posting status\n  Ready                Unknown   NodeStatusUnknown     Kubelet stopped posting status\n\nEvents:\n  Type     Reason                   Message\n  ----     ------                   -------\n  Normal   NodeNotReady             Node kube-worker-1 status is now: NotReady\n  Warning  ContainerRuntimeUnhealthy  Container runtime is unhealthy\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#kubelet-troubleshooting","title":"Kubelet Troubleshooting","text":"<p>Kubelet is the primary node agent - most node issues stem from kubelet problems.</p> <pre><code>sequenceDiagram\n    participant API as API Server\n    participant KB as Kubelet\n    participant CR as Container Runtime\n    participant OS as Operating System\n\n    API-&gt;&gt;KB: Send pod specifications\n    KB-&gt;&gt;KB: Health checks (every 10s)\n    KB-&gt;&gt;CR: Create containers\n    CR-&gt;&gt;OS: Spawn processes\n    KB-&gt;&gt;API: Report node status (heartbeat)\n\n    Note over KB,CR: Kubelet Issues:&lt;br/&gt;- Process crash&lt;br/&gt;- Config errors&lt;br/&gt;- Certificate expiry&lt;br/&gt;- Network problems\n\n    KB--xAPI: Heartbeat timeout (40s)\n    API-&gt;&gt;API: Mark node NotReady/Unknown</code></pre> <p>Check Kubelet Status: <pre><code># On the node (SSH access required)\nsudo systemctl status kubelet\n\n# Check if kubelet is running\nps aux | grep kubelet\n\n# View kubelet configuration\nsudo cat /var/lib/kubelet/config.yaml\n\n# Check kubelet version\nkubelet --version\n</code></pre></p> <p>View Kubelet Logs: <pre><code># On systemd-based systems (most common)\nsudo journalctl -u kubelet -f        # Follow logs in real-time\nsudo journalctl -u kubelet --since \"10 minutes ago\"\nsudo journalctl -u kubelet | grep -i error\n\n# On non-systemd systems\nsudo tail -f /var/log/kubelet.log\nsudo cat /var/log/kubelet.log | grep -i error\n\n# From control plane (requires NodeLogQuery feature gate)\nkubectl get --raw \"/api/v1/nodes/&lt;node-name&gt;/proxy/logs/?query=kubelet\"\nkubectl get --raw \"/api/v1/nodes/&lt;node-name&gt;/proxy/logs/?query=kubelet&amp;pattern=error\"\n</code></pre></p> <p>Common Kubelet Issues:</p> Issue Symptom Resolution Kubelet Crash Node NotReady, pods not starting Check logs: <code>journalctl -u kubelet</code>, restart: <code>systemctl restart kubelet</code> Certificate Expired \"certificate has expired\" errors Regenerate certificates: <code>kubeadm certs renew all</code> Config Error Kubelet won't start Validate <code>/var/lib/kubelet/config.yaml</code>, check <code>--config</code> flag Runtime Issue \"failed to get runtime\" Check container runtime: <code>systemctl status containerd/docker</code> Disk Pressure Node unschedulable Free disk space: <code>df -h</code>, clean logs, evict pods Network Plugin CNI errors Check CNI config: <code>ls /etc/cni/net.d/</code>, CNI pods: <code>kubectl get pods -n kube-system</code> <p>Restart Kubelet: <pre><code># Restart kubelet service\nsudo systemctl restart kubelet\n\n# Enable kubelet on boot\nsudo systemctl enable kubelet\n\n# Check kubelet after restart\nsudo systemctl status kubelet\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#container-runtime-troubleshooting","title":"Container Runtime Troubleshooting","text":"<pre><code># For containerd (most common in modern clusters)\nsudo systemctl status containerd\nsudo journalctl -u containerd --since \"10 minutes ago\"\nsudo crictl ps        # List running containers\nsudo crictl pods      # List pods\nsudo crictl version   # Check runtime version\n\n# For Docker (legacy)\nsudo systemctl status docker\nsudo docker ps\nsudo docker version\n\n# Check runtime socket\nls -l /run/containerd/containerd.sock  # containerd\nls -l /var/run/docker.sock             # Docker\n</code></pre> <p>Runtime Configuration: <pre><code># Kubelet runtime socket configuration\ngrep \"container-runtime-endpoint\" /var/lib/kubelet/config.yaml\n\n# Example containerd socket\nunix:///run/containerd/containerd.sock\n\n# Verify runtime is accessible\nsudo crictl --runtime-endpoint unix:///run/containerd/containerd.sock version\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#control-plane-troubleshooting","title":"Control Plane Troubleshooting","text":"","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#control-plane-architecture","title":"Control Plane Architecture","text":"<pre><code>graph TB\n    subgraph \"Control Plane Node\"\n        A[kube-apiserver&lt;br/&gt;Port 6443]\n        B[etcd&lt;br/&gt;Port 2379-2380]\n        C[kube-scheduler&lt;br/&gt;Port 10259]\n        D[kube-controller-manager&lt;br/&gt;Port 10257]\n    end\n\n    subgraph \"Worker Nodes\"\n        E[Kubelet&lt;br/&gt;Port 10250]\n        F[Container Runtime]\n    end\n\n    A &lt;--&gt; B\n    A &lt;--&gt; C\n    A &lt;--&gt; D\n    E --&gt; A\n    F --&gt; E\n\n    G[kubectl/API Clients] --&gt; A\n\n    style A fill:#ff9999\n    style B fill:#99ccff\n    style C fill:#ffcc99\n    style D fill:#cc99ff</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#component-health-checks","title":"Component Health Checks","text":"<p>API Server Health: <pre><code># Check API server reachability\nkubectl cluster-info\nkubectl get --raw /healthz\nkubectl get --raw /livez\nkubectl get --raw /readyz\n\n# Detailed health checks\nkubectl get --raw /livez?verbose=true\nkubectl get --raw /readyz?verbose=true\n\n# Check API server version\nkubectl version --short\n\n# API server endpoints\nkubectl get endpoints kubernetes -n default\n</code></pre></p> <p>Component Status (deprecated but useful for CKA): <pre><code># Check control plane component health\nkubectl get cs\nkubectl get componentstatuses\n\n# Output example:\n# NAME                 STATUS    MESSAGE   ERROR\n# scheduler            Healthy   ok\n# controller-manager   Healthy   ok\n# etcd-0               Healthy   ok\n</code></pre></p> <p>Static Pod Manifests (kubeadm clusters): <pre><code># Control plane pods run as static pods\nkubectl get pods -n kube-system | grep -E \"(api|scheduler|controller|etcd)\"\n\n# Check static pod manifest directory\nsudo ls -l /etc/kubernetes/manifests/\n\n# Static pod manifests:\n# - kube-apiserver.yaml\n# - kube-scheduler.yaml\n# - kube-controller-manager.yaml\n# - etcd.yaml\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#component-log-locations","title":"Component Log Locations","text":"<pre><code>graph TD\n    A[Component Logs] --&gt; B[Static Pods]\n    A --&gt; C[Systemd Services]\n    A --&gt; D[Log Files]\n\n    B --&gt; E[kubectl logs -n kube-system]\n    C --&gt; F[journalctl -u service]\n    D --&gt; G[/var/log/*.log]\n\n    E --&gt; H[kube-apiserver-*]\n    E --&gt; I[kube-scheduler-*]\n    E --&gt; J[kube-controller-*]\n    E --&gt; K[etcd-*]\n\n    F --&gt; L[kubelet]\n    F --&gt; M[containerd/docker]\n\n    G --&gt; N[/var/log/kube-apiserver.log]\n    G --&gt; O[/var/log/kube-scheduler.log]\n    G --&gt; P[/var/log/kube-controller-manager.log]\n\n    style A fill:#ff9999\n    style B fill:#99ccff\n    style C fill:#ffcc99\n    style D fill:#99ff99</code></pre> <p>Log Commands: <pre><code># API Server logs (kubeadm static pod)\nkubectl logs -n kube-system kube-apiserver-&lt;node-name&gt;\nkubectl logs -n kube-system kube-apiserver-&lt;node-name&gt; --previous  # Previous container\n\n# Scheduler logs\nkubectl logs -n kube-system kube-scheduler-&lt;node-name&gt;\n\n# Controller Manager logs\nkubectl logs -n kube-system kube-controller-manager-&lt;node-name&gt;\n\n# etcd logs\nkubectl logs -n kube-system etcd-&lt;node-name&gt;\n\n# Filter for errors\nkubectl logs -n kube-system kube-apiserver-&lt;node-name&gt; | grep -i error\nkubectl logs -n kube-system kube-apiserver-&lt;node-name&gt; | grep -i \"failed\\|error\\|warning\"\n\n# Legacy log file locations (if not using systemd)\nsudo tail -f /var/log/kube-apiserver.log\nsudo tail -f /var/log/kube-scheduler.log\nsudo tail -f /var/log/kube-controller-manager.log\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#api-server-troubleshooting","title":"API Server Troubleshooting","text":"<p>Common API Server Issues:</p> <pre><code>graph LR\n    A[API Server Issues] --&gt; B[Connection Refused]\n    A --&gt; C[Certificate Errors]\n    A --&gt; D[etcd Connection Failed]\n    A --&gt; E[High Latency]\n\n    B --&gt; F[Check port 6443&lt;br/&gt;Check firewall&lt;br/&gt;Check apiserver pod]\n    C --&gt; G[Verify certificates&lt;br/&gt;Check expiry&lt;br/&gt;Regenerate certs]\n    D --&gt; H[Check etcd health&lt;br/&gt;Verify etcd certs&lt;br/&gt;Check connectivity]\n    E --&gt; I[Check etcd performance&lt;br/&gt;Review audit logs&lt;br/&gt;Scale apiserver]\n\n    style A fill:#ff9999\n    style B fill:#99ccff\n    style C fill:#ffcc99\n    style D fill:#cc99ff\n    style E fill:#99ff99</code></pre> <p>Diagnostic Steps: <pre><code># Check API server pod status\nkubectl get pods -n kube-system -l component=kube-apiserver\n\n# If kubectl doesn't work, check directly\nsudo docker ps | grep kube-apiserver  # or crictl ps\nsudo crictl ps | grep kube-apiserver\n\n# Check API server process\nps aux | grep kube-apiserver\n\n# Test API server connectivity\ncurl -k https://localhost:6443/healthz\ncurl -k https://localhost:6443/version\n\n# Check API server configuration\nsudo cat /etc/kubernetes/manifests/kube-apiserver.yaml\n\n# Common config issues:\n# - Wrong etcd endpoints\n# - Certificate paths incorrect\n# - Service account key missing\n# - Admission plugins misconfigured\n</code></pre></p> <p>Restart API Server (kubeadm static pod): <pre><code># Move manifest temporarily (kubelet will stop pod)\nsudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/\n\n# Wait 20 seconds for pod to stop\n\n# Move manifest back (kubelet will restart pod)\nsudo mv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/\n\n# Verify restart\nkubectl get pods -n kube-system -w\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#scheduler-troubleshooting","title":"Scheduler Troubleshooting","text":"<p>Scheduler Issues manifest as pods stuck in Pending state with events like \"no nodes available\".</p> <pre><code># Check scheduler health\nkubectl get pods -n kube-system -l component=kube-scheduler\n\n# Check scheduler logs\nkubectl logs -n kube-system kube-scheduler-&lt;node-name&gt;\n\n# Common scheduler issues:\n# - \"no nodes available\" \u2192 Node NotReady, taints, resource exhaustion\n# - \"pod has unbound PVCs\" \u2192 PVC not bound\n# - \"insufficient cpu/memory\" \u2192 No node has resources\n# - \"didn't match pod affinity\" \u2192 Affinity rules not satisfied\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#controller-manager-troubleshooting","title":"Controller Manager Troubleshooting","text":"<p>Controller Issues affect resource reconciliation (deployments, replicasets, services, etc.).</p> <pre><code># Check controller manager health\nkubectl get pods -n kube-system -l component=kube-controller-manager\n\n# Check controller manager logs\nkubectl logs -n kube-system kube-controller-manager-&lt;node-name&gt;\n\n# Common controller issues:\n# - Service account token controller not working\n# - Endpoint controller not updating services\n# - Replication controller not creating pods\n# - Certificate controller not auto-approving CSRs\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#etcd-troubleshooting","title":"etcd Troubleshooting","text":"<p>etcd is the critical data store for all cluster state - etcd failure means cluster failure.</p> <pre><code>graph TB\n    A[etcd Cluster] --&gt; B[Member 1&lt;br/&gt;Port 2379-2380]\n    A --&gt; C[Member 2&lt;br/&gt;Port 2379-2380]\n    A --&gt; D[Member 3&lt;br/&gt;Port 2379-2380]\n\n    E[kube-apiserver] --&gt; B\n    E --&gt; C\n    E --&gt; D\n\n    F[etcd Health Checks] --&gt; G[Member List]\n    F --&gt; H[Endpoint Health]\n    F --&gt; I[Alarm List]\n\n    style A fill:#ff9999\n    style E fill:#99ccff\n    style F fill:#ffcc99</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#etcd-health-checks","title":"etcd Health Checks","text":"<p>Access etcd (kubeadm cluster): <pre><code># etcd runs as static pod\nkubectl get pods -n kube-system -l component=etcd\n\n# etcd pod name\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl version\n\n# Set etcd environment variables\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=https://127.0.0.1:2379\nexport ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt\nexport ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt\nexport ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key\n</code></pre></p> <p>Check etcd Health: <pre><code># Endpoint health (inside etcd pod)\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  endpoint health\n\n# Output:\n# https://127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.5ms\n\n# List etcd members\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  member list\n\n# Check endpoint status\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  endpoint status --write-out=table\n</code></pre></p> <p>etcd Logs: <pre><code># View etcd logs\nkubectl logs -n kube-system etcd-&lt;node-name&gt;\n\n# Check for common issues:\nkubectl logs -n kube-system etcd-&lt;node-name&gt; | grep -i \"error\\|warning\\|failed\"\n\n# etcd alarms (disk space, corruption)\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  alarm list\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#etcd-common-issues","title":"etcd Common Issues","text":"<pre><code>graph TD\n    A[etcd Issues] --&gt; B[Member Unavailable]\n    A --&gt; C[Disk Space Alarm]\n    A --&gt; D[Certificate Expiry]\n    A --&gt; E[Split Brain]\n\n    B --&gt; F[Check member health&lt;br/&gt;Restart etcd pod&lt;br/&gt;Replace member]\n    C --&gt; G[Free disk space&lt;br/&gt;Compact history&lt;br/&gt;Defragment]\n    D --&gt; H[Renew certificates&lt;br/&gt;Update manifests&lt;br/&gt;Restart pods]\n    E --&gt; I[Check quorum&lt;br/&gt;Network partition&lt;br/&gt;Time sync]\n\n    style A fill:#ff9999\n    style B fill:#99ccff\n    style C fill:#ffcc99\n    style D fill:#cc99ff\n    style E fill:#99ff99</code></pre> <p>Disk Space Issues: <pre><code># Check etcd data directory size\ndu -sh /var/lib/etcd\n\n# Compact etcd history\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  compact &lt;revision&gt;\n\n# Defragment etcd\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  defrag\n\n# Clear alarm\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  alarm disarm\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#certificate-troubleshooting","title":"Certificate Troubleshooting","text":"<p>Kubernetes uses PKI (Public Key Infrastructure) - certificate issues cause authentication failures.</p> <pre><code>graph TB\n    A[Kubernetes PKI] --&gt; B[CA Certificates]\n    A --&gt; C[API Server Certs]\n    A --&gt; D[Kubelet Certs]\n    A --&gt; E[etcd Certs]\n    A --&gt; F[Service Account Keys]\n\n    B --&gt; G[/etc/kubernetes/pki/ca.crt&lt;br/&gt;/etc/kubernetes/pki/ca.key]\n    C --&gt; H[/etc/kubernetes/pki/apiserver.crt&lt;br/&gt;/etc/kubernetes/pki/apiserver.key]\n    D --&gt; I[/var/lib/kubelet/pki/kubelet.crt&lt;br/&gt;/var/lib/kubelet/pki/kubelet.key]\n    E --&gt; J[/etc/kubernetes/pki/etcd/server.crt&lt;br/&gt;/etc/kubernetes/pki/etcd/server.key]\n\n    style A fill:#ff9999\n    style B fill:#99ccff\n    style C fill:#ffcc99\n    style D fill:#cc99ff\n    style E fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#check-certificate-expiry","title":"Check Certificate Expiry","text":"<p>kubeadm Certificate Commands: <pre><code># Check all certificate expiration dates\nsudo kubeadm certs check-expiration\n\n# Output example:\n# CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\n# admin.conf                 Nov 10, 2025 12:00 UTC   364d            ca                      no\n# apiserver                  Nov 10, 2025 12:00 UTC   364d            ca                      no\n# apiserver-etcd-client      Nov 10, 2025 12:00 UTC   364d            etcd-ca                 no\n# apiserver-kubelet-client   Nov 10, 2025 12:00 UTC   364d            ca                      no\n# controller-manager.conf    Nov 10, 2025 12:00 UTC   364d            ca                      no\n# etcd-healthcheck-client    Nov 10, 2025 12:00 UTC   364d            etcd-ca                 no\n# etcd-peer                  Nov 10, 2025 12:00 UTC   364d            etcd-ca                 no\n# etcd-server                Nov 10, 2025 12:00 UTC   364d            etcd-ca                 no\n# front-proxy-client         Nov 10, 2025 12:00 UTC   364d            front-proxy-ca          no\n# scheduler.conf             Nov 10, 2025 12:00 UTC   364d            ca                      no\n</code></pre></p> <p>Manual Certificate Inspection: <pre><code># Check API server certificate\nsudo openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text\nsudo openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -dates\n\n# Check kubelet certificate\nsudo openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -noout -dates\n\n# Check etcd certificate\nsudo openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -noout -dates\n\n# Quick expiry check (days remaining)\nsudo openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -checkend 0\necho $?  # 0 = valid, 1 = expired\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#renew-certificates","title":"Renew Certificates","text":"<p>kubeadm Certificate Renewal: <pre><code># Renew all certificates\nsudo kubeadm certs renew all\n\n# Renew specific certificate\nsudo kubeadm certs renew apiserver\nsudo kubeadm certs renew apiserver-kubelet-client\nsudo kubeadm certs renew etcd-server\n\n# After renewal, restart control plane components\nsudo mv /etc/kubernetes/manifests/*.yaml /tmp/\nsleep 20\nsudo mv /tmp/*.yaml /etc/kubernetes/manifests/\n\n# Update kubeconfig files\nsudo kubeadm certs renew admin.conf\nsudo cp /etc/kubernetes/admin.conf ~/.kube/config\nsudo chown $(id -u):$(id -g) ~/.kube/config\n</code></pre></p> <p>Certificate Errors: <pre><code>Common certificate errors:\n\n\"x509: certificate has expired or is not yet valid\"\n\u2192 Certificate expired, renew with kubeadm certs renew\n\n\"x509: certificate signed by unknown authority\"\n\u2192 CA mismatch, verify CA certificate consistency\n\n\"tls: failed to verify certificate: x509: certificate is valid for..., not...\"\n\u2192 Certificate SAN (Subject Alternative Name) missing required hostname/IP\n\n\"unable to authenticate the request due to an error: crypto/rsa: verification error\"\n\u2192 Service account key mismatch between apiserver and controller-manager\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#cluster-troubleshooting-decision-tree","title":"Cluster Troubleshooting Decision Tree","text":"<pre><code>graph TD\n    A[Cluster Issue] --&gt; B{Component Type?}\n\n    B --&gt;|Node| C[Node NotReady?]\n    B --&gt;|Control Plane| D[Which Component?]\n    B --&gt;|etcd| E[etcd Health Check]\n    B --&gt;|Network| F[Pod Communication?]\n\n    C --&gt;|Yes| G[Check kubelet logs&lt;br/&gt;systemctl status kubelet]\n    C --&gt;|No| H[Check node conditions&lt;br/&gt;kubectl describe node]\n\n    D --&gt;|API Server| I[kubectl logs -n kube-system&lt;br/&gt;kube-apiserver-*]\n    D --&gt;|Scheduler| J[kubectl logs -n kube-system&lt;br/&gt;kube-scheduler-*]\n    D --&gt;|Controller| K[kubectl logs -n kube-system&lt;br/&gt;kube-controller-*]\n\n    E --&gt; L[etcdctl endpoint health&lt;br/&gt;etcdctl member list]\n\n    F --&gt;|CNI| M[Check kube-proxy&lt;br/&gt;Check CNI pods]\n    F --&gt;|DNS| N[Check CoreDNS&lt;br/&gt;Test DNS resolution]\n\n    G --&gt; O{Found Issue?}\n    O --&gt;|Yes| P[Fix Configuration&lt;br/&gt;Restart Service]\n    O --&gt;|No| Q[Check Deeper:&lt;br/&gt;Runtime, Certs, Disk]\n\n    style A fill:#ff9999\n    style B fill:#99ccff\n    style O fill:#ffcc99\n    style P fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#cka-exam-practice-exercises","title":"CKA Exam Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#exercise-1-troubleshoot-notready-node","title":"Exercise 1: Troubleshoot NotReady Node","text":"<p>Scenario: Node <code>worker-1</code> is showing NotReady status. Investigate and fix the issue.</p> Solution <pre><code># 1. Check node status\nkubectl get nodes\n# Output: worker-1   NotReady   &lt;none&gt;   5m\n\n# 2. Describe node for details\nkubectl describe node worker-1\n# Check Conditions section for specific issues\n\n# 3. SSH to node and check kubelet\nssh worker-1\nsudo systemctl status kubelet\n# Output might show: failed, dead, or errors\n\n# 4. Check kubelet logs\nsudo journalctl -u kubelet -n 50\n# Look for error messages\n\n# 5. Common fixes:\n# If kubelet is dead:\nsudo systemctl start kubelet\n\n# If config error:\nsudo cat /var/lib/kubelet/config.yaml  # Validate syntax\n\n# If runtime issue:\nsudo systemctl status containerd\nsudo systemctl restart containerd\nsudo systemctl restart kubelet\n\n# 6. Verify fix\nkubectl get nodes\n# Output: worker-1   Ready   &lt;none&gt;   7m\n</code></pre>  **Common Issues**: - Kubelet service stopped: `systemctl start kubelet` - Container runtime down: `systemctl restart containerd` - Certificate expired: `kubeadm certs renew all` - Disk full: Free space in `/var/lib/kubelet` or `/var/lib/containerd`","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#exercise-2-debug-api-server-connection-issues","title":"Exercise 2: Debug API Server Connection Issues","text":"<p>Scenario: kubectl commands are failing with \"connection refused\". Troubleshoot the API server.</p> Solution <pre><code># 1. Test API server connectivity\ncurl -k https://localhost:6443/healthz\n# If connection refused, API server is down\n\n# 2. Check API server pod status\nsudo crictl ps | grep kube-apiserver\n# Or from another control plane node:\nkubectl get pods -n kube-system -l component=kube-apiserver\n\n# 3. Check API server manifest\nsudo cat /etc/kubernetes/manifests/kube-apiserver.yaml\n# Verify:\n# - etcd endpoints correct\n# - Certificate paths valid\n# - Service account key present\n\n# 4. Check API server container logs\nsudo crictl logs &lt;apiserver-container-id&gt;\n# Look for startup errors\n\n# 5. Common issues:\n# - etcd connection failed: Check etcd health\n# - Certificate error: Verify cert paths and expiry\n# - Port conflict: Check if port 6443 is in use\n\n# 6. Restart API server (move manifest)\nsudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/\nsleep 20\nsudo mv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/\n\n# 7. Verify API server is running\nkubectl get nodes\n# Success indicates API server is healthy\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#exercise-3-etcd-cluster-health-check","title":"Exercise 3: etcd Cluster Health Check","text":"<p>Scenario: Perform a comprehensive etcd health check and verify cluster membership.</p> Solution <pre><code># 1. Set etcd environment variables\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=https://127.0.0.1:2379\nexport ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt\nexport ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt\nexport ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key\n\n# 2. Check endpoint health\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=$ETCDCTL_ENDPOINTS \\\n  --cacert=$ETCDCTL_CACERT \\\n  --cert=$ETCDCTL_CERT \\\n  --key=$ETCDCTL_KEY \\\n  endpoint health\n# Output: https://127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.5ms\n\n# 3. List cluster members\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=$ETCDCTL_ENDPOINTS \\\n  --cacert=$ETCDCTL_CACERT \\\n  --cert=$ETCDCTL_CERT \\\n  --key=$ETCDCTL_KEY \\\n  member list\n# Output shows member ID, status, name, peer URLs, client URLs\n\n# 4. Check endpoint status\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=$ETCDCTL_ENDPOINTS \\\n  --cacert=$ETCDCTL_CACERT \\\n  --cert=$ETCDCTL_CERT \\\n  --key=$ETCDCTL_KEY \\\n  endpoint status --write-out=table\n# Shows: endpoint, ID, version, DB size, is leader, is learner, raft term\n\n# 5. Check for alarms\nkubectl exec -n kube-system etcd-&lt;node-name&gt; -- etcdctl \\\n  --endpoints=$ETCDCTL_ENDPOINTS \\\n  --cacert=$ETCDCTL_CACERT \\\n  --cert=$ETCDCTL_CERT \\\n  --key=$ETCDCTL_KEY \\\n  alarm list\n# No output = no alarms (good)\n\n# 6. View etcd logs for issues\nkubectl logs -n kube-system etcd-&lt;node-name&gt; | grep -i \"error\\|warning\"\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#exercise-4-certificate-expiration-check-and-renewal","title":"Exercise 4: Certificate Expiration Check and Renewal","text":"<p>Scenario: Check certificate expiration dates and renew certificates that will expire within 30 days.</p> Solution <pre><code># 1. Check all certificate expiration dates\nsudo kubeadm certs check-expiration\n# Output shows CERTIFICATE, EXPIRES, RESIDUAL TIME, CERTIFICATE AUTHORITY\n\n# 2. Check specific certificate manually\nsudo openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -dates\n# Output:\n# notBefore=Nov 10, 2024 12:00:00 GMT\n# notAfter=Nov 10, 2025 12:00:00 GMT\n\n# 3. Check if certificate expires in next 30 days (2592000 seconds)\nsudo openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -checkend 2592000\necho $?\n# 0 = still valid for 30 days, 1 = expires within 30 days\n\n# 4. Renew all certificates\nsudo kubeadm certs renew all\n# Output shows renewed certificates\n\n# 5. Renew specific kubeconfig files\nsudo kubeadm certs renew admin.conf\nsudo kubeadm certs renew controller-manager.conf\nsudo kubeadm certs renew scheduler.conf\n\n# 6. Update local kubeconfig\nsudo cp /etc/kubernetes/admin.conf ~/.kube/config\nsudo chown $(id -u):$(id -g) ~/.kube/config\n\n# 7. Restart control plane components\nsudo mv /etc/kubernetes/manifests/*.yaml /tmp/\nsleep 20\nsudo mv /tmp/*.yaml /etc/kubernetes/manifests/\n\n# 8. Verify control plane is healthy\nkubectl get nodes\nkubectl get pods -n kube-system\n\n# 9. Verify new certificate expiration\nsudo kubeadm certs check-expiration\n# Should show new expiration dates (1 year from now)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#exercise-5-multi-component-cluster-failure","title":"Exercise 5: Multi-Component Cluster Failure","text":"<p>Scenario: Multiple issues detected: - Node <code>worker-2</code> is NotReady - Pods stuck in Pending - API server logging certificate errors - etcd showing disk space alarm</p> <p>Systematically troubleshoot and fix all issues.</p> Solution <pre><code># Phase 1: Prioritize Critical Issues (etcd \u2192 API server \u2192 Node \u2192 Pods)\n\n# 1. Fix etcd disk space alarm (CRITICAL - cluster stability)\nkubectl exec -n kube-system etcd-master -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  alarm list\n# Output: memberID:xxx alarm:NOSPACE\n\n# Check etcd data directory size\nsudo du -sh /var/lib/etcd\n\n# Compact and defragment etcd\nkubectl exec -n kube-system etcd-master -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  compact $(kubectl exec -n kube-system etcd-master -- etcdctl \\\n    --endpoints=https://127.0.0.1:2379 \\\n    --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n    --cert=/etc/kubernetes/pki/etcd/server.crt \\\n    --key=/etc/kubernetes/pki/etcd/server.key \\\n    endpoint status --write-out=\"json\" | jq -r '.[0].Status.header.revision')\n\nkubectl exec -n kube-system etcd-master -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  defrag\n\n# Disarm alarm\nkubectl exec -n kube-system etcd-master -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  alarm disarm\n\n# Phase 2: Fix API Server Certificate Issues\n\n# 2. Check API server certificate errors\nkubectl logs -n kube-system kube-apiserver-master | grep -i certificate\n\n# Check certificate expiration\nsudo kubeadm certs check-expiration\n\n# Renew expired certificates\nsudo kubeadm certs renew all\n\n# Update kubeconfig\nsudo cp /etc/kubernetes/admin.conf ~/.kube/config\n\n# Restart API server\nsudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/\nsleep 20\nsudo mv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/\n\n# Verify API server is healthy\nkubectl get --raw /healthz\n\n# Phase 3: Fix NotReady Node\n\n# 3. Troubleshoot worker-2 NotReady\nkubectl describe node worker-2\n# Check Conditions section\n\n# SSH to worker-2\nssh worker-2\n\n# Check kubelet\nsudo systemctl status kubelet\nsudo journalctl -u kubelet -n 50\n\n# Common fixes:\nsudo systemctl restart kubelet  # If kubelet is dead\n\n# Check container runtime\nsudo systemctl status containerd\nsudo systemctl restart containerd\nsudo systemctl restart kubelet\n\n# Exit back to master\nexit\n\n# Verify node is Ready\nkubectl get nodes\n# Output: worker-2   Ready   &lt;none&gt;   10m\n\n# Phase 4: Fix Pending Pods\n\n# 4. Investigate pending pods\nkubectl get pods -A | grep Pending\nkubectl describe pod &lt;pending-pod&gt; -n &lt;namespace&gt;\n# Check Events section for reason\n\n# Common reasons and fixes:\n# - \"Insufficient cpu/memory\" \u2192 Node now Ready, pods should schedule\n# - \"No nodes available\" \u2192 Check node taints, labels, affinity rules\n# - \"pod has unbound PVCs\" \u2192 Check PVC status\n\n# Trigger rescheduling if needed\nkubectl get pods -A -o wide | grep Pending\nkubectl delete pod &lt;pod-name&gt; -n &lt;namespace&gt;  # If stuck\n# Deployment/ReplicaSet will recreate\n\n# Phase 5: Final Verification\n\n# 5. Comprehensive cluster health check\nkubectl get nodes\nkubectl get pods -A\nkubectl get cs\nkubectl cluster-info\n\n# Check etcd health\nkubectl exec -n kube-system etcd-master -- etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  endpoint health\n\n# All systems should be healthy now\n</code></pre>  **Troubleshooting Order**: 1. **etcd** (data store) - highest priority 2. **API server** (control plane access) 3. **Nodes** (workload capacity) 4. **Pods** (application layer)","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#quick-reference-commands","title":"Quick Reference Commands","text":"","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#node-troubleshooting_1","title":"Node Troubleshooting","text":"<pre><code>kubectl get nodes                           # List node status\nkubectl describe node &lt;node&gt;                # Detailed node info\nkubectl top node &lt;node&gt;                     # Resource usage\nsudo systemctl status kubelet               # Kubelet service status\nsudo journalctl -u kubelet -f               # Follow kubelet logs\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#control-plane-troubleshooting_1","title":"Control Plane Troubleshooting","text":"<pre><code>kubectl get cs                              # Component status\nkubectl get pods -n kube-system             # Control plane pods\nkubectl logs -n kube-system &lt;pod&gt;           # Component logs\nkubectl get --raw /healthz                  # API server health\nsudo ls /etc/kubernetes/manifests/          # Static pod manifests\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#etcd-troubleshooting_1","title":"etcd Troubleshooting","text":"<pre><code># Inside etcd pod or with etcdctl binary\netcdctl endpoint health --write-out=table\netcdctl member list --write-out=table\netcdctl endpoint status --write-out=table\netcdctl alarm list\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#certificate-troubleshooting_1","title":"Certificate Troubleshooting","text":"<pre><code>sudo kubeadm certs check-expiration         # Check all certs\nsudo kubeadm certs renew all                # Renew all certs\nsudo openssl x509 -in &lt;cert&gt; -noout -dates  # Check cert dates\n</code></pre>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#related-resources","title":"Related Resources","text":"<ul> <li>Kubernetes Architecture Fundamentals - Understanding cluster components</li> <li>kubectl Essentials - Master debugging commands</li> <li>Setting Up Your Lab Environment - Practice cluster setup</li> </ul>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/troubleshooting-clusters-nodes-components/#summary","title":"Summary","text":"<p>Cluster troubleshooting is systematic and methodical - always follow a logical diagnosis path from critical components (etcd, API server) down to application layers (nodes, pods). Master these troubleshooting techniques to confidently tackle the CKA exam's highest-weighted domain (30%) and become an effective Kubernetes cluster administrator.</p> <p>Key Takeaways: - \u2705 Node issues \u2192 Check kubelet logs, service status, and runtime health - \u2705 Control plane \u2192 Verify component logs, health endpoints, and static pod manifests - \u2705 etcd \u2192 Monitor health, membership, disk space, and certificate validity - \u2705 Certificates \u2192 Check expiration regularly, renew proactively, restart components after renewal - \u2705 Systematic approach \u2192 Prioritize critical components, follow decision trees, document findings</p> <p>Next: Application Troubleshooting and Log Analysis - Pod-level debugging and container troubleshooting</p>","tags":["kubernetes","k8s","cka-prep","troubleshooting","debugging","cluster-health"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/","title":"ConfigMaps, Secrets, and Volume Mounts","text":"<p>Every application needs configuration\u2014database endpoints, feature flags, API keys, TLS certificates. Kubernetes provides ConfigMaps for non-sensitive configuration data and Secrets for sensitive information like passwords and tokens. Understanding the distinction between these resources and the various consumption patterns (environment variables, volume mounts, projected volumes) is essential for the CKA exam's Storage domain (10% weight) and production Kubernetes operations.</p> <p>The key insight: ConfigMaps and Secrets decouple configuration from container images, enabling the same image to run across development, staging, and production with different configurations. This pattern is fundamental to cloud-native applications and the Twelve-Factor App methodology. Modern Kubernetes (2025) adds immutability for ConfigMaps/Secrets, enhanced encryption options, and improved integration with external secret management systems.</p> <p>While ConfigMaps store arbitrary key-value pairs, Secrets provide a specialized resource with base64 encoding (not encryption!), RBAC integration, and memory-backed storage options. Understanding when to use environment variables versus volume mounts, how to leverage projected volumes, and the security implications of each approach is critical for production deployments.</p>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#configmaps-vs-secrets-when-to-use-what","title":"ConfigMaps vs Secrets: When to Use What","text":"<pre><code>graph TB\n    Data{What Data?}\n\n    Data --&gt;|Non-Sensitive| CM[ConfigMap]\n    Data --&gt;|Sensitive| Secret[Secret]\n\n    CM --&gt; CMUse[Use Cases:&lt;br/&gt;- Application config&lt;br/&gt;- Feature flags&lt;br/&gt;- Non-sensitive URLs&lt;br/&gt;- Configuration files&lt;br/&gt;- HTML/CSS/JS]\n\n    Secret --&gt; SecretUse[Use Cases:&lt;br/&gt;- Passwords&lt;br/&gt;- API tokens&lt;br/&gt;- TLS certificates&lt;br/&gt;- SSH keys&lt;br/&gt;- Database credentials]\n\n    CMUse --&gt; Consume[Consumption Methods]\n    SecretUse --&gt; Consume\n\n    Consume --&gt; Env[Environment Variables]\n    Consume --&gt; VolMount[Volume Mounts]\n    Consume --&gt; Projected[Projected Volumes]\n\n    style CM fill:#e1f5ff\n    style Secret fill:#ffcccc\n    style Env fill:#fff4e1\n    style VolMount fill:#e8f5e8\n    style Projected fill:#f3e5f5</code></pre> <p>Decision Matrix:</p> Question ConfigMap Secret Contains passwords/keys? \u274c \u2705 Needs RBAC restrictions? Optional \u2705 Required Base64 encoded? No (plain text) Yes Stored in etcd? Yes (unencrypted by default) Yes (encrypted with encryption-at-rest) Audit logging for access? Not by default \u2705 Recommended Size limit 1MB 1MB","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#creating-configmaps","title":"Creating ConfigMaps","text":"","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#from-literal-values","title":"From Literal Values","text":"<pre><code># Single key-value pair\nkubectl create configmap app-config \\\n  --from-literal=database.host=postgres.production.svc \\\n  --from-literal=database.port=5432 \\\n  --from-literal=feature.new_ui=true\n\n# Verify creation\nkubectl get configmap app-config -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#from-files","title":"From Files","text":"<pre><code># Create config file\ncat &gt; app.properties &lt;&lt;EOF\nserver.port=8080\nserver.host=0.0.0.0\nlog.level=INFO\ncache.enabled=true\nEOF\n\n# Create ConfigMap from file\nkubectl create configmap app-properties --from-file=app.properties\n\n# The file name becomes the key\n# Access via: configMapKeyRef.key: app.properties\n\n# Custom key name\nkubectl create configmap app-props --from-file=config.properties=app.properties\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#from-directory","title":"From Directory","text":"<pre><code># Create config directory\nmkdir config\necho \"prod\" &gt; config/environment\necho \"https://api.production.com\" &gt; config/api_url\necho \"replica-set\" &gt; config/database_mode\n\n# Create ConfigMap from all files in directory\nkubectl create configmap env-config --from-file=config/\n\n# Each file becomes a separate key\nkubectl describe configmap env-config\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#from-yaml-manifest","title":"From YAML Manifest","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: production\ndata:\n  # Simple key-value pairs\n  database.host: \"postgres.production.svc\"\n  database.port: \"5432\"\n  cache.ttl: \"3600\"\n\n  # Multi-line configuration file\n  nginx.conf: |\n    server {\n      listen 80;\n      server_name example.com;\n      location / {\n        proxy_pass http://backend:8080;\n        proxy_set_header Host $host;\n      }\n    }\n\n  # JSON configuration\n  app.json: |\n    {\n      \"environment\": \"production\",\n      \"features\": {\n        \"newUI\": true,\n        \"betaFeatures\": false\n      }\n    }\n</code></pre> <pre><code># Apply ConfigMap\nkubectl apply -f configmap.yaml\n\n# Update ConfigMap (changes propagate to mounted volumes)\nkubectl apply -f configmap.yaml\n\n# Delete ConfigMap\nkubectl delete configmap app-config\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#creating-secrets","title":"Creating Secrets","text":"","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#understanding-secret-types","title":"Understanding Secret Types","text":"<pre><code>graph LR\n    SecretTypes[Secret Types]\n\n    SecretTypes --&gt; Opaque[Opaque&lt;br/&gt;Generic key-value]\n    SecretTypes --&gt; TLS[kubernetes.io/tls&lt;br/&gt;TLS certificates]\n    SecretTypes --&gt; DockerCfg[kubernetes.io/dockerconfigjson&lt;br/&gt;Registry credentials]\n    SecretTypes --&gt; SSH[kubernetes.io/ssh-auth&lt;br/&gt;SSH keys]\n    SecretTypes --&gt; Token[kubernetes.io/service-account-token&lt;br/&gt;ServiceAccount tokens]\n    SecretTypes --&gt; Basic[kubernetes.io/basic-auth&lt;br/&gt;Username/password]\n\n    style Opaque fill:#e1f5ff\n    style TLS fill:#fff4e1\n    style DockerCfg fill:#e8f5e8</code></pre> Secret Type Use Case Required Keys <code>Opaque</code> Generic secrets (default) Any key-value pairs <code>kubernetes.io/tls</code> TLS certificates <code>tls.crt</code>, <code>tls.key</code> <code>kubernetes.io/dockerconfigjson</code> Container registry auth <code>.dockerconfigjson</code> <code>kubernetes.io/ssh-auth</code> SSH authentication <code>ssh-privatekey</code> <code>kubernetes.io/basic-auth</code> HTTP basic auth <code>username</code>, <code>password</code> <code>kubernetes.io/service-account-token</code> ServiceAccount tokens Auto-managed by Kubernetes","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#creating-opaque-secrets","title":"Creating Opaque Secrets","text":"<pre><code># From literal values\nkubectl create secret generic db-credentials \\\n  --from-literal=username=admin \\\n  --from-literal=password='P@ssw0rd!2025'\n\n# From files\necho -n 'admin' &gt; username.txt\necho -n 'P@ssw0rd!2025' &gt; password.txt\nkubectl create secret generic db-creds --from-file=username.txt --from-file=password.txt\n\n# From env file\ncat &gt; db.env &lt;&lt;EOF\nDB_USER=admin\nDB_PASS=P@ssw0rd!2025\nEOF\nkubectl create secret generic db-env --from-env-file=db.env\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#creating-tls-secrets","title":"Creating TLS Secrets","text":"<pre><code># Generate self-signed certificate (example)\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n  -keyout tls.key \\\n  -out tls.crt \\\n  -subj \"/CN=example.com/O=example\"\n\n# Create TLS secret\nkubectl create secret tls example-tls \\\n  --cert=tls.crt \\\n  --key=tls.key\n\n# Use in Ingress\nkubectl get secret example-tls -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#creating-docker-registry-secrets","title":"Creating Docker Registry Secrets","text":"<pre><code># For private container registries\nkubectl create secret docker-registry regcred \\\n  --docker-server=https://index.docker.io/v1/ \\\n  --docker-username=myuser \\\n  --docker-password=mypassword \\\n  --docker-email=user@example.com\n\n# Use in Pod spec\n# spec.imagePullSecrets:\n#   - name: regcred\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#creating-secrets-from-yaml-security-warning","title":"Creating Secrets from YAML (\u26a0\ufe0f Security Warning)","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-secret\ntype: Opaque\ndata:\n  # Base64 encoded values (NOT encrypted!)\n  username: YWRtaW4=          # admin\n  password: UEBzc3cwcmQhMjAyNQ==  # P@ssw0rd!2025\n</code></pre> <p>\u26a0\ufe0f CRITICAL Security Note: - Base64 is encoding, not encryption - Anyone with access to Secret can decode: <code>echo YWRtaW4= | base64 -d</code> - Never commit Secrets to version control - Use external secret managers (Vault, AWS Secrets Manager) for production - Enable encryption-at-rest for etcd</p> <pre><code># Encode values for Secret YAML\necho -n 'admin' | base64              # YWRtaW4=\necho -n 'P@ssw0rd!2025' | base64      # UEBzc3cwcmQhMjAyNQ==\n\n# Decode Secret values\nkubectl get secret db-secret -o jsonpath='{.data.username}' | base64 -d\nkubectl get secret db-secret -o jsonpath='{.data.password}' | base64 -d\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#consuming-configmaps-and-secrets","title":"Consuming ConfigMaps and Secrets","text":"","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#method-1-environment-variables","title":"Method 1: Environment Variables","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  containers:\n    - name: app\n      image: myapp:1.0\n      env:\n        # Single value from ConfigMap\n        - name: DATABASE_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: database.host\n\n        # Single value from Secret\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: password\n\n        # Optional key (won't fail if missing)\n        - name: FEATURE_FLAG\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: optional.feature\n              optional: true\n</code></pre> <p>envFrom: Import All Keys as Environment Variables</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: env-from-pod\nspec:\n  containers:\n    - name: app\n      image: myapp:1.0\n      envFrom:\n        # Import all ConfigMap keys\n        - configMapRef:\n            name: app-config\n          prefix: CONFIG_  # Optional: prefix all keys\n\n        # Import all Secret keys\n        - secretRef:\n            name: db-secret\n          prefix: DB_\n\n# Results in environment variables:\n# CONFIG_database.host=postgres.production.svc\n# DB_username=admin\n# DB_password=P@ssw0rd!2025\n</code></pre> <p>Pros and Cons of Environment Variables:</p> <p>\u2705 Pros: - Simple to use - Available immediately at container start - No filesystem permissions issues - Good for simple configuration</p> <p>\u274c Cons: - No dynamic updates (requires pod restart) - Visible in <code>kubectl describe pod</code> output (security risk) - Limited to string values - Environment variable injection may leak in logs</p>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#method-2-volume-mounts","title":"Method 2: Volume Mounts","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-mount-pod\nspec:\n  containers:\n    - name: app\n      image: myapp:1.0\n      volumeMounts:\n        # Mount ConfigMap as files\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n\n        # Mount Secret as files\n        - name: secret-volume\n          mountPath: /etc/secrets\n          readOnly: true\n\n  volumes:\n    - name: config-volume\n      configMap:\n        name: app-config\n        items:  # Optional: specify which keys to mount\n          - key: nginx.conf\n            path: nginx.conf\n          - key: app.json\n            path: config.json\n\n    - name: secret-volume\n      secret:\n        secretName: db-secret\n        defaultMode: 0400  # Read-only for owner only\n</code></pre> <p>File Structure After Mounting: <pre><code>/etc/config/\n\u251c\u2500\u2500 nginx.conf      # Content from app-config.nginx.conf\n\u2514\u2500\u2500 config.json     # Content from app-config.app.json\n\n/etc/secrets/\n\u251c\u2500\u2500 username        # Content: admin\n\u2514\u2500\u2500 password        # Content: P@ssw0rd!2025\n</code></pre></p> <p>Pros and Cons of Volume Mounts:</p> <p>\u2705 Pros: - Dynamic updates: Changes propagate without pod restart (60s eventual consistency) - Multi-line files supported (config files, certificates) - File permissions control via <code>defaultMode</code> - Secrets can use memory-backed storage (tmpfs) - More secure (not visible in process environment)</p> <p>\u274c Cons: - More complex setup - Filesystem permissions can cause issues - Slight overhead from kubelet sync</p> <pre><code>sequenceDiagram\n    participant CM as ConfigMap/Secret\n    participant Kubelet as Kubelet\n    participant Pod as Container\n\n    CM-&gt;&gt;Kubelet: ConfigMap/Secret Updated\n    Note over Kubelet: Polling every ~60s\n    Kubelet-&gt;&gt;Pod: Sync to Volume Mount\n    Pod-&gt;&gt;Pod: Application Reads File\n    Note over Pod: Application can detect&lt;br/&gt;file change and reload config</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#method-3-subpath-for-specific-files","title":"Method 3: subPath for Specific Files","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: subpath-pod\nspec:\n  containers:\n    - name: nginx\n      image: nginx:1.25\n      volumeMounts:\n        # Mount only nginx.conf from ConfigMap\n        # Into existing directory without overwriting other files\n        - name: config-volume\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n          readOnly: true\n\n  volumes:\n    - name: config-volume\n      configMap:\n        name: nginx-config\n</code></pre> <p>subPath Use Cases: - Mount single file into existing directory - Avoid overwriting entire directory - Mix ConfigMap/Secret files with container image files</p> <p>\u26a0\ufe0f subPath Limitation: Dynamic updates don't work with <code>subPath</code>. Requires pod restart to pick up changes.</p>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#projected-volumes-combining-multiple-sources","title":"Projected Volumes: Combining Multiple Sources","text":"<p>Projected volumes aggregate multiple ConfigMaps, Secrets, and downwardAPI data into a single mount point:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: projected-volume-pod\n  labels:\n    app: myapp\n    tier: backend\nspec:\n  containers:\n    - name: app\n      image: myapp:1.0\n      volumeMounts:\n        - name: all-in-one\n          mountPath: /projected\n          readOnly: true\n\n  volumes:\n    - name: all-in-one\n      projected:\n        sources:\n          # Secret projection\n          - secret:\n              name: db-secret\n              items:\n                - key: username\n                  path: secrets/db-user\n                - key: password\n                  path: secrets/db-pass\n\n          # ConfigMap projection\n          - configMap:\n              name: app-config\n              items:\n                - key: app.json\n                  path: config/app.json\n\n          # Downward API projection (pod metadata)\n          - downwardAPI:\n              items:\n                - path: metadata/labels\n                  fieldRef:\n                    fieldPath: metadata.labels\n                - path: metadata/namespace\n                  fieldRef:\n                    fieldPath: metadata.namespace\n                - path: resources/cpu-limit\n                  resourceFieldRef:\n                    containerName: app\n                    resource: limits.cpu\n</code></pre> <p>Resulting Directory Structure: <pre><code>/projected/\n\u251c\u2500\u2500 secrets/\n\u2502   \u251c\u2500\u2500 db-user      # From Secret\n\u2502   \u2514\u2500\u2500 db-pass      # From Secret\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 app.json     # From ConfigMap\n\u2514\u2500\u2500 metadata/\n    \u251c\u2500\u2500 labels       # From Downward API\n    \u251c\u2500\u2500 namespace    # From Downward API\n    \u2514\u2500\u2500 resources/\n        \u2514\u2500\u2500 cpu-limit # From Downward API\n</code></pre></p> <p>Projected Volume Benefits: - Single mount point for multiple sources - Custom directory structure - Combines Secrets, ConfigMaps, and pod metadata - Simplifies complex configuration scenarios</p>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#immutable-configmaps-and-secrets-v121","title":"Immutable ConfigMaps and Secrets (v1.21+)","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: immutable-config\nimmutable: true  # Cannot be modified after creation\ndata:\n  app.version: \"2.5.0\"\n  release.date: \"2025-01-15\"\n</code></pre> <p>Benefits of Immutability: - \u2705 Protection: Prevents accidental modifications - \u2705 Performance: Kubelet doesn't watch for changes (reduces API server load) - \u2705 Caching: Better caching in kubelet - \u2705 GitOps: Immutable configs align with immutable infrastructure</p> <p>When to Use: - Configuration tied to specific application version - Shared configuration across many pods (better performance) - Production environments requiring change control</p> <p>How to Update Immutable ConfigMap: <pre><code># Cannot edit immutable ConfigMap\nkubectl edit configmap immutable-config  # Will fail\n\n# Must delete and recreate\nkubectl delete configmap immutable-config\nkubectl apply -f configmap-v2.yaml\n\n# Or create new version\nkubectl apply -f configmap-v2.yaml  # With different name\n# Update Deployment to use new ConfigMap name\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#encryption-at-rest-for-secrets","title":"Encryption at Rest for Secrets","text":"<p>By default, Secrets are stored unencrypted in etcd. Enable encryption at rest for production clusters:</p> <pre><code># /etc/kubernetes/enc/encryption-config.yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources:\n      - secrets\n      - configmaps\n    providers:\n      # New encryption key (encrypts new data)\n      - aescbc:\n          keys:\n            - name: key1\n              secret: &lt;base64-encoded-32-byte-key&gt;\n\n      # Old key (decrypts existing data during rotation)\n      - aescbc:\n          keys:\n            - name: key2\n              secret: &lt;base64-encoded-32-byte-key&gt;\n\n      # Identity provider (no encryption)\n      - identity: {}\n</code></pre> <pre><code># Generate encryption key\nhead -c 32 /dev/urandom | base64\n\n# Configure kube-apiserver\n# Add flag: --encryption-provider-config=/etc/kubernetes/enc/encryption-config.yaml\n\n# Verify encryption\nkubectl get secrets --all-namespaces -o json | kubectl replace -f -\n\n# Check if encrypted in etcd\nETCDCTL_API=3 etcdctl get /registry/secrets/default/db-secret\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#security-best-practices","title":"Security Best Practices","text":"<pre><code>flowchart TB\n    Secret[Secret Created]\n\n    Secret --&gt; Store{How Stored?}\n    Store --&gt;|\u274c Bad| Git[Committed to Git]\n    Store --&gt;|\u274c Bad| Plain[Plain YAML in repo]\n    Store --&gt;|\u2705 Good| Vault[External Secret Manager&lt;br/&gt;Vault, AWS Secrets Manager]\n    Store --&gt;|\u2705 Good| SealedSecret[Sealed Secrets&lt;br/&gt;Encrypted in Git]\n\n    Secret --&gt; Access{Who Accesses?}\n    Access --&gt;|\u274c Bad| AllPods[All pods in namespace]\n    Access --&gt;|\u2705 Good| RBAC[RBAC-restricted&lt;br/&gt;Specific ServiceAccount]\n\n    Secret --&gt; Consume{How Consumed?}\n    Consume --&gt;|\u26a0\ufe0f Caution| Env[Environment Variable&lt;br/&gt;Visible in describe]\n    Consume --&gt;|\u2705 Better| Volume[Volume Mount&lt;br/&gt;Not in env]\n    Consume --&gt;|\u2705 Best| MemoryVol[Memory-backed Volume&lt;br/&gt;tmpfs]\n\n    Git --&gt; Risk1[\ud83d\udea8 Credentials in Git history forever]\n    Plain --&gt; Risk2[\ud83d\udea8 Plaintext secrets exposed]\n    Env --&gt; Risk3[\u26a0\ufe0f Visible in pod describe, process list]\n\n    style Git fill:#ffcccc\n    style Plain fill:#ffcccc\n    style Env fill:#ffffcc\n    style Vault fill:#ccffcc\n    style SealedSecret fill:#ccffcc\n    style Volume fill:#ccffcc\n    style MemoryVol fill:#ccffcc</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#security-checklist","title":"\u2705 Security Checklist","text":"<ol> <li>Never commit Secrets to version control</li> <li>Use RBAC to restrict Secret access to specific ServiceAccounts</li> <li>Enable encryption at rest in production clusters</li> <li>Use external secret managers (Vault, AWS Secrets Manager, Azure Key Vault)</li> <li>Prefer volume mounts over environment variables for Secrets</li> <li>Set file permissions on Secret volumes (e.g., <code>defaultMode: 0400</code>)</li> <li>Audit Secret access via Kubernetes audit logs</li> <li>Rotate Secrets regularly and use short-lived credentials</li> <li>Use Sealed Secrets or SOPS for GitOps workflows</li> <li>Limit Secret size (1MB max, keep secrets minimal)</li> </ol>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#managing-configmaps-and-secrets","title":"Managing ConfigMaps and Secrets","text":"<pre><code># List resources\nkubectl get configmaps\nkubectl get secrets\n\n# Describe (shows keys but not values)\nkubectl describe configmap app-config\nkubectl describe secret db-secret\n\n# View full content (including Secret values)\nkubectl get configmap app-config -o yaml\nkubectl get secret db-secret -o yaml  # Values are base64 encoded\n\n# Edit (creates new revision)\nkubectl edit configmap app-config\nkubectl edit secret db-secret\n\n# Delete\nkubectl delete configmap app-config\nkubectl delete secret db-secret\n\n# Check which pods use a ConfigMap/Secret\nkubectl get pods -o json | jq '.items[] | select(.spec.volumes[]?.configMap.name==\"app-config\") | .metadata.name'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#troubleshooting-configmaps-and-secrets","title":"Troubleshooting ConfigMaps and Secrets","text":"","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#pod-fails-to-start-missing-configmapsecret","title":"Pod Fails to Start: Missing ConfigMap/Secret","text":"<pre><code># Check pod events\nkubectl describe pod myapp | grep -A 10 Events\n\n# Error: \"configmap \"app-config\" not found\"\n# Solution: Create the ConfigMap\nkubectl create configmap app-config --from-literal=key=value\n\n# Error: \"secret \"db-secret\" not found\"\n# Solution: Create the Secret\nkubectl create secret generic db-secret --from-literal=password=secret123\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#configmap-changes-not-reflected-in-pod","title":"ConfigMap Changes Not Reflected in Pod","text":"<pre><code># ConfigMap updated but pod still uses old values\n\n# Reason 1: Environment variables (never update)\n# Solution: Restart pod/deployment\nkubectl rollout restart deployment myapp\n\n# Reason 2: subPath mounts (don't update)\n# Solution: Remove subPath or restart pod\n\n# Reason 3: Immutable ConfigMap\n# Solution: Create new ConfigMap version, update pod spec\n\n# Reason 4: Kubelet sync delay (up to 60s)\n# Solution: Wait for eventual consistency\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#permission-denied-on-volume-mount","title":"Permission Denied on Volume Mount","text":"<pre><code># Error: \"permission denied\" when accessing mounted Secret\n\n# Check volume mount permissions\nkubectl get pod myapp -o yaml | grep -A 5 volumeMounts\n\n# Solution: Set defaultMode in Secret volume\n# volumes:\n#   - name: secret-volume\n#     secret:\n#       secretName: db-secret\n#       defaultMode: 0400  # Owner read-only\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#cka-exam-practice-exercises","title":"CKA Exam Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#exercise-1-create-configmap-from-files","title":"Exercise 1: Create ConfigMap from Files","text":"<p>Task: Create a ConfigMap named <code>nginx-config</code> from a file named <code>nginx.conf</code> containing a basic nginx configuration. Create a pod named <code>nginx-pod</code> that mounts this ConfigMap at <code>/etc/nginx/nginx.conf</code>.</p> Solution <pre><code># Create nginx config file\ncat &gt; nginx.conf &lt;&lt;'EOF'\nserver {\n    listen 80;\n    server_name example.com;\n    location / {\n        return 200 \"Hello from ConfigMap\";\n    }\n}\nEOF\n\n# Create ConfigMap from file\nkubectl create configmap nginx-config --from-file=nginx.conf\n\n# Create pod with ConfigMap mounted\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n    - name: nginx\n      image: nginx:1.25\n      volumeMounts:\n        - name: config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n  volumes:\n    - name: config\n      configMap:\n        name: nginx-config\nEOF\n\n# Verify\nkubectl exec nginx-pod -- cat /etc/nginx/nginx.conf\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#exercise-2-create-secret-and-use-as-environment-variables","title":"Exercise 2: Create Secret and Use as Environment Variables","text":"<p>Task: Create a Secret named <code>db-creds</code> with keys <code>username=admin</code> and <code>password=secret123</code>. Create a pod named <code>app</code> that exposes these as environment variables <code>DB_USER</code> and <code>DB_PASS</code>.</p> Solution <pre><code># Create Secret\nkubectl create secret generic db-creds \\\n  --from-literal=username=admin \\\n  --from-literal=password=secret123\n\n# Create pod\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app\nspec:\n  containers:\n    - name: app\n      image: busybox:1.36\n      command: ['sh', '-c', 'echo \"User: $DB_USER\"; echo \"Pass: $DB_PASS\"; sleep 3600']\n      env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-creds\n              key: username\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: db-creds\n              key: password\nEOF\n\n# Verify\nkubectl logs app\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#exercise-3-create-projected-volume","title":"Exercise 3: Create Projected Volume","text":"<p>Task: Create a ConfigMap named <code>app-config</code> with key <code>version=2.0</code>. Create a Secret named <code>api-key</code> with key <code>token=abc123</code>. Create a pod that projects both into <code>/config</code> directory with custom paths: <code>/config/app/version</code> and <code>/config/secret/token</code>.</p> Solution <pre><code># Create ConfigMap and Secret\nkubectl create configmap app-config --from-literal=version=2.0\nkubectl create secret generic api-key --from-literal=token=abc123\n\n# Create pod with projected volume\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: projected-pod\nspec:\n  containers:\n    - name: app\n      image: busybox:1.36\n      command: ['sh', '-c', 'ls -R /config &amp;&amp; sleep 3600']\n      volumeMounts:\n        - name: config-volume\n          mountPath: /config\n  volumes:\n    - name: config-volume\n      projected:\n        sources:\n          - configMap:\n              name: app-config\n              items:\n                - key: version\n                  path: app/version\n          - secret:\n              name: api-key\n              items:\n                - key: token\n                  path: secret/token\nEOF\n\n# Verify structure\nkubectl exec projected-pod -- ls -R /config\nkubectl exec projected-pod -- cat /config/app/version\nkubectl exec projected-pod -- cat /config/secret/token\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#exercise-4-immutable-configmap","title":"Exercise 4: Immutable ConfigMap","text":"<p>Task: Create an immutable ConfigMap named <code>release-info</code> with keys <code>version=1.0.0</code> and <code>date=2025-01-15</code>. Attempt to modify it and observe the error. Then create a new version with <code>version=1.0.1</code>.</p> Solution <pre><code># Create immutable ConfigMap\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: release-info\nimmutable: true\ndata:\n  version: \"1.0.0\"\n  date: \"2025-01-15\"\nEOF\n\n# Attempt to edit (will fail)\nkubectl edit configmap release-info\n# Error: \"field is immutable\"\n\n# Create new version\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: release-info-v2\nimmutable: true\ndata:\n  version: \"1.0.1\"\n  date: \"2025-02-01\"\nEOF\n\n# Update pod to use new ConfigMap name\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#exercise-5-troubleshoot-missing-configmap","title":"Exercise 5: Troubleshoot Missing ConfigMap","text":"<p>Task: A pod named <code>broken-app</code> in namespace <code>production</code> is failing with \"configmap not found\" error. Investigate and fix the issue.</p> Solution <pre><code># Check pod events\nkubectl describe pod broken-app -n production | grep -A 10 Events\n\n# Identify missing ConfigMap name (e.g., \"app-settings\")\n# Check if ConfigMap exists\nkubectl get configmap -n production\n\n# If missing, check pod spec for required keys\nkubectl get pod broken-app -n production -o yaml | grep -A 5 configMap\n\n# Create missing ConfigMap\nkubectl create configmap app-settings -n production \\\n  --from-literal=database.host=postgres \\\n  --from-literal=cache.enabled=true\n\n# Verify pod starts\nkubectl get pod broken-app -n production\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#quick-reference","title":"Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#essential-kubectl-commands","title":"Essential kubectl Commands","text":"<pre><code># ConfigMaps\nkubectl create configmap &lt;name&gt; --from-literal=key=value\nkubectl create configmap &lt;name&gt; --from-file=&lt;file&gt;\nkubectl create configmap &lt;name&gt; --from-file=&lt;dir&gt;\nkubectl get configmap\nkubectl describe configmap &lt;name&gt;\nkubectl edit configmap &lt;name&gt;\nkubectl delete configmap &lt;name&gt;\n\n# Secrets\nkubectl create secret generic &lt;name&gt; --from-literal=key=value\nkubectl create secret generic &lt;name&gt; --from-file=&lt;file&gt;\nkubectl create secret tls &lt;name&gt; --cert=&lt;cert&gt; --key=&lt;key&gt;\nkubectl create secret docker-registry &lt;name&gt; --docker-server=&lt;server&gt;\nkubectl get secret\nkubectl describe secret &lt;name&gt;\nkubectl get secret &lt;name&gt; -o jsonpath='{.data.key}' | base64 -d\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#configmapsecret-consumption-patterns","title":"ConfigMap/Secret Consumption Patterns","text":"<pre><code># Environment Variable\nenv:\n  - name: VAR_NAME\n    valueFrom:\n      configMapKeyRef:  # or secretKeyRef\n        name: config-name\n        key: key-name\n\n# All Keys as Environment Variables\nenvFrom:\n  - configMapRef:  # or secretRef\n      name: config-name\n    prefix: PREFIX_\n\n# Volume Mount\nvolumes:\n  - name: config-vol\n    configMap:  # or secret\n      name: config-name\n</code></pre>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/configmaps-secrets-volume-mounts/#related-posts","title":"Related Posts","text":"<ul> <li>Persistent Volumes and Claims - Persistent storage with PV/PVC</li> <li>Kubernetes Pods: The Atomic Unit - Pod configuration and volumes</li> <li>RBAC: Role-Based Access Control (Coming Soon) - Securing access to Secrets</li> </ul> <p>CKA Exam Domain: Storage (10%) Key Skills: ConfigMap/Secret creation, consumption patterns, troubleshooting configuration issues Production Focus: Security best practices, encryption at rest, external secret management</p>","tags":["kubernetes","k8s","cka-prep","configmaps","secrets","configuration"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/","title":"Custom Resources and Operators","text":"<p>Extend Kubernetes functionality with CustomResourceDefinitions and the Operator pattern</p> <p>CustomResourceDefinitions (CRDs) allow you to extend the Kubernetes API with custom resource types, enabling you to treat domain-specific objects as native Kubernetes resources. Operators combine CRDs with custom controllers to automate complex application management tasks using Kubernetes-native patterns. For the CKA exam, you'll need to inspect existing CRDs, understand their structure, query custom resources, and troubleshoot CRD-related issues. This guide covers CRD anatomy, version management, validation, and practical operator concepts to help you master Kubernetes extensibility.</p> <p>CKA Exam Relevance: Cluster Architecture, Installation &amp; Configuration (25% of exam weight)</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li>CustomResourceDefinitions: Extending the Kubernetes API</li> <li>CRD Structure: Schema, validation, versions, scope</li> <li>Custom Resource Lifecycle: Creating, querying, updating custom objects</li> <li>Operator Pattern: Controllers that manage custom resources</li> <li>CRD Features: Subresources, printer columns, categories, field selectors</li> <li>Version Management: Multiple API versions and conversion</li> <li>Troubleshooting: Common CRD issues and debugging techniques</li> <li>CKA Exam Strategies: Inspecting CRDs and querying custom resources</li> </ul>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#what-are-custom-resources","title":"\ud83c\udfaf What Are Custom Resources?","text":"<p>Custom Resources extend Kubernetes by adding new resource types beyond the built-in ones (Pods, Services, Deployments, etc.).</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#native-vs-custom-resources","title":"Native vs Custom Resources","text":"<pre><code>graph TD\n    subgraph \"Native Kubernetes Resources\"\n        Pod[Pod]\n        Deploy[Deployment]\n        Service[Service]\n        CM[ConfigMap]\n    end\n\n    subgraph \"Custom Resources via CRD\"\n        DB[PostgreSQL&lt;br/&gt;custom DB resource]\n        App[Application&lt;br/&gt;custom app resource]\n        Backup[Backup&lt;br/&gt;custom backup resource]\n    end\n\n    subgraph \"API Server\"\n        API[Kubernetes API]\n    end\n\n    Pod --&gt; API\n    Deploy --&gt; API\n    Service --&gt; API\n    CM --&gt; API\n\n    DB --&gt; API\n    App --&gt; API\n    Backup --&gt; API\n\n    style Pod fill:#e1f5ff\n    style Deploy fill:#e1f5ff\n    style Service fill:#e1f5ff\n    style CM fill:#e1f5ff\n    style DB fill:#fff4e1\n    style App fill:#fff4e1\n    style Backup fill:#fff4e1</code></pre> <p>Why use Custom Resources? - Domain-specific abstractions: Model application-specific concepts - Declarative management: Use kubectl and YAML like native resources - API consistency: Leverage Kubernetes API patterns (CRUD, watch, list) - Ecosystem integration: Work with Helm, GitOps, monitoring tools</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#customresourcedefinition-crd-basics","title":"\ud83d\udcdd CustomResourceDefinition (CRD) Basics","text":"<p>A CRD defines a new custom resource type in Kubernetes.</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#simple-crd-example","title":"Simple CRD Example","text":"<pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: crontabs.stable.example.com    # Must be &lt;plural&gt;.&lt;group&gt;\nspec:\n  group: stable.example.com             # API group\n  versions:\n  - name: v1                            # API version\n    served: true                        # Accept requests for this version\n    storage: true                       # Store objects in this version\n    schema:\n      openAPIV3Schema:                  # Validation schema\n        type: object\n        properties:\n          spec:\n            type: object\n            properties:\n              cronSpec:\n                type: string\n                pattern: '^(\\d+|\\*)(/\\d+)?(\\s+(\\d+|\\*)(/\\d+)?){4}$'\n              image:\n                type: string\n              replicas:\n                type: integer\n                minimum: 1\n                maximum: 10\n  scope: Namespaced                     # Namespaced or Cluster\n  names:\n    plural: crontabs                    # Used in URL: /apis/stable.example.com/v1/crontabs\n    singular: crontab                   # Used in CLI: kubectl get crontab\n    kind: CronTab                       # Used in YAML: kind: CronTab\n    shortNames:\n    - ct                                # Alias: kubectl get ct\n</code></pre> <pre><code># Apply CRD\nkubectl apply -f crontab-crd.yaml\n\n# Verify CRD exists\nkubectl get crd crontabs.stable.example.com\n\n# Describe CRD\nkubectl describe crd crontabs.stable.example.com\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#creating-custom-resource-instances","title":"Creating Custom Resource Instances","text":"<p>Once the CRD is applied, you can create instances:</p> <pre><code>apiVersion: stable.example.com/v1\nkind: CronTab\nmetadata:\n  name: my-cron-job\nspec:\n  cronSpec: \"*/5 * * * *\"\n  image: my-cron-image:v1\n  replicas: 3\n</code></pre> <pre><code># Create custom resource instance\nkubectl apply -f my-crontab.yaml\n\n# List custom resources\nkubectl get crontabs\nkubectl get ct              # Using short name\n\n# Describe custom resource\nkubectl describe crontab my-cron-job\n\n# Delete custom resource\nkubectl delete crontab my-cron-job\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#crd-structure-deep-dive","title":"\ud83c\udfd7\ufe0f CRD Structure Deep Dive","text":"","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#crd-naming-requirements","title":"CRD Naming Requirements","text":"<pre><code>graph LR\n    CRD[CRD Name] --&gt; Format[Format:&lt;br/&gt;plural.group]\n\n    Format --&gt; Example1[crontabs.stable.example.com]\n    Format --&gt; Example2[postgresqls.db.example.com]\n\n    Group[API Group] --&gt; DNS[Must be valid&lt;br/&gt;DNS subdomain]\n\n    Plural[Plural Name] --&gt; URL[Used in API URL&lt;br/&gt;/apis/group/version/plural]\n\n    Kind[Kind Name] --&gt; YAML[Used in YAML&lt;br/&gt;kind: CronTab]\n\n    style CRD fill:#e1f5ff\n    style Format fill:#fff4e1\n    style Group fill:#e8f5e8\n    style Plural fill:#f3e5f5\n    style Kind fill:#ffe5e5</code></pre> <p>Naming components: - Group: API group (e.g., <code>stable.example.com</code>, <code>db.example.com</code>) - Plural: Plural resource name (e.g., <code>crontabs</code>, <code>postgresqls</code>) - Singular: Singular name for CLI (e.g., <code>crontab</code>) - Kind: CamelCase type name in YAML (e.g., <code>CronTab</code>) - Short names: Aliases for kubectl (e.g., <code>ct</code>)</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#scope-namespaced-vs-cluster","title":"Scope: Namespaced vs Cluster","text":"<pre><code># Namespaced resource (most common)\nspec:\n  scope: Namespaced\n\n# Cluster-scoped resource\nspec:\n  scope: Cluster\n</code></pre> <pre><code># Namespaced resources require -n flag\nkubectl get crontabs -n production\n\n# Cluster-scoped resources available globally\nkubectl get storageclasses     # Example cluster-scoped native resource\n</code></pre> <p>Choose scope based on resource semantics: - Namespaced: Application-specific resources (databases, applications) - Cluster: Infrastructure resources (storage classes, cluster configs)</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#schema-validation","title":"\u2705 Schema Validation","text":"<p>CRDs use OpenAPI v3 schema for validation:</p> <pre><code>spec:\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        type: object\n        required: [\"spec\"]            # Required fields\n        properties:\n          spec:\n            type: object\n            required: [\"cronSpec\", \"image\"]\n            properties:\n              cronSpec:\n                type: string\n                pattern: '^(\\d+|\\*)(/\\d+)?(\\s+(\\d+|\\*)(/\\d+)?){4}$'  # Regex validation\n              image:\n                type: string\n                minLength: 1          # String length validation\n              replicas:\n                type: integer\n                minimum: 1            # Numeric validation\n                maximum: 10\n                default: 1            # Default value\n</code></pre> <p>Validation features: - Type checking: string, integer, boolean, array, object - Required fields: Enforce presence of fields - Pattern matching: Regex for strings - Numeric constraints: min, max, multipleOf - Enumerations: Restrict to specific values - Default values: Auto-populate missing fields</p> <pre><code># Invalid resource rejected\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: stable.example.com/v1\nkind: CronTab\nmetadata:\n  name: invalid-cron\nspec:\n  cronSpec: \"invalid cron syntax\"    # Fails pattern validation\n  replicas: 15                        # Exceeds maximum: 10\nEOF\n# Error: validation failure\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#additional-printer-columns","title":"\ud83d\udcca Additional Printer Columns","text":"<p>Custom columns in <code>kubectl get</code> output:</p> <pre><code>spec:\n  versions:\n  - name: v1\n    additionalPrinterColumns:\n    - name: Spec\n      type: string\n      description: The cron spec\n      jsonPath: .spec.cronSpec\n    - name: Replicas\n      type: integer\n      description: Number of replicas\n      jsonPath: .spec.replicas\n    - name: Age\n      type: date\n      jsonPath: .metadata.creationTimestamp\n</code></pre> <pre><code># kubectl get output with custom columns\nkubectl get crontabs\n\n# Output:\n# NAME           SPEC          REPLICAS   AGE\n# my-cron-job    */5 * * * *   3          5m\n# daily-backup   0 2 * * *     1          2d\n</code></pre> <p>Common JSONPath expressions: - <code>.metadata.name</code>: Object name - <code>.spec.field</code>: Spec field - <code>.status.phase</code>: Status field - <code>.metadata.creationTimestamp</code>: Creation time</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#categories-and-short-names","title":"\ud83c\udff7\ufe0f Categories and Short Names","text":"","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#categories","title":"Categories","text":"<p>Group custom resources with built-in types:</p> <pre><code>spec:\n  names:\n    categories:\n    - all                # Included in 'kubectl get all'\n    - databases          # Custom category\n</code></pre> <pre><code># List all resources including custom ones\nkubectl get all\n\n# List custom category\nkubectl get databases\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#short-names","title":"Short Names","text":"<p>Aliases for faster typing:</p> <pre><code>spec:\n  names:\n    shortNames:\n    - ct                # kubectl get ct\n    - crontab           # kubectl get crontab\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#subresources","title":"\ud83d\udd04 Subresources","text":"","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#status-subresource","title":"Status Subresource","text":"<p>Separate status updates from spec changes:</p> <pre><code>spec:\n  versions:\n  - name: v1\n    subresources:\n      status: {}        # Enable /status subresource\n</code></pre> <p>Without status subresource: - Spec and status updated together - No optimistic concurrency for status</p> <p>With status subresource: - Status updated via <code>/status</code> endpoint - Separate resource versions for spec and status - Optimistic concurrency control</p> <pre><code># Update spec\nkubectl edit crontab my-cron-job        # Updates .spec\n\n# Update status (requires controller)\nkubectl patch crontab my-cron-job --subresource=status \\\n  --type=merge -p '{\"status\":{\"phase\":\"Running\"}}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#scale-subresource","title":"Scale Subresource","text":"<p>Enable <code>kubectl scale</code> for custom resources:</p> <pre><code>spec:\n  versions:\n  - name: v1\n    subresources:\n      scale:\n        specReplicasPath: .spec.replicas\n        statusReplicasPath: .status.replicas\n        labelSelectorPath: .status.labelSelector\n</code></pre> <pre><code># Scale custom resource\nkubectl scale crontab my-cron-job --replicas=5\n\n# Get current scale\nkubectl get crontab my-cron-job -o jsonpath='{.spec.replicas}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#multiple-versions","title":"\ud83d\udd00 Multiple Versions","text":"<p>CRDs can serve multiple API versions simultaneously:</p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: crontabs.stable.example.com\nspec:\n  group: stable.example.com\n  versions:\n  - name: v1\n    served: true\n    storage: true       # ONE version must be storage version\n    schema:\n      openAPIV3Schema:\n        type: object\n        properties:\n          spec:\n            type: object\n            properties:\n              cronSpec:\n                type: string\n              image:\n                type: string\n  - name: v1beta1\n    served: true        # Accept requests\n    storage: false      # Not stored, converted from v1\n    deprecated: true    # Mark as deprecated\n    deprecationWarning: \"stable.example.com/v1beta1 is deprecated, use v1\"\n    schema:\n      openAPIV3Schema:\n        type: object\n        properties:\n          spec:\n            type: object\n            properties:\n              schedule:   # Different field name in v1beta1\n                type: string\n  conversion:\n    strategy: None      # Or Webhook for complex conversions\n</code></pre> <pre><code># Access v1 resource\nkubectl get crontabs.v1.stable.example.com\n\n# Access v1beta1 resource (with deprecation warning)\nkubectl get crontabs.v1beta1.stable.example.com\n# Warning: stable.example.com/v1beta1 is deprecated, use v1\n</code></pre> <p>Conversion strategies: - None: All versions have identical schemas (field names match) - Webhook: Custom conversion logic via webhook service</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#the-operator-pattern","title":"\ud83e\udd16 The Operator Pattern","text":"<p>Operators = CRDs + Custom Controllers</p> <pre><code>sequenceDiagram\n    participant User\n    participant API as Kubernetes API\n    participant Controller as Custom Controller&lt;br/&gt;(Operator)\n    participant Resources as Managed Resources&lt;br/&gt;(Pods, Services, etc.)\n\n    User-&gt;&gt;API: Create/Update Custom Resource&lt;br/&gt;(e.g., PostgreSQL CR)\n    API-&gt;&gt;API: Validate against CRD schema\n    API-&gt;&gt;Controller: Watch event: Custom Resource changed\n\n    Controller-&gt;&gt;API: Read Custom Resource spec\n    Controller-&gt;&gt;Controller: Reconciliation loop:&lt;br/&gt;Compare desired vs actual state\n\n    alt State Mismatch\n        Controller-&gt;&gt;Resources: Create/Update native resources&lt;br/&gt;(StatefulSet, Service, ConfigMap)\n        Resources--&gt;&gt;Controller: Confirm changes\n        Controller-&gt;&gt;API: Update Custom Resource status\n    else State Match\n        Controller-&gt;&gt;Controller: No action needed\n    end\n\n    Controller-&gt;&gt;API: Continuous watch for changes</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#operator-example-postgresql-operator","title":"Operator Example: PostgreSQL Operator","text":"<p>1. Define CRD for PostgreSQL: <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: postgresqls.db.example.com\nspec:\n  group: db.example.com\n  versions:\n  - name: v1\n    served: true\n    storage: true\n    schema:\n      openAPIV3Schema:\n        type: object\n        properties:\n          spec:\n            type: object\n            properties:\n              version:\n                type: string\n              storageSize:\n                type: string\n              replicas:\n                type: integer\n          status:\n            type: object\n            properties:\n              phase:\n                type: string\n              ready:\n                type: boolean\n    subresources:\n      status: {}\n  scope: Namespaced\n  names:\n    plural: postgresqls\n    singular: postgresql\n    kind: PostgreSQL\n    shortNames:\n    - pg\n</code></pre></p> <p>2. Create PostgreSQL instance: <pre><code>apiVersion: db.example.com/v1\nkind: PostgreSQL\nmetadata:\n  name: production-db\nspec:\n  version: \"14.5\"\n  storageSize: \"10Gi\"\n  replicas: 3\n</code></pre></p> <p>3. Operator controller reconciles: - Watches for PostgreSQL custom resources - Creates StatefulSet for database pods - Creates Services for database access - Creates PVCs for persistent storage - Creates ConfigMaps for configuration - Manages backups, upgrades, scaling - Updates <code>.status</code> with current state</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#operator-benefits","title":"Operator Benefits","text":"<p>Automation: - Complex lifecycle management (install, upgrade, backup, failover) - Domain-specific logic encoded in controller - Self-healing and auto-remediation</p> <p>Kubernetes-native: - Declarative configuration via YAML - kubectl integration - RBAC and auditing - GitOps compatibility</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#inspecting-crds-cka-essential","title":"\ud83d\udd0d Inspecting CRDs (CKA Essential)","text":"","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#listing-crds","title":"Listing CRDs","text":"<pre><code># List all CRDs in cluster\nkubectl get crd\nkubectl get customresourcedefinitions\n\n# Filter by group\nkubectl get crd | grep example.com\n\n# Show CRD details\nkubectl get crd crontabs.stable.example.com -o yaml\n\n# Describe CRD (human-readable)\nkubectl describe crd crontabs.stable.example.com\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#understanding-crd-output","title":"Understanding CRD Output","text":"<pre><code>kubectl get crd crontabs.stable.example.com\n\n# Output columns:\n# NAME                           CREATED AT\n# crontabs.stable.example.com    2025-11-11T10:00:00Z\n\n# Detailed view\nkubectl get crd crontabs.stable.example.com -o yaml\n</code></pre> <p>Key fields to inspect: <pre><code>spec:\n  group: stable.example.com\n  versions:\n  - name: v1\n    served: true\n    storage: true\n  scope: Namespaced\n  names:\n    plural: crontabs\n    kind: CronTab\nstatus:\n  conditions:\n  - type: Established\n    status: \"True\"               # CRD ready to use\n  acceptedNames:                 # Names accepted by API server\n    plural: crontabs\n    kind: CronTab\n  storedVersions:                # Versions used for storage\n  - v1\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#querying-custom-resources","title":"Querying Custom Resources","text":"<pre><code># List custom resources\nkubectl get crontabs\nkubectl get crontabs -A            # All namespaces\nkubectl get crontabs -n production # Specific namespace\n\n# Get specific resource\nkubectl get crontab my-cron-job -o yaml\n\n# Describe custom resource\nkubectl describe crontab my-cron-job\n\n# Use JSONPath for specific fields\nkubectl get crontab my-cron-job -o jsonpath='{.spec.cronSpec}'\n\n# Watch for changes\nkubectl get crontabs --watch\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#troubleshooting-crds","title":"\ud83d\udee0\ufe0f Troubleshooting CRDs","text":"","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#issue-1-crd-not-found","title":"Issue 1: CRD Not Found","text":"<p>Error: <pre><code>error: the server doesn't have a resource type \"crontabs\"\n</code></pre></p> <p>Diagnosis: <pre><code># Check if CRD exists\nkubectl get crd | grep crontab\n\n# If missing, CRD not applied\nkubectl get crd crontabs.stable.example.com\n# Error: customresourcedefinitions.apiextensions.k8s.io \"crontabs.stable.example.com\" not found\n</code></pre></p> <p>Solution: Apply CRD first <pre><code>kubectl apply -f crontab-crd.yaml\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#issue-2-validation-failure","title":"Issue 2: Validation Failure","text":"<p>Error: <pre><code>error: error validating \"my-crontab.yaml\": error validating data:\nValidationError(CronTab.spec.replicas): invalid type for com.example.stable.v1.CronTab.spec.replicas:\ngot \"string\", expected \"integer\"\n</code></pre></p> <p>Diagnosis: <pre><code># Check CRD schema\nkubectl get crd crontabs.stable.example.com -o yaml | grep -A 20 openAPIV3Schema\n\n# Verify required fields and types\n</code></pre></p> <p>Solution: Fix custom resource YAML to match schema <pre><code>spec:\n  replicas: 3      # Integer, not \"3\" string\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#issue-3-crd-not-established","title":"Issue 3: CRD Not Established","text":"<p>Error: <pre><code>error: the server could not find the requested resource\n</code></pre></p> <p>Diagnosis: <pre><code># Check CRD status\nkubectl get crd crontabs.stable.example.com -o jsonpath='{.status.conditions[?(@.type==\"Established\")].status}'\n# Output: False or missing\n\n# Check conditions\nkubectl describe crd crontabs.stable.example.com\n</code></pre></p> <p>Common causes: - Invalid schema (OpenAPI v3 validation errors) - Name conflicts with existing resources - API server issues</p> <p>Solution: Check API server logs <pre><code># On control plane node\nkubectl logs -n kube-system kube-apiserver-&lt;node&gt; | grep -i crd\n\n# Look for validation errors or conflicts\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#issue-4-deleting-crd-deletes-all-instances","title":"Issue 4: Deleting CRD Deletes All Instances","text":"<p>Behavior: Deleting a CRD cascades to all custom resource instances:</p> <pre><code># Delete CRD\nkubectl delete crd crontabs.stable.example.com\n# Deletes: crontabs.stable.example.com CRD\n# Also deletes: ALL CronTab custom resources\n\n# Trying to list custom resources\nkubectl get crontabs\n# Error: the server doesn't have a resource type \"crontabs\"\n</code></pre> <p>Protection: Use finalizers or backup custom resources before deleting CRD.</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#cka-exam-practice-exercises","title":"\ud83d\udcdd CKA Exam Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#exercise-1-inspect-existing-crd","title":"Exercise 1: Inspect Existing CRD","text":"<p>Scenario: A CRD named <code>applications.app.example.com</code> exists in the cluster. Find: 1. API group and version 2. Resource scope (Namespaced or Cluster) 3. Plural name and short names</p> Solution <pre><code># Get CRD details\nkubectl get crd applications.app.example.com -o yaml\n\n# Extract specific fields\necho \"API Group:\"\nkubectl get crd applications.app.example.com -o jsonpath='{.spec.group}'\n# Output: app.example.com\n\necho \"Version(s):\"\nkubectl get crd applications.app.example.com -o jsonpath='{.spec.versions[*].name}'\n# Output: v1 v1beta1\n\necho \"Scope:\"\nkubectl get crd applications.app.example.com -o jsonpath='{.spec.scope}'\n# Output: Namespaced\n\necho \"Plural name:\"\nkubectl get crd applications.app.example.com -o jsonpath='{.spec.names.plural}'\n# Output: applications\n\necho \"Short names:\"\nkubectl get crd applications.app.example.com -o jsonpath='{.spec.names.shortNames[*]}'\n# Output: app apps\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#exercise-2-list-and-describe-custom-resources","title":"Exercise 2: List and Describe Custom Resources","text":"<p>Scenario: Using the <code>applications.app.example.com</code> CRD from Exercise 1, list all Application resources and describe one named <code>web-app</code>.</p> Solution <pre><code># List all Application resources\nkubectl get applications\nkubectl get app          # Using short name\n\n# List in all namespaces\nkubectl get applications -A\n\n# Describe specific resource\nkubectl describe application web-app\n\n# Get YAML output\nkubectl get application web-app -o yaml\n\n# Get specific field (e.g., .spec.image)\nkubectl get application web-app -o jsonpath='{.spec.image}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#exercise-3-troubleshoot-missing-custom-resource","title":"Exercise 3: Troubleshoot Missing Custom Resource","text":"<p>Scenario: You try to create a custom resource but get \"error: the server doesn't have a resource type 'databases'\". Diagnose and fix.</p> Solution <pre><code># Step 1: Check if CRD exists\nkubectl get crd | grep database\n# Output: (empty) - CRD missing\n\n# Step 2: Find CRD name pattern (plural.group)\n# Assuming group is db.example.com\nkubectl get crd databases.db.example.com\n# Error: not found\n\n# Step 3: Check all CRDs for database-related resources\nkubectl get crd -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.names.kind}{\"\\n\"}{end}' | grep -i database\n\n# Step 4: If CRD definition exists, apply it\nkubectl apply -f database-crd.yaml\n\n# Step 5: Verify CRD is established\nkubectl get crd databases.db.example.com -o jsonpath='{.status.conditions[?(@.type==\"Established\")].status}'\n# Output: True\n\n# Step 6: Now create custom resource\nkubectl apply -f my-database.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#exercise-4-find-storage-version-of-crd","title":"Exercise 4: Find Storage Version of CRD","text":"<p>Scenario: A CRD <code>crontabs.stable.example.com</code> has multiple versions. Find which version is used for storage.</p> Solution <pre><code># Method 1: Check spec.versions[].storage field\nkubectl get crd crontabs.stable.example.com -o jsonpath='{.spec.versions[?(@.storage==true)].name}'\n# Output: v1\n\n# Method 2: Check status.storedVersions\nkubectl get crd crontabs.stable.example.com -o jsonpath='{.status.storedVersions[*]}'\n# Output: v1\n\n# Method 3: View full CRD YAML\nkubectl get crd crontabs.stable.example.com -o yaml | grep -A 2 \"storage:\"\n# Output:\n#   storage: true\n#   name: v1\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#exercise-5-check-crd-validation-schema","title":"Exercise 5: Check CRD Validation Schema","text":"<p>Scenario: Find the validation requirements for the <code>spec.replicas</code> field in <code>crontabs.stable.example.com</code> CRD.</p> Solution <pre><code># Extract schema for spec.replicas\nkubectl get crd crontabs.stable.example.com -o yaml | \\\n  yq '.spec.versions[0].schema.openAPIV3Schema.properties.spec.properties.replicas'\n\n# Output:\n# type: integer\n# minimum: 1\n# maximum: 10\n# default: 1\n\n# Or using jsonpath (if yq not available)\nkubectl get crd crontabs.stable.example.com \\\n  -o jsonpath='{.spec.versions[0].schema.openAPIV3Schema.properties.spec.properties.replicas}'\n\n# Test validation with invalid value\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: stable.example.com/v1\nkind: CronTab\nmetadata:\n  name: test-validation\nspec:\n  cronSpec: \"*/5 * * * *\"\n  image: test:v1\n  replicas: 15    # Exceeds maximum: 10\nEOF\n# Error: validation failure\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#cka-exam-tips","title":"\ud83c\udfaf CKA Exam Tips","text":"","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#fast-crd-inspection","title":"Fast CRD Inspection","text":"<pre><code># List all CRDs quickly\nkubectl get crd\n\n# Get CRD details (memorize flags)\nkubectl get crd &lt;name&gt; -o yaml          # Full YAML\nkubectl describe crd &lt;name&gt;             # Human-readable\nkubectl get crd &lt;name&gt; -o jsonpath='{...}'  # Specific fields\n\n# Check CRD readiness\nkubectl get crd &lt;name&gt; -o jsonpath='{.status.conditions[?(@.type==\"Established\")].status}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#custom-resource-queries","title":"Custom Resource Queries","text":"<pre><code># List custom resources (use CRD plural name)\nkubectl get &lt;plural-name&gt;\nkubectl get &lt;short-name&gt;\n\n# All namespaces\nkubectl get &lt;plural-name&gt; -A\n\n# Specific fields\nkubectl get &lt;plural-name&gt; &lt;name&gt; -o jsonpath='{.spec.field}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#common-exam-patterns","title":"Common Exam Patterns","text":"<ol> <li>Inspect CRD: <code>kubectl get crd &lt;name&gt; -o yaml</code></li> <li>List custom resources: <code>kubectl get &lt;plural&gt;</code></li> <li>Check schema: Look for <code>openAPIV3Schema</code></li> <li>Find storage version: Check <code>storage: true</code> in versions</li> <li>Verify CRD ready: Check <code>status.conditions[].type: Established</code></li> </ol>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#quick-reference","title":"\ud83d\udcda Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#crd-essential-commands","title":"CRD Essential Commands","text":"<pre><code># List CRDs\nkubectl get crd\nkubectl get customresourcedefinitions\n\n# Get specific CRD\nkubectl get crd &lt;crd-name&gt; -o yaml\nkubectl describe crd &lt;crd-name&gt;\n\n# Apply/Delete CRD\nkubectl apply -f my-crd.yaml\nkubectl delete crd &lt;crd-name&gt;          # \u26a0\ufe0f Deletes all instances!\n\n# List custom resources\nkubectl get &lt;plural-name&gt;\nkubectl get &lt;plural-name&gt; -A           # All namespaces\nkubectl get &lt;plural-name&gt; -o wide      # Additional columns\n\n# Manage custom resources\nkubectl apply -f my-custom-resource.yaml\nkubectl edit &lt;kind&gt; &lt;name&gt;\nkubectl delete &lt;kind&gt; &lt;name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#crd-inspection-checklist","title":"CRD Inspection Checklist","text":"<pre><code>\u2705 CRD exists: kubectl get crd &lt;name&gt;\n\u2705 CRD established: Check status.conditions\n\u2705 API group: spec.group\n\u2705 Versions: spec.versions[].name\n\u2705 Storage version: spec.versions[].storage == true\n\u2705 Scope: spec.scope (Namespaced or Cluster)\n\u2705 Names: spec.names (plural, singular, kind, shortNames)\n\u2705 Schema: spec.versions[].schema.openAPIV3Schema\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Previous: Post 16 - Security Contexts and Pod Security Standards</li> <li>Next: Post 18 - Helm: Kubernetes Package Manager</li> <li>Reference: Kubernetes CRD Documentation</li> <li>Reference: Operator Pattern</li> <li>Series: Kubernetes CKA Mastery</li> </ul>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/custom-resources-operators-crds/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>CRDs extend Kubernetes API with custom resource types</li> <li>Custom resources act like native resources (kubectl, YAML, API)</li> <li>Schema validation ensures data integrity via OpenAPI v3</li> <li>Operators combine CRDs with controllers for automation</li> <li>Multiple versions enable API evolution with backward compatibility</li> <li>Subresources (status, scale) add advanced capabilities</li> <li>CRD deletion cascades to all custom resource instances</li> <li>CKA exam tip: Master <code>kubectl get crd</code> and custom resource queries</li> </ul> <p>Next Steps: Package and deploy complex applications using Helm charts and templates.</p>","tags":["kubernetes","k8s","cka-prep","architecture"]},{"location":"blog/2025/11/11/kubectl-essentials/","title":"kubectl Essentials: Your Kubernetes Swiss Army Knife","text":"<p>Master kubectl for CKA exam success. Learn imperative commands for speed, output formats for precision, and productivity patterns that save critical exam minutes.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#overview","title":"Overview","text":"<p>kubectl is the command-line interface to Kubernetes and your primary tool during the CKA exam. While the exam allows access to official documentation, proficiency with kubectl commands determines whether you finish in time.</p> <p>CKA Exam Domain: All domains (kubectl is used for every task)</p> <p>Key Insight: CKA exam success correlates directly with kubectl speed. Candidates who master imperative commands and output formats consistently score higher and finish with time to spare.</p> <p>What You'll Learn: - Essential kubectl commands by category - Imperative vs declarative approaches for exam efficiency - Output formats and JSONPath for data extraction - Context and namespace management patterns - kubectl explain for in-exam documentation - Time-saving aliases and autocomplete workflows</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#command-categories-overview","title":"Command Categories Overview","text":"<p>kubectl commands follow functional patterns that map to exam task types.</p> <pre><code>graph TB\n    subgraph \"Resource Lifecycle\"\n        CREATE[Create/Generate&lt;br/&gt;run, create, expose]\n        READ[Inspect&lt;br/&gt;get, describe, logs]\n        UPDATE[Modify&lt;br/&gt;edit, patch, scale, set]\n        DELETE[Remove&lt;br/&gt;delete]\n    end\n\n    subgraph \"Debugging &amp; Troubleshooting\"\n        EXEC[Execute&lt;br/&gt;exec, cp]\n        DEBUG[Debug&lt;br/&gt;debug, logs, port-forward]\n        METRICS[Metrics&lt;br/&gt;top]\n    end\n\n    subgraph \"Configuration &amp; Context\"\n        CONFIG[Configure&lt;br/&gt;config, explain]\n        APPLY[Apply&lt;br/&gt;apply, replace]\n        ROLLOUT[Rollouts&lt;br/&gt;rollout, scale]\n    end\n\n    CREATE --&gt; UPDATE\n    UPDATE --&gt; ROLLOUT\n    READ --&gt; DEBUG\n    DEBUG --&gt; EXEC\n    CONFIG --&gt; APPLY\n\n    style CREATE fill:#e1f5ff\n    style READ fill:#e8f5e8\n    style UPDATE fill:#fff4e1\n    style DELETE fill:#ffe5e5\n    style DEBUG fill:#f5e1ff</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#essential-commands-by-category","title":"Essential Commands by Category","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#resource-creation-commands","title":"Resource Creation Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-run-pods","title":"kubectl run (Pods)","text":"<p>Purpose: Create pods imperatively - fastest method for simple pods</p> <pre><code># Basic pod creation\nkubectl run nginx --image=nginx\n\n# Pod with port specification\nkubectl run nginx --image=nginx --port=80\n\n# Pod with labels\nkubectl run nginx --image=nginx --labels=\"app=web,tier=frontend\"\n\n# Pod with environment variables\nkubectl run nginx --image=nginx --env=\"DB_HOST=mysql\" --env=\"DB_PORT=3306\"\n\n# Generate YAML without creating (CRITICAL for exam)\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n</code></pre> <p>Exam Pattern: Use <code>--dry-run=client -o yaml</code> to generate templates, then edit as needed.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-create-workloads","title":"kubectl create (Workloads)","text":"<p>Purpose: Generate deployments, jobs, services with imperative commands</p> <pre><code># Deployment creation\nkubectl create deployment webapp --image=nginx --replicas=3\n\n# Job creation\nkubectl create job hello --image=busybox:1.28 -- echo \"Hello World\"\n\n# CronJob creation\nkubectl create cronjob hello \\\n  --image=busybox:1.28 \\\n  --schedule=\"*/5 * * * *\" \\\n  -- /bin/sh -c \"date; echo Hello from CronJob\"\n\n# Service creation\nkubectl create service clusterip my-service --tcp=8080:80\nkubectl create service nodeport my-service --tcp=8080:80 --node-port=30080\n\n# ConfigMap from literals\nkubectl create configmap app-config \\\n  --from-literal=env=production \\\n  --from-literal=debug=false\n\n# Secret creation\nkubectl create secret generic db-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=secretpass\n\n# All with YAML generation\nkubectl create deployment webapp --image=nginx --dry-run=client -o yaml &gt; deploy.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-expose-services","title":"kubectl expose (Services)","text":"<p>Purpose: Expose existing resources as services</p> <pre><code># Expose pod\nkubectl expose pod nginx --port=80 --type=NodePort\n\n# Expose deployment\nkubectl expose deployment webapp --port=80 --target-port=8080 --type=LoadBalancer\n\n# Expose with specific name\nkubectl expose deployment webapp --port=80 --name=web-service --type=ClusterIP\n\n# Generate service YAML\nkubectl expose deployment webapp --port=80 --dry-run=client -o yaml &gt; service.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#resource-inspection-commands","title":"Resource Inspection Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-get-list-resources","title":"kubectl get (List Resources)","text":"<p>Purpose: Query cluster state - most frequently used command</p> <pre><code># Basic resource listing\nkubectl get pods                        # Current namespace\nkubectl get pods -A                     # All namespaces\nkubectl get pods -n kube-system        # Specific namespace\nkubectl get all                         # All resources in namespace\n\n# Wide output (additional columns)\nkubectl get pods -o wide                # Shows IP, Node, etc.\nkubectl get nodes -o wide              # Shows internal IP, OS, etc.\n\n# Show labels\nkubectl get pods --show-labels\n\n# Filter by labels\nkubectl get pods -l app=nginx\nkubectl get pods -l 'env in (dev,staging)'\nkubectl get pods -l app=nginx,tier!=frontend\n\n# Multiple resource types\nkubectl get pods,services,deployments\nkubectl get deploy,rs,pods\n</code></pre> <p>Output Formats (covered in detail later): - <code>-o wide</code> - Additional columns - <code>-o yaml</code> - Full YAML representation - <code>-o json</code> - Full JSON representation - <code>-o name</code> - Resource names only - <code>-o custom-columns</code> - User-defined columns - <code>-o jsonpath</code> - JSONPath expressions</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-describe-detailed-information","title":"kubectl describe (Detailed Information)","text":"<p>Purpose: Get detailed resource information with events</p> <pre><code># Describe resources\nkubectl describe pod nginx\nkubectl describe node worker-1\nkubectl describe deployment webapp\nkubectl describe service my-service\n\n# Describe from file\nkubectl describe -f deployment.yaml\n\n# Common use case: debugging\nkubectl describe pod failing-pod  # Check Events section for issues\n</code></pre> <p>Key Information in describe Output: - Events: Recent state changes, errors, scheduling decisions - Status: Current resource state - Spec: Resource configuration - Conditions: Health checks and readiness</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#resource-modification-commands","title":"Resource Modification Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-edit-interactive-editing","title":"kubectl edit (Interactive Editing)","text":"<p>Purpose: Edit resources in your default editor</p> <pre><code># Edit pod\nkubectl edit pod nginx\n\n# Edit deployment\nkubectl edit deployment webapp\n\n# Edit with specific editor\nKUBE_EDITOR=\"vim\" kubectl edit service my-service\n</code></pre> <p>Exam Tip: <code>kubectl edit</code> opens full resource YAML. Use with caution - easy to accidentally modify important fields. Prefer <code>kubectl patch</code> or <code>kubectl set</code> for targeted changes.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-patch-partial-updates","title":"kubectl patch (Partial Updates)","text":"<p>Purpose: Update specific fields without full resource replacement</p> <pre><code># Update image\nkubectl patch pod nginx -p '{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"image\":\"nginx:1.21\"}]}}'\n\n# Scale using patch\nkubectl patch deployment webapp -p '{\"spec\":{\"replicas\":5}}'\n\n# Strategic merge patch (default)\nkubectl patch deployment webapp --type=strategic -p '{\"spec\":{\"replicas\":3}}'\n\n# JSON patch\nkubectl patch deployment webapp --type=json -p='[{\"op\": \"replace\", \"path\": \"/spec/replicas\", \"value\":5}]'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-scale-replica-management","title":"kubectl scale (Replica Management)","text":"<p>Purpose: Change replica count for deployments, replica sets</p> <pre><code># Scale deployment\nkubectl scale deployment webapp --replicas=5\n\n# Scale replica set\nkubectl scale rs my-replicaset --replicas=3\n\n# Scale from file\nkubectl scale --replicas=3 -f deployment.yaml\n\n# Conditional scaling\nkubectl scale deployment webapp --current-replicas=3 --replicas=5\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-set-update-resource-fields","title":"kubectl set (Update Resource Fields)","text":"<p>Purpose: Update specific resource fields with simple syntax</p> <pre><code># Update image\nkubectl set image deployment/webapp nginx=nginx:1.21\nkubectl set image deployment/webapp nginx=nginx:1.21 --record\n\n# Update resources\nkubectl set resources deployment webapp \\\n  --limits=cpu=200m,memory=512Mi \\\n  --requests=cpu=100m,memory=256Mi\n\n# Update service account\nkubectl set serviceaccount deployment webapp my-service-account\n\n# Update selector\nkubectl set selector service my-service app=nginx,tier=frontend\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#deployment-management-commands","title":"Deployment Management Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-rollout-deployment-updates","title":"kubectl rollout (Deployment Updates)","text":"<p>Purpose: Manage deployment rollouts and history</p> <pre><code># Check rollout status\nkubectl rollout status deployment/webapp\nkubectl rollout status deployment/webapp --watch\n\n# View rollout history\nkubectl rollout history deployment/webapp\nkubectl rollout history deployment/webapp --revision=3\n\n# Undo rollout (rollback)\nkubectl rollout undo deployment/webapp                    # Rollback to previous\nkubectl rollout undo deployment/webapp --to-revision=2    # Rollback to specific\n\n# Restart deployment (rolling restart)\nkubectl rollout restart deployment/webapp\n\n# Pause/Resume rollout\nkubectl rollout pause deployment/webapp\nkubectl rollout resume deployment/webapp\n</code></pre> <p>Exam Scenario Flow: <pre><code>sequenceDiagram\n    participant User\n    participant Deployment\n    participant ReplicaSet\n    participant Pods\n\n    User-&gt;&gt;Deployment: kubectl set image deploy/webapp nginx=nginx:1.21\n    Deployment-&gt;&gt;ReplicaSet: Create new ReplicaSet (nginx:1.21)\n    ReplicaSet-&gt;&gt;Pods: Create new pods gradually\n\n    Note over Deployment,Pods: Rolling update in progress\n\n    User-&gt;&gt;Deployment: kubectl rollout status deploy/webapp\n    Deployment--&gt;&gt;User: Waiting for rollout to finish...\n\n    alt Update fails\n        User-&gt;&gt;Deployment: kubectl rollout undo deploy/webapp\n        Deployment-&gt;&gt;ReplicaSet: Scale up old ReplicaSet\n        ReplicaSet-&gt;&gt;Pods: Create pods with old image\n    else Update succeeds\n        Deployment-&gt;&gt;ReplicaSet: Scale down old ReplicaSet\n        ReplicaSet-&gt;&gt;Pods: Terminate old pods\n    end\n\n    User-&gt;&gt;Deployment: kubectl rollout history deploy/webapp\n    Deployment--&gt;&gt;User: Show revision history</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#debugging-commands","title":"Debugging Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-logs-container-logs","title":"kubectl logs (Container Logs)","text":"<p>Purpose: View container stdout/stderr logs</p> <pre><code># Basic logs\nkubectl logs pod-name\nkubectl logs pod-name -c container-name        # Multi-container pods\n\n# Follow logs (stream)\nkubectl logs -f pod-name\n\n# Previous container instance (after crash)\nkubectl logs pod-name --previous\nkubectl logs pod-name -c container-name --previous\n\n# Logs from deployment\nkubectl logs deployment/webapp\nkubectl logs deployment/webapp -c nginx\n\n# Logs with labels\nkubectl logs -l app=nginx                      # All pods with label\nkubectl logs -f -l app=nginx --all-containers  # Stream all containers\n\n# Tail last N lines\nkubectl logs pod-name --tail=50\nkubectl logs pod-name --since=1h              # Last hour\nkubectl logs pod-name --since-time=2024-01-01T00:00:00Z\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-exec-execute-commands","title":"kubectl exec (Execute Commands)","text":"<p>Purpose: Run commands inside containers</p> <pre><code># Single command\nkubectl exec pod-name -- ls /app\nkubectl exec pod-name -- env\nkubectl exec pod-name -- cat /etc/resolv.conf\n\n# Interactive shell\nkubectl exec -it pod-name -- /bin/bash\nkubectl exec -it pod-name -- /bin/sh\n\n# Multi-container pod\nkubectl exec -it pod-name -c container-name -- /bin/bash\n\n# Deployment exec (first pod)\nkubectl exec deployment/webapp -- env\nkubectl exec -it deployment/webapp -- /bin/bash\n</code></pre> <p>Common Exam Patterns: <pre><code># Check pod networking\nkubectl exec -it pod-name -- ping google.com\nkubectl exec -it pod-name -- nslookup kubernetes.default\n\n# Verify volume mounts\nkubectl exec pod-name -- ls /mnt/data\n\n# Test service connectivity\nkubectl exec -it pod-name -- curl http://service-name:8080\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-port-forward-local-access","title":"kubectl port-forward (Local Access)","text":"<p>Purpose: Forward local port to pod/service for debugging</p> <pre><code># Forward to pod\nkubectl port-forward pod/nginx 8080:80        # Local:8080 -&gt; Pod:80\n\n# Forward to service\nkubectl port-forward service/webapp 8080:80\n\n# Forward to deployment\nkubectl port-forward deployment/webapp 8080:80\n\n# Listen on all interfaces (use with caution)\nkubectl port-forward --address 0.0.0.0 pod/nginx 8080:80\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-debug-ephemeral-containers","title":"kubectl debug (Ephemeral Containers)","text":"<p>Purpose: Debug running pods with ephemeral debugging containers</p> <pre><code># Debug pod with new container\nkubectl debug pod-name -it --image=busybox:1.28\n\n# Debug node\nkubectl debug node/worker-1 -it --image=ubuntu\n\n# Copy pod and debug\nkubectl debug pod-name --copy-to=pod-name-debug --container=debugger --image=busybox\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#imperative-vs-declarative-exam-strategy","title":"Imperative vs Declarative: Exam Strategy","text":"<p>The CKA exam requires balancing speed with maintainability. Understanding when to use each approach is critical.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#imperative-commands-recommended-for-cka","title":"Imperative Commands (Recommended for CKA)","text":"<p>Definition: Direct CLI commands that immediately execute operations</p> <p>Advantages: - Fast execution (30-60 seconds vs 2-3 minutes) - No file management overhead - Perfect for exam time constraints - Easy to remember patterns</p> <p>Disadvantages: - Not reproducible - No version control - Limited complexity handling</p> <p>Exam Use Cases: <pre><code># Simple pod creation\nkubectl run nginx --image=nginx\n\n# Quick deployment\nkubectl create deployment webapp --image=nginx --replicas=3\n\n# Expose service\nkubectl expose deployment webapp --port=80 --type=NodePort\n\n# Scale resources\nkubectl scale deployment webapp --replicas=5\n\n# Update image\nkubectl set image deployment/webapp nginx=nginx:1.21\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#declarative-configuration-limited-exam-use","title":"Declarative Configuration (Limited Exam Use)","text":"<p>Definition: YAML manifests describing desired state, applied with <code>kubectl apply</code></p> <p>Advantages: - Reproducible deployments - Version controllable - Handles complex configurations - Production best practice</p> <p>Disadvantages: - Slower for simple tasks - File management overhead - Verbose for basic operations</p> <p>Exam Use Cases: <pre><code># Complex multi-container pods\nkubectl apply -f complex-pod.yaml\n\n# Resources with specific configurations\nkubectl apply -f deployment-with-resources.yaml\n\n# Multiple related resources\nkubectl apply -f ./manifests/\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#hybrid-approach-optimal-for-cka","title":"Hybrid Approach (Optimal for CKA)","text":"<p>Strategy: Use imperative commands to generate YAML templates, edit as needed, then apply.</p> <pre><code># 1. Generate base YAML\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# 2. Edit for specific requirements\nvim pod.yaml\n# Add: resource limits, volumes, labels, etc.\n\n# 3. Apply declaratively\nkubectl apply -f pod.yaml\n\n# 4. Verify\nkubectl get pods\nkubectl describe pod nginx\n</code></pre> <p>Time Comparison:</p> Task Purely Imperative Hybrid Approach Pure Declarative Simple pod 10 seconds 30 seconds 2 minutes Deployment with 3 replicas 15 seconds 45 seconds 3 minutes Pod with volumes + resources Impossible 90 seconds 4 minutes Multi-container pod Impossible 2 minutes 5 minutes","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#decision-tree-imperative-vs-declarative","title":"Decision Tree: Imperative vs Declarative","text":"<pre><code>flowchart TD\n    Start([New Resource Needed]) --&gt; Simple{Simple&lt;br/&gt;Configuration?}\n\n    Simple --&gt;|Yes| SingleContainer{Single&lt;br/&gt;Container?}\n    Simple --&gt;|No| Complex[Use Hybrid Approach]\n\n    SingleContainer --&gt;|Yes| Imperative[Use Pure Imperative&lt;br/&gt;kubectl run/create]\n    SingleContainer --&gt;|No| Complex\n\n    Complex --&gt; Generate[kubectl create --dry-run]\n    Generate --&gt; Edit[Edit YAML file]\n    Edit --&gt; Apply[kubectl apply -f]\n    Apply --&gt; Verify[kubectl get/describe]\n\n    Imperative --&gt; QuickVerify[Quick kubectl get]\n\n    style Imperative fill:#99ff99\n    style Complex fill:#fff4e1\n    style Generate fill:#e1f5ff</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#output-formats-extracting-information","title":"Output Formats: Extracting Information","text":"<p>kubectl supports multiple output formats for data extraction. Mastering these saves critical exam time.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#standard-output-formats","title":"Standard Output Formats","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#default-human-readable","title":"Default (Human-Readable)","text":"<pre><code>kubectl get pods\n# NAME        READY   STATUS    RESTARTS   AGE\n# nginx       1/1     Running   0          5m\n# webapp-1    2/2     Running   1          10m\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#wide-output","title":"Wide Output","text":"<pre><code>kubectl get pods -o wide\n# NAME     READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE\n# nginx    1/1     Running   0          5m    10.244.0.5   worker-1   &lt;none&gt;\n</code></pre> <p>Use Case: Get pod IPs, node placement, readiness gates in one view</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#yaml-output","title":"YAML Output","text":"<pre><code>kubectl get pod nginx -o yaml\n</code></pre> <p>Use Cases: - Create templates from existing resources - Understand full resource structure - Debug configuration issues - Generate manifests for reproduction</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#json-output","title":"JSON Output","text":"<pre><code>kubectl get pod nginx -o json\n</code></pre> <p>Use Cases: - Programmatic parsing - Integration with scripts - API response inspection</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#name-output","title":"Name Output","text":"<pre><code>kubectl get pods -o name\n# pod/nginx\n# pod/webapp-1\n# pod/webapp-2\n</code></pre> <p>Use Case: Pipe to other commands, scripting</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#advanced-output-formats","title":"Advanced Output Formats","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#custom-columns","title":"Custom Columns","text":"<p>Purpose: Define exactly which fields to display in table format</p> <p>Basic Syntax: <pre><code>kubectl get pods -o custom-columns=&lt;COLUMN_NAME&gt;:&lt;JSON_PATH&gt;\n</code></pre></p> <p>Examples: <pre><code># Pod name and image\nkubectl get pods -o custom-columns=NAME:.metadata.name,IMAGE:.spec.containers[0].image\n\n# Pod name, namespace, node\nkubectl get pods -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,NODE:.spec.nodeName\n\n# Multiple container images\nkubectl get pods -o custom-columns=NAME:.metadata.name,IMAGES:.spec.containers[*].image\n\n# Filter with JSONPath\nkubectl get pods -A -o custom-columns='POD:.metadata.name,IMAGE:.spec.containers[?(@.image!=\"registry.k8s.io/pause:3.9\")].image'\n</code></pre></p> <p>Common Patterns: <pre><code># Service types and IPs\nkubectl get svc -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,CLUSTER-IP:.spec.clusterIP,EXTERNAL-IP:.status.loadBalancer.ingress[0].ip\n\n# Node resource capacity\nkubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory\n\n# PV claim status\nkubectl get pv -o custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage,STATUS:.status.phase,CLAIM:.spec.claimRef.name\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#jsonpath-queries","title":"JSONPath Queries","text":"<p>Purpose: Extract specific data using JSONPath expressions</p> <p>Basic Syntax: <pre><code>kubectl get &lt;resource&gt; -o jsonpath='{&lt;JSONPath_expression&gt;}'\n</code></pre></p> <p>Essential Patterns:</p> <pre><code># All pod names\nkubectl get pods -o jsonpath='{.items[*].metadata.name}'\n\n# First pod name\nkubectl get pods -o jsonpath='{.items[0].metadata.name}'\n\n# Pod IPs with newlines\nkubectl get pods -o jsonpath='{range .items[*]}{.status.podIP}{\"\\n\"}{end}'\n\n# Node names and CPU capacity\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\n\"}{end}'\n\n# Filter by condition\nkubectl get pods -o jsonpath='{.items[?(@.metadata.labels.app==\"nginx\")].metadata.name}'\n\n# All container images (unique)\nkubectl get pods -A -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\\n' | sort -u\n\n# Service cluster IPs\nkubectl get svc -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.clusterIP}{\"\\n\"}{end}'\n</code></pre> <p>Complex Examples: <pre><code># Pods with high restart counts\nkubectl get pods -o jsonpath='{range .items[?(@.status.containerStatuses[0].restartCount&gt;5)]}{.metadata.name}{\"\\t\"}{.status.containerStatuses[0].restartCount}{\"\\n\"}{end}'\n\n# Users from kubeconfig\nkubectl config view -o jsonpath='{.users[*].name}'\n\n# Context cluster mapping\nkubectl config view -o jsonpath='{range .contexts[*]}{.name}{\"\\t\"}{.context.cluster}{\"\\n\"}{end}'\n</code></pre></p> <p>JSONPath Tips: - Use <code>{range}...{end}</code> for iteration - Use <code>{\\n}</code> for newlines, <code>{\\t}</code> for tabs - Filter with <code>[?(@.field==\"value\")]</code> - Access array elements with <code>[index]</code> or <code>[*]</code> for all - No regex support in JSONPath</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#output-format-comparison","title":"Output Format Comparison","text":"<pre><code>graph LR\n    Task[Information Need] --&gt; Decision{Data Type?}\n\n    Decision --&gt;|Quick Overview| Wide[Use -o wide]\n    Decision --&gt;|Specific Fields| Choice{How Many Fields?}\n    Decision --&gt;|Full Resource| Format{Human or Machine?}\n    Decision --&gt;|Resource Names| Name[Use -o name]\n\n    Choice --&gt;|1-2 fields| JSONPath[Use -o jsonpath]\n    Choice --&gt;|3+ fields| CustomCol[Use -o custom-columns]\n\n    Format --&gt;|Human| YAML[Use -o yaml]\n    Format --&gt;|Machine| JSON[Use -o json]\n\n    style Wide fill:#99ff99\n    style JSONPath fill:#e1f5ff\n    style CustomCol fill:#fff4e1\n    style YAML fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#context-and-namespace-management","title":"Context and Namespace Management","text":"<p>Managing multiple clusters and namespaces efficiently is a core CKA exam skill.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#understanding-contexts","title":"Understanding Contexts","text":"<p>Context Definition: A context groups cluster + user + namespace into a named configuration.</p> <p>Context Components: - Cluster: API server endpoint and CA certificate - User: Authentication credentials (client cert, token, etc.) - Namespace: Default namespace for commands (optional)</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#context-management-commands","title":"Context Management Commands","text":"<pre><code># List all contexts\nkubectl config get-contexts\n# CURRENT   NAME           CLUSTER        AUTHINFO       NAMESPACE\n# *         prod-context   prod-cluster   prod-admin     production\n#           dev-context    dev-cluster    dev-user       development\n\n# Show current context\nkubectl config current-context\n# prod-context\n\n# Switch context\nkubectl config use-context dev-context\n\n# Create new context\nkubectl config set-context staging \\\n  --cluster=prod-cluster \\\n  --user=staging-user \\\n  --namespace=staging\n\n# Modify existing context\nkubectl config set-context dev-context --namespace=testing\n\n# Set namespace for current context\nkubectl config set-context --current --namespace=kube-system\n\n# Delete context\nkubectl config delete-context old-context\n\n# Rename context\nkubectl config rename-context old-name new-name\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#namespace-operations","title":"Namespace Operations","text":"<pre><code># List namespaces\nkubectl get namespaces\nkubectl get ns  # Short form\n\n# Create namespace\nkubectl create namespace development\nkubectl create ns production\n\n# Describe namespace\nkubectl describe namespace development\n\n# Delete namespace (deletes all resources!)\nkubectl delete namespace development\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#multi-cluster-workflow","title":"Multi-Cluster Workflow","text":"<pre><code>sequenceDiagram\n    participant Exam as Exam Question\n    participant User\n    participant kubectl\n    participant Cluster1 as Cluster 1\n    participant Cluster2 as Cluster 2\n\n    Exam-&gt;&gt;User: \"Deploy to cluster 'k8s-prod', namespace 'production'\"\n\n    User-&gt;&gt;kubectl: kubectl config get-contexts\n    kubectl--&gt;&gt;User: List available contexts\n\n    User-&gt;&gt;kubectl: kubectl config use-context k8s-prod\n    kubectl--&gt;&gt;Cluster1: Switch connection\n\n    User-&gt;&gt;kubectl: kubectl config set-context --current --namespace=production\n    kubectl--&gt;&gt;kubectl: Set default namespace\n\n    User-&gt;&gt;kubectl: kubectl run app --image=nginx\n    kubectl-&gt;&gt;Cluster1: Create pod in production namespace\n\n    Note over Exam,User: Next question uses different cluster\n\n    Exam-&gt;&gt;User: \"Check pods in cluster 'k8s-dev', namespace 'default'\"\n\n    User-&gt;&gt;kubectl: kubectl config use-context k8s-dev\n    kubectl--&gt;&gt;Cluster2: Switch connection\n\n    User-&gt;&gt;kubectl: kubectl get pods\n    kubectl-&gt;&gt;Cluster2: Query default namespace\n    Cluster2--&gt;&gt;User: Pod list</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#namespace-best-practices","title":"Namespace Best Practices","text":"<p>Always Specify Namespace Explicitly (Exam Safety): <pre><code># Risky (uses context default)\nkubectl get pods\n\n# Safe (explicit namespace)\nkubectl get pods -n production\nkubectl get pods --namespace=production\n\n# All namespaces\nkubectl get pods -A\nkubectl get pods --all-namespaces\n</code></pre></p> <p>Context Switching Aliases (Pre-configured in Exam): <pre><code># Context switcher\nalias kx='f() { [ \"$1\" ] &amp;&amp; kubectl config use-context $1 || kubectl config current-context ; } ; f'\n\n# Usage:\nkx                  # Show current context\nkx prod-context     # Switch to prod-context\n\n# Namespace switcher\nalias kn='f() { [ \"$1\" ] &amp;&amp; kubectl config set-context --current --namespace $1 || kubectl config view --minify | grep namespace | cut -d\" \" -f6 ; } ; f'\n\n# Usage:\nkn                  # Show current namespace\nkn production       # Switch to production namespace\n</code></pre></p> <p>Exam Verification Checklist: <pre><code># Before EVERY task, verify:\nkubectl config current-context          # Am I in the right cluster?\nkubectl config view --minify | grep namespace  # What's my default namespace?\n\n# Or use the exam-provided aliases:\nkx  # Show context\nkn  # Show namespace\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-explain-in-exam-documentation","title":"kubectl explain: In-Exam Documentation","text":"<p>kubectl explain provides API documentation directly in your terminal. This tool is available during the exam and is faster than searching web docs.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#basic-usage","title":"Basic Usage","text":"<pre><code># Top-level resource\nkubectl explain pod\nkubectl explain deployment\nkubectl explain service\n\n# Nested field\nkubectl explain pod.spec\nkubectl explain deployment.spec.template\nkubectl explain service.spec\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exploring-resource-structure","title":"Exploring Resource Structure","text":"<pre><code># View pod spec fields\nkubectl explain pod.spec\n# FIELDS:\n#   containers    &lt;[]Container&gt; -required-\n#   volumes       &lt;[]Volume&gt;\n#   restartPolicy &lt;string&gt;\n#   nodeName      &lt;string&gt;\n\n# Drill into containers\nkubectl explain pod.spec.containers\n# FIELDS:\n#   name          &lt;string&gt; -required-\n#   image         &lt;string&gt; -required-\n#   command       &lt;[]string&gt;\n#   args          &lt;[]string&gt;\n#   env           &lt;[]EnvVar&gt;\n#   ports         &lt;[]ContainerPort&gt;\n#   volumeMounts  &lt;[]VolumeMount&gt;\n\n# Check container ports structure\nkubectl explain pod.spec.containers.ports\n# FIELDS:\n#   containerPort &lt;integer&gt; -required-\n#   hostPort      &lt;integer&gt;\n#   name          &lt;string&gt;\n#   protocol      &lt;string&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#common-exam-use-cases","title":"Common Exam Use Cases","text":"<p>Scenario 1: What fields are available for liveness probes? <pre><code>kubectl explain pod.spec.containers.livenessProbe\n# Shows: exec, httpGet, tcpSocket, initialDelaySeconds, periodSeconds, etc.\n\nkubectl explain pod.spec.containers.livenessProbe.httpGet\n# Shows: path, port, host, scheme, httpHeaders\n</code></pre></p> <p>Scenario 2: How to configure deployment strategy? <pre><code>kubectl explain deployment.spec.strategy\n# Shows: type (RollingUpdate, Recreate), rollingUpdate\n\nkubectl explain deployment.spec.strategy.rollingUpdate\n# Shows: maxSurge, maxUnavailable\n</code></pre></p> <p>Scenario 3: What volume types are available? <pre><code>kubectl explain pod.spec.volumes\n# Shows MANY types: configMap, secret, emptyDir, persistentVolumeClaim, hostPath, nfs, etc.\n\nkubectl explain pod.spec.volumes.persistentVolumeClaim\n# Shows: claimName, readOnly\n</code></pre></p> <p>Scenario 4: Resource limits and requests? <pre><code>kubectl explain pod.spec.containers.resources\n# Shows: limits, requests\n\nkubectl explain pod.spec.containers.resources.limits\n# Shows: can specify cpu, memory, ephemeral-storage\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#field-type-notation","title":"Field Type Notation","text":"<p>Understanding Output: - <code>&lt;string&gt;</code> - String field - <code>&lt;integer&gt;</code> - Integer field - <code>&lt;boolean&gt;</code> - Boolean (true/false) - <code>&lt;[]Type&gt;</code> - Array of Type - <code>&lt;Object&gt;</code> - Nested object - <code>&lt;map[string]string&gt;</code> - Key-value map - <code>-required-</code> - Required field marker</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#integration-with-workflow","title":"Integration with Workflow","text":"<p>Exam Pattern: <pre><code># 1. Generate base template\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# 2. Check available fields\nkubectl explain pod.spec.containers.resources\n\n# 3. Edit with correct field names\nvim pod.yaml\n# Add resources based on explain output\n\n# 4. Verify and apply\nkubectl apply -f pod.yaml\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#productivity-patterns","title":"Productivity Patterns","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#time-saving-aliases-pre-configured-in-exam","title":"Time-Saving Aliases (Pre-configured in Exam)","text":"<p>Core Alias (Already Active): <pre><code>alias k=kubectl\ncomplete -o default -F __start_kubectl k\n</code></pre></p> <p>Recommended Personal Additions: <pre><code># Dry-run YAML generation\nexport dry='--dry-run=client -o yaml'\nexport now='--force --grace-period=0'\n\n# Usage examples:\nk run nginx --image=nginx $dry &gt; pod.yaml\nk create deploy webapp --image=nginx $dry &gt; deploy.yaml\nk delete pod nginx $now  # Immediate deletion\n</code></pre></p> <p>Resource Shortcuts: <pre><code>alias kgp='kubectl get pods'\nalias kgd='kubectl get deployments'\nalias kgs='kubectl get services'\nalias kga='kubectl get all'\nalias kgpa='kubectl get pods -A'\n\nalias kdesc='kubectl describe'\nalias kl='kubectl logs'\nalias klf='kubectl logs -f'\nalias kex='kubectl exec -it'\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#autocomplete-usage-pre-configured","title":"Autocomplete Usage (Pre-configured)","text":"<p>Tab Completion Examples: <pre><code># Resource type completion\nk get po&lt;TAB&gt;            # Completes to: k get pods\nk get dep&lt;TAB&gt;           # Completes to: k get deployments\n\n# Resource name completion\nk get pods ng&lt;TAB&gt;       # Completes to existing pod name\nk describe node wor&lt;TAB&gt; # Completes to node name\n\n# Namespace completion\nk get pods -n kube-&lt;TAB&gt; # Completes to: k get pods -n kube-system\n\n# Flag completion\nk get pods --out&lt;TAB&gt;    # Completes to: k get pods --output\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#common-command-patterns","title":"Common Command Patterns","text":"<p>Deployment Lifecycle: <pre><code># Create\nkubectl create deployment webapp --image=nginx --replicas=3\n\n# Expose\nkubectl expose deployment webapp --port=80 --type=NodePort\n\n# Update\nkubectl set image deployment/webapp nginx=nginx:1.21\n\n# Scale\nkubectl scale deployment webapp --replicas=5\n\n# Check status\nkubectl rollout status deployment/webapp\n\n# Rollback if needed\nkubectl rollout undo deployment/webapp\n</code></pre></p> <p>Debugging Workflow: <pre><code># 1. Identify issue\nkubectl get pods                # Find problematic pod\n\n# 2. Get details\nkubectl describe pod pod-name   # Check Events section\n\n# 3. Check logs\nkubectl logs pod-name           # Current logs\nkubectl logs pod-name --previous # After crash\n\n# 4. Interactive debug\nkubectl exec -it pod-name -- /bin/sh\n\n# 5. Network test\nkubectl exec -it pod-name -- ping service-name\nkubectl exec -it pod-name -- curl http://service:8080\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#cka-exam-strategies","title":"CKA Exam Strategies","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#essential-commands-use-daily","title":"Essential Commands (Use Daily)","text":"<p>Top 10 Most-Used in Exam: 1. <code>kubectl run</code> - Create pods quickly 2. <code>kubectl create</code> - Generate resources 3. <code>kubectl get</code> - List and inspect 4. <code>kubectl describe</code> - Debug issues 5. <code>kubectl logs</code> - View application logs 6. <code>kubectl apply</code> - Deploy configurations 7. <code>kubectl delete</code> - Remove resources 8. <code>kubectl exec</code> - Container debugging 9. <code>kubectl edit</code> - Quick modifications 10. <code>kubectl explain</code> - Field reference</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#time-management","title":"Time Management","text":"<p>Command Time Budget: - Simple pod creation: 10-30 seconds - Deployment with service: 45-90 seconds - Complex multi-container pod: 2-3 minutes - Debugging scenario: 3-5 minutes</p> <p>Speed Optimization: <pre><code># SLOW (2-3 minutes)\nvim pod.yaml  # Write from scratch\nkubectl apply -f pod.yaml\n\n# FAST (30 seconds)\nkubectl run nginx --image=nginx $dry &gt; pod.yaml\nvim pod.yaml  # Edit generated template\nkubectl apply -f pod.yaml\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#common-exam-scenarios","title":"Common Exam Scenarios","text":"<p>Scenario 1: Create and Expose <pre><code># Create deployment\nkubectl create deployment webapp --image=nginx --replicas=3\n\n# Expose service\nkubectl expose deployment webapp --port=80 --type=NodePort\n\n# Verify\nkubectl get deploy,svc,pods\n</code></pre></p> <p>Scenario 2: Fix Failing Pod <pre><code># Identify\nkubectl get pods\nkubectl describe pod failing-pod\nkubectl logs failing-pod\n\n# Fix approach\nkubectl get pod failing-pod -o yaml &gt; fix.yaml\nvim fix.yaml  # Fix the issue\nkubectl delete pod failing-pod\nkubectl apply -f fix.yaml\n</code></pre></p> <p>Scenario 3: Scale and Update <pre><code># Scale\nkubectl scale deployment webapp --replicas=5\n\n# Update image\nkubectl set image deployment/webapp nginx=nginx:1.21\n\n# Monitor rollout\nkubectl rollout status deployment/webapp\n\n# Rollback if needed\nkubectl rollout undo deployment/webapp\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-1-command-mastery-15-minutes","title":"Exercise 1: Command Mastery (15 minutes)","text":"<p>Objective: Build muscle memory for essential commands</p> <p>Tasks: 1. Create pod 'web' with nginx:1.21 image and port 80 2. Create deployment 'api' with 3 replicas using httpd image 3. Expose 'api' deployment as NodePort on port 8080 4. Get all pod IPs using JSONPath 5. List pod names only 6. Scale 'api' to 5 replicas 7. Update 'api' image to httpd:2.4.57 8. View rollout history 9. Delete all resources</p> <p>Solution: <pre><code># 1\nkubectl run web --image=nginx:1.21 --port=80\n\n# 2\nkubectl create deployment api --image=httpd --replicas=3\n\n# 3\nkubectl expose deployment api --port=8080 --type=NodePort\n\n# 4\nkubectl get pods -o jsonpath='{range .items[*]}{.status.podIP}{\"\\n\"}{end}'\n\n# 5\nkubectl get pods -o name\n\n# 6\nkubectl scale deployment api --replicas=5\n\n# 7\nkubectl set image deployment/api httpd=httpd:2.4.57\n\n# 8\nkubectl rollout history deployment/api\n\n# 9\nkubectl delete deployment api\nkubectl delete pod web\nkubectl delete service api\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-2-output-formats-20-minutes","title":"Exercise 2: Output Formats (20 minutes)","text":"<p>Objective: Master data extraction with output formats</p> <p>Tasks: 1. List all pod names in kube-system namespace 2. Get pod names and node placement with custom columns 3. Extract all container images in cluster 4. Get services with type and cluster IP 5. Find pods with label app=nginx</p> <p>Solution: <pre><code># 1\nkubectl get pods -n kube-system -o name\n\n# 2\nkubectl get pods -A -o custom-columns=POD:.metadata.name,NODE:.spec.nodeName\n\n# 3\nkubectl get pods -A -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\\n' | sort -u\n\n# 4\nkubectl get svc -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,CLUSTER-IP:.spec.clusterIP\n\n# 5\nkubectl get pods -l app=nginx\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-3-context-management-15-minutes","title":"Exercise 3: Context Management (15 minutes)","text":"<p>Objective: Practice multi-cluster context switching</p> <p>Tasks: 1. List all contexts 2. Show current context 3. Create context 'dev-ctx' for development namespace 4. Switch to 'dev-ctx' 5. Set default namespace to 'kube-system' for current context 6. Verify namespace setting</p> <p>Solution: <pre><code># 1\nkubectl config get-contexts\n\n# 2\nkubectl config current-context\n\n# 3\nkubectl config set-context dev-ctx --namespace=development\n\n# 4\nkubectl config use-context dev-ctx\n\n# 5\nkubectl config set-context --current --namespace=kube-system\n\n# 6\nkubectl config view --minify | grep namespace\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-4-imperative-vs-declarative-25-minutes","title":"Exercise 4: Imperative vs Declarative (25 minutes)","text":"<p>Objective: Practice hybrid workflow</p> <p>Tasks: 1. Generate pod YAML for nginx with resource limits (don't create) 2. Add resource requests: cpu=100m, memory=128Mi 3. Add resource limits: cpu=200m, memory=256Mi 4. Add liveness probe: HTTP GET on port 80, path / 5. Apply and verify 6. Generate deployment YAML with 3 replicas 7. Modify to add nodeSelector: disk=ssd 8. Apply and verify pods scheduled on correct nodes</p> <p>Solution: <pre><code># 1\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# 2-4 (edit pod.yaml)\nvim pod.yaml\n# Add under containers:\n#   resources:\n#     requests:\n#       cpu: 100m\n#       memory: 128Mi\n#     limits:\n#       cpu: 200m\n#       memory: 256Mi\n#   livenessProbe:\n#     httpGet:\n#       path: /\n#       port: 80\n\n# 5\nkubectl apply -f pod.yaml\nkubectl describe pod nginx\n\n# 6\nkubectl create deployment webapp --image=nginx --replicas=3 --dry-run=client -o yaml &gt; deploy.yaml\n\n# 7\nvim deploy.yaml\n# Add under spec.template.spec:\n#   nodeSelector:\n#     disk: ssd\n\n# 8\nkubectl apply -f deploy.yaml\nkubectl get pods -o wide\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-5-debugging-simulation-30-minutes","title":"Exercise 5: Debugging Simulation (30 minutes)","text":"<p>Objective: Practice troubleshooting workflow</p> <p>Tasks: 1. Create pod with wrong image name 2. Identify why pod is failing 3. Get YAML and fix 4. Create deployment, then break it by scaling to 100 replicas 5. Observe pending pods 6. Identify resource constraints 7. Fix by scaling down</p> <p>Solution: <pre><code># 1\nkubectl run broken --image=nginxxx  # Wrong image\n\n# 2\nkubectl get pods\nkubectl describe pod broken  # Check Events: ImagePullBackOff\n\n# 3\nkubectl get pod broken -o yaml &gt; fixed.yaml\nvim fixed.yaml  # Change image to nginx\nkubectl delete pod broken\nkubectl apply -f fixed.yaml\n\n# 4\nkubectl create deployment overload --image=nginx\nkubectl scale deployment overload --replicas=100\n\n# 5\nkubectl get pods | grep Pending\n\n# 6\nkubectl describe pod &lt;pending-pod-name&gt;\n# Events show: Insufficient cpu/memory\n\n# 7\nkubectl scale deployment overload --replicas=3\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#quick-reference","title":"Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#command-syntax-patterns","title":"Command Syntax Patterns","text":"<pre><code># Resource Management\nkubectl &lt;verb&gt; &lt;resource&gt; &lt;name&gt; [options]\nkubectl get pods nginx -o yaml\nkubectl describe deployment webapp\n\n# Imperative Creation\nkubectl run &lt;name&gt; --image=&lt;image&gt; [options]\nkubectl create &lt;resource&gt; &lt;name&gt; [options]\nkubectl expose &lt;resource&gt; &lt;name&gt; [options]\n\n# Declarative Operations\nkubectl apply -f &lt;file&gt;\nkubectl delete -f &lt;file&gt;\n\n# Resource Modification\nkubectl edit &lt;resource&gt; &lt;name&gt;\nkubectl patch &lt;resource&gt; &lt;name&gt; -p '&lt;patch&gt;'\nkubectl scale &lt;resource&gt; &lt;name&gt; --replicas=&lt;n&gt;\nkubectl set image &lt;resource&gt;/&lt;name&gt; &lt;container&gt;=&lt;image&gt;\n\n# Debugging\nkubectl logs &lt;pod&gt; [-c &lt;container&gt;] [options]\nkubectl exec &lt;pod&gt; [-c &lt;container&gt;] -- &lt;command&gt;\nkubectl port-forward &lt;resource&gt; &lt;local&gt;:&lt;remote&gt;\n\n# Context &amp; Config\nkubectl config &lt;subcommand&gt;\nkubectl config use-context &lt;context&gt;\nkubectl config set-context --current --namespace=&lt;ns&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#output-format-quick-reference","title":"Output Format Quick Reference","text":"<pre><code># Standard formats\n-o wide               # Additional columns\n-o yaml               # Full YAML\n-o json               # Full JSON\n-o name               # Resource names only\n\n# Advanced formats\n-o custom-columns=&lt;spec&gt;\n-o jsonpath='{&lt;path&gt;}'\n-o jsonpath-file=&lt;file&gt;\n\n# Examples\nkubectl get pods -o wide\nkubectl get pods -o jsonpath='{.items[*].metadata.name}'\nkubectl get pods -o custom-columns=NAME:.metadata.name,IP:.status.podIP\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#common-flags","title":"Common Flags","text":"<pre><code>-n, --namespace         # Specify namespace\n-A, --all-namespaces    # All namespaces\n-l, --selector          # Label selector\n-f, --filename          # File path\n--dry-run=client        # Don't create, just print\n-o, --output            # Output format\n-w, --watch             # Watch for changes\n--sort-by               # Sort output\n--field-selector        # Field selector\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Imperative commands save exam minutes - Use <code>kubectl run</code>, <code>create</code>, <code>expose</code> for speed</p> <p>\u2705 --dry-run=client -o yaml is critical - Generate templates, never write YAML from scratch</p> <p>\u2705 kubectl explain is your friend - Faster than searching docs during exam</p> <p>\u2705 Output formats extract data precisely - Master JSONPath and custom-columns</p> <p>\u2705 Context awareness prevents mistakes - Always verify cluster and namespace</p> <p>\u2705 Autocomplete is pre-configured - Use tab completion aggressively</p> <p>\u2705 Aliases save time only if practiced - Use <code>k</code>, <code>$dry</code>, <code>$now</code> extensively before exam</p> <p>\u2705 Hybrid approach balances speed and complexity - Imperative generation + declarative application</p> <p>\u2705 Verification is non-negotiable - Always check resources created correctly</p> <p>\u2705 Practice makes permanent - Build muscle memory through repetition</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#next-steps","title":"Next Steps","text":"<p>After mastering kubectl, continue with:</p> <p>Post 4: Pod Lifecycle and Management - Deep dive into the fundamental Kubernetes workload unit</p> <p>Related Posts: - Kubernetes Architecture Fundamentals - Understanding cluster components - Setting Up Your Kubernetes Lab - Build your practice environment - Kubernetes CKA Mastery - Complete Learning Path - Full exam preparation series</p> <p>External Resources: - kubectl Official Documentation - kubectl Cheat Sheet - kubectl Commands Reference - JSONPath Support in kubectl - CKA Exam Curriculum</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/","title":"Kubernetes Architecture Fundamentals","text":"<p>Deep dive into Kubernetes cluster architecture, control plane components, and the distributed systems design that powers container orchestration at scale.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#overview","title":"Overview","text":"<p>Kubernetes is a distributed system designed to manage containerized applications across a cluster of machines. Understanding its architecture is foundational for the CKA exam and real-world cluster administration.</p> <p>CKA Exam Domain: Cluster Architecture, Installation &amp; Configuration (25%)</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#cluster-architecture","title":"Cluster Architecture","text":"<p>A Kubernetes cluster consists of two types of nodes:</p> <ol> <li>Control Plane Nodes: Run the core components that manage the cluster</li> <li>Worker Nodes: Run application workloads (pods)</li> </ol> <pre><code>graph TB\n    subgraph \"Control Plane Nodes\"\n        API[API Server&lt;br/&gt;:6443]\n        SCHED[Scheduler]\n        CM[Controller&lt;br/&gt;Manager]\n        CCM[Cloud Controller&lt;br/&gt;Manager]\n        ETCD[(etcd&lt;br/&gt;:2379-2380)]\n    end\n\n    subgraph \"Worker Node 1\"\n        KUB1[kubelet]\n        KPXY1[kube-proxy]\n        POD1[Pods]\n        CRI1[Container&lt;br/&gt;Runtime]\n    end\n\n    subgraph \"Worker Node 2\"\n        KUB2[kubelet]\n        KPXY2[kube-proxy]\n        POD2[Pods]\n        CRI2[Container&lt;br/&gt;Runtime]\n    end\n\n    subgraph \"Worker Node 3\"\n        KUB3[kubelet]\n        KPXY3[kube-proxy]\n        POD3[Pods]\n        CRI3[Container&lt;br/&gt;Runtime]\n    end\n\n    API --&gt;|watches| ETCD\n    API --&gt; SCHED\n    API --&gt; CM\n    API --&gt; CCM\n\n    KUB1 --&gt;|API calls| API\n    KUB2 --&gt;|API calls| API\n    KUB3 --&gt;|API calls| API\n\n    KUB1 --&gt; CRI1 --&gt; POD1\n    KUB2 --&gt; CRI2 --&gt; POD2\n    KUB3 --&gt; CRI3 --&gt; POD3\n\n    KPXY1 -.-&gt;|iptables/IPVS| POD1\n    KPXY2 -.-&gt;|iptables/IPVS| POD2\n    KPXY3 -.-&gt;|iptables/IPVS| POD3\n\n    style API fill:#e1f5ff\n    style ETCD fill:#ffe5e5\n    style SCHED fill:#fff4e1\n    style CM fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#control-plane-components","title":"Control Plane Components","text":"<p>The control plane makes global decisions about the cluster and detects/responds to cluster events.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#1-api-server-kube-apiserver","title":"1. API Server (kube-apiserver)","text":"<p>Purpose: Front-end for the Kubernetes control plane. All communication goes through the API server.</p> <p>Key Responsibilities: - Exposes the Kubernetes API (REST interface) - Validates and configures API objects (pods, services, replication controllers) - Serves as the only component that directly interacts with etcd - Handles authentication, authorization, and admission control</p> <p>Default Port: <code>6443</code> (HTTPS)</p> <pre><code># Check API server status\nkubectl get --raw='/healthz?verbose'\n\n# View API server configuration\nkubectl -n kube-system get pod kube-apiserver-&lt;node-name&gt; -o yaml\n\n# Check API server logs\nkubectl -n kube-system logs kube-apiserver-&lt;node-name&gt;\n</code></pre> <p>API Request Flow:</p> <pre><code>sequenceDiagram\n    participant U as User/kubectl\n    participant API as API Server\n    participant AUTH as Authentication\n    participant AUTHZ as Authorization\n    participant ADM as Admission&lt;br/&gt;Controllers\n    participant ETCD as etcd\n\n    U-&gt;&gt;+API: HTTP Request&lt;br/&gt;(create Pod)\n\n    API-&gt;&gt;+AUTH: Authenticate\n    Note right of AUTH: Verify user identity&lt;br/&gt;(certs, tokens, SA)\n    AUTH--&gt;&gt;-API: User ID\n\n    API-&gt;&gt;+AUTHZ: Authorize\n    Note right of AUTHZ: Check RBAC rules&lt;br/&gt;(can user create pod?)\n    AUTHZ--&gt;&gt;-API: Authorized\n\n    API-&gt;&gt;+ADM: Admission Control\n    Note right of ADM: Validate &amp; Mutate&lt;br/&gt;(defaults, quotas, PSP)\n    ADM--&gt;&gt;-API: Admitted\n\n    API-&gt;&gt;+ETCD: Write to etcd\n    ETCD--&gt;&gt;-API: Persisted\n\n    API--&gt;&gt;-U: 201 Created\n\n    Note over API,ETCD: Object now exists in desired state</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#2-etcd","title":"2. etcd","text":"<p>Purpose: Consistent, highly-available key-value store used as Kubernetes' backing store for all cluster data.</p> <p>Key Characteristics: - Distributed consensus using Raft algorithm - Stores all cluster state and configuration - Only the API server writes to etcd - Supports watch operations for real-time updates</p> <p>Default Ports: - <code>2379</code> - Client requests - <code>2380</code> - Server-to-server communication</p> <pre><code># Check etcd cluster health\nETCDCTL_API=3 etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  endpoint health\n\n# List etcd members\nETCDCTL_API=3 etcdctl member list\n\n# Get all keys (see what's stored)\nETCDCTL_API=3 etcdctl get / --prefix --keys-only\n\n# Backup etcd\nETCDCTL_API=3 etcdctl snapshot save snapshot.db\n</code></pre> <p>High Availability: For production clusters, run etcd with at least 3 nodes (odd number for quorum).</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#3-scheduler-kube-scheduler","title":"3. Scheduler (kube-scheduler)","text":"<p>Purpose: Watches for newly created pods with no assigned node and selects a node for them to run on.</p> <p>Scheduling Algorithm: 1. Filtering: Find nodes that satisfy pod requirements (feasible nodes)    - Resource requests (CPU, memory)    - Node selectors    - Taints and tolerations    - Affinity/anti-affinity rules</p> <ol> <li>Scoring: Rank feasible nodes</li> <li>Spread pods across nodes for availability</li> <li>Prefer nodes with available resources</li> <li> <p>Consider pod priorities</p> </li> <li> <p>Binding: Assign pod to highest-scoring node</p> </li> </ol> <pre><code>graph TD\n    START([New Pod Created]) --&gt; FILTER[Filtering Phase]\n\n    FILTER --&gt; CHECK1{Resource&lt;br/&gt;Requirements?}\n    CHECK1 --&gt;|Fail| UNSCHEDULABLE[Pod Unschedulable]\n    CHECK1 --&gt;|Pass| CHECK2{Node&lt;br/&gt;Selectors?}\n\n    CHECK2 --&gt;|Fail| UNSCHEDULABLE\n    CHECK2 --&gt;|Pass| CHECK3{Taints/&lt;br/&gt;Tolerations?}\n\n    CHECK3 --&gt;|Fail| UNSCHEDULABLE\n    CHECK3 --&gt;|Pass| CHECK4{Affinity&lt;br/&gt;Rules?}\n\n    CHECK4 --&gt;|Fail| UNSCHEDULABLE\n    CHECK4 --&gt;|Pass| FEASIBLE[Feasible Nodes]\n\n    FEASIBLE --&gt; SCORE[Scoring Phase]\n    SCORE --&gt; RANK[Rank Nodes&lt;br/&gt;by Score]\n    RANK --&gt; BIND[Bind Pod to&lt;br/&gt;Top Node]\n    BIND --&gt; SUCCESS([Pod Scheduled])\n\n    style START fill:#e1f5ff\n    style SUCCESS fill:#e8f5e8\n    style UNSCHEDULABLE fill:#ffe5e5</code></pre> <pre><code># View scheduler configuration\nkubectl -n kube-system get pod kube-scheduler-&lt;node-name&gt; -o yaml\n\n# Check scheduler logs\nkubectl -n kube-system logs kube-scheduler-&lt;node-name&gt;\n\n# View events (scheduling decisions)\nkubectl get events --sort-by='.lastTimestamp'\n\n# See why a pod is not scheduled\nkubectl describe pod &lt;pod-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#4-controller-manager-kube-controller-manager","title":"4. Controller Manager (kube-controller-manager)","text":"<p>Purpose: Runs controller processes that regulate the state of the cluster.</p> <p>Key Controllers: - Node Controller: Monitors node health, marks nodes as unreachable - Replication Controller: Maintains correct number of pod replicas - Endpoints Controller: Populates Endpoints objects (joins Services &amp; Pods) - Service Account &amp; Token Controllers: Create default accounts and API access tokens</p> <p>Control Loop Pattern:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Watch: Controller starts\n    Watch --&gt; Compare: Detect change\n    Compare --&gt; DesiredState: Check desired state\n    DesiredState --&gt; CurrentState: Check current state\n    CurrentState --&gt; Match: States match?\n\n    Match --&gt; Watch: Yes, no action\n    Match --&gt; Reconcile: No, take action\n\n    Reconcile --&gt; CreateResources: Create missing resources\n    Reconcile --&gt; UpdateResources: Update existing resources\n    Reconcile --&gt; DeleteResources: Delete extra resources\n\n    CreateResources --&gt; Watch\n    UpdateResources --&gt; Watch\n    DeleteResources --&gt; Watch</code></pre> <pre><code># View controller manager logs\nkubectl -n kube-system logs kube-controller-manager-&lt;node-name&gt;\n\n# Check which controllers are enabled\nkubectl -n kube-system get pod kube-controller-manager-&lt;node-name&gt; -o yaml | grep enable\n\n# Watch controller actions in events\nkubectl get events --watch\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#5-cloud-controller-manager-cloud-controller-manager","title":"5. Cloud Controller Manager (cloud-controller-manager)","text":"<p>Purpose: Embeds cloud-specific control logic. Allows cloud providers to integrate with Kubernetes.</p> <p>Key Controllers: - Node Controller: Check cloud provider to determine if a deleted node has been removed - Route Controller: Set up routes in the cloud infrastructure - Service Controller: Create, update, delete cloud load balancers</p> <p>Note: Only relevant when running Kubernetes on cloud platforms (AWS, GCP, Azure).</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#worker-node-components","title":"Worker Node Components","text":"<p>Worker nodes run application workloads and maintain running pods.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#1-kubelet","title":"1. kubelet","text":"<p>Purpose: Agent that runs on each node, ensures containers are running in pods.</p> <p>Key Responsibilities: - Registers node with API server - Watches for pod assignments to its node - Ensures containers described in PodSpec are running and healthy - Reports node and pod status back to API server - Performs container health checks (liveness, readiness probes)</p> <p>Default Port: <code>10250</code></p> <pre><code># Check kubelet status (on node directly)\nsystemctl status kubelet\n\n# View kubelet configuration\ncat /var/lib/kubelet/config.yaml\n\n# Check kubelet logs\njournalctl -u kubelet -f\n\n# View node status from control plane\nkubectl get nodes\nkubectl describe node &lt;node-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#2-kube-proxy","title":"2. kube-proxy","text":"<p>Purpose: Network proxy that maintains network rules on nodes, enabling communication to pods.</p> <p>Key Responsibilities: - Implements Service abstraction - Maintains iptables/IPVS rules for service IPs - Forwards traffic to correct backend pods - Performs load balancing across pod replicas</p> <p>Modes: - iptables (default): Uses Linux iptables for packet filtering - IPVS: Uses Linux IPVS for better performance at scale - userspace: Legacy mode (rarely used)</p> <pre><code># Check kube-proxy mode\nkubectl -n kube-system logs kube-proxy-&lt;pod-name&gt; | grep \"proxy mode\"\n\n# View kube-proxy configuration\nkubectl -n kube-system get cm kube-proxy -o yaml\n\n# Check iptables rules (on node)\niptables-save | grep -i kube\n\n# View IPVS rules (if using IPVS mode)\nipvsadm -Ln\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#3-container-runtime","title":"3. Container Runtime","text":"<p>Purpose: Software responsible for running containers.</p> <p>Supported Runtimes (via Container Runtime Interface - CRI): - containerd: Lightweight, industry-standard (default for most distributions) - CRI-O: Lightweight alternative specifically for Kubernetes - Docker Engine: Via cri-dockerd shim (removed as default in Kubernetes 1.24)</p> <pre><code># Check container runtime\nkubectl get nodes -o wide\n\n# On node: check containerd\nsystemctl status containerd\ncrictl ps\n\n# List images\ncrictl images\n\n# Pull image\ncrictl pull nginx:latest\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#component-communication","title":"Component Communication","text":"<p>All components communicate through the API server. No direct component-to-component communication exists.</p> <pre><code>graph LR\n    subgraph \"Control Plane\"\n        API[API Server]\n        SCHED[Scheduler]\n        CM[Controller&lt;br/&gt;Manager]\n        ETCD[(etcd)]\n    end\n\n    subgraph \"Worker Nodes\"\n        KUB1[kubelet]\n        KUB2[kubelet]\n        KUB3[kubelet]\n    end\n\n    CLIENT[kubectl/Users]\n\n    CLIENT --&gt;|REST API| API\n    SCHED --&gt;|watch/update| API\n    CM --&gt;|watch/update| API\n    API &lt;--&gt;|read/write| ETCD\n    KUB1 --&gt;|watch/update| API\n    KUB2 --&gt;|watch/update| API\n    KUB3 --&gt;|watch/update| API\n\n    style API fill:#e1f5ff\n    style ETCD fill:#ffe5e5</code></pre> <p>Communication Patterns:</p> <ol> <li>kubectl \u2192 API Server: Users interact with cluster via kubectl</li> <li>API Server \u2194 etcd: All state stored in etcd</li> <li>Scheduler \u2192 API Server: Watches for unscheduled pods, updates bindings</li> <li>Controller Manager \u2192 API Server: Watches resources, reconciles state</li> <li>kubelet \u2192 API Server: Reports node/pod status, watches for assigned pods</li> </ol>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#kubernetes-object-model","title":"Kubernetes Object Model","text":"<p>Kubernetes manages objects that represent the desired state of your cluster.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#object-anatomy","title":"Object Anatomy","text":"<pre><code>apiVersion: v1              # API version\nkind: Pod                   # Object type\nmetadata:                   # Object metadata\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx\n    tier: frontend\n  annotations:\n    description: \"Example pod\"\nspec:                       # Desired state\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\nstatus:                     # Current state (managed by system)\n  phase: Running\n  conditions: [...]\n</code></pre> <p>Key Fields: - apiVersion: API group and version (<code>v1</code>, <code>apps/v1</code>, <code>networking.k8s.io/v1</code>) - kind: Object type (Pod, Deployment, Service) - metadata: Identifying information (name, namespace, labels) - spec: Desired state defined by user - status: Current state observed by system (read-only)</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#common-exam-tasks","title":"Common Exam Tasks","text":"","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#scenario-1-check-cluster-component-health","title":"Scenario 1: Check Cluster Component Health","text":"<pre><code># Check all control plane components\nkubectl get componentstatuses\n\n# Check system pods\nkubectl -n kube-system get pods\n\n# Verify API server\nkubectl get --raw='/healthz?verbose'\n\n# Check etcd health\nkubectl -n kube-system exec etcd-&lt;node&gt; -- etcdctl endpoint health\n\n# View node status\nkubectl get nodes\nkubectl describe node &lt;node-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#scenario-2-troubleshoot-kubelet-issues","title":"Scenario 2: Troubleshoot kubelet Issues","text":"<pre><code># On worker node:\nsystemctl status kubelet\njournalctl -u kubelet -f\n\n# Check kubelet config\ncat /var/lib/kubelet/config.yaml\n\n# Restart kubelet\nsystemctl restart kubelet\n\n# From control plane:\nkubectl describe node &lt;node-name&gt;\nkubectl get events --field-selector involvedObject.kind=Node\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#scenario-3-backup-and-restore-etcd","title":"Scenario 3: Backup and Restore etcd","text":"<pre><code># Backup\nETCDCTL_API=3 etcdctl snapshot save /backup/snapshot.db \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key\n\n# Verify backup\nETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db\n\n# Restore (advanced - exam may require)\nETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\\n  --data-dir=/var/lib/etcd-restore\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#practice-exercises","title":"Practice Exercises","text":"<ol> <li>Inspect Cluster Architecture</li> <li>List all control plane pods</li> <li>Check which nodes are running control plane components</li> <li> <p>Identify the API server endpoint and port</p> </li> <li> <p>Component Analysis</p> </li> <li>View logs from each control plane component</li> <li>Check resource usage of control plane pods</li> <li> <p>Identify which container runtime each node is using</p> </li> <li> <p>Troubleshooting Simulation</p> </li> <li>Simulate kubelet failure (stop service) and observe effects</li> <li>Check events to see scheduling decisions</li> <li>Examine etcd data to see how objects are stored</li> </ol>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Kubernetes is a distributed system with control plane and worker nodes</p> <p>\u2705 API server is the central hub - all communication flows through it</p> <p>\u2705 etcd stores all cluster state - critical for backups and disaster recovery</p> <p>\u2705 Scheduler assigns pods to nodes using filtering and scoring</p> <p>\u2705 Controllers maintain desired state through continuous reconciliation loops</p> <p>\u2705 kubelet is the node agent ensuring containers run as specified</p> <p>\u2705 kube-proxy handles networking enabling Service abstraction</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#quick-reference-commands","title":"Quick Reference Commands","text":"<pre><code># Cluster info\nkubectl cluster-info\nkubectl version\nkubectl api-resources\nkubectl api-versions\n\n# Component health\nkubectl get componentstatuses\nkubectl -n kube-system get pods\nkubectl get nodes\n\n# Component logs\nkubectl -n kube-system logs &lt;component-pod&gt;\njournalctl -u kubelet (on node)\njournalctl -u containerd (on node)\n\n# etcd operations\nETCDCTL_API=3 etcdctl endpoint health\nETCDCTL_API=3 etcdctl snapshot save &lt;file&gt;\nETCDCTL_API=3 etcdctl member list\n\n# Node inspection\nkubectl describe node &lt;node-name&gt;\nkubectl get nodes -o wide\nkubectl top nodes (requires metrics-server)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#next-steps","title":"Next Steps","text":"<p>Continue to Post 2: Setting Up Your Kubernetes Lab Environment to build your hands-on learning environment for practicing CKA exam scenarios.</p> <p>Related Posts: - Kubernetes CKA Mastery - Complete Learning Path - Post 3: kubectl Essentials (coming soon) - Post 15: RBAC and Security (coming soon)</p> <p>External Resources: - Kubernetes Components (Official Docs) - Kubernetes Architecture Diagram (CNCF) - CKA Exam Curriculum</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/","title":"CoreDNS and Service Discovery Deep Dive","text":"<p>Master Kubernetes DNS resolution and service discovery with CoreDNS. Learn DNS patterns, troubleshooting techniques, and advanced configuration strategies essential for the CKA exam and production Kubernetes environments.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#introduction","title":"Introduction","text":"<p>In the dynamic world of Kubernetes, where pods are ephemeral and IP addresses constantly change, DNS becomes the critical foundation for reliable service communication. Without DNS, your microservices architecture would crumble\u2014applications couldn't discover each other, load balancing would fail, and the entire orchestration system would become unmanageable.</p> <p>CoreDNS serves as Kubernetes' DNS server, automatically creating DNS records for every Service and Pod in your cluster. When a pod needs to communicate with a backend service, it doesn't need to know the service's IP address\u2014it simply queries DNS for <code>backend-service</code> and CoreDNS returns the appropriate ClusterIP. This seamless service discovery is what makes Kubernetes' dynamic environment practical for real-world applications.</p> <p>For the Certified Kubernetes Administrator (CKA) exam, DNS troubleshooting is a critical skill. You'll encounter scenarios where applications can't communicate, services aren't resolving, or DNS queries are failing. Understanding CoreDNS architecture, configuration, and debugging techniques is essential for both the exam and production operations.</p> <p>In this comprehensive guide, you'll master:</p> <ul> <li>DNS architecture: CoreDNS components, plugins, and how DNS resolution works</li> <li>Service DNS patterns: ClusterIP services, headless services, and SRV records</li> <li>Pod DNS configuration: DNS policies, search domains, and FQDN construction</li> <li>CoreDNS configuration: Corefile structure, plugin chains, and custom DNS entries</li> <li>Advanced features: NodeLocal DNSCache (2025), DNS policies, and performance optimization</li> <li>Troubleshooting mastery: Systematic debugging workflows using nslookup, dig, and CoreDNS logs</li> <li>CKA exam skills: Fast DNS testing, common scenarios, and time-saving techniques</li> </ul> <p>CKA Exam Relevance: DNS and service discovery appear across multiple exam domains\u2014Services &amp; Networking (20%), Troubleshooting (30%), and Cluster Architecture (25%). You must demonstrate proficiency in diagnosing DNS issues, understanding CoreDNS configuration, and validating service discovery under exam time pressure.</p> <p>Whether you're preparing for the CKA certification or building production-grade Kubernetes expertise, understanding CoreDNS is fundamental. Let's explore how DNS makes Kubernetes networking work seamlessly.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#dns-in-kubernetes-architecture","title":"DNS in Kubernetes Architecture","text":"","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#coredns-overview","title":"CoreDNS Overview","text":"<p>CoreDNS replaced kube-dns as the default Kubernetes DNS server starting in Kubernetes 1.13, and for good reason. Built in Go and designed as a modular, plugin-based DNS server, CoreDNS is faster, more flexible, and significantly easier to configure than its predecessor.</p> <p>At its core, CoreDNS runs as a Deployment in the <code>kube-system</code> namespace, typically with two replicas for high availability. Each CoreDNS pod watches the Kubernetes API server for Service and Endpoint changes, automatically creating and updating DNS records as resources are created, modified, or deleted.</p> <p>Here's the basic architecture:</p> <pre><code>graph TB\n    subgraph \"Application Pod\"\n        App[Application Container]\n        Resolver[/etc/resolv.conf]\n    end\n\n    subgraph \"kube-system Namespace\"\n        CoreDNS1[CoreDNS Pod 1]\n        CoreDNS2[CoreDNS Pod 2]\n        ConfigMap[CoreDNS ConfigMap]\n    end\n\n    subgraph \"Kubernetes API\"\n        APIServer[API Server]\n        ServiceRegistry[Services]\n        EndpointRegistry[Endpoints]\n    end\n\n    App --&gt;|DNS Query| Resolver\n    Resolver --&gt;|UDP/TCP 53| CoreDNS1\n    Resolver --&gt;|UDP/TCP 53| CoreDNS2\n    CoreDNS1 --&gt;|Watch| APIServer\n    CoreDNS2 --&gt;|Watch| APIServer\n    APIServer --&gt;|Service Events| ServiceRegistry\n    APIServer --&gt;|Endpoint Events| EndpointRegistry\n    ConfigMap --&gt;|Configuration| CoreDNS1\n    ConfigMap --&gt;|Configuration| CoreDNS2\n\n    style App fill:#e1f5ff\n    style CoreDNS1 fill:#fff4e1\n    style CoreDNS2 fill:#fff4e1\n    style ConfigMap fill:#f0f0f0</code></pre> <p>Key Components:</p> <ul> <li>CoreDNS Deployment: Runs the DNS server pods with resource limits and anti-affinity rules</li> <li>CoreDNS Service: ClusterIP service (usually <code>10.96.0.10</code>) that kubelet configures in pod <code>/etc/resolv.conf</code></li> <li>CoreDNS ConfigMap: Contains the Corefile configuration defining DNS resolution behavior</li> <li>Kubernetes Plugin: CoreDNS plugin that watches the API and generates DNS records</li> <li>Cache Plugin: In-memory cache for DNS queries to reduce API load</li> <li>Forward Plugin: Forwards non-cluster queries to upstream DNS servers</li> </ul>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#dns-record-types-in-kubernetes","title":"DNS Record Types in Kubernetes","text":"<p>CoreDNS creates several types of DNS records automatically:</p> <p>Service DNS Records (A/AAAA): <pre><code>&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;\n</code></pre></p> <p>Example: <code>nginx.default.svc.cluster.local</code> \u2192 <code>10.96.100.50</code> (ClusterIP)</p> <p>Headless Service Records (A/AAAA): <pre><code>&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt; \u2192 Pod IPs\n&lt;pod-hostname&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt; \u2192 Pod IP\n</code></pre></p> <p>Example: <code>nginx.default.svc.cluster.local</code> \u2192 <code>10.244.1.10, 10.244.2.15</code> (Pod IPs)</p> <p>Pod DNS Records (A/AAAA): <pre><code>&lt;pod-ip-with-dashes&gt;.&lt;namespace&gt;.pod.&lt;cluster-domain&gt;\n</code></pre></p> <p>Example: <code>10-244-1-10.default.pod.cluster.local</code> \u2192 <code>10.244.1.10</code></p> <p>SRV Records (Service Discovery with Port Information): <pre><code>_&lt;port-name&gt;._&lt;protocol&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;\n</code></pre></p> <p>Example: <code>_http._tcp.nginx.default.svc.cluster.local</code> \u2192 <code>0 100 80 nginx.default.svc.cluster.local</code></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#dns-resolution-flow","title":"DNS Resolution Flow","text":"<p>Understanding how DNS queries flow through the system is critical for troubleshooting:</p> <pre><code>sequenceDiagram\n    participant App as Application Pod\n    participant Kubelet as kubelet\n    participant CoreDNS as CoreDNS Pod\n    participant Cache as DNS Cache\n    participant API as Kubernetes API\n    participant Upstream as Upstream DNS\n\n    Note over App,Kubelet: Pod Creation\n    Kubelet-&gt;&gt;App: Inject /etc/resolv.conf&lt;br/&gt;nameserver 10.96.0.10&lt;br/&gt;search default.svc.cluster.local\n\n    Note over App,CoreDNS: DNS Query: \"backend\"\n    App-&gt;&gt;CoreDNS: Query \"backend\" (short name)\n    CoreDNS-&gt;&gt;CoreDNS: Apply search domains:&lt;br/&gt;backend.default.svc.cluster.local\n    CoreDNS-&gt;&gt;Cache: Check cache\n\n    alt Cache Hit\n        Cache-&gt;&gt;CoreDNS: Return cached IP\n        CoreDNS-&gt;&gt;App: 10.96.200.100\n    else Cache Miss\n        CoreDNS-&gt;&gt;API: Query Service \"backend\" in \"default\"\n        API-&gt;&gt;CoreDNS: ClusterIP: 10.96.200.100\n        CoreDNS-&gt;&gt;Cache: Store in cache (30s TTL)\n        CoreDNS-&gt;&gt;App: 10.96.200.100\n    end\n\n    Note over App,Upstream: External Query: \"google.com\"\n    App-&gt;&gt;CoreDNS: Query \"google.com\"\n    CoreDNS-&gt;&gt;CoreDNS: Not cluster domain\n    CoreDNS-&gt;&gt;Upstream: Forward to 8.8.8.8\n    Upstream-&gt;&gt;CoreDNS: 142.250.185.46\n    CoreDNS-&gt;&gt;App: 142.250.185.46</code></pre> <p>Resolution Steps:</p> <ol> <li>Application initiates query: App container queries DNS using the nameserver from <code>/etc/resolv.conf</code></li> <li>Search domain expansion: If querying a short name (e.g., <code>backend</code>), the resolver appends search domains</li> <li>CoreDNS receives query: Query reaches CoreDNS service (typically <code>10.96.0.10:53</code>)</li> <li>Cache check: CoreDNS checks its in-memory cache for the record</li> <li>Kubernetes plugin lookup: If cache miss, the kubernetes plugin queries the API for Service/Endpoint data</li> <li>Response construction: CoreDNS builds the DNS response with the appropriate A/AAAA records</li> <li>Cache storage: Result is cached with TTL (typically 30 seconds for cluster records)</li> <li>External forwarding: Non-cluster queries are forwarded to upstream DNS servers</li> </ol>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#search-domains-and-fqdn-construction","title":"Search Domains and FQDN Construction","text":"<p>Every pod's <code>/etc/resolv.conf</code> contains search domains that enable short-name DNS queries:</p> <pre><code>nameserver 10.96.0.10\nsearch default.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5\n</code></pre> <p>Search Domain Behavior:</p> <ul> <li>Query: <code>backend</code> \u2192 Tries <code>backend.default.svc.cluster.local</code>, then <code>backend.svc.cluster.local</code>, then <code>backend.cluster.local</code></li> <li>Query: <code>backend.production</code> \u2192 Tries <code>backend.production.default.svc.cluster.local</code>, then <code>backend.production.svc.cluster.local</code>, etc.</li> <li>Query: <code>backend.production.svc.cluster.local</code> \u2192 Direct FQDN query, no search domain expansion</li> </ul> <p>ndots Configuration:</p> <p>The <code>ndots:5</code> option determines when search domains are applied. If a query has fewer than 5 dots, search domains are tried first. This means:</p> <ul> <li><code>backend</code> (0 dots) \u2192 Search domains applied</li> <li><code>backend.production</code> (1 dot) \u2192 Search domains applied</li> <li><code>backend.production.svc.cluster.local.</code> (4 dots + trailing <code>.</code>) \u2192 Direct query, no search</li> </ul> <p>Important: Always use the trailing dot (<code>.</code>) for external domains to avoid unnecessary search domain queries: <pre><code># Inefficient - tries search domains first\ncurl http://api.example.com/data\n\n# Efficient - direct query\ncurl http://api.example.com./data\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#coredns-configuration-deep-dive","title":"CoreDNS Configuration Deep Dive","text":"","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#corefile-structure","title":"Corefile Structure","text":"<p>CoreDNS is configured via a ConfigMap named <code>coredns</code> in the <code>kube-system</code> namespace. The ConfigMap contains a Corefile\u2014a configuration format similar to nginx or Apache\u2014that defines DNS server behavior through a plugin chain.</p> <p>Let's examine a typical Corefile:</p> <pre><code>kubectl get configmap coredns -n kube-system -o yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n           lameduck 5s\n        }\n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n           pods insecure\n           fallthrough in-addr.arpa ip6.arpa\n           ttl 30\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf {\n           max_concurrent 1000\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n</code></pre> <p>Corefile Sections Explained:</p> <ul> <li><code>.:53</code>: Server block listening on port 53 for all domains (<code>.</code> matches everything)</li> <li>Plugin order matters: Plugins are executed in the order listed (errors \u2192 health \u2192 ready \u2192 kubernetes \u2192 ...)</li> <li>Each plugin: Configures specific DNS behavior (logging, health checks, Kubernetes records, caching, etc.)</li> </ul>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#coredns-plugins","title":"CoreDNS Plugins","text":"<p>Let's break down each plugin and its purpose:</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#1-errors-plugin","title":"1. errors Plugin","text":"<p><pre><code>errors\n</code></pre> Enables error logging to stdout. Essential for troubleshooting DNS issues.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#2-health-plugin","title":"2. health Plugin","text":"<p><pre><code>health {\n   lameduck 5s\n}\n</code></pre> Exposes a health check endpoint at <code>:8080/health</code>. The <code>lameduck</code> period delays the health check failure during pod shutdown, allowing time for graceful termination.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#3-ready-plugin","title":"3. ready Plugin","text":"<p><pre><code>ready\n</code></pre> Exposes a readiness check at <code>:8181/ready</code>. This endpoint returns 200 OK when all plugins are ready to serve queries.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#4-kubernetes-plugin","title":"4. kubernetes Plugin","text":"<pre><code>kubernetes cluster.local in-addr.arpa ip6.arpa {\n   pods insecure\n   fallthrough in-addr.arpa ip6.arpa\n   ttl 30\n}\n</code></pre> <p>This is the core plugin that provides Kubernetes DNS functionality:</p> <ul> <li><code>cluster.local</code>: The cluster domain for DNS records</li> <li><code>in-addr.arpa ip6.arpa</code>: Reverse DNS zones for PTR lookups</li> <li><code>pods insecure</code>: Enables pod DNS records using pod IP address (<code>10-244-1-10.default.pod.cluster.local</code>)</li> <li><code>insecure</code> mode doesn't verify pod existence (faster but less strict)</li> <li>Use <code>pods verified</code> for production to validate pods exist</li> <li><code>fallthrough</code>: Pass queries for reverse DNS zones to the next plugin if not found</li> <li><code>ttl 30</code>: DNS records have a 30-second time-to-live</li> </ul> <p>Advanced Options: <pre><code>kubernetes cluster.local in-addr.arpa ip6.arpa {\n   pods verified\n   endpoint_pod_names\n   upstream\n   ttl 30\n   fallthrough in-addr.arpa ip6.arpa\n}\n</code></pre></p> <ul> <li><code>endpoint_pod_names</code>: Use pod names from Endpoints instead of generated names</li> <li><code>upstream</code>: Query upstream servers for external queries instead of returning NXDOMAIN</li> </ul>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#5-prometheus-plugin","title":"5. prometheus Plugin","text":"<p><pre><code>prometheus :9153\n</code></pre> Exposes Prometheus metrics at <code>:9153/metrics</code> for monitoring DNS performance, query rates, and cache hit ratios.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#6-forward-plugin","title":"6. forward Plugin","text":"<pre><code>forward . /etc/resolv.conf {\n   max_concurrent 1000\n}\n</code></pre> <p>Forwards non-cluster DNS queries to upstream DNS servers:</p> <ul> <li><code>. /etc/resolv.conf</code>: Forward all non-matched queries to nameservers in CoreDNS pod's <code>/etc/resolv.conf</code></li> <li>Alternative: <code>forward . 8.8.8.8 8.8.4.4</code> (use specific DNS servers)</li> <li><code>max_concurrent 1000</code>: Limit concurrent upstream queries to prevent overwhelming upstream servers</li> </ul> <p>Custom Upstream Example: <pre><code>forward . 1.1.1.1 1.0.0.1 {\n   max_concurrent 1000\n   policy sequential\n   health_check 5s\n}\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#7-cache-plugin","title":"7. cache Plugin","text":"<p><pre><code>cache 30\n</code></pre> Caches DNS responses for 30 seconds to reduce load on the Kubernetes API and upstream DNS servers.</p> <p>Advanced Cache Configuration: <pre><code>cache 30 {\n   success 9984 30\n   denial 9984 5\n   prefetch 10 60s 10%\n}\n</code></pre></p> <ul> <li><code>success 9984 30</code>: Cache successful responses for 30 seconds (max 9984 entries)</li> <li><code>denial 9984 5</code>: Cache NXDOMAIN responses for 5 seconds</li> <li><code>prefetch 10 60s 10%</code>: Prefetch records that will expire in 60s if accessed by &gt;10% of requests</li> </ul>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#8-loop-plugin","title":"8. loop Plugin","text":"<p><pre><code>loop\n</code></pre> Detects and prevents DNS forwarding loops that could cause infinite query cycles.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#9-reload-plugin","title":"9. reload Plugin","text":"<p><pre><code>reload\n</code></pre> Watches the Corefile for changes and automatically reloads configuration without restarting CoreDNS pods.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#10-loadbalance-plugin","title":"10. loadbalance Plugin","text":"<p><pre><code>loadbalance\n</code></pre> Randomizes the order of A/AAAA records in DNS responses, providing basic client-side load balancing.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#custom-dns-records","title":"Custom DNS Records","text":"<p>You can add custom DNS entries for internal services or override external domains:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n           lameduck 5s\n        }\n        ready\n\n        # Custom static DNS entries\n        hosts {\n           192.168.1.100 internal-db.company.local\n           192.168.1.101 legacy-app.company.local\n           fallthrough\n        }\n\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n           pods insecure\n           fallthrough in-addr.arpa ip6.arpa\n           ttl 30\n        }\n\n        prometheus :9153\n        forward . /etc/resolv.conf\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n</code></pre> <p>Static Host Entries: <pre><code>hosts {\n   192.168.1.100 internal-db.company.local\n   fallthrough\n}\n</code></pre></p> <p>The <code>fallthrough</code> directive ensures that queries not matching static entries continue down the plugin chain.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#rewrite-plugin-for-dns-aliasing","title":"Rewrite Plugin for DNS Aliasing","text":"<p>Create DNS aliases or rewrite queries:</p> <pre><code>rewrite name api.internal.local backend.default.svc.cluster.local\n</code></pre> <p>This rewrites queries for <code>api.internal.local</code> to <code>backend.default.svc.cluster.local</code>.</p> <p>Advanced Rewrite: <pre><code>rewrite {\n   name regex (.*)\\.old-domain\\.com {1}.new-domain.com\n}\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#service-discovery-patterns","title":"Service Discovery Patterns","text":"","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#clusterip-service-dns","title":"ClusterIP Service DNS","text":"<p>The most common service type uses a stable ClusterIP for internal load balancing:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  namespace: production\nspec:\n  type: ClusterIP\n  selector:\n    app: nginx\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <p>DNS Resolution: <pre><code>graph LR\n    Query[DNS Query] --&gt;|nginx| SearchDomain[Search Domain Expansion]\n    SearchDomain --&gt;|nginx.production.svc.cluster.local| CoreDNS\n    CoreDNS --&gt;|Kubernetes Plugin| API[API Server]\n    API --&gt;|Service ClusterIP| Response[10.96.100.50]\n    Response --&gt; Client[Client Pod]\n\n    style Query fill:#e1f5ff\n    style CoreDNS fill:#fff4e1\n    style Response fill:#d4edda</code></pre></p> <p>Available DNS Names (from same namespace <code>production</code>): <pre><code>nginx                              \u2192 10.96.100.50 (short name)\nnginx.production                   \u2192 10.96.100.50 (namespace qualified)\nnginx.production.svc               \u2192 10.96.100.50 (service scoped)\nnginx.production.svc.cluster.local \u2192 10.96.100.50 (FQDN)\n</code></pre></p> <p>From Different Namespace: <pre><code># Short name fails - only works within same namespace\ncurl http://nginx/\n# Error: could not resolve host\n\n# Must use namespace-qualified name\ncurl http://nginx.production/\n# Success: 200 OK\n\n# FQDN always works\ncurl http://nginx.production.svc.cluster.local/\n# Success: 200 OK\n</code></pre></p> <p>SRV Records for Port Discovery: <pre><code>dig SRV _http._tcp.nginx.production.svc.cluster.local\n\n; ANSWER SECTION:\n_http._tcp.nginx.production.svc.cluster.local. 30 IN SRV 0 100 80 nginx.production.svc.cluster.local.\n</code></pre></p> <p>SRV record format: <code>priority weight port target</code></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#headless-service-dns","title":"Headless Service DNS","text":"<p>Headless services (with <code>clusterIP: None</code>) return pod IPs directly instead of a stable ClusterIP, enabling direct pod-to-pod communication:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-headless\n  namespace: production\nspec:\n  clusterIP: None  # Headless service\n  selector:\n    app: nginx\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <p>DNS Behavior:</p> <pre><code>graph TB\n    subgraph \"DNS Query Flow\"\n        Query[Query: nginx-headless.production.svc.cluster.local]\n        CoreDNS[CoreDNS]\n        API[Kubernetes API]\n    end\n\n    subgraph \"Pod Endpoints\"\n        Pod1[nginx-0&lt;br/&gt;10.244.1.10]\n        Pod2[nginx-1&lt;br/&gt;10.244.2.15]\n        Pod3[nginx-2&lt;br/&gt;10.244.3.20]\n    end\n\n    Query --&gt; CoreDNS\n    CoreDNS --&gt; API\n    API --&gt;|Endpoint IPs| CoreDNS\n    CoreDNS --&gt;|A Records| Response[\"10.244.1.10&lt;br/&gt;10.244.2.15&lt;br/&gt;10.244.3.20\"]\n\n    Response -.-&gt; Pod1\n    Response -.-&gt; Pod2\n    Response -.-&gt; Pod3\n\n    style Query fill:#e1f5ff\n    style CoreDNS fill:#fff4e1\n    style Response fill:#d4edda</code></pre> <p>DNS Resolution Returns All Pod IPs: <pre><code>nslookup nginx-headless.production.svc.cluster.local\n\nServer:    10.96.0.10\nAddress:   10.96.0.10#53\n\nName:   nginx-headless.production.svc.cluster.local\nAddress: 10.244.1.10\nName:   nginx-headless.production.svc.cluster.local\nAddress: 10.244.2.15\nName:   nginx-headless.production.svc.cluster.local\nAddress: 10.244.3.20\n</code></pre></p> <p>Individual Pod DNS (for StatefulSets): <pre><code>&lt;pod-name&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local\n</code></pre></p> <p>Example with StatefulSet: <pre><code>nslookup nginx-0.nginx-headless.production.svc.cluster.local\n\nServer:    10.96.0.10\nAddress:   10.96.0.10#53\n\nName:   nginx-0.nginx-headless.production.svc.cluster.local\nAddress: 10.244.1.10\n</code></pre></p> <p>Use Cases for Headless Services: - StatefulSets: Stable DNS names for individual pods (<code>mongo-0</code>, <code>mongo-1</code>, <code>mongo-2</code>) - Database clusters: Direct communication with specific database replicas - Custom load balancing: Applications implementing their own load balancing logic - Service discovery: Discovering all pod endpoints for a service</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#pod-dns-configuration","title":"Pod DNS Configuration","text":"<p>Pods have automatic DNS records based on their IP address:</p> <p>Default Pod DNS: <pre><code>&lt;pod-ip-with-dashes&gt;.&lt;namespace&gt;.pod.&lt;cluster-domain&gt;\n</code></pre></p> <p>Example: <code>10-244-1-10.default.pod.cluster.local</code> \u2192 <code>10.244.1.10</code></p> <p>Pod with Hostname and Subdomain: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  namespace: production\nspec:\n  hostname: web-server\n  subdomain: nginx-headless\n  containers:\n  - name: nginx\n    image: nginx:1.25\n</code></pre></p> <p>DNS becomes: <pre><code>web-server.nginx-headless.production.svc.cluster.local \u2192 10.244.1.10\n</code></pre></p> <p>This is particularly useful for StatefulSets where predictable DNS names are required.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#troubleshooting-dns-issues","title":"Troubleshooting DNS Issues","text":"","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#systematic-debugging-workflow","title":"Systematic Debugging Workflow","text":"<p>When facing DNS issues, follow this methodical approach:</p> <pre><code>graph TD\n    Start[DNS Issue Reported] --&gt; Check1{Can pod resolve&lt;br/&gt;cluster services?}\n    Check1 --&gt;|No| Debug1[Test DNS from pod]\n    Check1 --&gt;|Yes| Check2{Can pod resolve&lt;br/&gt;external domains?}\n\n    Debug1 --&gt; Test1[kubectl exec -it pod -- nslookup kubernetes.default]\n    Test1 --&gt; Result1{Success?}\n    Result1 --&gt;|No| Fix1[Check CoreDNS pods,&lt;br/&gt;resolv.conf, network policy]\n    Result1 --&gt;|Yes| Check2\n\n    Check2 --&gt;|No| Debug2[Check upstream DNS]\n    Check2 --&gt;|Yes| Check3{Specific service&lt;br/&gt;not resolving?}\n\n    Debug2 --&gt; Test2[kubectl exec -it pod -- nslookup google.com]\n    Test2 --&gt; Result2{Success?}\n    Result2 --&gt;|No| Fix2[Check forward plugin,&lt;br/&gt;upstream DNS servers,&lt;br/&gt;network connectivity]\n    Result2 --&gt;|Yes| Check3\n\n    Check3 --&gt;|Yes| Debug3[Verify service exists]\n    Debug3 --&gt; Test3[kubectl get svc service-name -n namespace]\n    Test3 --&gt; Result3{Service exists?}\n    Result3 --&gt;|No| Fix3[Create service or&lt;br/&gt;fix service name]\n    Result3 --&gt;|Yes| Debug4[Check endpoints]\n\n    Debug4 --&gt; Test4[kubectl get endpoints service-name -n namespace]\n    Test4 --&gt; Result4{Endpoints ready?}\n    Result4 --&gt;|No| Fix4[Check pod selector,&lt;br/&gt;pod readiness,&lt;br/&gt;pod labels]\n    Result4 --&gt;|Yes| Debug5[Check DNS record]\n\n    Debug5 --&gt; Test5[kubectl exec -it pod -- nslookup service.namespace.svc.cluster.local]\n    Test5 --&gt; Result5{Resolves correctly?}\n    Result5 --&gt;|No| Fix5[Check CoreDNS logs,&lt;br/&gt;restart CoreDNS,&lt;br/&gt;check Corefile config]\n    Result5 --&gt;|Yes| Fix6[Issue may be&lt;br/&gt;application-level,&lt;br/&gt;not DNS]\n\n    style Start fill:#e1f5ff\n    style Fix1 fill:#f8d7da\n    style Fix2 fill:#f8d7da\n    style Fix3 fill:#f8d7da\n    style Fix4 fill:#f8d7da\n    style Fix5 fill:#f8d7da\n    style Fix6 fill:#fff3cd</code></pre>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#essential-dns-testing-commands","title":"Essential DNS Testing Commands","text":"<p>Test DNS from a debug pod: <pre><code># Create a debug pod with DNS tools\nkubectl run debug-dns --image=nicolaka/netshoot -it --rm -- /bin/bash\n\n# Inside the pod:\n# Test cluster DNS\nnslookup kubernetes.default\nnslookup kubernetes.default.svc.cluster.local\n\n# Test external DNS\nnslookup google.com\n\n# Use dig for detailed information\ndig kubernetes.default\ndig @10.96.0.10 kubernetes.default  # Query CoreDNS directly\n\n# Test specific service\nnslookup nginx.production.svc.cluster.local\n\n# Test headless service\nnslookup nginx-headless.production.svc.cluster.local\n\n# Test SRV records\ndig SRV _http._tcp.nginx.production.svc.cluster.local\n</code></pre></p> <p>Check pod's DNS configuration: <pre><code># View resolv.conf\nkubectl exec -it pod-name -- cat /etc/resolv.conf\n\n# Expected output:\n# nameserver 10.96.0.10\n# search default.svc.cluster.local svc.cluster.local cluster.local\n# options ndots:5\n\n# Verify DNS connectivity\nkubectl exec -it pod-name -- ping -c 3 10.96.0.10\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#common-dns-issues-and-solutions","title":"Common DNS Issues and Solutions","text":"","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#issue-1-pod-cannot-resolve-cluster-services","title":"Issue 1: Pod Cannot Resolve Cluster Services","text":"<p>Symptoms: <pre><code>nslookup kubernetes.default\nServer:    10.96.0.10\nAddress:   10.96.0.10#53\n\n** server can't find kubernetes.default: NXDOMAIN\n</code></pre></p> <p>Debugging Steps: <pre><code># 1. Check CoreDNS pods are running\nkubectl get pods -n kube-system -l k8s-app=kube-dns\nNAME                      READY   STATUS    RESTARTS   AGE\ncoredns-5d78c9869d-abcde  1/1     Running   0          5d\ncoredns-5d78c9869d-fghij  1/1     Running   0          5d\n\n# 2. Check CoreDNS service exists and has endpoints\nkubectl get svc -n kube-system kube-dns\nkubectl get endpoints -n kube-system kube-dns\n\n# 3. Check CoreDNS logs for errors\nkubectl logs -n kube-system -l k8s-app=kube-dns --tail=100\n\n# 4. Verify pod's resolv.conf points to CoreDNS\nkubectl exec -it pod-name -- cat /etc/resolv.conf\n\n# 5. Test DNS directly from CoreDNS service IP\nkubectl exec -it pod-name -- nslookup kubernetes.default 10.96.0.10\n</code></pre></p> <p>Common Causes: - CoreDNS pods not running (check pod status) - Wrong nameserver in <code>/etc/resolv.conf</code> (kubelet configuration issue) - Network policy blocking DNS traffic (port 53 UDP/TCP) - CoreDNS service ClusterIP changed (check service YAML)</p> <p>Solutions: <pre><code># Restart CoreDNS pods\nkubectl rollout restart deployment coredns -n kube-system\n\n# Check network policies blocking DNS\nkubectl get networkpolicy -A\nkubectl describe networkpolicy &lt;policy-name&gt;\n\n# Verify kubelet DNS configuration\nkubectl get configmap kubelet-config -n kube-system -o yaml | grep clusterDNS\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#issue-2-cannot-resolve-external-domains","title":"Issue 2: Cannot Resolve External Domains","text":"<p>Symptoms: <pre><code>nslookup google.com\nServer:    10.96.0.10\nAddress:   10.96.0.10#53\n\n** server can't find google.com: NXDOMAIN\n</code></pre></p> <p>Debugging Steps: <pre><code># 1. Check CoreDNS forward configuration\nkubectl get configmap coredns -n kube-system -o yaml | grep -A 5 forward\n\n# 2. Check CoreDNS can reach upstream DNS\nkubectl exec -it -n kube-system &lt;coredns-pod&gt; -- cat /etc/resolv.conf\nkubectl exec -it -n kube-system &lt;coredns-pod&gt; -- nslookup google.com\n\n# 3. Test from CoreDNS pod directly\nkubectl exec -it -n kube-system &lt;coredns-pod&gt; -- ping -c 3 8.8.8.8\n</code></pre></p> <p>Common Causes: - Upstream DNS servers unreachable (firewall/network issue) - Wrong upstream DNS configuration in Corefile - No internet connectivity from cluster nodes - DNS forwarding disabled or misconfigured</p> <p>Solutions: <pre><code># Update upstream DNS servers in Corefile\nkubectl edit configmap coredns -n kube-system\n\n# Change:\n# forward . /etc/resolv.conf\n# To:\n# forward . 8.8.8.8 8.8.4.4 1.1.1.1\n\n# CoreDNS will auto-reload with reload plugin\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#issue-3-service-exists-but-dns-doesnt-resolve","title":"Issue 3: Service Exists But DNS Doesn't Resolve","text":"<p>Symptoms: <pre><code>kubectl get svc nginx\nNAME    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nnginx   ClusterIP   10.96.100.50   &lt;none&gt;        80/TCP    5m\n\nnslookup nginx.default.svc.cluster.local\n** server can't find nginx.default.svc.cluster.local: NXDOMAIN\n</code></pre></p> <p>Debugging Steps: <pre><code># 1. Check if service has endpoints\nkubectl get endpoints nginx\nNAME    ENDPOINTS         AGE\nnginx   &lt;none&gt;            5m  # No endpoints!\n\n# 2. Check pod labels match service selector\nkubectl get svc nginx -o yaml | grep -A 5 selector\nkubectl get pods -l app=nginx --show-labels\n\n# 3. Check pods are ready\nkubectl get pods -l app=nginx\n\n# 4. Force DNS cache clear by restarting CoreDNS\nkubectl rollout restart deployment coredns -n kube-system\n</code></pre></p> <p>Common Causes: - Service selector doesn't match any pods (label mismatch) - Pods exist but aren't ready (readiness probe failing) - DNS cache stale (wait 30s or restart CoreDNS) - Service in wrong namespace</p> <p>Solutions: <pre><code># Fix service selector to match pod labels\nkubectl edit svc nginx\n\n# Check why pods aren't ready\nkubectl describe pod &lt;pod-name&gt;\n\n# Verify labels match\nkubectl get pods --show-labels\nkubectl get svc nginx -o jsonpath='{.spec.selector}'\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#issue-4-headless-service-returns-no-ips","title":"Issue 4: Headless Service Returns No IPs","text":"<p>Debugging: <pre><code># Check endpoints exist\nkubectl get endpoints nginx-headless\n\n# Check pods are ready and have IPs\nkubectl get pods -l app=nginx -o wide\n\n# Verify service is truly headless\nkubectl get svc nginx-headless -o yaml | grep clusterIP\n# Should show: clusterIP: None\n\n# Test DNS resolution\nkubectl run debug --image=nicolaka/netshoot -it --rm -- nslookup nginx-headless.default.svc.cluster.local\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#coredns-log-analysis","title":"CoreDNS Log Analysis","text":"<p>View CoreDNS logs: <pre><code># Follow CoreDNS logs\nkubectl logs -n kube-system -l k8s-app=kube-dns -f\n\n# Filter for errors\nkubectl logs -n kube-system -l k8s-app=kube-dns | grep -i error\n\n# Look for specific query\nkubectl logs -n kube-system -l k8s-app=kube-dns | grep \"nginx.default\"\n</code></pre></p> <p>Enable query logging (debugging only - high volume): <pre><code># Add to Corefile\n.:53 {\n    log  # Enable query logging\n    errors\n    # ... rest of config\n}\n</code></pre></p> <p>Common Error Patterns: <pre><code>[ERROR] plugin/errors: 2 example.com. A: read udp 10.96.0.10:53-&gt;8.8.8.8:53: i/o timeout\n\u2192 Upstream DNS unreachable\n\n[ERROR] plugin/errors: 2 nginx.default.svc.cluster.local. A: no such service\n\u2192 Service doesn't exist\n\n[INFO] NXDOMAIN: nginx.production.svc.cluster.local. A: 127.0.0.1:53\n\u2192 Service not found (check namespace, service name)\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#advanced-dns-topics","title":"Advanced DNS Topics","text":"","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#nodelocal-dnscache-2025-best-practice","title":"NodeLocal DNSCache (2025 Best Practice)","text":"<p>NodeLocal DNSCache is a DaemonSet that runs a DNS caching agent on cluster nodes, significantly improving DNS performance and reliability by:</p> <ul> <li>Reducing CoreDNS load (especially in large clusters)</li> <li>Minimizing DNS query latency (queries stay on local node)</li> <li>Improving reliability (local cache survives CoreDNS pod restarts)</li> <li>Reducing network hops (no DNAT traversal)</li> </ul> <p>Architecture:</p> <pre><code>graph TB\n    subgraph \"Node 1\"\n        Pod1[Application Pod]\n        NodeCache1[NodeLocal DNSCache&lt;br/&gt;169.254.20.10]\n        Kubelet1[kubelet]\n    end\n\n    subgraph \"Node 2\"\n        Pod2[Application Pod]\n        NodeCache2[NodeLocal DNSCache&lt;br/&gt;169.254.20.10]\n        Kubelet2[kubelet]\n    end\n\n    subgraph \"kube-system Namespace\"\n        CoreDNS1[CoreDNS Pod]\n        CoreDNS2[CoreDNS Pod]\n    end\n\n    Pod1 --&gt;|DNS Query| NodeCache1\n    Pod2 --&gt;|DNS Query| NodeCache2\n\n    NodeCache1 --&gt;|Cache Miss| CoreDNS1\n    NodeCache1 --&gt;|Cache Miss| CoreDNS2\n    NodeCache2 --&gt;|Cache Miss| CoreDNS1\n    NodeCache2 --&gt;|Cache Miss| CoreDNS2\n\n    Kubelet1 -.-&gt;|Configures&lt;br/&gt;169.254.20.10| Pod1\n    Kubelet2 -.-&gt;|Configures&lt;br/&gt;169.254.20.10| Pod2\n\n    style Pod1 fill:#e1f5ff\n    style Pod2 fill:#e1f5ff\n    style NodeCache1 fill:#d4edda\n    style NodeCache2 fill:#d4edda\n    style CoreDNS1 fill:#fff4e1\n    style CoreDNS2 fill:#fff4e1</code></pre> <p>Installation (Kubernetes 1.18+): <pre><code># Download NodeLocal DNSCache manifest\nkubectl apply -f https://k8s.io/examples/admin/dns/nodelocaldns.yaml\n\n# Verify DaemonSet running on all nodes\nkubectl get daemonset node-local-dns -n kube-system\nNAME             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE\nnode-local-dns   3         3         3       3            3\n</code></pre></p> <p>How It Works: - NodeLocal DNSCache listens on link-local IP <code>169.254.20.10</code> - kubelet configures pods to use <code>169.254.20.10</code> as nameserver - Cache miss queries are forwarded to ClusterIP <code>10.96.0.10</code> (CoreDNS) - Cache hit queries are served immediately from node-local cache</p> <p>Performance Benefits: - 50-90% reduction in DNS query latency - 70% reduction in CoreDNS CPU usage - Improved reliability during CoreDNS pod restarts</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#dns-policies","title":"DNS Policies","text":"<p>Pods can use different DNS policies to control DNS resolution behavior:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-example\nspec:\n  dnsPolicy: ClusterFirst  # Default\n  containers:\n  - name: test\n    image: nginx\n</code></pre> <p>DNS Policy Options:</p> Policy Behavior Use Case ClusterFirst Use cluster DNS (CoreDNS), fallback to node's DNS for non-cluster domains Default, most common ClusterFirstWithHostNet Like ClusterFirst but for pods with <code>hostNetwork: true</code> Pods using host networking Default Inherit DNS config from node's <code>/etc/resolv.conf</code> Legacy workloads None No DNS configuration (must specify <code>dnsConfig</code>) Custom DNS setup <p>Custom DNS Configuration: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: custom-dns\nspec:\n  dnsPolicy: None\n  dnsConfig:\n    nameservers:\n      - 1.1.1.1\n      - 8.8.8.8\n    searches:\n      - my-namespace.svc.cluster.local\n      - svc.cluster.local\n      - cluster.local\n    options:\n      - name: ndots\n        value: \"2\"\n      - name: timeout\n        value: \"5\"\n  containers:\n  - name: test\n    image: nginx\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#ndots-configuration-tuning","title":"ndots Configuration Tuning","text":"<p>The <code>ndots:5</code> default can cause performance issues for external domain queries, as the DNS resolver tries all search domains before the direct query:</p> <p>Problem: <pre><code># Query: api.example.com (1 dot, less than ndots:5)\n# DNS queries attempted:\n1. api.example.com.default.svc.cluster.local (NXDOMAIN)\n2. api.example.com.svc.cluster.local (NXDOMAIN)\n3. api.example.com.cluster.local (NXDOMAIN)\n4. api.example.com (SUCCESS)\n</code></pre></p> <p>This creates 3 unnecessary queries!</p> <p>Solution 1: Reduce ndots: <pre><code>spec:\n  dnsConfig:\n    options:\n      - name: ndots\n        value: \"2\"  # Reduce from 5 to 2\n</code></pre></p> <p>Solution 2: Use FQDNs with trailing dot: <pre><code># Application code - always use trailing dot for external domains\ncurl http://api.example.com./data\n</code></pre></p> <p>Recommendation: For microservices making many external API calls, reducing ndots to 2-3 can significantly improve DNS performance.</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#cka-exam-dns-skills","title":"CKA Exam DNS Skills","text":"","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#fast-dns-testing-techniques","title":"Fast DNS Testing Techniques","text":"<p>Quick service resolution test: <pre><code># From any pod, test service DNS\nkubectl exec -it &lt;pod-name&gt; -- nslookup kubernetes.default\n\n# Create temporary debug pod\nkubectl run tmp --image=nicolaka/netshoot --rm -it -- nslookup nginx.production\n</code></pre></p> <p>One-liner DNS verification: <pre><code># Test multiple DNS names quickly\nfor svc in kubernetes nginx backend; do\n  kubectl exec -it &lt;pod-name&gt; -- nslookup $svc 2&gt;&amp;1 | grep -E \"Name:|Address:\"\ndone\n</code></pre></p> <p>Check DNS end-to-end: <pre><code># Verify full DNS chain: pod \u2192 CoreDNS \u2192 service \u2192 endpoint\nkubectl run test --image=busybox --rm -it -- wget -O- http://nginx.default.svc.cluster.local\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#common-cka-exam-scenarios","title":"Common CKA Exam Scenarios","text":"<p>Scenario 1: \"Application can't connect to database service\"</p> <pre><code># Fast troubleshooting workflow\n# 1. Verify service exists\nkubectl get svc database\n\n# 2. Check endpoints\nkubectl get endpoints database\n\n# 3. Test DNS from application pod\nkubectl exec -it app-pod -- nslookup database\n\n# 4. If DNS fails, check CoreDNS\nkubectl get pods -n kube-system -l k8s-app=kube-dns\nkubectl logs -n kube-system -l k8s-app=kube-dns --tail=50\n</code></pre> <p>Scenario 2: \"Fix CoreDNS configuration to use custom upstream DNS\"</p> <pre><code># Edit Corefile\nkubectl edit configmap coredns -n kube-system\n\n# Change forward plugin\n# FROM:\nforward . /etc/resolv.conf\n\n# TO:\nforward . 8.8.8.8 1.1.1.1\n\n# CoreDNS auto-reloads (no restart needed with reload plugin)\n</code></pre> <p>Scenario 3: \"Create headless service for StatefulSet\"</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mongo\nspec:\n  clusterIP: None  # Headless!\n  selector:\n    app: mongo\n  ports:\n  - port: 27017\n    targetPort: 27017\n</code></pre>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#time-saving-tips","title":"Time-Saving Tips","text":"<ol> <li>Use short names within namespace: <code>curl http://backend</code> instead of <code>backend.default.svc.cluster.local</code></li> <li>Keep debug pod ready: <code>kubectl run debug --image=nicolaka/netshoot --rm -it -- /bin/bash</code></li> <li>Alias common checks:    <pre><code>alias kdns='kubectl get pods -n kube-system -l k8s-app=kube-dns'\nalias kdnslogs='kubectl logs -n kube-system -l k8s-app=kube-dns --tail=100'\n</code></pre></li> <li>Know CoreDNS restart: <code>kubectl rollout restart deployment coredns -n kube-system</code></li> <li>Test DNS quickly: <code>kubectl exec -it &lt;pod&gt; -- nslookup kubernetes.default</code></li> </ol>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-coredns-service-discovery/#conclusion","title":"Conclusion","text":"<p>DNS and service discovery are the invisible glue that makes Kubernetes networking seamless. CoreDNS automatically creates DNS records for every Service and Pod, enabling applications to discover each other using simple, memorable names instead of ephemeral IP addresses. Understanding CoreDNS architecture, configuration, and troubleshooting is essential for both the CKA exam and production Kubernetes operations.</p> <p>Key Takeaways:</p> <p>\u2705 CoreDNS is Kubernetes' DNS server, running as a Deployment in kube-system with plugin-based configuration \u2705 Service DNS follows predictable patterns: <code>&lt;service&gt;.&lt;namespace&gt;.svc.&lt;cluster.local&gt;</code> \u2705 Headless services return pod IPs instead of ClusterIP, enabling direct pod-to-pod communication \u2705 Search domains enable short names within namespaces, but can impact external query performance \u2705 Corefile configuration defines DNS behavior through plugin chains (kubernetes, forward, cache, etc.) \u2705 Systematic troubleshooting follows the DNS resolution flow: pod \u2192 CoreDNS \u2192 API \u2192 endpoints \u2705 NodeLocal DNSCache (2025 best practice) dramatically improves DNS performance and reliability \u2705 CKA exam success requires fast DNS testing skills and confident troubleshooting workflows</p> <p>Practice Exercises:</p> <ol> <li>Create a ClusterIP service and test DNS resolution from different namespaces</li> <li>Deploy a headless service with StatefulSet and verify individual pod DNS names</li> <li>Modify Corefile to use custom upstream DNS servers (1.1.1.1, 8.8.8.8)</li> <li>Troubleshoot a broken DNS scenario (intentionally delete CoreDNS pods)</li> <li>Configure custom DNS policy with reduced ndots value</li> <li>Deploy NodeLocal DNSCache and verify improved query latency</li> <li>Use dig/nslookup to inspect SRV records for service port discovery</li> </ol> <p>Further Learning:</p> <ul> <li>CoreDNS Official Documentation</li> <li>Kubernetes DNS Specification</li> <li>NodeLocal DNSCache Guide</li> <li>DNS Performance Best Practices</li> </ul> <p>Master CoreDNS and service discovery, and you'll have the confidence to troubleshoot any DNS issue in your CKA exam or production environment. DNS is fundamental\u2014get it right, and everything else becomes easier. Now go practice with real clusters and build that muscle memory!</p>","tags":["kubernetes","k8s","cka-prep","coredns","dns","service-discovery","networking"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/","title":"Deployments, ReplicaSets, and Rolling Updates","text":"<p>Master Kubernetes workload controllers for production-grade application management. Learn deployment strategies, rolling updates, rollback procedures, and the complete controller hierarchy essential for CKA exam success.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#introduction","title":"Introduction","text":"<p>Deployments are the cornerstone of stateless application management in Kubernetes. While you can create individual pods, deployments provide self-healing, scaling, rolling updates, and declarative version management - capabilities essential for production environments.</p> <p>CKA Exam Domain: Workloads &amp; Scheduling (15% of exam)</p> <p>Why Deployments Matter for CKA: - Exam Frequency: Deployment tasks appear in nearly every CKA exam - Time Efficiency: Imperative deployment commands save critical exam minutes - Practical Relevance: 90% of production Kubernetes workloads use deployments - Foundation Knowledge: Understanding deployments builds intuition for other controllers</p> <p>Real-World Context: According to the 2024 CNCF survey, deployments manage over 80% of stateless workloads in production clusters. In the CKA exam, you'll create deployments, scale them, update images, rollback failed updates, and troubleshoot deployment issues - often under time pressure.</p> <p>What You'll Learn: - Deployment fundamentals and the controller hierarchy - ReplicaSet mechanics and pod lifecycle management - Rolling update strategies with surge and unavailability controls - Rollout management, history tracking, and rollback procedures - Other workload controllers: DaemonSets, StatefulSets, Jobs, CronJobs - CKA-specific commands, shortcuts, and time-saving patterns - Hands-on exercises replicating exam scenarios</p> <p>Prerequisites: Basic understanding of pods, YAML syntax, and kubectl commands. Review the \"Kubernetes Objects &amp; YAML\" post if needed.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#deployment-fundamentals","title":"Deployment Fundamentals","text":"","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#what-are-deployments","title":"What Are Deployments?","text":"<p>A Deployment is a Kubernetes controller that manages stateless applications through declarative configuration. Instead of manually creating and managing pods, you declare the desired state (image, replicas, update strategy), and the deployment controller continuously works to maintain that state.</p> <p>Core Benefits: - Self-Healing: Automatically replaces failed pods - Scaling: Horizontal scaling with a single command - Rolling Updates: Zero-downtime application updates - Rollback: Quick reversion to previous versions - Version History: Track and manage deployment revisions - Declarative Management: Infrastructure-as-code compatibility</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#the-controller-hierarchy","title":"The Controller Hierarchy","text":"<p>Deployments don't manage pods directly. They use a three-tier hierarchy:</p> <pre><code>graph TB\n    subgraph \"User Layer\"\n        USER[kubectl apply deployment.yaml]\n    end\n\n    subgraph \"Controller Layer\"\n        DEPLOY[Deployment Controller&lt;br/&gt;Manages: ReplicaSets]\n        RS[ReplicaSet Controller&lt;br/&gt;Manages: Pods]\n    end\n\n    subgraph \"Workload Layer\"\n        POD1[Pod 1&lt;br/&gt;app:nginx v1.21]\n        POD2[Pod 2&lt;br/&gt;app:nginx v1.21]\n        POD3[Pod 3&lt;br/&gt;app:nginx v1.21]\n    end\n\n    USER --&gt; DEPLOY\n    DEPLOY --&gt;|Creates/Updates| RS\n    RS --&gt;|Creates| POD1\n    RS --&gt;|Creates| POD2\n    RS --&gt;|Creates| POD3\n\n    style DEPLOY fill:#e1f5ff\n    style RS fill:#e8f5e8\n    style POD1 fill:#fff4e1\n    style POD2 fill:#fff4e1\n    style POD3 fill:#fff4e1</code></pre> <p>Hierarchy Responsibilities:</p> <ol> <li>Deployment: Manages application versions and update strategies</li> <li>Creates new ReplicaSets for updates</li> <li>Scales old ReplicaSets down during rollouts</li> <li>Maintains revision history</li> <li> <p>Handles rollback operations</p> </li> <li> <p>ReplicaSet: Ensures pod replica count matches desired state</p> </li> <li>Creates/deletes pods to match replica count</li> <li>Monitors pod health</li> <li> <p>Adopts orphaned pods matching its selector</p> </li> <li> <p>Pods: Run the actual application containers</p> </li> <li>Execute application workloads</li> <li>Report health status</li> <li>Managed entirely by their parent ReplicaSet</li> </ol>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#desired-state-reconciliation","title":"Desired State Reconciliation","text":"<p>Kubernetes operates on a reconciliation loop - the controller constantly compares desired state (your specification) with actual state (cluster reality) and takes corrective action.</p> <p>Reconciliation Example: <pre><code>Desired State: 3 nginx:1.21 pods\nActual State:  2 nginx:1.21 pods (1 pod crashed)\nAction:        Create 1 new pod \u2192 Desired = Actual\n</code></pre></p> <p>This self-healing behavior happens automatically without human intervention.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#basic-deployment-yaml-structure","title":"Basic Deployment YAML Structure","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3                    # Desired pod count\n  selector:\n    matchLabels:\n      app: nginx                 # Must match template labels\n  template:                      # Pod template\n    metadata:\n      labels:\n        app: nginx               # Pod labels\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\n</code></pre> <p>Critical YAML Elements:</p> <ul> <li><code>apiVersion: apps/v1</code>: Deployment API group (v1 is stable)</li> <li><code>kind: Deployment</code>: Resource type</li> <li><code>replicas</code>: Number of pod copies to maintain</li> <li><code>selector.matchLabels</code>: How deployment finds its pods</li> <li><code>template</code>: Pod specification (same as standalone pod YAML)</li> <li>Label Matching Rule: <code>selector.matchLabels</code> must match <code>template.metadata.labels</code></li> </ul> <p>CKA Tip: The label matching rule is strictly enforced. Mismatched labels cause <code>error: selector does not match template labels</code> - a common exam mistake.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#replicasets-deep-dive","title":"ReplicaSets Deep Dive","text":"","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#what-replicasets-do","title":"What ReplicaSets Do","text":"<p>A ReplicaSet is a controller that maintains a stable set of replica pods at any given time. It's primarily used by deployments but can be created independently (though this is discouraged).</p> <p>ReplicaSet Controller Responsibilities: 1. Count Maintenance: Ensure replica count matches <code>spec.replicas</code> 2. Pod Creation: Create pods from the template when count is low 3. Pod Deletion: Remove excess pods when count is high 4. Health Monitoring: Watch for pod failures and replace them 5. Label Selection: Find and manage pods matching the selector 6. Ownership: Set controller references on managed pods</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#pod-template-specifications","title":"Pod Template Specifications","text":"<p>The ReplicaSet's pod template defines the blueprint for all created pods:</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: nginx-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: nginx\n        tier: frontend\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 80\n</code></pre> <p>Template Components: - metadata.labels: Must match selector (required) - spec.containers: Container specifications - spec.volumes: Shared storage (if needed) - spec.nodeSelector: Node placement constraints - spec.affinity: Advanced scheduling rules</p> <p>CKA Insight: ReplicaSet templates are immutable - changes don't affect existing pods. You must delete old pods to get new configuration, which is why deployments handle updates instead.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#label-selectors-and-matching","title":"Label Selectors and Matching","text":"<p>Label selectors determine which pods a ReplicaSet manages:</p> <pre><code># Equality-based selector\nselector:\n  matchLabels:\n    app: nginx\n    environment: production\n\n# Set-based selector (more flexible)\nselector:\n  matchExpressions:\n  - key: app\n    operator: In\n    values: [nginx, apache]\n  - key: tier\n    operator: NotIn\n    values: [cache]\n  - key: environment\n    operator: Exists\n</code></pre> <p>Selector Operators: - <code>In</code>: Label value in specified set - <code>NotIn</code>: Label value not in specified set - <code>Exists</code>: Label key exists (value irrelevant) - <code>DoesNotExist</code>: Label key absent</p> <p>Selection Logic: <pre><code># ReplicaSet finds pods using label query\nkubectl get pods -l app=nginx,environment=production\n\n# Must return exactly 'replicas' count of pods\n# If less: Create more pods\n# If more: Delete excess pods\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#scale-operations","title":"Scale Operations","text":"<p>Scaling changes replica count imperatively or declaratively:</p> <pre><code># Imperative scaling (fast for exam)\nkubectl scale deployment nginx-deployment --replicas=5\n\n# Declarative scaling\nkubectl edit deployment nginx-deployment  # Change spec.replicas\nkubectl apply -f deployment.yaml          # Updated replicas in file\n\n# Autoscaling (HorizontalPodAutoscaler)\nkubectl autoscale deployment nginx-deployment --min=3 --max=10 --cpu-percent=80\n</code></pre> <p>Scaling Behavior: - Scale Up: ReplicaSet creates (replicas - current) new pods - Scale Down: ReplicaSet deletes (current - replicas) pods (newest first) - Immediate: Scaling is instant (no gradual rollout)</p> <p>CKA Time-Saver: Use <code>kubectl scale</code> for quick replica changes. Use <code>kubectl autoscale</code> only when explicitly required.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#orphaned-pods-and-adoption","title":"Orphaned Pods and Adoption","text":"<p>ReplicaSets adopt pods that match their selector, even if not originally created by that ReplicaSet:</p> <pre><code># Create standalone pod with matching labels\nkubectl run orphan --image=nginx --labels=app=nginx\n\n# Create ReplicaSet with same selector\nkubectl apply -f replicaset.yaml  # selector: app=nginx, replicas: 3\n\n# ReplicaSet adopts orphan pod\n# Only creates 2 new pods (3 - 1 existing = 2)\n</code></pre> <p>Adoption Mechanics: 1. ReplicaSet queries API for pods matching selector 2. Counts matching pods (including orphans) 3. Creates/deletes pods to match desired count 4. Sets <code>ownerReferences</code> on adopted pods</p> <p>CKA Warning: Be careful with label overlap. Pods can be unexpectedly adopted or deleted if selectors overlap.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#deployment-strategies","title":"Deployment Strategies","text":"<p>Kubernetes supports multiple deployment strategies for different use cases.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#rolling-updates-default-strategy","title":"Rolling Updates (Default Strategy)","text":"<p>Rolling updates gradually replace old pods with new ones, maintaining availability:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 10\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2              # Max pods above desired count\n      maxUnavailable: 1        # Max pods below desired count\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n</code></pre> <p>Rolling Update Parameters:</p> <ul> <li><code>maxSurge</code>: Maximum pods above <code>replicas</code> during update</li> <li>Absolute number: <code>maxSurge: 2</code> \u2192 10 + 2 = 12 max pods</li> <li>Percentage: <code>maxSurge: 25%</code> \u2192 10 + 2.5 = 12 max pods (rounds up)</li> <li> <p>Default: <code>25%</code></p> </li> <li> <p><code>maxUnavailable</code>: Maximum pods below <code>replicas</code> during update</p> </li> <li>Absolute number: <code>maxUnavailable: 1</code> \u2192 10 - 1 = 9 min available</li> <li>Percentage: <code>maxUnavailable: 10%</code> \u2192 10 - 1 = 9 min available</li> <li>Default: <code>25%</code></li> </ul> <p>Update Process Visualization:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Deployment\n    participant NewRS as New ReplicaSet\n    participant OldRS as Old ReplicaSet\n    participant Pods\n\n    User-&gt;&gt;Deployment: Update image: nginx:1.22\n    Deployment-&gt;&gt;NewRS: Create ReplicaSet (nginx:1.22, replicas=0)\n    Deployment-&gt;&gt;NewRS: Scale up by maxSurge (0 \u2192 2)\n    NewRS-&gt;&gt;Pods: Create 2 new pods\n\n    Note over Pods: Wait for new pods Ready\n\n    Deployment-&gt;&gt;OldRS: Scale down by maxUnavailable (10 \u2192 9)\n    OldRS-&gt;&gt;Pods: Delete 1 old pod\n\n    Deployment-&gt;&gt;NewRS: Scale up (2 \u2192 4)\n    NewRS-&gt;&gt;Pods: Create 2 new pods\n\n    Note over Pods: Repeat until complete\n\n    Deployment-&gt;&gt;OldRS: Scale down (1 \u2192 0)\n    Deployment-&gt;&gt;NewRS: Scale up (9 \u2192 10)\n\n    Note over Deployment: Update complete</code></pre> <p>Rolling Update Calculation: <pre><code>Given: replicas=10, maxSurge=2, maxUnavailable=1\n\nMax pods during update: 10 + 2 = 12\nMin available pods:     10 - 1 = 9\n\nUpdate proceeds in waves:\nWave 1: Create 2 new (total: 12), delete 1 old (available: 11)\nWave 2: Create 2 new (total: 13), delete 1 old (available: 11)\n...continue until all old pods replaced\n</code></pre></p> <p>CKA Strategy: Default rolling update works well for most exam scenarios. Only modify if explicitly required.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#recreate-strategy","title":"Recreate Strategy","text":"<p>Recreate terminates all old pods before creating new ones - causes downtime:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 10\n  strategy:\n    type: Recreate         # All old pods deleted first\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.22\n</code></pre> <p>Recreate Process: 1. Scale old ReplicaSet to 0 (delete all pods) 2. Wait for all old pods to terminate 3. Create new ReplicaSet with full replica count 4. Wait for all new pods to become ready</p> <p>Use Cases: - Stateful applications requiring clean shutdown - Database migrations needing downtime - Incompatible versions that can't coexist - Resource constraints preventing extra pods during update</p> <p>Downtime: Complete unavailability from step 1 to step 4 completion.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#bluegreen-deployments","title":"Blue/Green Deployments","text":"<p>Blue/Green maintains two complete environments and switches traffic atomically:</p> <pre><code># Blue deployment (current production)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-blue\n  labels:\n    version: blue\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: nginx\n      version: blue\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: blue\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n\n---\n# Green deployment (new version)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-green\n  labels:\n    version: green\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: nginx\n      version: green\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: green\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.22\n\n---\n# Service (traffic routing)\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx\n    version: blue      # Switch to 'green' for cutover\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <p>Blue/Green Process: 1. Deploy green environment alongside blue 2. Test green environment thoroughly 3. Switch service selector from <code>version: blue</code> to <code>version: green</code> 4. Monitor for issues 5. Keep blue environment running for quick rollback 6. Delete blue environment after validation period</p> <p>Advantages: - Instant Rollback: Change service selector back to blue - Zero Downtime: Both environments ready before cutover - Full Testing: Test production-like environment before cutover</p> <p>Disadvantages: - Resource Cost: 2x resources during transition - Database Complexity: Schema migrations require careful coordination</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#canary-deployments","title":"Canary Deployments","text":"<p>Canary deployments route small percentage of traffic to new version:</p> <pre><code># Stable deployment (90% traffic)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-stable\nspec:\n  replicas: 9\n  selector:\n    matchLabels:\n      app: nginx\n      track: stable\n  template:\n    metadata:\n      labels:\n        app: nginx\n        track: stable\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n\n---\n# Canary deployment (10% traffic)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-canary\nspec:\n  replicas: 1           # 1 of 10 total = 10% traffic\n  selector:\n    matchLabels:\n      app: nginx\n      track: canary\n  template:\n    metadata:\n      labels:\n        app: nginx\n        track: canary\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.22\n\n---\n# Service (routes to both)\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx          # Matches both stable and canary\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <p>Canary Process: 1. Deploy canary with small replica count (10%) 2. Monitor error rates, latency, user feedback 3. Gradually increase canary replicas (10% \u2192 25% \u2192 50%) 4. Decrease stable replicas correspondingly 5. Once validated, promote canary to 100% 6. Delete old stable deployment</p> <p>Traffic Distribution: <pre><code>Total pods: 10\nStable: 9 pods (90% traffic)\nCanary: 1 pod  (10% traffic)\n\n\u2192 Gradually shift:\nStable: 5 pods (50%), Canary: 5 pods (50%)\n\u2192 Finally:\nStable: 0 pods (0%),  Canary: 10 pods (100%)\n</code></pre></p> <p>CKA Note: Canary deployments rarely appear in CKA exam. Understand the concept but prioritize rolling updates and rollbacks.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#strategy-comparison","title":"Strategy Comparison","text":"Strategy Downtime Resource Cost Rollback Speed Complexity Use Case Rolling Update None Low (slight surge) Medium (gradual) Low Default, stateless apps Recreate Yes (full) None Medium (redeploy) Low Stateful apps, migrations Blue/Green None High (2x) Instant Medium Critical apps, easy rollback Canary None Medium Medium High Risk mitigation, gradual validation <p>CKA Exam Strategy: - 90% of exam tasks: Use default rolling update - Explicit downtime acceptable: Use recreate - \"Zero downtime\" emphasized: Verify rolling update configuration - \"Gradual rollout\" mentioned: Consider canary approach (but likely rolling update)</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#rollout-management","title":"Rollout Management","text":"<p>Kubernetes tracks deployment history and provides rollout control commands.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#kubectl-rollout-commands","title":"kubectl rollout Commands","text":"<p>Check Rollout Status: <pre><code># Watch rollout progress\nkubectl rollout status deployment/nginx-deployment\n\n# Output examples:\n# \"Waiting for deployment 'nginx-deployment' rollout to finish: 2 of 3 updated replicas...\"\n# \"deployment 'nginx-deployment' successfully rolled out\"\n</code></pre></p> <p>View Rollout History: <pre><code># List all revisions\nkubectl rollout history deployment/nginx-deployment\n\n# Output:\n# REVISION  CHANGE-CAUSE\n# 1         &lt;none&gt;\n# 2         kubectl set image deployment/nginx-deployment nginx=nginx:1.22\n# 3         kubectl set image deployment/nginx-deployment nginx=nginx:1.23\n\n# View specific revision details\nkubectl rollout history deployment/nginx-deployment --revision=2\n</code></pre></p> <p>Pause Rollout: <pre><code># Pause ongoing rollout\nkubectl rollout pause deployment/nginx-deployment\n\n# Make multiple changes while paused\nkubectl set image deployment/nginx-deployment nginx=nginx:1.24\nkubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=500m\n\n# Resume rollout (batches all changes into one revision)\nkubectl rollout resume deployment/nginx-deployment\n</code></pre></p> <p>Restart Rollout: <pre><code># Restart all pods (useful for config updates)\nkubectl rollout restart deployment/nginx-deployment\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#revision-history-tracking","title":"Revision History Tracking","text":"<p>Kubernetes stores old ReplicaSets for rollback purposes:</p> <pre><code># Default revision history limit\nkubectl get deployment nginx-deployment -o yaml | grep revisionHistoryLimit\n# spec.revisionHistoryLimit: 10 (default)\n\n# View all ReplicaSets (including old revisions)\nkubectl get replicasets\n\n# Output:\n# NAME                         DESIRED   CURRENT   READY   AGE\n# nginx-deployment-7d9c8f8d5   3         3         3       5m    # Current revision\n# nginx-deployment-6b8c7d6c4   0         0         0       10m   # Old revision\n# nginx-deployment-5a7b6c5b3   0         0         0       15m   # Old revision\n</code></pre> <p>Old ReplicaSets: - Scaled to 0 replicas (no pods running) - Retain pod template for rollback - Count limited by <code>revisionHistoryLimit</code> - Oldest deleted when limit exceeded</p> <p>Change Revision Limit: <pre><code>spec:\n  revisionHistoryLimit: 5  # Keep only 5 old ReplicaSets\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#rollback-procedures","title":"Rollback Procedures","text":"<p>Rollback to Previous Revision: <pre><code># Undo last rollout (rollback to revision N-1)\nkubectl rollout undo deployment/nginx-deployment\n\n# Process:\n# 1. Find previous ReplicaSet\n# 2. Scale previous ReplicaSet up\n# 3. Scale current ReplicaSet down\n# 4. Creates new revision number\n</code></pre></p> <p>Rollback to Specific Revision: <pre><code># Rollback to revision 2\nkubectl rollout undo deployment/nginx-deployment --to-revision=2\n\n# Verify rollback\nkubectl rollout status deployment/nginx-deployment\nkubectl describe deployment nginx-deployment | grep Image\n</code></pre></p> <p>Rollback Visualization:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Deployment\n    participant RevN as Revision N (current)\n    participant RevN1 as Revision N-1 (previous)\n\n    User-&gt;&gt;Deployment: kubectl rollout undo\n    Deployment-&gt;&gt;Deployment: Find previous revision (N-1)\n    Deployment-&gt;&gt;RevN1: Scale up (0 \u2192 3)\n\n    Note over RevN1: Create pods from old template\n\n    Deployment-&gt;&gt;RevN: Scale down (3 \u2192 2)\n    Deployment-&gt;&gt;RevN: Scale down (2 \u2192 1)\n    Deployment-&gt;&gt;RevN: Scale down (1 \u2192 0)\n\n    Note over Deployment: Rollback complete&lt;br/&gt;Creates new revision N+1\n\n    Deployment--&gt;&gt;User: Successfully rolled back</code></pre> <p>CKA Rollback Pattern: 1. Verify issue: <code>kubectl rollout status</code> shows failure 2. Check history: <code>kubectl rollout history</code> to see revisions 3. Rollback: <code>kubectl rollout undo</code> (or <code>--to-revision=N</code>) 4. Verify fix: <code>kubectl rollout status</code> confirms success 5. Validate: <code>kubectl get pods</code> shows healthy pods</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#pauseresume-rollouts","title":"Pause/Resume Rollouts","text":"<p>Pausing allows batching multiple changes into single revision:</p> <pre><code># Start with deployment\nkubectl create deployment nginx --image=nginx:1.21 --replicas=3\n\n# Pause rollout\nkubectl rollout pause deployment/nginx\n\n# Make multiple changes (no rollout triggered)\nkubectl set image deployment/nginx nginx=nginx:1.22\nkubectl set resources deployment/nginx -c=nginx --requests=cpu=100m,memory=64Mi\nkubectl set resources deployment/nginx -c=nginx --limits=cpu=200m,memory=128Mi\n\n# Resume rollout (all changes applied together)\nkubectl rollout resume deployment/nginx\n</code></pre> <p>Pause Benefits: - Atomic Changes: Multiple updates in single revision - Testing: Verify manifest changes before rollout - Coordination: Synchronize with other system changes</p> <p>CKA Usage: Rarely used in exam but valuable for complex updates.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#change-cause-annotations","title":"Change-Cause Annotations","text":"<p>Track why deployments were updated using annotations:</p> <pre><code># Annotate during update\nkubectl set image deployment/nginx nginx=nginx:1.22 \\\n  --record  # Deprecated but useful for history\n\n# Better approach: annotate explicitly\nkubectl annotate deployment/nginx kubernetes.io/change-cause=\"Update to nginx 1.22 for security patch\"\n\n# View in history\nkubectl rollout history deployment/nginx\n\n# Output:\n# REVISION  CHANGE-CAUSE\n# 1         &lt;none&gt;\n# 2         Update to nginx 1.22 for security patch\n</code></pre> <p>CKA Tip: <code>--record</code> flag is deprecated. Use explicit annotations for change tracking in production scenarios.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#other-workload-controllers","title":"Other Workload Controllers","text":"<p>Beyond deployments, Kubernetes provides specialized controllers for different workload patterns.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#daemonsets-one-pod-per-node","title":"DaemonSets (One Pod Per Node)","text":"<p>DaemonSets ensure exactly one pod runs on each node (or subset of nodes):</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: prom/node-exporter:latest\n        ports:\n        - containerPort: 9100\n      hostNetwork: true        # Access node metrics\n      hostPID: true            # Access node processes\n</code></pre> <p>DaemonSet Behavior: - Node Addition: Automatically create pod on new nodes - Node Removal: Delete pod when node deleted - Node Selector: Use <code>nodeSelector</code> or <code>affinity</code> to target node subset</p> <p>Common Use Cases: - Monitoring Agents: Prometheus node-exporter, Datadog agent - Log Collectors: Fluentd, Logstash - Network Plugins: CNI agents, kube-proxy - Storage Plugins: CSI drivers - Security Agents: Falco, Sysdig</p> <p>DaemonSet vs Deployment: <pre><code># DaemonSet\nkubectl get daemonset node-exporter\n# DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE\n# 5         5         5       5            5\n# (Matches node count)\n\n# Deployment\nkubectl get deployment nginx\n# READY   UP-TO-DATE   AVAILABLE\n# 3/3     3            3\n# (Explicitly set replicas)\n</code></pre></p> <p>CKA Commands: <pre><code># Create DaemonSet\nkubectl create -f daemonset.yaml\n\n# Update DaemonSet image\nkubectl set image daemonset/node-exporter node-exporter=prom/node-exporter:v1.3.0\n\n# Check DaemonSet rollout\nkubectl rollout status daemonset/node-exporter\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#statefulsets-ordered-stable-workloads","title":"StatefulSets (Ordered, Stable Workloads)","text":"<p>StatefulSets manage stateful applications requiring stable network identities and persistent storage:</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: mysql       # Headless service name\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        ports:\n        - containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n  volumeClaimTemplates:    # Creates PVC per pod\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre> <p>StatefulSet Guarantees:</p> <ol> <li>Stable Network Identity: Predictable pod names</li> <li><code>mysql-0</code>, <code>mysql-1</code>, <code>mysql-2</code> (not random hashes)</li> <li> <p>DNS: <code>mysql-0.mysql.default.svc.cluster.local</code></p> </li> <li> <p>Ordered Deployment: Sequential pod creation</p> </li> <li> <p>Creates <code>mysql-0</code>, waits for Ready, then <code>mysql-1</code>, etc.</p> </li> <li> <p>Ordered Termination: Reverse sequential deletion</p> </li> <li> <p>Deletes <code>mysql-2</code>, then <code>mysql-1</code>, then <code>mysql-0</code></p> </li> <li> <p>Stable Storage: PersistentVolumeClaims bound to pods</p> </li> <li><code>data-mysql-0</code> always attached to <code>mysql-0</code></li> <li>PVCs persist even if pod deleted</li> </ol> <p>StatefulSet vs Deployment:</p> <pre><code>graph TB\n    subgraph \"Deployment (Stateless)\"\n        D[Deployment]\n        DP1[nginx-7d9c8-abc12&lt;br/&gt;Random Name]\n        DP2[nginx-7d9c8-def34&lt;br/&gt;Random Name]\n        DP3[nginx-7d9c8-ghi56&lt;br/&gt;Random Name]\n\n        D --&gt; DP1\n        D --&gt; DP2\n        D --&gt; DP3\n    end\n\n    subgraph \"StatefulSet (Stateful)\"\n        S[StatefulSet]\n        SP1[mysql-0&lt;br/&gt;Stable Name&lt;br/&gt;PVC: data-mysql-0]\n        SP2[mysql-1&lt;br/&gt;Stable Name&lt;br/&gt;PVC: data-mysql-1]\n        SP3[mysql-2&lt;br/&gt;Stable Name&lt;br/&gt;PVC: data-mysql-2]\n\n        S --&gt; SP1\n        SP1 --&gt; SP2\n        SP2 --&gt; SP3\n    end\n\n    style DP1 fill:#fff4e1\n    style DP2 fill:#fff4e1\n    style DP3 fill:#fff4e1\n    style SP1 fill:#e1f5ff\n    style SP2 fill:#e8f5e8\n    style SP3 fill:#f5e1ff</code></pre> <p>Common Use Cases: - Databases: MySQL, PostgreSQL, MongoDB - Distributed Systems: Kafka, Zookeeper, Elasticsearch - Stateful Applications: Requires persistent identity or storage</p> <p>CKA Commands: <pre><code># Create StatefulSet\nkubectl apply -f statefulset.yaml\n\n# Scale StatefulSet\nkubectl scale statefulset mysql --replicas=5\n\n# Delete StatefulSet (keeps PVCs)\nkubectl delete statefulset mysql\n\n# Delete StatefulSet and PVCs\nkubectl delete statefulset mysql\nkubectl delete pvc -l app=mysql\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#jobs-run-to-completion","title":"Jobs (Run-to-Completion)","text":"<p>Jobs run pods to completion (successful termination) rather than continuously:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: database-backup\nspec:\n  template:\n    spec:\n      containers:\n      - name: backup\n        image: mysql:8.0\n        command:\n        - /bin/bash\n        - -c\n        - mysqldump -h mysql -u root -p$MYSQL_ROOT_PASSWORD mydb &gt; /backup/dump.sql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: password\n      restartPolicy: OnFailure  # Required for Jobs\n  backoffLimit: 4              # Retry failed jobs up to 4 times\n  completions: 1               # Run 1 pod successfully\n  parallelism: 1               # Run 1 pod at a time\n</code></pre> <p>Job Parameters:</p> <ul> <li><code>completions</code>: Number of successful pod completions required</li> <li> <p><code>completions: 3</code> \u2192 Run until 3 pods succeed</p> </li> <li> <p><code>parallelism</code>: Number of pods running simultaneously</p> </li> <li> <p><code>parallelism: 2</code> \u2192 Run 2 pods at once</p> </li> <li> <p><code>backoffLimit</code>: Number of retries before marking job failed</p> </li> <li> <p>Default: 6 retries</p> </li> <li> <p><code>activeDeadlineSeconds</code>: Maximum job runtime</p> </li> <li>Job fails if exceeds deadline</li> </ul> <p>Job Patterns:</p> <pre><code># Single Job (run once)\ncompletions: 1\nparallelism: 1\n\n# Parallel Job (run N times in parallel)\ncompletions: 10\nparallelism: 5      # 5 pods at a time until 10 complete\n\n# Work Queue Job (process queue until empty)\ncompletions: null   # No fixed count\nparallelism: 5      # 5 workers processing queue\n</code></pre> <p>CKA Commands: <pre><code># Create job\nkubectl create job backup --image=mysql:8.0 -- mysqldump ...\n\n# View job status\nkubectl get jobs\n\n# View job pods\nkubectl get pods --selector=job-name=backup\n\n# View job logs\nkubectl logs job/backup\n\n# Delete completed jobs\nkubectl delete job backup\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#cronjobs-scheduled-jobs","title":"CronJobs (Scheduled Jobs)","text":"<p>CronJobs create Jobs on a schedule:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: database-backup\nspec:\n  schedule: \"0 2 * * *\"        # 2am daily (cron syntax)\n  jobTemplate:                 # Job spec (same as Job)\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: mysql:8.0\n            command:\n            - /bin/bash\n            - -c\n            - mysqldump -h mysql -u root -p$MYSQL_ROOT_PASSWORD mydb &gt; /backup/dump-$(date +%Y%m%d).sql\n          restartPolicy: OnFailure\n  successfulJobsHistoryLimit: 3  # Keep last 3 successful jobs\n  failedJobsHistoryLimit: 1      # Keep last 1 failed job\n  concurrencyPolicy: Forbid      # Don't run if previous job still running\n</code></pre> <p>Cron Schedule Syntax: <pre><code># \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n# \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n# \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31)\n# \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n# \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of week (0 - 6) (Sunday to Saturday)\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# * * * * *\n\nExamples:\n\"0 2 * * *\"       # 2am daily\n\"*/5 * * * *\"     # Every 5 minutes\n\"0 */2 * * *\"     # Every 2 hours\n\"0 9 * * 1-5\"     # 9am weekdays\n\"0 0 1 * *\"       # 1st of month\n</code></pre></p> <p>Concurrency Policies: - <code>Allow</code> (default): Allow concurrent jobs - <code>Forbid</code>: Skip new job if previous still running - <code>Replace</code>: Cancel previous job, start new one</p> <p>CKA Commands: <pre><code># Create CronJob\nkubectl create cronjob backup --image=mysql:8.0 --schedule=\"0 2 * * *\" -- mysqldump ...\n\n# View CronJobs\nkubectl get cronjobs\n\n# Manually trigger CronJob (create job immediately)\nkubectl create job backup-manual --from=cronjob/backup\n\n# Suspend CronJob\nkubectl patch cronjob backup -p '{\"spec\":{\"suspend\":true}}'\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#controller-comparison","title":"Controller Comparison","text":"Controller Replicas Pod Names Restart Policy Storage Use Case Deployment Fixed count Random hash Always Ephemeral Stateless apps (web, API) ReplicaSet Fixed count Random hash Always Ephemeral Managed by Deployment DaemonSet 1 per node Random hash Always Ephemeral Node agents (monitoring, logging) StatefulSet Fixed count Ordered (mysql-0) Always Persistent Databases, distributed systems Job Completions Random hash OnFailure/Never Ephemeral Batch processing, migrations CronJob Scheduled Random hash OnFailure/Never Ephemeral Scheduled tasks (backups, reports) <p>CKA Controller Selection: - Web application: Deployment - Database cluster: StatefulSet - Log aggregator on each node: DaemonSet - Database migration: Job - Daily backup: CronJob</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#cka-exam-skills","title":"CKA Exam Skills","text":"","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#imperative-deployment-creation","title":"Imperative Deployment Creation","text":"<p>Create Deployment Quickly: <pre><code># Basic deployment\nkubectl create deployment nginx --image=nginx:1.21\n\n# Deployment with replicas\nkubectl create deployment nginx --image=nginx:1.21 --replicas=3\n\n# Generate YAML without creating\nkubectl create deployment nginx --image=nginx:1.21 --dry-run=client -o yaml &gt; deployment.yaml\n\n# Create and expose\nkubectl create deployment nginx --image=nginx:1.21 --replicas=3\nkubectl expose deployment nginx --port=80 --target-port=80\n</code></pre></p> <p>CKA Time-Saver: Use <code>kubectl create deployment</code> for basic deployments. Only write YAML for complex configurations.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#scaling-commands","title":"Scaling Commands","text":"<pre><code># Scale deployment\nkubectl scale deployment nginx --replicas=5\n\n# Scale multiple deployments\nkubectl scale deployment nginx frontend backend --replicas=3\n\n# Conditional scaling\nkubectl scale deployment nginx --current-replicas=3 --replicas=5\n\n# Autoscale (HPA)\nkubectl autoscale deployment nginx --min=3 --max=10 --cpu-percent=80\n</code></pre> <p>Verify Scaling: <pre><code>kubectl get deployment nginx\nkubectl get replicaset\nkubectl get pods -l app=nginx\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#image-updates","title":"Image Updates","text":"<pre><code># Update deployment image\nkubectl set image deployment/nginx nginx=nginx:1.22\n\n# Update multiple containers\nkubectl set image deployment/nginx nginx=nginx:1.22 sidecar=sidecar:2.0\n\n# Verify update\nkubectl rollout status deployment/nginx\nkubectl describe deployment nginx | grep Image\n</code></pre> <p>CKA Pattern: <pre><code># Update image and watch rollout\nkubectl set image deployment/nginx nginx=nginx:1.22 &amp;&amp; \\\nkubectl rollout status deployment/nginx\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#rollback-procedures_1","title":"Rollback Procedures","text":"<pre><code># View rollout history\nkubectl rollout history deployment/nginx\n\n# Rollback to previous version\nkubectl rollout undo deployment/nginx\n\n# Rollback to specific revision\nkubectl rollout undo deployment/nginx --to-revision=2\n\n# Verify rollback\nkubectl rollout status deployment/nginx\nkubectl get pods\n</code></pre> <p>CKA Rollback Scenario: <pre><code># Problem: Deployment stuck in rollout\nkubectl rollout status deployment/nginx\n# \"Waiting for rollout to finish: 1 of 3 updated replicas...\"\n\n# Investigate\nkubectl get pods\n# nginx-deployment-7d9c8f8d5-xyz12   0/1     ImagePullBackOff\n\n# Rollback immediately\nkubectl rollout undo deployment/nginx\n\n# Verify fix\nkubectl get pods\n# All pods Running\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#troubleshooting-deployments","title":"Troubleshooting Deployments","text":"<p>Common Issues and Solutions:</p> <ol> <li> <p>ImagePullBackOff: <pre><code># Symptom\nkubectl get pods\n# nginx-7d9c8-abc12   0/1   ImagePullBackOff\n\n# Diagnose\nkubectl describe pod nginx-7d9c8-abc12\n# Events: Failed to pull image \"nginx:typo\"\n\n# Fix\nkubectl set image deployment/nginx nginx=nginx:1.22\n</code></pre></p> </li> <li> <p>CrashLoopBackOff: <pre><code># Symptom\nkubectl get pods\n# nginx-7d9c8-abc12   0/1   CrashLoopBackOff\n\n# Diagnose\nkubectl logs nginx-7d9c8-abc12\n# Error: Configuration file not found\n\n# Fix (edit config)\nkubectl edit deployment nginx\n</code></pre></p> </li> <li> <p>Insufficient Resources: <pre><code># Symptom\nkubectl get pods\n# nginx-7d9c8-abc12   0/1   Pending\n\n# Diagnose\nkubectl describe pod nginx-7d9c8-abc12\n# Events: 0/3 nodes are available: insufficient cpu\n\n# Fix (reduce resources or scale cluster)\nkubectl set resources deployment/nginx -c=nginx --requests=cpu=100m\n</code></pre></p> </li> <li> <p>Label Selector Mismatch: <pre><code># Symptom\nkubectl get deployment nginx\n# READY: 0/3\n\nkubectl get replicaset\n# DESIRED: 3, CURRENT: 0\n\n# Diagnose\nkubectl get deployment nginx -o yaml | grep -A5 selector\n# selector.matchLabels doesn't match template.metadata.labels\n\n# Fix\nkubectl edit deployment nginx  # Align selector and labels\n</code></pre></p> </li> </ol> <p>CKA Troubleshooting Workflow: 1. Check pod status: <code>kubectl get pods</code> 2. Describe pod: <code>kubectl describe pod &lt;name&gt;</code> 3. Check logs: <code>kubectl logs &lt;name&gt;</code> 4. Check events: <code>kubectl get events --sort-by=.metadata.creationTimestamp</code> 5. Fix and verify: Update deployment, watch rollout</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exam-time-savers","title":"Exam Time-Savers","text":"<p>Aliases (provided in exam environment): <pre><code>alias k=kubectl\nalias kgp='kubectl get pods'\nalias kgd='kubectl get deployment'\nalias kd='kubectl describe'\n</code></pre></p> <p>kubectl Shortcuts: <pre><code># Short resource names\nkubectl get deploy       # Instead of 'deployments'\nkubectl get rs           # Instead of 'replicasets'\nkubectl get po           # Instead of 'pods'\n\n# Wide output (more columns)\nkubectl get pods -o wide\n\n# Watch mode (auto-refresh)\nkubectl get pods -w\n\n# All namespaces\nkubectl get pods -A\n</code></pre></p> <p>YAML Generation: <pre><code># Generate deployment YAML\nkubectl create deployment nginx --image=nginx:1.21 --dry-run=client -o yaml\n\n# Generate service YAML\nkubectl expose deployment nginx --port=80 --dry-run=client -o yaml\n\n# Combine and apply\nkubectl create deployment nginx --image=nginx:1.21 --dry-run=client -o yaml | kubectl apply -f -\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-1-create-and-scale-deployment","title":"Exercise 1: Create and Scale Deployment","text":"<p>Scenario: Deploy nginx web application with 3 replicas.</p> <pre><code># Create deployment\nkubectl create deployment web --image=nginx:1.21 --replicas=3\n\n# Verify deployment\nkubectl get deployment web\nkubectl get pods -l app=web\n\n# Scale to 5 replicas\nkubectl scale deployment web --replicas=5\n\n# Verify scaling\nkubectl get pods -l app=web -w  # Watch pods being created\n</code></pre> <p>Expected Output: <pre><code>NAME   READY   UP-TO-DATE   AVAILABLE   AGE\nweb    5/5     5            5           2m\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-2-rolling-update-and-rollback","title":"Exercise 2: Rolling Update and Rollback","text":"<p>Scenario: Update nginx to version 1.22, then rollback due to issues.</p> <pre><code># Initial deployment\nkubectl create deployment nginx --image=nginx:1.21 --replicas=3\n\n# Update to 1.22\nkubectl set image deployment/nginx nginx=nginx:1.22\n\n# Watch rollout\nkubectl rollout status deployment/nginx\n\n# Check history\nkubectl rollout history deployment/nginx\n\n# Simulate issue - rollback\nkubectl rollout undo deployment/nginx\n\n# Verify rollback\nkubectl describe deployment nginx | grep Image:\n# Should show nginx:1.21\n</code></pre> <p>Expected History: <pre><code>REVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment/nginx nginx=nginx:1.22\n3         kubectl rollout undo deployment/nginx\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-3-deployment-with-resource-limits","title":"Exercise 3: Deployment with Resource Limits","text":"<p>Scenario: Create deployment with resource requests and limits.</p> <pre><code># Generate base YAML\nkubectl create deployment app --image=nginx:1.21 --replicas=2 --dry-run=client -o yaml &gt; app.yaml\n\n# Edit app.yaml to add resources\n</code></pre> <pre><code># app.yaml (add under containers)\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n</code></pre> <pre><code># Apply deployment\nkubectl apply -f app.yaml\n\n# Verify resources\nkubectl describe deployment app | grep -A5 Limits\n</code></pre> <p>Expected Output: <pre><code>    Limits:\n      cpu:     500m\n      memory:  128Mi\n    Requests:\n      cpu:        250m\n      memory:     64Mi\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-4-daemonset-creation","title":"Exercise 4: DaemonSet Creation","text":"<p>Scenario: Deploy monitoring agent to all nodes.</p> <pre><code># monitoring-agent.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-monitor\nspec:\n  selector:\n    matchLabels:\n      app: node-monitor\n  template:\n    metadata:\n      labels:\n        app: node-monitor\n    spec:\n      containers:\n      - name: monitor\n        image: prom/node-exporter:latest\n        ports:\n        - containerPort: 9100\n</code></pre> <pre><code># Create DaemonSet\nkubectl apply -f monitoring-agent.yaml\n\n# Verify one pod per node\nkubectl get daemonset node-monitor\nkubectl get pods -l app=node-monitor -o wide\n\n# Count should match node count\nkubectl get nodes --no-headers | wc -l\nkubectl get pods -l app=node-monitor --no-headers | wc -l\n</code></pre>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-5-statefulset-with-headless-service","title":"Exercise 5: StatefulSet with Headless Service","text":"<p>Scenario: Deploy a 3-replica MySQL StatefulSet.</p> <pre><code># mysql-statefulset.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  clusterIP: None           # Headless service\n  selector:\n    app: mysql\n  ports:\n  - port: 3306\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: mysql\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"password\"  # Use secrets in production!\n        ports:\n        - containerPort: 3306\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre> <pre><code># Create StatefulSet\nkubectl apply -f mysql-statefulset.yaml\n\n# Watch ordered pod creation\nkubectl get pods -l app=mysql -w\n\n# Verify stable names\nkubectl get pods -l app=mysql\n# mysql-0, mysql-1, mysql-2\n\n# Verify PVCs\nkubectl get pvc\n# data-mysql-0, data-mysql-1, data-mysql-2\n\n# Test stable network identity\nkubectl run -it --rm debug --image=busybox --restart=Never -- nslookup mysql-0.mysql\n</code></pre> <p>Expected Pod Order: <pre><code>mysql-0   1/1   Running   # Created first\nmysql-1   1/1   Running   # Created second (after mysql-0 Ready)\nmysql-2   1/1   Running   # Created third (after mysql-1 Ready)\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#summary","title":"Summary","text":"<p>Deployments form the foundation of stateless workload management in Kubernetes. Key takeaways for CKA success:</p> <p>Essential Concepts: - Deployment Hierarchy: Deployment \u2192 ReplicaSet \u2192 Pods - Rolling Updates: Default strategy for zero-downtime updates - Rollback Capability: Revision history enables quick recovery - Controller Specialization: DaemonSet (per-node), StatefulSet (stateful), Job (batch)</p> <p>CKA Exam Priorities: 1. Imperative Commands: <code>kubectl create deployment</code>, <code>kubectl scale</code>, <code>kubectl set image</code> 2. Rollout Management: <code>kubectl rollout status</code>, <code>kubectl rollout undo</code> 3. Troubleshooting: Diagnose ImagePullBackOff, CrashLoopBackOff, Pending pods 4. YAML Generation: Use <code>--dry-run=client -o yaml</code> for complex resources</p> <p>Time-Saving Strategies: - Use imperative commands for simple deployments - Generate YAML templates for complex configurations - Master <code>kubectl rollout</code> commands for update scenarios - Understand label selectors to avoid common pitfalls</p> <p>Next Steps: - Practice deployment creation and scaling until automatic - Build muscle memory for rollout and rollback commands - Understand when to use each controller type - Review Services and Ingress for complete application deployment</p> <p>Related Topics: - Services: Expose deployments with stable networking - ConfigMaps &amp; Secrets: Inject configuration into deployments - Persistent Volumes: StatefulSet storage requirements - Resource Quotas: Control deployment resource consumption</p> <p>With deployments mastered, you control application lifecycle management - a core competency for CKA certification and production Kubernetes operations.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/","title":"Helm: Kubernetes Package Manager","text":"<p>Simplify application deployment with charts, releases, and templating</p> <p>Helm is the de facto package manager for Kubernetes, enabling you to define, install, and upgrade complex applications using reusable packages called charts. For the CKA exam, you'll need to understand Helm's architecture, work with charts and releases, troubleshoot deployments, and use Helm commands effectively. This guide covers Helm 3 fundamentals, chart structure, templating with values, release management, and practical troubleshooting techniques to help you master Kubernetes application packaging.</p> <p>CKA Exam Relevance: Cluster Architecture, Installation &amp; Configuration (25% of exam weight)</p>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li>Helm Architecture: Charts, releases, repositories</li> <li>Chart Structure: Templates, values, metadata</li> <li>Helm Commands: install, upgrade, rollback, list, uninstall</li> <li>Templating: Go templates, values injection, built-in functions</li> <li>Chart Repositories: Adding, searching, and managing repos</li> <li>Release Management: Versioning, rollbacks, history</li> <li>Hooks: Pre/post-install, upgrade lifecycle management</li> <li>Troubleshooting: Common issues and debugging techniques</li> </ul>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#what-is-helm","title":"\ud83c\udfaf What is Helm?","text":"<p>Helm is a package manager for Kubernetes that simplifies deploying and managing applications.</p>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#why-use-helm","title":"Why Use Helm?","text":"<pre><code>graph TD\n    subgraph \"Without Helm\"\n        M1[deployment.yaml&lt;br/&gt;50 lines]\n        M2[service.yaml&lt;br/&gt;20 lines]\n        M3[configmap.yaml&lt;br/&gt;30 lines]\n        M4[ingress.yaml&lt;br/&gt;25 lines]\n        M5[pvc.yaml&lt;br/&gt;15 lines]\n\n        Manual[Manual kubectl apply&lt;br/&gt;for each file]\n\n        M1 --&gt; Manual\n        M2 --&gt; Manual\n        M3 --&gt; Manual\n        M4 --&gt; Manual\n        M5 --&gt; Manual\n    end\n\n    subgraph \"With Helm\"\n        Chart[Helm Chart&lt;br/&gt;myapp/]\n        Values[values.yaml&lt;br/&gt;Configuration]\n\n        Install[helm install myapp .]\n\n        Chart --&gt; Install\n        Values --&gt; Install\n    end\n\n    Manual --&gt; Complex[Complex&lt;br/&gt;Error-prone&lt;br/&gt;Hard to version]\n    Install --&gt; Simple[Simple&lt;br/&gt;Repeatable&lt;br/&gt;Versioned]\n\n    style Complex fill:#ffe5e5\n    style Simple fill:#e8f5e8</code></pre> <p>Helm Benefits: - Package Management: Bundle related Kubernetes resources - Templating: Parameterize manifests for reuse - Version Control: Track releases and rollback easily - Dependency Management: Charts can depend on other charts - Repeatability: Same chart deploys identically across environments</p> <p>Helm Use Cases: - Deploy complex applications (databases, monitoring stacks) - Share applications via public/private repositories - Manage multi-environment deployments (dev, staging, prod) - Standardize deployments across teams</p>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#helm-architecture","title":"\ud83c\udfd7\ufe0f Helm Architecture","text":"<pre><code>graph LR\n    subgraph \"Helm Client\"\n        CLI[helm CLI]\n    end\n\n    subgraph \"Kubernetes Cluster\"\n        API[Kubernetes API Server]\n        Resources[Kubernetes Resources&lt;br/&gt;Deployments, Services, etc.]\n    end\n\n    subgraph \"Helm Concepts\"\n        Chart[Chart&lt;br/&gt;Package]\n        Values[values.yaml&lt;br/&gt;Configuration]\n        Release[Release&lt;br/&gt;Deployed instance]\n        Repo[Chart Repository&lt;br/&gt;Storage]\n    end\n\n    CLI --&gt;|1. Install/Upgrade| Chart\n    Chart --&gt;|2. Template with| Values\n    Values --&gt;|3. Generate manifests| CLI\n    CLI --&gt;|4. Apply| API\n    API --&gt;|5. Create/Update| Resources\n    CLI --&gt;|6. Track| Release\n\n    Repo --&gt;|Fetch charts| Chart\n\n    style Chart fill:#e1f5ff\n    style Values fill:#fff4e1\n    style Release fill:#e8f5e8\n    style Resources fill:#f3e5f5</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#core-concepts","title":"Core Concepts","text":"<p>Chart: A Helm package containing templates and metadata - Template files (Deployment, Service, etc.) - <code>values.yaml</code> for default configuration - <code>Chart.yaml</code> for metadata - Optional dependencies</p> <p>Release: An instance of a chart deployed to a cluster - Each <code>helm install</code> creates a new release - Releases have unique names and versions - Release history tracked in Kubernetes Secrets</p> <p>Repository: A collection of packaged charts - Public repos: ArtifactHub, Helm Hub - Private repos: ChartMuseum, Harbor, S3</p> <p>Values: Configuration parameters that customize chart behavior - Default values in <code>values.yaml</code> - Override via <code>--set</code> or <code>-f custom-values.yaml</code></p>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#chart-structure","title":"\ud83d\udce6 Chart Structure","text":"<p>A typical Helm chart directory structure:</p> <pre><code>mychart/\n\u251c\u2500\u2500 Chart.yaml          # Chart metadata\n\u251c\u2500\u2500 values.yaml         # Default configuration values\n\u251c\u2500\u2500 charts/             # Dependent charts (subcharts)\n\u251c\u2500\u2500 templates/          # Kubernetes manifest templates\n\u2502   \u251c\u2500\u2500 NOTES.txt       # Post-install notes\n\u2502   \u251c\u2500\u2500 _helpers.tpl    # Template helpers\n\u2502   \u251c\u2500\u2500 deployment.yaml # Deployment template\n\u2502   \u251c\u2500\u2500 service.yaml    # Service template\n\u2502   \u251c\u2500\u2500 ingress.yaml    # Ingress template\n\u2502   \u2514\u2500\u2500 tests/          # Test resources\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u251c\u2500\u2500 .helmignore         # Files to ignore when packaging\n\u2514\u2500\u2500 README.md           # Chart documentation\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#chartyaml","title":"Chart.yaml","text":"<p>Metadata file defining the chart:</p> <pre><code>apiVersion: v2                    # Chart API version (v2 for Helm 3)\nname: mychart                     # Chart name\nversion: 1.0.0                    # Chart version (SemVer)\nappVersion: \"1.16.0\"             # Application version\ndescription: A Helm chart for my application\ntype: application                 # application or library\nkeywords:\n  - web\n  - frontend\nmaintainers:\n  - name: Your Name\n    email: you@example.com\ndependencies:                     # Optional: Chart dependencies\n  - name: postgresql\n    version: \"11.x.x\"\n    repository: https://charts.bitnami.com/bitnami\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#valuesyaml","title":"values.yaml","text":"<p>Default configuration for the chart:</p> <pre><code>replicaCount: 2\n\nimage:\n  repository: nginx\n  tag: \"1.27\"\n  pullPolicy: IfNotPresent\n\nservice:\n  type: ClusterIP\n  port: 80\n\ningress:\n  enabled: false\n  className: nginx\n  hosts:\n    - host: example.com\n      paths:\n        - path: /\n          pathType: Prefix\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\nautoscaling:\n  enabled: false\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 80\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#template-example","title":"Template Example","text":"<p>templates/deployment.yaml with Go templating:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      labels:\n        {{- include \"mychart.selectorLabels\" . | nindent 8 }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n        imagePullPolicy: {{ .Values.image.pullPolicy }}\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        resources:\n          {{- toYaml .Values.resources | nindent 10 }}\n</code></pre> <p>Template syntax: - <code>{{ .Values.key }}</code>: Access values - <code>{{ .Chart.Name }}</code>: Access chart metadata - <code>{{ .Release.Name }}</code>: Access release info - <code>{{- include \"template\" . }}</code>: Include helper templates - <code>{{ toYaml .Values.resources | nindent 10 }}</code>: Convert to YAML with indentation</p>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#helm-commands","title":"\ud83d\udd27 Helm Commands","text":"","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#installing-charts","title":"Installing Charts","text":"<pre><code># Install from repository\nhelm install myrelease bitnami/nginx\n\n# Install from local directory\nhelm install myrelease ./mychart\n\n# Install with custom values\nhelm install myrelease ./mychart -f custom-values.yaml\n\n# Install with --set overrides\nhelm install myrelease ./mychart \\\n  --set replicaCount=3 \\\n  --set image.tag=1.28\n\n# Install in specific namespace\nhelm install myrelease ./mychart -n production --create-namespace\n\n# Dry-run (template only, don't install)\nhelm install myrelease ./mychart --dry-run --debug\n\n# Generate name automatically\nhelm install ./mychart --generate-name\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#listing-releases","title":"Listing Releases","text":"<pre><code># List releases in current namespace\nhelm list\n\n# List all releases in all namespaces\nhelm list -A\n\n# List uninstalled releases (history)\nhelm list --uninstalled\n\n# Show all release states\nhelm list --all\n\n# Filter by status\nhelm list --deployed\nhelm list --failed\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#upgrading-releases","title":"Upgrading Releases","text":"<pre><code># Upgrade release with new values\nhelm upgrade myrelease ./mychart -f new-values.yaml\n\n# Upgrade and install if not exists\nhelm upgrade --install myrelease ./mychart\n\n# Upgrade with --set\nhelm upgrade myrelease ./mychart --set replicaCount=5\n\n# Force resource updates\nhelm upgrade myrelease ./mychart --force\n\n# Wait for resources to be ready\nhelm upgrade myrelease ./mychart --wait --timeout 5m\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#rolling-back-releases","title":"Rolling Back Releases","text":"<pre><code># Show release history\nhelm history myrelease\n\n# Rollback to previous version\nhelm rollback myrelease\n\n# Rollback to specific revision\nhelm rollback myrelease 3\n\n# Dry-run rollback\nhelm rollback myrelease 2 --dry-run\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#uninstalling-releases","title":"Uninstalling Releases","text":"<pre><code># Uninstall release\nhelm uninstall myrelease\n\n# Uninstall but keep history\nhelm uninstall myrelease --keep-history\n\n# Uninstall with timeout\nhelm uninstall myrelease --timeout 2m\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#getting-release-information","title":"Getting Release Information","text":"<pre><code># Show release status\nhelm status myrelease\n\n# Show release values\nhelm get values myrelease\n\n# Show all release values (including defaults)\nhelm get values myrelease --all\n\n# Show release manifest\nhelm get manifest myrelease\n\n# Show release notes\nhelm get notes myrelease\n\n# Show release hooks\nhelm get hooks myrelease\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#working-with-repositories","title":"\ud83d\udcda Working with Repositories","text":"","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#managing-repositories","title":"Managing Repositories","text":"<pre><code># Add repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo add stable https://charts.helm.sh/stable\n\n# List repositories\nhelm repo list\n\n# Update repository index\nhelm repo update\n\n# Remove repository\nhelm repo remove bitnami\n\n# Search repositories\nhelm search repo nginx\nhelm search repo bitnami/postgres --versions\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#searching-charts","title":"Searching Charts","text":"<pre><code># Search all repos for keyword\nhelm search repo database\n\n# Search specific repo\nhelm search repo bitnami/mysql\n\n# Search with versions\nhelm search repo nginx --versions\n\n# Search ArtifactHub (public hub)\nhelm search hub wordpress\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#installing-from-repository","title":"Installing from Repository","text":"<pre><code># Install latest version\nhelm install mydb bitnami/postgresql\n\n# Install specific version\nhelm install mydb bitnami/postgresql --version 11.9.0\n\n# Show chart information before installing\nhelm show chart bitnami/postgresql\nhelm show values bitnami/postgresql\nhelm show readme bitnami/postgresql\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#templating-and-values","title":"\ud83c\udfa8 Templating and Values","text":"","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#template-functions","title":"Template Functions","text":"<p>Common built-in functions:</p> <pre><code># String functions\n{{ .Values.name | upper }}              # MYAPP\n{{ .Values.name | lower }}              # myapp\n{{ .Values.name | quote }}              # \"myapp\"\n{{ .Values.name | trunc 5 }}            # myapp (truncate)\n\n# Default values\n{{ .Values.optional | default \"default-value\" }}\n\n# Type conversions\n{{ .Values.port | toString }}           # \"80\"\n{{ .Values.enabled | toJson }}          # true\n\n# Conditionals\n{{- if .Values.ingress.enabled }}\n# ingress configuration\n{{- end }}\n\n{{- if eq .Values.service.type \"LoadBalancer\" }}\n# LoadBalancer-specific config\n{{- end }}\n\n# Loops\n{{- range .Values.hosts }}\n- {{ . }}\n{{- end }}\n\n# YAML/JSON conversion\n{{- toYaml .Values.resources | nindent 8 }}\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#helper-templates-_helperstpl","title":"Helper Templates (_helpers.tpl)","text":"<p>Reusable template snippets:</p> <pre><code>{{/*\nExpand the name of the chart.\n*/}}\n{{- define \"mychart.name\" -}}\n{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n\n{{/*\nCreate a fully qualified app name.\n*/}}\n{{- define \"mychart.fullname\" -}}\n{{- if .Values.fullnameOverride }}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- $name := default .Chart.Name .Values.nameOverride }}\n{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n\n{{/*\nCommon labels\n*/}}\n{{- define \"mychart.labels\" -}}\nhelm.sh/chart: {{ include \"mychart.chart\" . }}\n{{ include \"mychart.selectorLabels\" . }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end }}\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#values-precedence","title":"Values Precedence","text":"<p>Override order (highest to lowest priority):</p> <ol> <li><code>--set</code> flags: <code>helm install --set key=value</code></li> <li><code>-f</code> files (last file wins): <code>helm install -f values1.yaml -f values2.yaml</code></li> <li>Chart's <code>values.yaml</code> (defaults)</li> </ol> <pre><code># Multiple override methods\nhelm install myrelease ./mychart \\\n  -f prod-values.yaml \\              # Override file\n  --set replicaCount=3 \\             # Command-line override\n  --set image.tag=v2.0               # Another override\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#hooks","title":"\ud83e\ude9d Hooks","text":"<p>Hooks execute at specific points in a release lifecycle:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-migration\n  annotations:\n    \"helm.sh/hook\": pre-upgrade              # Hook type\n    \"helm.sh/hook-weight\": \"0\"               # Execution order\n    \"helm.sh/hook-delete-policy\": hook-succeeded  # Cleanup policy\nspec:\n  template:\n    spec:\n      containers:\n      - name: migration\n        image: myapp:{{ .Values.image.tag }}\n        command: [\"./run-migrations.sh\"]\n      restartPolicy: Never\n</code></pre> <p>Hook types: - <code>pre-install</code>: Before resources are created - <code>post-install</code>: After all resources are created - <code>pre-delete</code>: Before deletion request - <code>post-delete</code>: After all resources deleted - <code>pre-upgrade</code>: Before upgrade - <code>post-upgrade</code>: After upgrade - <code>pre-rollback</code>: Before rollback - <code>post-rollback</code>: After rollback - <code>test</code>: When <code>helm test</code> runs</p> <p>Hook deletion policies: - <code>before-hook-creation</code>: Delete previous hook before new hook - <code>hook-succeeded</code>: Delete after successful execution - <code>hook-failed</code>: Delete after failed execution</p>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#creating-custom-charts","title":"\ud83d\udee0\ufe0f Creating Custom Charts","text":"","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#create-new-chart","title":"Create New Chart","text":"<pre><code># Generate chart scaffold\nhelm create mychart\n\n# Chart structure created:\nmychart/\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 values.yaml\n\u251c\u2500\u2500 charts/\n\u2514\u2500\u2500 templates/\n    \u251c\u2500\u2500 deployment.yaml\n    \u251c\u2500\u2500 service.yaml\n    \u251c\u2500\u2500 ingress.yaml\n    \u251c\u2500\u2500 _helpers.tpl\n    \u2514\u2500\u2500 NOTES.txt\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#packaging-charts","title":"Packaging Charts","text":"<pre><code># Package chart into .tgz archive\nhelm package mychart/\n# Output: mychart-1.0.0.tgz\n\n# Package with specific destination\nhelm package mychart/ -d ./packages/\n\n# Update dependencies before packaging\nhelm package mychart/ --dependency-update\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#chart-dependencies","title":"Chart Dependencies","text":"<p>Chart.yaml with dependencies:</p> <pre><code>dependencies:\n  - name: postgresql\n    version: \"11.9.0\"\n    repository: https://charts.bitnami.com/bitnami\n    condition: postgresql.enabled          # Enable/disable via values\n  - name: redis\n    version: \"17.x.x\"\n    repository: https://charts.bitnami.com/bitnami\n    tags:\n      - cache                              # Enable via tag\n</code></pre> <p>Manage dependencies:</p> <pre><code># Download dependencies to charts/ directory\nhelm dependency update mychart/\n\n# List dependencies\nhelm dependency list mychart/\n\n# Build dependencies (equivalent to update)\nhelm dependency build mychart/\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#troubleshooting-helm","title":"\ud83d\udd0d Troubleshooting Helm","text":"","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#debugging-templates","title":"Debugging Templates","text":"<pre><code># Render templates without installing (dry-run)\nhelm install myrelease ./mychart --dry-run --debug\n\n# Template only (no validation)\nhelm template myrelease ./mychart\n\n# Template with values\nhelm template myrelease ./mychart -f prod-values.yaml\n\n# Show specific template\nhelm template myrelease ./mychart --show-only templates/deployment.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#common-issues","title":"Common Issues","text":"<p>Issue 1: Template Rendering Error</p> <pre><code># Error: parse error at (mychart/templates/deployment.yaml:10)\nhelm install myrelease ./mychart --dry-run --debug\n\n# Check line 10 in deployment.yaml for syntax errors\n# Common: Missing closing braces, incorrect indentation\n</code></pre> <p>Issue 2: Release Already Exists</p> <pre><code># Error: cannot re-use a name that is still in use\n\n# List existing releases\nhelm list -A\n\n# Uninstall existing release\nhelm uninstall myrelease\n\n# Or use different name\nhelm install myrelease-v2 ./mychart\n</code></pre> <p>Issue 3: Failed Installation</p> <pre><code># Check release status\nhelm status myrelease\n\n# View release history\nhelm history myrelease\n\n# Check failed resources\nkubectl get all -n &lt;namespace&gt; -l app.kubernetes.io/instance=myrelease\n\n# Uninstall and retry\nhelm uninstall myrelease\nhelm install myrelease ./mychart --wait --timeout 5m\n</code></pre> <p>Issue 4: Values Not Applied</p> <pre><code># Verify applied values\nhelm get values myrelease\n\n# Check template rendering\nhelm template myrelease ./mychart -f values.yaml --debug\n\n# Ensure correct values file path\nhelm upgrade myrelease ./mychart -f ./config/prod-values.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#cka-exam-practice-exercises","title":"\ud83d\udcdd CKA Exam Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#exercise-1-install-chart-with-custom-values","title":"Exercise 1: Install Chart with Custom Values","text":"<p>Scenario: Install the <code>bitnami/nginx</code> chart as release <code>web-server</code> in namespace <code>production</code> with 3 replicas.</p> Solution <pre><code># Add bitnami repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n\n# Create namespace\nkubectl create namespace production\n\n# Install with custom replica count\nhelm install web-server bitnami/nginx \\\n  --namespace production \\\n  --set replicaCount=3\n\n# Verify installation\nhelm list -n production\nkubectl get pods -n production\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#exercise-2-upgrade-release-and-rollback","title":"Exercise 2: Upgrade Release and Rollback","text":"<p>Scenario: Upgrade the <code>web-server</code> release to use nginx image tag <code>1.25</code>, verify it works, then rollback.</p> Solution <pre><code># Show current release\nhelm status web-server -n production\n\n# Upgrade with new image tag\nhelm upgrade web-server bitnami/nginx \\\n  -n production \\\n  --set image.tag=1.25\n\n# Verify upgrade\nkubectl get pods -n production\nhelm history web-server -n production\n\n# If issues occur, rollback to previous version\nhelm rollback web-server -n production\n\n# Verify rollback\nhelm history web-server -n production\nkubectl get pods -n production -o jsonpath='{.items[0].spec.containers[0].image}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#exercise-3-create-custom-chart","title":"Exercise 3: Create Custom Chart","text":"<p>Scenario: Create a chart named <code>myapp</code> that deploys a simple nginx application with configurable replica count.</p> Solution <pre><code># Generate chart scaffold\nhelm create myapp\n\n# Edit values.yaml\ncat &lt;&lt;EOF &gt; myapp/values.yaml\nreplicaCount: 2\n\nimage:\n  repository: nginx\n  tag: \"1.27\"\n  pullPolicy: IfNotPresent\n\nservice:\n  type: ClusterIP\n  port: 80\nEOF\n\n# Test template rendering\nhelm template myrelease ./myapp\n\n# Install locally\nhelm install myapp-release ./myapp\n\n# Verify deployment\nkubectl get all -l app.kubernetes.io/name=myapp\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#exercise-4-troubleshoot-failed-release","title":"Exercise 4: Troubleshoot Failed Release","text":"<p>Scenario: A release <code>broken-app</code> failed to install. Diagnose and fix the issue.</p> Solution <pre><code># Check release status\nhelm status broken-app\n\n# View release history\nhelm history broken-app\n\n# Check rendered manifest for errors\nhelm get manifest broken-app\n\n# Check events and pod status\nkubectl get events --sort-by='.lastTimestamp'\nkubectl get pods -l app.kubernetes.io/instance=broken-app\n\n# Common fixes:\n# 1. Image pull errors\nkubectl describe pod &lt;pod-name&gt;\n# Fix: Update image tag in values\n\n# 2. Resource quota exceeded\nkubectl describe ns &lt;namespace&gt;\n# Fix: Increase quotas or reduce requests\n\n# 3. Template syntax error\nhelm template broken-app ./chart --debug\n# Fix: Correct template syntax\n\n# Uninstall and reinstall with fix\nhelm uninstall broken-app\nhelm install broken-app ./chart -f fixed-values.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#exercise-5-export-and-backup-release","title":"Exercise 5: Export and Backup Release","text":"<p>Scenario: Export all configuration for release <code>production-app</code> to files for backup.</p> Solution <pre><code># Create backup directory\nmkdir -p helm-backups/production-app\n\n# Export values\nhelm get values production-app &gt; helm-backups/production-app/values.yaml\n\n# Export manifest\nhelm get manifest production-app &gt; helm-backups/production-app/manifest.yaml\n\n# Export notes\nhelm get notes production-app &gt; helm-backups/production-app/notes.txt\n\n# Export history\nhelm history production-app &gt; helm-backups/production-app/history.txt\n\n# Package chart if available\nhelm package ./production-app-chart -d helm-backups/\n\n# Verify backup\nls -lh helm-backups/production-app/\n\n# To restore later:\n# helm install production-app ./helm-backups/production-app-1.0.0.tgz \\\n#   -f helm-backups/production-app/values.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#cka-exam-tips","title":"\ud83c\udfaf CKA Exam Tips","text":"","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#fast-helm-operations","title":"Fast Helm Operations","text":"<pre><code># Quick install with overrides\nhelm install NAME CHART --set key=value\n\n# Upgrade or install in one command\nhelm upgrade --install NAME CHART\n\n# Template without installing (validation)\nhelm template NAME CHART --debug\n\n# List all releases quickly\nhelm list -A\n\n# Rollback immediately if issues\nhelm rollback NAME\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#common-exam-patterns","title":"Common Exam Patterns","text":"<ol> <li>Install from repo: <code>helm install NAME bitnami/CHART</code></li> <li>Custom values: Use <code>--set</code> for quick changes, <code>-f</code> for files</li> <li>Verify: <code>helm list</code>, <code>helm status</code>, <code>kubectl get all</code></li> <li>Troubleshoot: <code>helm template --debug</code>, <code>helm get manifest</code></li> <li>Rollback: <code>helm history</code>, <code>helm rollback</code></li> </ol>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#memorize-key-commands","title":"Memorize Key Commands","text":"<pre><code># Repository management\nhelm repo add NAME URL\nhelm repo update\nhelm search repo KEYWORD\n\n# Release management\nhelm install NAME CHART\nhelm upgrade NAME CHART\nhelm rollback NAME [REVISION]\nhelm uninstall NAME\nhelm list\n\n# Information\nhelm status NAME\nhelm history NAME\nhelm get values NAME\nhelm get manifest NAME\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#quick-reference","title":"\ud83d\udcda Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#essential-helm-commands","title":"Essential Helm Commands","text":"<pre><code># Install\nhelm install RELEASE CHART [flags]\n  --namespace, -n: Namespace\n  --set: Set values on command line\n  -f, --values: Specify values file\n  --dry-run: Simulate install\n  --wait: Wait for resources to be ready\n  --timeout: Time to wait (default 5m)\n\n# Upgrade\nhelm upgrade RELEASE CHART [flags]\n  --install: Install if not exists\n  --force: Force resource updates\n  --reset-values: Reset to chart defaults\n\n# Rollback\nhelm rollback RELEASE [REVISION] [flags]\n\n# List\nhelm list [flags]\n  -A, --all-namespaces: All namespaces\n  --deployed: Only deployed releases\n  --failed: Only failed releases\n\n# Get information\nhelm get values RELEASE\nhelm get manifest RELEASE\nhelm status RELEASE\nhelm history RELEASE\n\n# Repository\nhelm repo add NAME URL\nhelm repo list\nhelm repo update\nhelm search repo KEYWORD\n\n# Chart management\nhelm create CHART\nhelm package CHART\nhelm dependency update CHART\n</code></pre>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Previous: Post 17 - Custom Resources and Operators</li> <li>Next: Post 19 - Kustomize: Template-Free Configuration</li> <li>Reference: Helm Documentation</li> <li>Reference: ArtifactHub - Public Chart Repository</li> <li>Series: Kubernetes CKA Mastery</li> </ul>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/helm-kubernetes-package-manager/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Helm simplifies Kubernetes deployment with reusable packages (charts)</li> <li>Charts contain templates parameterized by values</li> <li>Releases track deployed instances with version history</li> <li>Templating uses Go templates with built-in functions</li> <li>Repositories share charts publicly or privately</li> <li>helm upgrade modifies releases, helm rollback reverts changes</li> <li>Hooks execute at specific lifecycle points</li> <li>helm template --debug is essential for troubleshooting</li> <li>CKA exam tip: Master <code>helm install</code>, <code>upgrade</code>, <code>rollback</code>, and <code>list</code></li> </ul> <p>Next Steps: Learn Kustomize for template-free configuration management and GitOps workflows.</p>","tags":["kubernetes","k8s","cka-prep","helm"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/","title":"Ingress and Gateway API: Modern Traffic Management","text":"<p>In production Kubernetes environments, managing external access to your services is critical. While Services handle internal cluster networking, Ingress provides sophisticated HTTP/HTTPS routing from the outside world. This guide covers everything you need to know for the CKA exam and production deployments, including the modern Gateway API that's reshaping Kubernetes networking in 2025.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#why-ingress-matters","title":"Why Ingress Matters","text":"<p>Imagine running 50 microservices in your cluster. Without Ingress, you'd need 50 LoadBalancer Services\u2014each with its own expensive cloud load balancer and public IP address. That's not just costly; it's operationally nightmarish.</p> <p>Ingress solves this by providing:</p> <ul> <li>Cost efficiency: One load balancer for multiple services</li> <li>Advanced routing: Path-based, host-based, header-based routing</li> <li>SSL/TLS termination: Centralized certificate management</li> <li>Name-based virtual hosting: Multiple domains on one IP</li> <li>Protocol support: HTTP, HTTPS, WebSocket, gRPC</li> </ul> <p>For the CKA exam, you'll need to demonstrate hands-on competency with Ingress resources, troubleshoot misconfigurations, and understand controller selection. In production, mastering Ingress means the difference between elegant traffic management and a tangled mess of load balancers.</p> <p>The landscape is evolving. While traditional Ingress remains the standard (and what's tested on the CKA), the Gateway API represents Kubernetes networking's future\u2014offering role-oriented design, better extensibility, and more expressive routing rules. Understanding both is essential for modern Kubernetes practitioners.</p> <pre><code>graph TB\n    subgraph \"External Traffic\"\n        Client[Client Browser]\n    end\n\n    subgraph \"Kubernetes Cluster\"\n        LB[Cloud Load Balancer&lt;br/&gt;External IP: 203.0.113.10]\n\n        subgraph \"Ingress Controller Pod\"\n            IC[NGINX/Traefik/Contour&lt;br/&gt;Processes routing rules]\n        end\n\n        subgraph \"Ingress Resources\"\n            I1[Ingress: api.example.com]\n            I2[Ingress: app.example.com]\n        end\n\n        subgraph \"Backend Services\"\n            S1[Service: api-service&lt;br/&gt;ClusterIP: 10.96.0.10]\n            S2[Service: app-service&lt;br/&gt;ClusterIP: 10.96.0.20]\n\n            P1[Pod: api-v1&lt;br/&gt;10.244.1.5]\n            P2[Pod: api-v2&lt;br/&gt;10.244.2.8]\n            P3[Pod: app-frontend&lt;br/&gt;10.244.1.12]\n        end\n    end\n\n    Client --&gt;|HTTPS Request&lt;br/&gt;Host: api.example.com| LB\n    LB --&gt;|Forwards to&lt;br/&gt;NodePort/HostPort| IC\n    IC -.-&gt;|Reads rules| I1\n    IC -.-&gt;|Reads rules| I2\n    IC --&gt;|Routes based on&lt;br/&gt;Host header| S1\n    IC --&gt;|Routes based on&lt;br/&gt;Host header| S2\n    S1 --&gt; P1\n    S1 --&gt; P2\n    S2 --&gt; P3\n\n    style IC fill:#e1f5ff\n    style I1 fill:#fff4e1\n    style I2 fill:#fff4e1\n    style LB fill:#ffe1e1</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#ingress-fundamentals","title":"Ingress Fundamentals","text":"","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#what-is-ingress","title":"What is Ingress?","text":"<p>Ingress is an API object that defines rules for routing external HTTP(S) traffic to services inside your cluster. Think of it as a sophisticated reverse proxy configuration expressed as Kubernetes YAML. Unlike Services which operate at Layer 4 (TCP/UDP), Ingress works at Layer 7 (HTTP/HTTPS), giving you application-aware routing.</p> <p>The Two-Part Architecture:</p> <ol> <li>Ingress Resource: The YAML configuration defining routing rules</li> <li>Ingress Controller: The actual software that reads these resources and implements the routing logic</li> </ol> <p>This separation is crucial\u2014you can have dozens of Ingress resources, but they do nothing without a controller to act on them.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#ingress-controller-architecture","title":"Ingress Controller Architecture","text":"<p>An Ingress controller is essentially three components working together:</p> <pre><code>graph LR\n    subgraph \"Ingress Controller Components\"\n        W[Watcher&lt;br/&gt;Monitors Ingress resources]\n        C[Configuration Generator&lt;br/&gt;Translates to proxy config]\n        P[Proxy Server&lt;br/&gt;NGINX/Envoy/HAProxy]\n    end\n\n    subgraph \"Kubernetes API\"\n        API[API Server&lt;br/&gt;Ingress resources]\n    end\n\n    subgraph \"Traffic Flow\"\n        Traffic[External Traffic] --&gt; P\n        P --&gt; Backend[Backend Services]\n    end\n\n    API -.-&gt;|Watch events| W\n    W --&gt;|Resource changes| C\n    C --&gt;|Generate config| P\n    P -.-&gt;|Reload| P\n\n    style W fill:#e1f5ff\n    style C fill:#ffe1e1\n    style P fill:#e1ffe1</code></pre> <p>How it works:</p> <ol> <li>Watcher continuously monitors the Kubernetes API for Ingress resource changes</li> <li>Configuration Generator translates Ingress rules into proxy-specific config (NGINX conf, Envoy YAML, etc.)</li> <li>Proxy Server reloads configuration and routes traffic accordingly</li> </ol>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#popular-controllers-2025-comparison","title":"Popular Controllers (2025 Comparison)","text":"Controller Best For Strengths Considerations NGINX Ingress General production use Battle-tested, extensive features, large community Configuration via annotations can be complex Traefik Dynamic environments Auto-discovery, native Let's Encrypt, beautiful UI Higher resource usage Contour Enterprise, security-focused Envoy-based, graceful shutdowns, Gateway API support Steeper learning curve HAProxy Ingress High performance Excellent performance, advanced load balancing Smaller community than NGINX <p>2025 Recommendation for CKA Prep:</p> <p>Use NGINX Ingress Controller for exam preparation. It's the de facto standard, well-documented, and most likely to match exam scenarios. Install with:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.11.2/deploy/static/provider/cloud/deploy.yaml\n</code></pre> <p>Production Recommendation:</p> <p>Consider Contour if you're investing in Gateway API (it has excellent support), or stick with NGINX for mature, stable deployments with extensive community resources.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#ingress-resources","title":"Ingress Resources","text":"","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#basic-structure","title":"Basic Structure","text":"<p>Every Ingress resource follows this pattern:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  namespace: production\n  annotations:\n    # Controller-specific configuration\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx  # Critical: specifies which controller handles this\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /v1\n        pathType: Prefix\n        backend:\n          service:\n            name: api-v1-service\n            port:\n              number: 8080\n</code></pre> <p>Key Fields:</p> <ul> <li>ingressClassName: Specifies which controller processes this Ingress (required in v1)</li> <li>rules: Array of routing rules</li> <li>host: Optional hostname for virtual hosting</li> <li>paths: Array of path-based routing rules</li> <li>pathType: <code>Exact</code>, <code>Prefix</code>, or <code>ImplementationSpecific</code></li> </ul>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#path-based-routing","title":"Path-Based Routing","text":"<p>Route different paths to different services\u2014perfect for microservices architectures:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: path-routing\n  namespace: default\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: shop.example.com\n    http:\n      paths:\n      # Product catalog microservice\n      - path: /products\n        pathType: Prefix\n        backend:\n          service:\n            name: catalog-service\n            port:\n              number: 80\n\n      # Shopping cart microservice\n      - path: /cart\n        pathType: Prefix\n        backend:\n          service:\n            name: cart-service\n            port:\n              number: 8080\n\n      # Exact match for homepage\n      - path: /\n        pathType: Exact\n        backend:\n          service:\n            name: frontend-service\n            port:\n              number: 3000\n</code></pre> <p>PathType Semantics:</p> <ul> <li>Exact: Matches the URL path exactly (case-sensitive)</li> <li>Prefix: Matches based on URL path prefix split by <code>/</code></li> <li>ImplementationSpecific: Interpretation depends on IngressClass</li> </ul> <p>CKA Tip: Understand prefix matching behavior. <code>/foo</code> matches <code>/foo</code>, <code>/foo/</code>, and <code>/foo/bar</code>, but not <code>/foobar</code>.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#host-based-routing","title":"Host-Based Routing","text":"<p>Single IP address, multiple domains\u2014critical for multi-tenant environments:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: multi-tenant\n  namespace: saas-platform\nspec:\n  ingressClassName: nginx\n  rules:\n  # Customer A's subdomain\n  - host: acme.platform.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: acme-tenant-service\n            port:\n              number: 80\n\n  # Customer B's subdomain\n  - host: globex.platform.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: globex-tenant-service\n            port:\n              number: 80\n\n  # Default backend for unmatched hosts\n  defaultBackend:\n    service:\n      name: default-404-service\n      port:\n        number: 80\n</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#tlsssl-termination","title":"TLS/SSL Termination","text":"<p>Production traffic must be encrypted. Ingress handles TLS termination at the edge:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tls-ingress\n  namespace: production\n  annotations:\n    # Force HTTPS redirect\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    # Modern TLS configuration\n    nginx.ingress.kubernetes.io/ssl-protocols: \"TLSv1.2 TLSv1.3\"\nspec:\n  ingressClassName: nginx\n  tls:\n  # TLS configuration\n  - hosts:\n    - secure.example.com\n    - api.example.com\n    secretName: example-tls-cert  # Secret containing tls.crt and tls.key\n  rules:\n  - host: secure.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: secure-service\n            port:\n              number: 443\n</code></pre> <p>Creating the TLS Secret:</p> <pre><code># From certificate files\nkubectl create secret tls example-tls-cert \\\n  --cert=path/to/tls.crt \\\n  --key=path/to/tls.key \\\n  --namespace=production\n\n# Using cert-manager (recommended for production)\n# See Advanced Patterns section\n</code></pre> <pre><code>sequenceDiagram\n    participant Client\n    participant LB as Load Balancer\n    participant IC as Ingress Controller\n    participant Secret as TLS Secret\n    participant Backend as Backend Pod\n\n    Note over Client,Backend: TLS Termination Flow\n\n    Client-&gt;&gt;LB: HTTPS Request (encrypted)\n    LB-&gt;&gt;IC: Forward to Ingress Controller\n\n    IC-&gt;&gt;Secret: Read TLS certificate\n    Secret--&gt;&gt;IC: tls.crt + tls.key\n\n    IC-&gt;&gt;IC: TLS handshake&lt;br/&gt;Decrypt request\n\n    Note over IC: Certificate validated&lt;br/&gt;Traffic decrypted\n\n    IC-&gt;&gt;Backend: HTTP Request (plain text)&lt;br/&gt;Internal cluster communication\n    Backend--&gt;&gt;IC: HTTP Response\n\n    IC-&gt;&gt;IC: Encrypt response\n    IC--&gt;&gt;LB: HTTPS Response (encrypted)\n    LB--&gt;&gt;Client: HTTPS Response (encrypted)\n\n    style IC fill:#e1f5ff\n    style Secret fill:#ffe1e1</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#controller-annotations","title":"Controller Annotations","text":"<p>Annotations provide controller-specific configuration beyond the Ingress spec. Here are production-critical NGINX examples:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: advanced-config\n  annotations:\n    # Rate limiting (DDoS protection)\n    nginx.ingress.kubernetes.io/limit-rps: \"100\"\n\n    # Connection limits\n    nginx.ingress.kubernetes.io/limit-connections: \"10\"\n\n    # Request body size (file uploads)\n    nginx.ingress.kubernetes.io/proxy-body-size: \"50m\"\n\n    # Timeouts\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"60\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"60\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"60\"\n\n    # CORS headers\n    nginx.ingress.kubernetes.io/enable-cors: \"true\"\n    nginx.ingress.kubernetes.io/cors-allow-origin: \"https://app.example.com\"\n\n    # Sticky sessions (affinity)\n    nginx.ingress.kubernetes.io/affinity: \"cookie\"\n    nginx.ingress.kubernetes.io/session-cookie-name: \"route\"\n\n    # Custom error pages\n    nginx.ingress.kubernetes.io/custom-http-errors: \"404,503\"\n    nginx.ingress.kubernetes.io/default-backend: \"custom-error-service\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 8080\n</code></pre> <p>CKA Exam Warning: Annotations are controller-specific. NGINX annotations won't work with Traefik. Always check documentation for your chosen controller.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#gateway-api-the-future-of-kubernetes-networking","title":"Gateway API: The Future of Kubernetes Networking","text":"","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#why-gateway-api-exists","title":"Why Gateway API Exists","text":"<p>Traditional Ingress has served Kubernetes well, but it has fundamental limitations:</p> <p>Ingress Limitations:</p> <ol> <li>Annotations hell: Controller-specific configuration scattered across annotations</li> <li>Limited expressiveness: No header matching, query parameter routing, traffic splitting</li> <li>Single role design: No separation between cluster operators and application developers</li> <li>Protocol limitations: Primarily HTTP/HTTPS, limited gRPC/TCP support</li> </ol> <p>Gateway API Solutions:</p> <ul> <li>Role-oriented: Separate resources for infrastructure (GatewayClass, Gateway) and application routing (HTTPRoute, TCPRoute)</li> <li>Expressive: First-class support for header matching, traffic splitting, redirects, rewrites</li> <li>Extensible: Policy attachment mechanism instead of annotations</li> <li>Protocol-rich: Native support for HTTP, HTTPS, gRPC, TCP, TLS, UDP</li> </ul> <p>As of 2025, Gateway API v1.4.0 is production-ready and rapidly gaining adoption. It's not yet on the CKA exam, but forward-thinking practitioners should learn it now.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#gateway-api-resource-hierarchy","title":"Gateway API Resource Hierarchy","text":"<pre><code>graph TB\n    subgraph \"Infrastructure Resources (Cluster Operator)\"\n        GC[GatewayClass&lt;br/&gt;Cluster-scoped&lt;br/&gt;Controller specification]\n        G[Gateway&lt;br/&gt;Namespace-scoped&lt;br/&gt;Load balancer config]\n    end\n\n    subgraph \"Routing Resources (Application Developer)\"\n        HR[HTTPRoute&lt;br/&gt;HTTP/HTTPS routing rules]\n        GR[GRPCRoute&lt;br/&gt;gRPC-specific routing]\n        TR[TCPRoute&lt;br/&gt;TCP routing rules]\n        UR[UDPRoute&lt;br/&gt;UDP routing rules]\n    end\n\n    subgraph \"Policy Resources (Security/Operations)\"\n        BP[BackendPolicy&lt;br/&gt;Retry, timeout, circuit breaking]\n        TP[TimeoutPolicy&lt;br/&gt;Request/idle timeouts]\n        RP[RateLimitPolicy&lt;br/&gt;Traffic shaping]\n    end\n\n    GC --&gt;|Defines controller| G\n    G --&gt;|Attaches to| HR\n    G --&gt;|Attaches to| GR\n    G --&gt;|Attaches to| TR\n    G --&gt;|Attaches to| UR\n\n    HR -.-&gt;|Policy attachment| BP\n    HR -.-&gt;|Policy attachment| TP\n    HR -.-&gt;|Policy attachment| RP\n\n    style GC fill:#ffe1e1\n    style G fill:#e1f5ff\n    style HR fill:#e1ffe1\n    style BP fill:#fff4e1</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#core-resources","title":"Core Resources","text":"<p>1. GatewayClass (Cluster Operator)</p> <p>Defines which controller implementation to use\u2014similar to IngressClass but more powerful:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: contour-gateway\nspec:\n  controllerName: projectcontour.io/gateway-controller\n  parametersRef:\n    group: projectcontour.io\n    kind: ContourDeployment\n    name: contour-gateway-config\n</code></pre> <p>2. Gateway (Platform Team)</p> <p>Represents the load balancer configuration\u2014infrastructure that application teams will use:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: production-gateway\n  namespace: infrastructure\nspec:\n  gatewayClassName: contour-gateway\n  listeners:\n  # HTTPS listener\n  - name: https\n    protocol: HTTPS\n    port: 443\n    hostname: \"*.example.com\"\n    tls:\n      mode: Terminate\n      certificateRefs:\n      - kind: Secret\n        name: wildcard-tls-cert\n    allowedRoutes:\n      namespaces:\n        from: All  # Allow routes from any namespace\n\n  # HTTP listener (redirect to HTTPS)\n  - name: http\n    protocol: HTTP\n    port: 80\n    hostname: \"*.example.com\"\n    allowedRoutes:\n      namespaces:\n        from: All\n</code></pre> <p>3. HTTPRoute (Application Developers)</p> <p>Defines HTTP routing rules\u2014what application teams interact with:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: api-route\n  namespace: production\nspec:\n  parentRefs:\n  - name: production-gateway\n    namespace: infrastructure\n\n  hostnames:\n  - api.example.com\n\n  rules:\n  # Header-based routing\n  - matches:\n    - headers:\n      - name: X-API-Version\n        value: v2\n    backendRefs:\n    - name: api-v2-service\n      port: 8080\n\n  # Path-based routing with weight (traffic splitting)\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /users\n    backendRefs:\n    - name: users-v1-service\n      port: 80\n      weight: 90  # 90% of traffic\n    - name: users-v2-service\n      port: 80\n      weight: 10  # 10% of traffic (canary)\n\n  # Query parameter matching\n  - matches:\n    - queryParams:\n      - name: beta\n        value: \"true\"\n    backendRefs:\n    - name: beta-service\n      port: 8080\n</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#advantages-over-ingress","title":"Advantages Over Ingress","text":"Capability Ingress Gateway API Role separation No Yes (GatewayClass/Gateway vs Routes) Header matching Via annotations Native in HTTPRoute Traffic splitting Via annotations Native with weights Query parameters Not supported Native matching Cross-namespace routing Limited First-class support Retry/timeout policies Via annotations Separate policy resources Protocol support HTTP/HTTPS HTTP, HTTPS, gRPC, TCP, UDP, TLS Redirect/rewrite Via annotations Native in rules <p>Real-World Example:</p> <pre><code># Gateway API: Native canary deployment\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: canary-deployment\nspec:\n  parentRefs:\n  - name: production-gateway\n  rules:\n  - backendRefs:\n    - name: stable-service\n      port: 80\n      weight: 95  # 95% to stable\n    - name: canary-service\n      port: 80\n      weight: 5   # 5% to canary\n</code></pre> <pre><code># Ingress: Requires complex annotations\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: canary-deployment\n  annotations:\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"5\"\nspec:\n  # ... rest of configuration\n</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#migration-strategies","title":"Migration Strategies","text":"<p>1. Parallel Adoption (Recommended)</p> <p>Run Ingress and Gateway API side-by-side:</p> <pre><code># Install Gateway API CRDs\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.4.0/standard-install.yaml\n\n# Install Contour Gateway (excellent Gateway API support)\nkubectl apply -f https://projectcontour.io/quickstart/contour-gateway.yaml\n\n# Migrate services incrementally\n# - New services use Gateway API\n# - Existing services remain on Ingress\n# - Migrate critical services last\n</code></pre> <p>2. Testing Strategy</p> <pre><code># Create test Gateway for validation\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: test-gateway\n  namespace: testing\nspec:\n  gatewayClassName: contour-gateway\n  listeners:\n  - name: http\n    protocol: HTTP\n    port: 8080  # Different port to avoid conflicts\n    allowedRoutes:\n      namespaces:\n        from: Same\n\n---\n# Test HTTPRoute\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: test-route\n  namespace: testing\nspec:\n  parentRefs:\n  - name: test-gateway\n  rules:\n  - backendRefs:\n    - name: test-service\n      port: 80\n</code></pre> <p>3. Production Cutover</p> <p>Once validated:</p> <ol> <li>Deploy Gateway resource in production namespace</li> <li>Create HTTPRoutes for all services</li> <li>Update DNS to point to new Gateway load balancer IP</li> <li>Monitor traffic carefully (use weights for gradual shift)</li> <li>Deprecate Ingress resources after validation period</li> </ol>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#2025-maturity-status","title":"2025 Maturity Status","text":"<p>Gateway API v1.4.0 (Current):</p> <ul> <li>\u2705 GA: GatewayClass, Gateway, HTTPRoute</li> <li>\u2705 Beta: GRPCRoute, ReferenceGrant</li> <li>\u2705 Alpha: TCPRoute, UDPRoute, TLSRoute, BackendTLSPolicy</li> </ul> <p>Production-Ready Controllers (2025):</p> Controller Gateway API Support Maturity Contour v1.4.0, excellent Production-ready Istio v1.4.0, comprehensive Production-ready NGINX Gateway Fabric v1.4.0, growing Production-ready Envoy Gateway v1.4.0, native Production-ready Traefik v1.4.0, mature Production-ready <p>Recommendation: Gateway API is ready for production in 2025. If starting new projects, prefer Gateway API over traditional Ingress.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#advanced-patterns","title":"Advanced Patterns","text":"","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#path-rewriting","title":"Path Rewriting","text":"<p>Transform incoming paths before forwarding to backends:</p> <pre><code># NGINX Ingress: Annotation-based\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: path-rewrite\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      # /api/users -&gt; /users\n      - path: /api(/|$)(.*)\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: backend-service\n            port:\n              number: 8080\n</code></pre> <pre><code># Gateway API: Native support\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: path-rewrite\nspec:\n  parentRefs:\n  - name: production-gateway\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /api\n    filters:\n    - type: URLRewrite\n      urlRewrite:\n        path:\n          type: ReplacePrefixMatch\n          replacePrefixMatch: /\n    backendRefs:\n    - name: backend-service\n      port: 8080\n</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#traffic-splitting-canary-deployments","title":"Traffic Splitting (Canary Deployments)","text":"<p>Gradually shift traffic to new versions:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: canary-rollout\n  namespace: production\nspec:\n  parentRefs:\n  - name: production-gateway\n\n  hostnames:\n  - shop.example.com\n\n  rules:\n  - backendRefs:\n    # Stable version (90%)\n    - name: shop-v1\n      port: 80\n      weight: 90\n\n    # Canary version (10%)\n    - name: shop-v2\n      port: 80\n      weight: 10\n\n---\n# Progressive rollout plan:\n# Week 1: 90/10 split\n# Week 2: 75/25 split (update weights)\n# Week 3: 50/50 split\n# Week 4: 0/100 split (full cutover)\n# Week 5: Remove shop-v1 service\n</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#header-manipulation","title":"Header Manipulation","text":"<p>Add, modify, or remove headers:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: header-manipulation\nspec:\n  parentRefs:\n  - name: production-gateway\n\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /api\n\n    filters:\n    # Add custom headers\n    - type: RequestHeaderModifier\n      requestHeaderModifier:\n        add:\n        - name: X-Request-Source\n          value: gateway-api\n        - name: X-Forwarded-Proto\n          value: https\n\n        # Remove headers\n        remove:\n        - X-Internal-Token\n\n    # Response headers\n    - type: ResponseHeaderModifier\n      responseHeaderModifier:\n        add:\n        - name: X-Cache-Status\n          value: MISS\n        - name: Strict-Transport-Security\n          value: max-age=31536000; includeSubDomains\n\n    backendRefs:\n    - name: api-service\n      port: 8080\n</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#rate-limiting","title":"Rate Limiting","text":"<p>Protect backends from traffic spikes:</p> <pre><code># NGINX Ingress: Annotation-based\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rate-limited\n  annotations:\n    # 10 requests per second per IP\n    nginx.ingress.kubernetes.io/limit-rps: \"10\"\n\n    # Burst capacity\n    nginx.ingress.kubernetes.io/limit-burst-multiplier: \"5\"\n\n    # Connection limits\n    nginx.ingress.kubernetes.io/limit-connections: \"20\"\n\n    # Custom error response\n    nginx.ingress.kubernetes.io/limit-rate-status-code: \"429\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 8080\n</code></pre> <p>For Gateway API, rate limiting is typically implemented via policy attachment (implementation-specific):</p> <pre><code># Example: Contour BackendPolicy (implementation-specific)\napiVersion: projectcontour.io/v1alpha1\nkind: RateLimitPolicy\nmetadata:\n  name: api-rate-limit\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    name: api-route\n  rateLimits:\n  - limit: 100\n    unit: minute\n    local:\n      type: PerIP\n</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#cert-manager-integration","title":"cert-manager Integration","text":"<p>Automate TLS certificate management:</p> <pre><code># Install cert-manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.15.0/cert-manager.yaml\n\n---\n# Create ClusterIssuer for Let's Encrypt\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: admin@example.com\n    privateKeySecretRef:\n      name: letsencrypt-prod-key\n    solvers:\n    - http01:\n        ingress:\n          ingressClassName: nginx\n\n---\n# Ingress with automatic certificate\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: auto-tls\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - secure.example.com\n    secretName: secure-example-tls  # cert-manager creates this automatically\n  rules:\n  - host: secure.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: web-service\n            port:\n              number: 80\n</code></pre> <p>Certificate Lifecycle:</p> <ol> <li>cert-manager watches Ingress resources</li> <li>Detects <code>cert-manager.io/cluster-issuer</code> annotation</li> <li>Initiates ACME challenge with Let's Encrypt</li> <li>Creates temporary Ingress for HTTP-01 challenge</li> <li>Obtains certificate and stores in Secret</li> <li>Automatically renews 30 days before expiration</li> </ol> <pre><code>graph TB\n    subgraph \"cert-manager Automation\"\n        CM[cert-manager Controller]\n        CI[ClusterIssuer&lt;br/&gt;Let's Encrypt config]\n    end\n\n    subgraph \"Ingress Resource\"\n        I[Ingress with annotation&lt;br/&gt;cert-manager.io/cluster-issuer]\n    end\n\n    subgraph \"ACME Challenge\"\n        LE[Let's Encrypt CA]\n        Challenge[HTTP-01 Challenge&lt;br/&gt;Temporary Ingress]\n    end\n\n    subgraph \"Certificate Storage\"\n        Secret[Secret&lt;br/&gt;tls.crt + tls.key]\n    end\n\n    subgraph \"Renewal Process\"\n        Renewal[Auto-renewal&lt;br/&gt;30 days before expiry]\n    end\n\n    I --&gt;|Watches| CM\n    CM -.-&gt;|Uses| CI\n    CM --&gt;|Initiates| Challenge\n    Challenge --&gt;|Requests cert| LE\n    LE --&gt;|Validates &amp; issues| CM\n    CM --&gt;|Creates/updates| Secret\n    Secret --&gt;|Used by| I\n\n    CM --&gt;|Monitors expiry| Renewal\n    Renewal -.-&gt;|Triggers| Challenge\n\n    style CM fill:#e1f5ff\n    style Challenge fill:#ffe1e1\n    style Secret fill:#e1ffe1</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#cka-exam-skills","title":"CKA Exam Skills","text":"","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#creating-ingress-imperatively","title":"Creating Ingress Imperatively","text":"<p>The CKA is a hands-on performance exam. Speed matters. Here's how to create Ingress resources quickly:</p> <pre><code># Basic Ingress (doesn't exist in kubectl create, use dry-run)\nkubectl create ingress simple-ingress \\\n  --rule=\"example.com/path*=service:80\" \\\n  --dry-run=client -o yaml &gt; ingress.yaml\n\n# Edit and apply\nvim ingress.yaml  # Add ingressClassName: nginx\nkubectl apply -f ingress.yaml\n\n# Faster: Use kubectl run with generator (legacy but useful)\nkubectl create ingress api-ingress \\\n  --class=nginx \\\n  --rule=\"api.example.com/v1*=api-svc:8080\" \\\n  --rule=\"api.example.com/v2*=api-v2-svc:8080\"\n\n# Multiple hosts\nkubectl create ingress multi-host \\\n  --class=nginx \\\n  --rule=\"app1.example.com/*=app1-svc:80\" \\\n  --rule=\"app2.example.com/*=app2-svc:80\"\n\n# With TLS\nkubectl create ingress tls-ingress \\\n  --class=nginx \\\n  --rule=\"secure.example.com/*=web-svc:443,tls=secure-tls-secret\"\n</code></pre> <p>Exam Tip: Practice these commands repeatedly. In the exam, you can't afford to write 40 lines of YAML for a simple Ingress.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#troubleshooting-workflow","title":"Troubleshooting Workflow","text":"<p>When Ingress isn't working, follow this systematic approach:</p> <pre><code># 1. Verify Ingress controller is running\nkubectl get pods -n ingress-nginx\n# Should show: ingress-nginx-controller-xxx Running\n\n# 2. Check Ingress resource exists and is valid\nkubectl get ingress -A\nkubectl describe ingress &lt;name&gt; -n &lt;namespace&gt;\n# Look for: Address field populated, Events for errors\n\n# 3. Verify IngressClass configuration\nkubectl get ingressclass\n# Should match spec.ingressClassName in your Ingress\n\n# 4. Check backend Service exists\nkubectl get svc &lt;service-name&gt; -n &lt;namespace&gt;\n# Verify port matches Ingress backend port\n\n# 5. Test backend Service directly (bypass Ingress)\nkubectl run test-pod --image=curlimages/curl -it --rm -- \\\n  curl http://&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local:&lt;port&gt;\n# If this fails, problem is backend, not Ingress\n\n# 6. Check controller logs\nkubectl logs -n ingress-nginx deployment/ingress-nginx-controller --tail=50\n# Look for: Configuration errors, backend sync issues\n\n# 7. Verify DNS/hostname resolution\nkubectl run test-dns --image=busybox -it --rm -- nslookup &lt;hostname&gt;\n\n# 8. Test from within cluster\nkubectl run test-curl --image=curlimages/curl -it --rm -- \\\n  curl -H \"Host: example.com\" http://&lt;ingress-controller-service-ip&gt;\n\n# 9. Check Ingress controller Service\nkubectl get svc -n ingress-nginx\n# Type should be LoadBalancer or NodePort\n# EXTERNAL-IP should be assigned (if LoadBalancer)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#common-misconfigurations","title":"Common Misconfigurations","text":"<p>1. Missing IngressClass</p> <pre><code># \u274c WRONG: No ingressClassName specified\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: broken-ingress\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: web-svc\n            port:\n              number: 80\n\n# \u2705 CORRECT: Explicitly set ingressClassName\nspec:\n  ingressClassName: nginx  # Critical in v1 API\n  rules:\n  # ... rest of spec\n</code></pre> <p>Symptom: Ingress exists but no Address assigned, controller ignores it.</p> <p>2. Wrong PathType</p> <pre><code># \u274c WRONG: PathType doesn't match intent\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /api\n        pathType: Exact  # Only matches /api, not /api/users\n        backend:\n          service:\n            name: api-svc\n            port:\n              number: 8080\n\n# \u2705 CORRECT: Use Prefix for path hierarchies\n      - path: /api\n        pathType: Prefix  # Matches /api, /api/, /api/users, etc.\n</code></pre> <p>Symptom: 404 errors for valid paths.</p> <p>3. Service Port Mismatch</p> <pre><code># Service definition\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-svc\nspec:\n  ports:\n  - port: 8080  # Service port\n    targetPort: 80  # Container port\n\n---\n# \u274c WRONG: Using targetPort in Ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: web-svc\n            port:\n              number: 80  # WRONG: Should be 8080 (Service port)\n\n# \u2705 CORRECT: Use Service port, not targetPort\n            port:\n              number: 8080  # Matches Service spec.ports[0].port\n</code></pre> <p>Symptom: 503 Service Temporarily Unavailable.</p> <p>4. Namespace Confusion</p> <pre><code># \u274c WRONG: Ingress in different namespace than Service\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: cross-namespace\n  namespace: default  # Ingress is here\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: web-svc  # Service is in 'production' namespace\n            port:\n              number: 80\n\n# \u2705 CORRECT: Ingress must be in same namespace as Service\nmetadata:\n  namespace: production  # Match Service namespace\n</code></pre> <p>Symptom: Backend service not found errors.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#fast-debugging-techniques","title":"Fast Debugging Techniques","text":"<p>Technique 1: Quick validation pod</p> <pre><code># Create debug pod with networking tools\nkubectl run debug --image=nicolaka/netshoot -it --rm -- bash\n\n# Inside pod:\n# Test Service directly\ncurl http://api-svc.production.svc.cluster.local:8080\n\n# Test Ingress controller\ncurl -H \"Host: api.example.com\" http://&lt;ingress-controller-ip&gt;\n\n# DNS resolution\nnslookup api.example.com\n</code></pre> <p>Technique 2: Controller dry-run</p> <pre><code># Check what controller would generate (NGINX)\nkubectl exec -n ingress-nginx deployment/ingress-nginx-controller -- \\\n  cat /etc/nginx/nginx.conf | grep -A 20 \"server_name example.com\"\n</code></pre> <p>Technique 3: Event watching</p> <pre><code># Watch Ingress events in real-time\nkubectl get events -n production --watch | grep ingress\n\n# Filter for errors\nkubectl get events -n production --field-selector type=Warning\n</code></pre> <p>Technique 4: Compare working vs broken</p> <pre><code># Export working Ingress\nkubectl get ingress working-ingress -o yaml &gt; working.yaml\n\n# Export broken Ingress\nkubectl get ingress broken-ingress -o yaml &gt; broken.yaml\n\n# Compare\ndiff working.yaml broken.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kubernetes-ingress-gateway-api/#conclusion","title":"Conclusion","text":"<p>Mastering Ingress and Gateway API is essential for production Kubernetes operations and CKA exam success. Here's your action plan:</p> <p>For CKA Exam Preparation:</p> <ol> <li>Practice creating Ingress resources imperatively with <code>kubectl create ingress</code></li> <li>Master the troubleshooting workflow\u2014it's predictable and systematic</li> <li>Understand PathType semantics (Exact vs Prefix)</li> <li>Know how to configure TLS with Secrets</li> <li>Practice with NGINX Ingress Controller (most common in exams)</li> </ol> <p>For Production Systems:</p> <ol> <li>Use cert-manager for automated TLS certificate management</li> <li>Implement rate limiting and request timeouts via annotations</li> <li>Monitor controller logs and metrics</li> <li>Consider Gateway API for new projects\u2014it's the future</li> <li>Separate infrastructure (Gateway) from application routing (HTTPRoute)</li> </ol> <p>Migration Path:</p> <ul> <li>2025-2026: Run Ingress and Gateway API in parallel</li> <li>2026+: Migrate to Gateway API as primary routing mechanism</li> <li>Long-term: Gateway API will likely supersede Ingress in Kubernetes exams</li> </ul> <p>The networking landscape is evolving, but the fundamentals remain: understand routing semantics, master troubleshooting, and build with production resilience in mind. Whether you're preparing for the CKA or architecting enterprise systems, these skills are your foundation.</p> <p>Now go create some Ingress resources, break them, fix them, and build confidence through hands-on practice. That's how mastery happens.</p> <p>Next in the CKA series: Network Policies and CNI Plugins\u2014securing pod-to-pod communication and understanding cluster networking foundations.</p>","tags":["kubernetes","k8s","cka-prep","ingress","gateway-api","networking","traffic-management"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/","title":"Kustomize: Template-Free Kubernetes Configuration Management","text":"<p>Master Kustomize for declarative, overlay-based configuration customization without templates</p> <p>Kustomize is Kubernetes' native configuration management tool that allows you to customize application configurations without templates. Unlike Helm, Kustomize uses a template-free approach with overlays and patches, making it ideal for GitOps workflows and environment-specific customizations.</p> <p>Why Kustomize Matters for CKA: - Built into <code>kubectl</code> (no separate installation required) - Template-free configuration reduces complexity - GitOps-friendly with declarative overlays - Strategic merge and JSON patches for targeted modifications - ConfigMap and Secret generators for dynamic content</p>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#what-is-kustomize","title":"\ud83c\udfaf What is Kustomize?","text":"<p>Kustomize is a standalone tool integrated into <code>kubectl</code> that lets you customize Kubernetes YAML configurations through:</p> <ul> <li>Bases: Common configuration shared across environments</li> <li>Overlays: Environment-specific customizations</li> <li>Patches: Targeted modifications to base resources</li> <li>Transformers: Cross-cutting changes (labels, annotations, namespaces)</li> <li>Generators: Dynamic ConfigMap and Secret creation</li> </ul> <pre><code>graph TD\n    A[Base Configuration] --&gt; B[Dev Overlay]\n    A --&gt; C[Staging Overlay]\n    A --&gt; D[Production Overlay]\n\n    B --&gt; B1[Dev Resources]\n    C --&gt; C1[Staging Resources]\n    D --&gt; D1[Prod Resources]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e8\n    style D fill:#ffe5e5</code></pre> <p>Key Philosophy: - No templating: Pure YAML with no <code>{{ }}</code> syntax - Declarative overlays: Layer customizations instead of conditionals - Reusable bases: Share common configuration across environments - Kubernetes-native: Uses standard Kubernetes resources</p>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#kustomize-directory-structure","title":"\ud83d\udcc2 Kustomize Directory Structure","text":"<pre><code>kustomize-app/\n\u251c\u2500\u2500 base/                      # Common configuration\n\u2502   \u251c\u2500\u2500 kustomization.yaml     # Base kustomization file\n\u2502   \u251c\u2500\u2500 deployment.yaml        # Base deployment\n\u2502   \u251c\u2500\u2500 service.yaml           # Base service\n\u2502   \u2514\u2500\u2500 configmap.yaml         # Base config\n\u2502\n\u2514\u2500\u2500 overlays/                  # Environment-specific customizations\n    \u251c\u2500\u2500 dev/\n    \u2502   \u251c\u2500\u2500 kustomization.yaml # Dev customizations\n    \u2502   \u251c\u2500\u2500 replica-count.yaml # Dev-specific patches\n    \u2502   \u2514\u2500\u2500 dev-config.env     # Dev environment variables\n    \u2502\n    \u251c\u2500\u2500 staging/\n    \u2502   \u251c\u2500\u2500 kustomization.yaml\n    \u2502   \u2514\u2500\u2500 staging-patch.yaml\n    \u2502\n    \u2514\u2500\u2500 production/\n        \u251c\u2500\u2500 kustomization.yaml\n        \u251c\u2500\u2500 prod-patch.yaml\n        \u2514\u2500\u2500 prod-secrets.yaml\n</code></pre> <pre><code>graph LR\n    A[base/] --&gt; A1[kustomization.yaml]\n    A --&gt; A2[deployment.yaml]\n    A --&gt; A3[service.yaml]\n\n    B[overlays/dev/] --&gt; B1[kustomization.yaml]\n    B1 -.references.-&gt; A1\n    B --&gt; B2[dev patches]\n\n    C[overlays/prod/] --&gt; C1[kustomization.yaml]\n    C1 -.references.-&gt; A1\n    C --&gt; C2[prod patches]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#kustomization-file-structure","title":"\ud83e\udde9 Kustomization File Structure","text":"<p>The <code>kustomization.yaml</code> file is the heart of Kustomize configuration:</p>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#base-kustomization","title":"Base Kustomization","text":"<pre><code># base/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# Resources to include\nresources:\n  - deployment.yaml\n  - service.yaml\n  - configmap.yaml\n\n# Common labels applied to all resources\ncommonLabels:\n  app: myapp\n  managed-by: kustomize\n\n# Common annotations\ncommonAnnotations:\n  description: \"Base configuration for myapp\"\n\n# Namespace for all resources\nnamespace: default\n\n# Name prefix for all resources\nnamePrefix: myapp-\n\n# Name suffix\nnameSuffix: -v1\n\n# Images to replace\nimages:\n  - name: nginx\n    newName: nginx\n    newTag: \"1.27\"\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#overlay-kustomization","title":"Overlay Kustomization","text":"<pre><code># overlays/production/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# Reference base configuration\nbases:\n  - ../../base\n\n# Override namespace\nnamespace: production\n\n# Override name suffix\nnameSuffix: -prod\n\n# Patches for production-specific changes\npatchesStrategicMerge:\n  - replica-count.yaml\n  - resource-limits.yaml\n\n# JSON patches for targeted modifications\npatchesJson6902:\n  - target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: myapp\n    path: add-security-context.yaml\n\n# ConfigMap generator\nconfigMapGenerator:\n  - name: app-config\n    envs:\n      - prod-config.env\n\n# Secret generator\nsecretGenerator:\n  - name: app-secrets\n    literals:\n      - api-key=prod-key-placeholder\n\n# Images for production\nimages:\n  - name: nginx\n    newName: nginx\n    newTag: \"1.27-alpine\"\n\n# Replicas transformer\nreplicas:\n  - name: myapp\n    count: 5\n</code></pre> <pre><code>graph TD\n    A[kustomization.yaml] --&gt; B[resources]\n    A --&gt; C[commonLabels]\n    A --&gt; D[namespace]\n    A --&gt; E[patches]\n    A --&gt; F[generators]\n    A --&gt; G[transformers]\n\n    B --&gt; B1[deployment.yaml]\n    B --&gt; B2[service.yaml]\n\n    E --&gt; E1[strategicMerge]\n    E --&gt; E2[json6902]\n\n    F --&gt; F1[configMapGenerator]\n    F --&gt; F2[secretGenerator]\n\n    G --&gt; G1[images]\n    G --&gt; G2[replicas]\n    G --&gt; G3[namePrefix]\n\n    style A fill:#e1f5ff\n    style E fill:#fff4e1\n    style F fill:#e8f5e8\n    style G fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#patching-strategies","title":"\ud83d\udd27 Patching Strategies","text":"<p>Kustomize supports two primary patching methods:</p>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#1-strategic-merge-patches","title":"1. Strategic Merge Patches","text":"<p>Strategic merge patches overlay fields onto existing resources, merging arrays intelligently.</p> <p>Example: Increase Replicas</p> <pre><code># overlays/production/replica-count.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 5  # Override base replica count\n</code></pre> <p>Example: Add Resource Limits</p> <pre><code># overlays/production/resource-limits.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n      - name: nginx\n        resources:\n          limits:\n            cpu: \"1000m\"\n            memory: \"512Mi\"\n          requests:\n            cpu: \"500m\"\n            memory: \"256Mi\"\n</code></pre> <p>How Strategic Merge Works:</p> <pre><code>graph LR\n    A[Base YAML] --&gt; C[Merged Result]\n    B[Patch YAML] --&gt; C\n\n    A1[replicas: 2] --&gt; C1[replicas: 5]\n    B1[replicas: 5] --&gt; C1\n\n    A2[image: nginx:1.27] --&gt; C2[image: nginx:1.27-alpine]\n    B2[image: nginx:1.27-alpine] --&gt; C2\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#2-json6902-patches","title":"2. JSON6902 Patches","text":"<p>JSON6902 patches use JSONPath-like syntax for precise modifications (add, remove, replace, move, copy, test).</p> <p>Example: Add Security Context</p> <pre><code># overlays/production/add-security-context.yaml\n- op: add\n  path: /spec/template/spec/securityContext\n  value:\n    runAsNonRoot: true\n    runAsUser: 1000\n    fsGroup: 2000\n\n- op: add\n  path: /spec/template/spec/containers/0/securityContext\n  value:\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop: [\"ALL\"]\n    readOnlyRootFilesystem: true\n</code></pre> <p>Example: Replace Image Tag</p> <pre><code>- op: replace\n  path: /spec/template/spec/containers/0/image\n  value: nginx:1.27-alpine\n</code></pre> <p>Example: Remove Resource</p> <pre><code>- op: remove\n  path: /spec/template/spec/containers/0/resources/limits/cpu\n</code></pre> <p>JSON6902 Operations:</p> Operation Description Example add Add new field Add security context remove Remove field Remove resource limit replace Replace value Change image tag move Move field location Reorganize structure copy Copy field value Duplicate configuration test Assert field value Validation before patch","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#transformers","title":"\ud83c\udfa8 Transformers","text":"<p>Transformers apply cross-cutting changes to all resources:</p>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#common-labels-and-annotations","title":"Common Labels and Annotations","text":"<pre><code># kustomization.yaml\ncommonLabels:\n  app: myapp\n  version: v1.2.3\n  environment: production\n\ncommonAnnotations:\n  managed-by: kustomize\n  contact: devops@example.com\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#name-prefix-and-suffix","title":"Name Prefix and Suffix","text":"<pre><code># Add prefix to all resource names\nnamePrefix: prod-\n\n# Add suffix to all resource names\nnameSuffix: -blue-green\n</code></pre> <p>Result: <pre><code>Deployment: myapp \u2192 prod-myapp-blue-green\nService: myapp-svc \u2192 prod-myapp-svc-blue-green\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#namespace-transformer","title":"Namespace Transformer","text":"<pre><code># Apply namespace to all resources\nnamespace: production\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#image-transformer","title":"Image Transformer","text":"<pre><code>images:\n  - name: nginx\n    newName: nginx\n    newTag: \"1.27-alpine\"\n\n  - name: redis\n    newName: redis\n    newTag: \"7.2\"\n    digest: sha256:abc123...  # Optional: use digest for immutability\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#replica-transformer","title":"Replica Transformer","text":"<pre><code>replicas:\n  - name: myapp\n    count: 5\n\n  - name: worker\n    count: 10\n</code></pre> <pre><code>graph TD\n    A[Base Resources] --&gt; B[Apply Transformers]\n\n    B --&gt; C[commonLabels]\n    B --&gt; D[namespace]\n    B --&gt; E[namePrefix/Suffix]\n    B --&gt; F[images]\n    B --&gt; G[replicas]\n\n    C --&gt; H[Transformed Resources]\n    D --&gt; H\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style H fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#generators","title":"\ud83d\udd11 Generators","text":"<p>Generators create ConfigMaps and Secrets dynamically:</p>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#configmap-generator","title":"ConfigMap Generator","text":"<p>From Literal Values:</p> <pre><code>configMapGenerator:\n  - name: app-config\n    literals:\n      - LOG_LEVEL=info\n      - MAX_CONNECTIONS=100\n      - TIMEOUT=30s\n</code></pre> <p>From Files:</p> <pre><code>configMapGenerator:\n  - name: app-config\n    files:\n      - application.properties\n      - log4j.xml\n</code></pre> <p>From .env Files:</p> <pre><code>configMapGenerator:\n  - name: app-config\n    envs:\n      - config.env\n</code></pre> <pre><code># config.env\nLOG_LEVEL=info\nMAX_CONNECTIONS=100\nTIMEOUT=30s\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#secret-generator","title":"Secret Generator","text":"<p>From Literal Values:</p> <pre><code>secretGenerator:\n  - name: app-secrets\n    literals:\n      - api-key=secret-key-placeholder\n      - db-password=db-pass-placeholder\n</code></pre> <p>From Files:</p> <pre><code>secretGenerator:\n  - name: tls-secret\n    files:\n      - tls.crt\n      - tls.key\n    type: kubernetes.io/tls\n</code></pre> <p>Immutable Secrets:</p> <pre><code>secretGenerator:\n  - name: app-secrets\n    literals:\n      - api-key=secret-value\n    options:\n      immutable: true\n      disableNameSuffixHash: false  # Append hash to name\n</code></pre> <p>Generated ConfigMap/Secret Names:</p> <p>Kustomize automatically appends a content hash to generated ConfigMaps and Secrets:</p> <pre><code>app-config \u2192 app-config-k4hf9g6bk2\napp-secrets \u2192 app-secrets-2f75hb8tc4\n</code></pre> <p>This triggers rolling updates when content changes (name change forces pod restart).</p> <pre><code>graph LR\n    A[config.env] --&gt; B[configMapGenerator]\n    C[secrets.txt] --&gt; D[secretGenerator]\n\n    B --&gt; E[app-config-k4hf9g6bk2]\n    D --&gt; F[app-secrets-2f75hb8tc4]\n\n    E --&gt; G[Deployment references]\n    F --&gt; G\n\n    G --&gt; H[Pods restart on hash change]\n\n    style A fill:#e1f5ff\n    style C fill:#ffe5e5\n    style E fill:#e8f5e8\n    style F fill:#fff4e1</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#kustomize-workflow","title":"\ud83d\ude80 Kustomize Workflow","text":"","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#basic-commands","title":"Basic Commands","text":"<pre><code># Build kustomized YAML (preview without applying)\nkubectl kustomize overlays/production\n\n# Apply kustomized configuration\nkubectl apply -k overlays/production\n\n# Build and save to file\nkubectl kustomize overlays/production &gt; prod-manifests.yaml\n\n# Diff before applying\nkubectl diff -k overlays/production\n\n# Delete resources\nkubectl delete -k overlays/production\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#validation-and-debugging","title":"Validation and Debugging","text":"<pre><code># Validate kustomization structure\nkustomize build overlays/production\n\n# Dry-run to check for errors\nkubectl apply -k overlays/production --dry-run=client -o yaml\n\n# Server-side dry-run with validation\nkubectl apply -k overlays/production --dry-run=server\n\n# Show differences between base and overlay\ndiff &lt;(kubectl kustomize base) &lt;(kubectl kustomize overlays/production)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#gitops-with-kustomize","title":"\ud83c\udfd7\ufe0f GitOps with Kustomize","text":"<p>Kustomize's template-free approach makes it ideal for GitOps:</p>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#repository-structure","title":"Repository Structure","text":"<pre><code>gitops-repo/\n\u251c\u2500\u2500 apps/\n\u2502   \u251c\u2500\u2500 frontend/\n\u2502   \u2502   \u251c\u2500\u2500 base/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 service.yaml\n\u2502   \u2502   \u2514\u2500\u2500 overlays/\n\u2502   \u2502       \u251c\u2500\u2500 dev/\n\u2502   \u2502       \u251c\u2500\u2500 staging/\n\u2502   \u2502       \u2514\u2500\u2500 production/\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 backend/\n\u2502       \u251c\u2500\u2500 base/\n\u2502       \u2514\u2500\u2500 overlays/\n\u2502\n\u2514\u2500\u2500 infrastructure/\n    \u251c\u2500\u2500 namespaces/\n    \u251c\u2500\u2500 rbac/\n    \u2514\u2500\u2500 monitoring/\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#gitops-workflow","title":"GitOps Workflow","text":"<pre><code>graph TD\n    A[Developer commits to Git] --&gt; B[CI Pipeline]\n    B --&gt; C[Update Kustomize overlay]\n    C --&gt; D[Git push to config repo]\n\n    D --&gt; E[GitOps Operator watches repo]\n    E --&gt; F[Detect changes in overlays]\n    F --&gt; G[kubectl apply -k overlay]\n\n    G --&gt; H[Kubernetes cluster]\n    H --&gt; I[Deployment updated]\n\n    I --&gt; J[ConfigMap hash changed?]\n    J --&gt;|Yes| K[Rolling restart pods]\n    J --&gt;|No| L[No pod restart]\n\n    style A fill:#e1f5ff\n    style E fill:#fff4e1\n    style H fill:#e8f5e8\n    style I fill:#ffe5e5</code></pre> <p>Benefits for GitOps:</p> <ul> <li>\u2705 Declarative: All configuration in Git</li> <li>\u2705 Auditable: Git history tracks all changes</li> <li>\u2705 Reviewable: Pull requests for config changes</li> <li>\u2705 Rollback-friendly: Git revert for instant rollback</li> <li>\u2705 Environment parity: Overlays ensure consistency</li> </ul>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#common-cka-exam-tasks","title":"\ud83c\udfaf Common CKA Exam Tasks","text":"","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#task-1-create-base-configuration","title":"Task 1: Create Base Configuration","text":"<pre><code># Create directory structure\nmkdir -p kustomize-app/base\ncd kustomize-app/base\n\n# Create deployment\ncat &lt;&lt;EOF &gt; deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.27\n        ports:\n        - containerPort: 80\nEOF\n\n# Create service\ncat &lt;&lt;EOF &gt; service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 80\nEOF\n\n# Create kustomization\ncat &lt;&lt;EOF &gt; kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - deployment.yaml\n  - service.yaml\n\ncommonLabels:\n  app: nginx\n  managed-by: kustomize\nEOF\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#task-2-create-production-overlay","title":"Task 2: Create Production Overlay","text":"<pre><code># Create overlay directory\nmkdir -p ../overlays/production\ncd ../overlays/production\n\n# Create replica patch\ncat &lt;&lt;EOF &gt; replica-patch.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 5\nEOF\n\n# Create production kustomization\ncat &lt;&lt;EOF &gt; kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nbases:\n  - ../../base\n\nnamespace: production\n\nnameSuffix: -prod\n\npatchesStrategicMerge:\n  - replica-patch.yaml\n\nimages:\n  - name: nginx\n    newTag: \"1.27-alpine\"\n\ncommonLabels:\n  environment: production\nEOF\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#task-3-generate-configmap-from-environment-file","title":"Task 3: Generate ConfigMap from Environment File","text":"<pre><code># Create config file\ncat &lt;&lt;EOF &gt; prod-config.env\nLOG_LEVEL=info\nMAX_WORKERS=10\nCACHE_ENABLED=true\nEOF\n\n# Add to kustomization\ncat &lt;&lt;EOF &gt;&gt; kustomization.yaml\n\nconfigMapGenerator:\n  - name: app-config\n    envs:\n      - prod-config.env\nEOF\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#task-4-apply-and-verify","title":"Task 4: Apply and Verify","text":"<pre><code># Preview kustomized output\nkubectl kustomize .\n\n# Apply configuration\nkubectl apply -k .\n\n# Verify resources\nkubectl get deployments,services,configmaps -n production\n\n# Check generated ConfigMap name (with hash)\nkubectl get configmap -n production\n# Output: app-config-k4hf9g6bk2\n\n# Verify deployment uses correct ConfigMap\nkubectl describe deployment nginx-prod -n production | grep -A5 ConfigMap\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#advanced-patterns","title":"\ud83d\udd0d Advanced Patterns","text":"","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#multi-base-composition","title":"Multi-Base Composition","text":"<pre><code># overlays/production/kustomization.yaml\nbases:\n  - ../../base/app\n  - ../../base/database\n  - ../../base/monitoring\n\n# Combine multiple bases into single overlay\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#component-reuse","title":"Component Reuse","text":"<pre><code># components/monitoring/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1alpha1\nkind: Component\n\nresources:\n  - servicemonitor.yaml\n  - prometheusrule.yaml\n\n# Use in overlay:\ncomponents:\n  - ../../components/monitoring\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#helm-chart-integration","title":"Helm Chart Integration","text":"<pre><code># Use Helm charts with Kustomize\nhelmCharts:\n  - name: prometheus\n    repo: https://prometheus-community.github.io/helm-charts\n    version: 25.8.0\n    releaseName: prometheus\n    namespace: monitoring\n    valuesInline:\n      server:\n        persistentVolume:\n          enabled: false\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#variable-substitution","title":"Variable Substitution","text":"<pre><code># kustomization.yaml\nvars:\n  - name: APP_NAME\n    objref:\n      kind: Deployment\n      name: myapp\n      apiVersion: apps/v1\n    fieldref:\n      fieldpath: metadata.name\n\n# Use in resources\nconfigMapGenerator:\n  - name: app-vars\n    literals:\n      - app-name=$(APP_NAME)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#practice-exercises","title":"\ud83d\udcdd Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#exercise-1-basic-kustomize-setup","title":"Exercise 1: Basic Kustomize Setup","text":"<p>Task: Create a base configuration for an Nginx deployment with 2 replicas, then create a production overlay that increases replicas to 5 and changes the namespace to <code>production</code>.</p> <p>Solution:</p> <pre><code># Create base\nmkdir -p myapp/{base,overlays/production}\n\n# Base deployment\ncat &lt;&lt;EOF &gt; myapp/base/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.27\nEOF\n\n# Base kustomization\ncat &lt;&lt;EOF &gt; myapp/base/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - deployment.yaml\nEOF\n\n# Production overlay patch\ncat &lt;&lt;EOF &gt; myapp/overlays/production/replica-patch.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 5\nEOF\n\n# Production kustomization\ncat &lt;&lt;EOF &gt; myapp/overlays/production/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nbases:\n  - ../../base\nnamespace: production\npatchesStrategicMerge:\n  - replica-patch.yaml\nEOF\n\n# Apply\nkubectl apply -k myapp/overlays/production\n\n# Verify\nkubectl get deployment nginx -n production\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#exercise-2-configmap-generator","title":"Exercise 2: ConfigMap Generator","text":"<p>Task: Create a ConfigMap using a generator from an environment file, and ensure the deployment references it. The ConfigMap should contain <code>APP_ENV=production</code> and <code>LOG_LEVEL=info</code>.</p> <p>Solution:</p> <pre><code># Create config file\ncat &lt;&lt;EOF &gt; myapp/overlays/production/prod.env\nAPP_ENV=production\nLOG_LEVEL=info\nEOF\n\n# Update production kustomization\ncat &lt;&lt;EOF &gt;&gt; myapp/overlays/production/kustomization.yaml\n\nconfigMapGenerator:\n  - name: app-config\n    envs:\n      - prod.env\nEOF\n\n# Update deployment to use ConfigMap\ncat &lt;&lt;EOF &gt; myapp/overlays/production/configmap-patch.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  template:\n    spec:\n      containers:\n      - name: nginx\n        envFrom:\n        - configMapRef:\n            name: app-config\nEOF\n\n# Add patch to kustomization\ncat &lt;&lt;EOF &gt;&gt; myapp/overlays/production/kustomization.yaml\n\npatchesStrategicMerge:\n  - replica-patch.yaml\n  - configmap-patch.yaml\nEOF\n\n# Apply and verify\nkubectl apply -k myapp/overlays/production\nkubectl get configmap -n production  # Note the hash suffix\nkubectl describe deployment nginx -n production | grep -A5 ConfigMap\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#exercise-3-json6902-patch-for-security-context","title":"Exercise 3: JSON6902 Patch for Security Context","text":"<p>Task: Add a security context to the Nginx container using a JSON6902 patch that sets <code>runAsNonRoot: true</code> and <code>runAsUser: 1000</code>.</p> <p>Solution:</p> <pre><code># Create JSON6902 patch\ncat &lt;&lt;EOF &gt; myapp/overlays/production/security-patch.yaml\n- op: add\n  path: /spec/template/spec/securityContext\n  value:\n    runAsNonRoot: true\n    runAsUser: 1000\n    fsGroup: 2000\n\n- op: add\n  path: /spec/template/spec/containers/0/securityContext\n  value:\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop: [\"ALL\"]\nEOF\n\n# Update kustomization to use JSON patch\ncat &lt;&lt;EOF &gt;&gt; myapp/overlays/production/kustomization.yaml\n\npatchesJson6902:\n  - target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: nginx\n    path: security-patch.yaml\nEOF\n\n# Apply\nkubectl apply -k myapp/overlays/production\n\n# Verify security context\nkubectl get deployment nginx -n production -o yaml | grep -A10 securityContext\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#exercise-4-multi-environment-setup","title":"Exercise 4: Multi-Environment Setup","text":"<p>Task: Create three overlays (dev, staging, production) with different replica counts (1, 3, 5) and namespaces.</p> <p>Solution:</p> <pre><code># Create overlay directories\nmkdir -p myapp/overlays/{dev,staging,production}\n\n# Dev overlay\ncat &lt;&lt;EOF &gt; myapp/overlays/dev/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nbases:\n  - ../../base\nnamespace: dev\nreplicas:\n  - name: nginx\n    count: 1\nnameSuffix: -dev\nEOF\n\n# Staging overlay\ncat &lt;&lt;EOF &gt; myapp/overlays/staging/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nbases:\n  - ../../base\nnamespace: staging\nreplicas:\n  - name: nginx\n    count: 3\nnameSuffix: -staging\nEOF\n\n# Production overlay\ncat &lt;&lt;EOF &gt; myapp/overlays/production/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nbases:\n  - ../../base\nnamespace: production\nreplicas:\n  - name: nginx\n    count: 5\nnameSuffix: -prod\nEOF\n\n# Apply all environments\nkubectl apply -k myapp/overlays/dev\nkubectl apply -k myapp/overlays/staging\nkubectl apply -k myapp/overlays/production\n\n# Verify\nkubectl get deployments --all-namespaces | grep nginx\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#exercise-5-image-transformer","title":"Exercise 5: Image Transformer","text":"<p>Task: Use the image transformer to update the Nginx image to <code>nginx:1.27-alpine</code> in production, keeping <code>nginx:1.27</code> in dev.</p> <p>Solution:</p> <pre><code># Update production kustomization\ncat &lt;&lt;EOF &gt;&gt; myapp/overlays/production/kustomization.yaml\n\nimages:\n  - name: nginx\n    newName: nginx\n    newTag: \"1.27-alpine\"\nEOF\n\n# Dev keeps base image (no transformer needed)\n# Already using base nginx:1.27\n\n# Apply both\nkubectl apply -k myapp/overlays/dev\nkubectl apply -k myapp/overlays/production\n\n# Verify images\nkubectl get deployment nginx-dev -n dev -o jsonpath='{.spec.template.spec.containers[0].image}'\n# Output: nginx:1.27\n\nkubectl get deployment nginx-prod -n production -o jsonpath='{.spec.template.spec.containers[0].image}'\n# Output: nginx:1.27-alpine\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#quick-reference","title":"\ud83d\udcda Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#essential-kustomize-commands","title":"Essential Kustomize Commands","text":"<pre><code># Build kustomized YAML\nkubectl kustomize &lt;directory&gt;\nkustomize build &lt;directory&gt;\n\n# Apply kustomized configuration\nkubectl apply -k &lt;directory&gt;\n\n# Delete resources\nkubectl delete -k &lt;directory&gt;\n\n# Diff before applying\nkubectl diff -k &lt;directory&gt;\n\n# Dry-run validation\nkubectl apply -k &lt;directory&gt; --dry-run=client -o yaml\nkubectl apply -k &lt;directory&gt; --dry-run=server\n\n# Show field changes\nkubectl kustomize &lt;dir1&gt; &gt; base.yaml\nkubectl kustomize &lt;dir2&gt; &gt; overlay.yaml\ndiff base.yaml overlay.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#kustomization-file-sections","title":"Kustomization File Sections","text":"Section Purpose <code>resources</code> List of YAML files to include <code>bases</code> Reference to base kustomizations (deprecated, use <code>resources</code>) <code>patches</code> Strategic merge patches <code>patchesJson6902</code> JSON6902 patches <code>configMapGenerator</code> Generate ConfigMaps <code>secretGenerator</code> Generate Secrets <code>commonLabels</code> Labels for all resources <code>commonAnnotations</code> Annotations for all resources <code>namespace</code> Namespace for all resources <code>namePrefix</code> Prefix for resource names <code>nameSuffix</code> Suffix for resource names <code>images</code> Image name/tag transformations <code>replicas</code> Replica count overrides","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#patch-operations-json6902","title":"Patch Operations (JSON6902)","text":"Operation Description Example <code>add</code> Add new field Add security context <code>remove</code> Remove field Remove resource limit <code>replace</code> Replace value Change image tag <code>move</code> Move field Reorganize structure <code>copy</code> Copy field Duplicate config <code>test</code> Assert value Validation check","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#related-posts","title":"\ud83d\udd17 Related Posts","text":"<ul> <li>Helm: Kubernetes Package Manager - Template-based configuration management</li> <li>ConfigMaps, Secrets, and Volume Mounts - Configuration injection patterns</li> <li>Namespaces and Resource Quotas - Multi-tenancy and isolation</li> </ul>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/kustomize-template-free-configuration/#additional-resources","title":"\ud83d\udcd6 Additional Resources","text":"<ul> <li>Kustomize Official Documentation</li> <li>Kubernetes Kustomize Guide</li> <li>Kustomize GitHub Repository</li> </ul> <p>CKA Exam Domain: Cluster Architecture, Installation &amp; Configuration (25%)</p> <p>Key Takeaways: - \u2705 Kustomize provides template-free configuration customization - \u2705 Use bases for common config, overlays for environment-specific changes - \u2705 Strategic merge patches for simple changes, JSON6902 for precise modifications - \u2705 Generators automatically create ConfigMaps/Secrets with content hashes - \u2705 Transformers apply cross-cutting changes (labels, namespaces, images) - \u2705 Integrated into <code>kubectl</code> - no separate installation needed - \u2705 Ideal for GitOps workflows with declarative, auditable configuration</p> <p>Next: Troubleshooting Clusters, Nodes, and Components</p>","tags":["kubernetes","k8s","cka-prep","kustomize","configuration","gitops"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/","title":"Setting Up Your Kubernetes Lab Environment","text":"<p>Master the art of building Kubernetes clusters for CKA exam preparation. Learn kubeadm for production-grade setups, kind for rapid testing, and essential kubectl configuration for efficient cluster management.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#overview","title":"Overview","text":"<p>A proper lab environment is critical for CKA exam success. The exam uses kubeadm-based clusters, making hands-on practice with real cluster setup essential. This guide covers four primary methods for building Kubernetes environments:</p> <ol> <li>kubeadm - Production-grade multi-node clusters (exam environment)</li> <li>kind - Fast container-based clusters for rapid iteration</li> <li>Minikube - Single-node local development with rich addons</li> <li>kubectl - Essential CLI tool configuration and management</li> </ol> <p>CKA Exam Domain: Cluster Architecture, Installation &amp; Configuration (25%)</p> <p>Key Finding: While the CKA exam uses kubeadm clusters, kind offers the fastest iteration for practice exercises. A combination of both provides optimal preparation.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#lab-environment-decision-tree","title":"Lab Environment Decision Tree","text":"<p>Choosing the right lab setup depends on your immediate needs and learning phase.</p> <pre><code>flowchart TD\n    Start([Need Kubernetes Lab?]) --&gt; Purpose{What's the purpose?}\n\n    Purpose --&gt;|CKA Exam Prep| Exam{Practice&lt;br/&gt;or&lt;br/&gt;Full Simulation?}\n    Purpose --&gt;|Local Development| Dev{Single or&lt;br/&gt;Multi-Node?}\n    Purpose --&gt;|CI/CD Testing| CICD[Use kind&lt;br/&gt;Fast iteration]\n\n    Exam --&gt;|Quick Practice| kind_exam[Use kind&lt;br/&gt;30-second clusters]\n    Exam --&gt;|Full Simulation| kubeadm_exam[Use kubeadm&lt;br/&gt;Production-like setup]\n\n    Dev --&gt;|Single Node&lt;br/&gt;+ Addons| Minikube[Use Minikube&lt;br/&gt;Rich ecosystem]\n    Dev --&gt;|Multi-Node&lt;br/&gt;Testing| kind_dev[Use kind&lt;br/&gt;Config-based]\n    Dev --&gt;|Production Sim| kubeadm_dev[Use kubeadm&lt;br/&gt;Real cluster]\n\n    CICD --&gt; fast{Speed&lt;br/&gt;Important?}\n    fast --&gt;|Yes| kind_fast[Use kind&lt;br/&gt;Container-based]\n    fast --&gt;|No| minikube_ci[Use Minikube&lt;br/&gt;More features]\n\n    style kubeadm_exam fill:#ff9999\n    style kubeadm_dev fill:#ff9999\n    style Minikube fill:#99ccff\n    style kind_exam fill:#99ff99\n    style kind_dev fill:#99ff99\n    style kind_fast fill:#99ff99\n    style CICD fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeadm-production-grade-cluster-setup","title":"kubeadm: Production-Grade Cluster Setup","text":"<p>kubeadm is the official Kubernetes tool for bootstrapping production-grade clusters. This is the tool used in the CKA exam environment.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#prerequisites","title":"Prerequisites","text":"<p>System Requirements (Per Node): - Ubuntu 20.04/22.04/24.04 or equivalent Debian-based system - 2+ CPUs - 2GB+ RAM - Network connectivity between all nodes - Unique hostname, MAC address, and product_uuid per node - Swap disabled (critical requirement)</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#complete-installation-process","title":"Complete Installation Process","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-1-disable-swap-critical","title":"Step 1: Disable Swap (CRITICAL)","text":"<p>Kubernetes scheduler relies on accurate resource allocation. Swap can cause unpredictable behavior.</p> <pre><code># Temporary disable\nsudo swapoff -a\n\n# Permanent disable - comment out swap in /etc/fstab\nsudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\n\n# Verify (swap line should show 0)\nfree -h\n</code></pre> <p>Exam Alert</p> <p>The CKA exam expects swap to be disabled. Forgetting this step causes kubeadm init to fail.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-2-install-container-runtime-containerd","title":"Step 2: Install Container Runtime (containerd)","text":"<pre><code># Install containerd\nsudo apt-get update\nsudo apt-get install -y containerd\n\n# Configure containerd\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\n\n# Restart and enable containerd\nsudo systemctl restart containerd\nsudo systemctl enable containerd\n\n# Verify containerd is running\nsystemctl status containerd\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-3-install-kubeadm-kubelet-kubectl","title":"Step 3: Install kubeadm, kubelet, kubectl","text":"<pre><code># Update package index and install prerequisites\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\n\n# Add Kubernetes GPG key (NEW REPOSITORY)\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | \\\n  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# Add Kubernetes repository\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | \\\n  sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n# Install Kubernetes components\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\n\n# Hold packages at current version (prevent auto-upgrade)\nsudo apt-mark hold kubelet kubeadm kubectl\n\n# Enable kubelet\nsudo systemctl enable --now kubelet\n</code></pre> <p>Repository Change</p> <p>The old repository <code>https://apt.kubernetes.io</code> is deprecated. Always use <code>pkgs.k8s.io</code> for new installations.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-4-initialize-control-plane-node","title":"Step 4: Initialize Control Plane Node","text":"<pre><code># Basic initialization with pod network CIDR\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n\n# For HA setup with load balancer\nsudo kubeadm init \\\n  --control-plane-endpoint \"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\" \\\n  --upload-certs \\\n  --pod-network-cidr=10.244.0.0/16\n</code></pre> <p>Expected Output: <pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a Pod network to the cluster.\n\nYou can now join any number of machines by running the following on each node:\n\n  kubeadm join 192.168.0.200:6443 \\\n    --token 9vr73a.a8uxyaju799qwdjv \\\n    --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d\n</code></pre></p> <p>Save the join command - you'll need it for worker nodes.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-5-configure-kubectl-access","title":"Step 5: Configure kubectl Access","text":"<pre><code># Configure kubectl for regular user\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n# Verify cluster access\nkubectl cluster-info\nkubectl get nodes\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-6-install-cni-network-plugin-critical","title":"Step 6: Install CNI Network Plugin (CRITICAL)","text":"<p>Without a CNI plugin, nodes remain in \"NotReady\" state and pods cannot communicate.</p> <p>Option A: Calico (Recommended for CKA)</p> <pre><code># Install Calico operator\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\n\n# Apply custom resources\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml\n\n# Verify installation (wait for all pods to be Running)\nwatch kubectl get pods -n calico-system\n</code></pre> <p>Option B: Flannel</p> <pre><code>kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n</code></pre> <p>Option C: Weave Net</p> <pre><code>kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml\n</code></pre> <p>After CNI installation, verify nodes are Ready:</p> <pre><code>kubectl get nodes\n# NAME            STATUS   ROLES           AGE   VERSION\n# control-plane   Ready    control-plane   5m    v1.34.0\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-7-join-worker-nodes","title":"Step 7: Join Worker Nodes","text":"<p>On each worker node (after completing Steps 1-3):</p> <pre><code># Use the join command from Step 4 output\nsudo kubeadm join 192.168.0.200:6443 \\\n  --token 9vr73a.a8uxyaju799qwdjv \\\n  --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d\n</code></pre> <p>If token expired (tokens are valid for 24 hours):</p> <pre><code># Generate new join command on control plane\nkubeadm token create --print-join-command\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeadm-cluster-architecture","title":"kubeadm Cluster Architecture","text":"<pre><code>graph TB\n    subgraph \"Control Plane Node\"\n        APIServer[API Server&lt;br/&gt;:6443]\n        Scheduler[Scheduler]\n        Controller[Controller Manager]\n        etcd[(etcd&lt;br/&gt;:2379-2380)]\n        KubeletCP[Kubelet&lt;br/&gt;:10250]\n        ProxyCP[kube-proxy]\n    end\n\n    subgraph \"Worker Node 1\"\n        KubeletW1[Kubelet&lt;br/&gt;:10250]\n        ProxyW1[kube-proxy]\n        Pod1[Pod]\n        Pod2[Pod]\n        CRI1[containerd]\n    end\n\n    subgraph \"Worker Node 2\"\n        KubeletW2[Kubelet&lt;br/&gt;:10250]\n        ProxyW2[kube-proxy]\n        Pod3[Pod]\n        Pod4[Pod]\n        CRI2[containerd]\n    end\n\n    subgraph \"CNI Network Layer\"\n        CNI[Calico/Flannel&lt;br/&gt;10.244.0.0/16]\n    end\n\n    APIServer --&gt; Scheduler\n    APIServer --&gt; Controller\n    APIServer --&gt; etcd\n    APIServer -.-&gt;|watches| KubeletCP\n    APIServer -.-&gt;|watches| KubeletW1\n    APIServer -.-&gt;|watches| KubeletW2\n\n    KubeletW1 --&gt; CRI1\n    KubeletW2 --&gt; CRI2\n    CRI1 --&gt; Pod1\n    CRI1 --&gt; Pod2\n    CRI2 --&gt; Pod3\n    CRI2 --&gt; Pod4\n\n    CNI --&gt; Pod1\n    CNI --&gt; Pod2\n    CNI --&gt; Pod3\n    CNI --&gt; Pod4\n\n    style APIServer fill:#e1f5ff\n    style etcd fill:#ffe5e5\n    style CNI fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#common-kubeadm-issues-and-solutions","title":"Common kubeadm Issues and Solutions","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#issue-1-swap-is-enabled-production-deployments-should-disable-swap","title":"Issue 1: \"swap is enabled; production deployments should disable swap\"","text":"<p>Cause: Swap not properly disabled</p> <p>Solution: <pre><code>sudo swapoff -a\nsudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\nsudo reboot\nfree -h  # Verify swap shows 0\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#issue-2-container-runtime-network-not-ready-cni-config-uninitialized","title":"Issue 2: \"container runtime network not ready: cni config uninitialized\"","text":"<p>Cause: No CNI plugin installed</p> <p>Solution: <pre><code># Check CNI config directory\nls /etc/cni/net.d/\n\n# If empty, install CNI plugin (Calico/Flannel)\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml\n\n# Verify CNI pods running\nkubectl get pods -n calico-system\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#issue-3-port-6443-already-in-use","title":"Issue 3: Port 6443 already in use","text":"<p>Cause: Previous kubeadm installation not cleaned up</p> <p>Solution: <pre><code># Reset kubeadm completely\nsudo kubeadm reset\n\n# Clean up directories\nsudo rm -rf /etc/cni/net.d\nsudo rm -rf $HOME/.kube/config\n\n# Try initialization again\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#issue-4-worker-node-join-fails","title":"Issue 4: Worker node join fails","text":"<p>Causes: Token expired, network connectivity, firewall rules</p> <p>Solutions: <pre><code># Generate new join command\nkubeadm token create --print-join-command\n\n# Check connectivity from worker to control plane\nping &lt;control-plane-ip&gt;\nnc -zv &lt;control-plane-ip&gt; 6443\n\n# Required firewall ports:\n# Control Plane: 6443, 2379-2380, 10250-10252\n# Worker Nodes: 10250, 30000-32767\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeadm-configuration-file-approach","title":"kubeadm Configuration File Approach","text":"<p>For repeatable setups, use configuration files:</p> <pre><code># kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta4\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.0.200\n  bindPort: 6443\n---\napiVersion: kubeadm.k8s.io/v1beta4\nkind: ClusterConfiguration\nnetworking:\n  podSubnet: 10.244.0.0/16\n  serviceSubnet: 10.96.0.0/16\nkubernetesVersion: v1.34.0\n---\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\n</code></pre> <pre><code># Initialize with config file\nsudo kubeadm init --config=kubeadm-config.yaml\n\n# Generate default config template\nkubeadm config print init-defaults &gt; kubeadm-config.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kind-kubernetes-in-docker","title":"kind: Kubernetes in Docker","text":"<p>kind runs Kubernetes clusters using Docker containers as \"nodes\". Excellent for rapid iteration and CKA practice.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#installation","title":"Installation","text":"<p>Linux/macOS: <pre><code># Using Homebrew (macOS)\nbrew install kind\n\n# Direct binary download (Linux)\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/latest/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Verify installation\nkind version\n</code></pre></p> <p>Prerequisites: - Docker installed and running - kubectl installed - 4GB+ RAM allocated to Docker - 2+ CPUs for multi-node clusters</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#basic-usage","title":"Basic Usage","text":"<pre><code># Create default single-node cluster\nkind create cluster\n\n# Create cluster with custom name\nkind create cluster --name cka-practice\n\n# List clusters\nkind get clusters\n\n# Delete cluster\nkind delete cluster --name cka-practice\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#multi-node-cluster-configuration","title":"Multi-Node Cluster Configuration","text":"<p>2-Node Cluster (1 Control Plane + 1 Worker):</p> <pre><code># kind-2node.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n  - role: worker\n</code></pre> <pre><code>kind create cluster --config kind-2node.yaml --name cka-lab\n</code></pre> <p>3-Node Cluster with Port Mapping:</p> <pre><code># kind-3node.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n    extraPortMappings:\n    - containerPort: 30080\n      hostPort: 8080\n      protocol: TCP\n  - role: worker\n  - role: worker\n</code></pre> <pre><code>kind create cluster --config kind-3node.yaml --name multinode\n</code></pre> <p>High Availability Control Plane (3 Control Plane + 3 Worker):</p> <pre><code># kind-ha.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n  - role: control-plane\n  - role: control-plane\n  - role: worker\n  - role: worker\n  - role: worker\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kind-cluster-lifecycle","title":"kind Cluster Lifecycle","text":"<pre><code>sequenceDiagram\n    participant User\n    participant kind\n    participant Docker\n    participant kubectl\n\n    User-&gt;&gt;kind: kind create cluster --config kind-3node.yaml\n    kind-&gt;&gt;Docker: Pull kindest/node:v1.34.0 image\n    Docker--&gt;&gt;kind: Image ready\n\n    kind-&gt;&gt;Docker: Create control-plane container\n    kind-&gt;&gt;Docker: Create worker-1 container\n    kind-&gt;&gt;Docker: Create worker-2 container\n    Docker--&gt;&gt;kind: 3 containers running\n\n    kind-&gt;&gt;kind: Bootstrap Kubernetes in containers\n    kind-&gt;&gt;kind: Configure CNI (kindnetd)\n    kind-&gt;&gt;kind: Wait for cluster ready\n\n    kind-&gt;&gt;kubectl: Update kubeconfig\n    kind--&gt;&gt;User: Cluster ready (30-60 seconds)\n\n    User-&gt;&gt;kubectl: kubectl get nodes\n    kubectl--&gt;&gt;User: 3 nodes Ready\n\n    Note over User,kubectl: Practice CKA scenarios\n\n    User-&gt;&gt;kind: kind delete cluster\n    kind-&gt;&gt;Docker: Remove all containers\n    Docker--&gt;&gt;kind: Cleanup complete\n    kind--&gt;&gt;User: Cluster deleted</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#load-docker-images-into-kind","title":"Load Docker Images into kind","text":"<p>For testing custom applications without pushing to registry:</p> <pre><code># Build image locally\ndocker build -t my-app:1.0 .\n\n# Load into kind cluster\nkind load docker-image my-app:1.0 --name cka-lab\n\n# Verify image available in cluster\ndocker exec -it cka-lab-control-plane crictl images | grep my-app\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#when-to-use-kind","title":"When to Use kind","text":"<p>Best For: - CKA rapid practice: Create/destroy clusters in seconds - Multi-node testing: Easy configuration for complex topologies - CI/CD pipelines: Fast, reproducible test environments - Integration testing: Test applications in real cluster - Quick experiments: Try configurations without VM overhead</p> <p>Not Ideal For: - Learning basics: Minikube has better addon ecosystem - Production simulation: kubeadm provides real multi-machine setup - Resource-constrained systems: Requires Docker overhead</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#minikube-local-development-environment","title":"Minikube: Local Development Environment","text":"<p>Minikube creates single-node Kubernetes clusters on your local machine. Ideal for learning with rich addon support.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#installation_1","title":"Installation","text":"<p>macOS: <pre><code>brew install minikube\nminikube version\n</code></pre></p> <p>Linux: <pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\nminikube version\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#driver-options","title":"Driver Options","text":"<p>Docker Driver (Recommended): <pre><code># Start with Docker driver\nminikube start --driver=docker\n\n# Set as default\nminikube config set driver docker\n</code></pre></p> <p>QEMU Driver (ARM/M1 Macs): <pre><code>minikube start --driver=qemu\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#resource-configuration","title":"Resource Configuration","text":"<pre><code># Specify CPUs, memory, disk\nminikube start \\\n  --driver=docker \\\n  --cpus=4 \\\n  --memory=8192 \\\n  --disk-size=40g \\\n  --kubernetes-version=v1.34.0\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#common-minikube-commands","title":"Common Minikube Commands","text":"<pre><code># Start cluster\nminikube start\n\n# Stop cluster (preserves state)\nminikube stop\n\n# Delete cluster\nminikube delete\n\n# Check status\nminikube status\n\n# SSH into node\nminikube ssh\n\n# Open dashboard\nminikube dashboard\n\n# List addons\nminikube addons list\n\n# Enable addons\nminikube addons enable ingress\nminikube addons enable metrics-server\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#minikube-addons-for-cka-practice","title":"Minikube Addons for CKA Practice","text":"<pre><code># Metrics Server (for kubectl top)\nminikube addons enable metrics-server\n\n# Ingress Controller\nminikube addons enable ingress\n\n# Storage Provisioner (dynamic PVs)\nminikube addons enable storage-provisioner\n\n# Dashboard\nminikube addons enable dashboard\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubectl-installation-and-configuration","title":"kubectl Installation and Configuration","text":"<p>kubectl is the Kubernetes command-line tool. Mastery is essential for CKA exam success.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#installation_2","title":"Installation","text":"<p>Linux: <pre><code># Download latest release\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n\n# Install\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\n# Verify\nkubectl version --client\n</code></pre></p> <p>macOS: <pre><code># Using Homebrew\nbrew install kubectl\n\n# Verify\nkubectl version --client\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#version-compatibility","title":"Version Compatibility","text":"<p>kubectl version must be within \u00b11 minor version of cluster version.</p> <pre><code># Check versions\nkubectl version --short\n\n# Example compatible versions:\n# Cluster: v1.34.0\n# kubectl: v1.33.x, v1.34.x, v1.35.x \u2705\n# kubectl: v1.32.x, v1.36.x \u274c\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#shell-completion-setup-critical-for-exam-speed","title":"Shell Completion Setup (CRITICAL for Exam Speed)","text":"<p>Bash (Linux): <pre><code># Install bash-completion package\nsudo apt-get install bash-completion\n\n# Add completion to current shell\nsource &lt;(kubectl completion bash)\n\n# Add to .bashrc for persistence\necho 'source &lt;(kubectl completion bash)' &gt;&gt; ~/.bashrc\n\n# Alias completion (essential for exam)\necho 'alias k=kubectl' &gt;&gt; ~/.bashrc\necho 'complete -o default -F __start_kubectl k' &gt;&gt; ~/.bashrc\n\n# Reload\nsource ~/.bashrc\n</code></pre></p> <p>Zsh: <pre><code># Add completion to .zshrc\necho 'source &lt;(kubectl completion zsh)' &gt;&gt; ~/.zshrc\n\n# Alias completion\necho 'alias k=kubectl' &gt;&gt; ~/.zshrc\necho 'complete -o default -F __start_kubectl k' &gt;&gt; ~/.zshrc\n\n# Reload\nsource ~/.zshrc\n</code></pre></p> <p>Exam Tip</p> <p>Practice using <code>k</code> instead of <code>kubectl</code> extensively. Tab completion saves critical minutes during the exam.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeconfig-management","title":"kubeconfig Management","text":"<p>kubeconfig files contain cluster connection details. Managing multiple clusters efficiently is essential for the exam.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeconfig-file-structure","title":"kubeconfig File Structure","text":"<pre><code>apiVersion: v1\nkind: Config\ncurrent-context: my-cluster\n\nclusters:\n- cluster:\n    certificate-authority-data: &lt;base64-encoded-ca&gt;\n    server: https://192.168.1.100:6443\n  name: my-cluster\n\nusers:\n- name: my-user\n  user:\n    client-certificate-data: &lt;base64-encoded-cert&gt;\n    client-key-data: &lt;base64-encoded-key&gt;\n\ncontexts:\n- context:\n    cluster: my-cluster\n    user: my-user\n    namespace: default\n  name: my-cluster-context\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#context-management-flow","title":"Context Management Flow","text":"<pre><code>graph TB\n    subgraph \"kubeconfig File\"\n        Clusters[Clusters&lt;br/&gt;- prod-cluster&lt;br/&gt;- dev-cluster&lt;br/&gt;- cka-cluster]\n        Users[Users&lt;br/&gt;- admin-user&lt;br/&gt;- dev-user&lt;br/&gt;- readonly-user]\n        Contexts[Contexts&lt;br/&gt;- prod-admin&lt;br/&gt;- dev-user&lt;br/&gt;- cka-practice]\n        CurrentContext[Current Context:&lt;br/&gt;cka-practice]\n    end\n\n    subgraph \"Context: cka-practice\"\n        CKACluster[Cluster: cka-cluster]\n        CKAUser[User: admin-user]\n        CKANamespace[Namespace: default]\n    end\n\n    subgraph \"Actual Clusters\"\n        ProdCluster[Production&lt;br/&gt;10.0.1.100:6443]\n        DevCluster[Development&lt;br/&gt;10.0.2.100:6443]\n        CKAClusterActual[CKA Lab&lt;br/&gt;10.0.3.100:6443]\n    end\n\n    Contexts --&gt; CurrentContext\n    CurrentContext --&gt; CKACluster\n    CurrentContext --&gt; CKAUser\n    CurrentContext --&gt; CKANamespace\n\n    CKACluster -.-&gt;|kubectl commands| CKAClusterActual\n\n    style CurrentContext fill:#ffff99\n    style CKAClusterActual fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#essential-context-commands","title":"Essential Context Commands","text":"<pre><code># List all contexts\nkubectl config get-contexts\n\n# Show current context\nkubectl config current-context\n\n# Switch context\nkubectl config use-context cka-practice\n\n# Set default namespace for current context\nkubectl config set-context --current --namespace=kube-system\n\n# Create new context\nkubectl config set-context dev-user \\\n  --cluster=dev-cluster \\\n  --namespace=development \\\n  --user=dev-user\n\n# Rename context\nkubectl config rename-context old-name new-name\n\n# Delete context\nkubectl config delete-context old-context\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#multiple-cluster-management","title":"Multiple Cluster Management","text":"<p>Strategy 1: Merged Configuration (Recommended for Exam):</p> <pre><code># Merge multiple kubeconfig files\nexport KUBECONFIG=~/.kube/config:~/.kube/dev-config:~/.kube/prod-config\nkubectl config view --flatten &gt; ~/.kube/merged-config\n\n# Set as default\nexport KUBECONFIG=~/.kube/merged-config\n\n# Add to .bashrc for persistence\necho 'export KUBECONFIG=~/.kube/merged-config' &gt;&gt; ~/.bashrc\n</code></pre> <p>Strategy 2: Context-Specific Commands:</p> <pre><code># Use specific context for single command\nkubectl --context=prod-cluster get nodes\n\n# Use specific kubeconfig for single command\nkubectl --kubeconfig=~/.kube/prod-config get pods\n\n# Override namespace for single command\nkubectl -n kube-system get pods\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#security-best-practices","title":"Security Best Practices","text":"<pre><code># Protect kubeconfig files\nchmod 600 ~/.kube/config\nchmod 0400 ~/.kube/prod-config  # Read-only for production\n\n# Never commit kubeconfig to version control\necho '.kube/' &gt;&gt; ~/.gitignore\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#common-exam-tasks","title":"Common Exam Tasks","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#scenario-1-rapid-cluster-setup-for-practice","title":"Scenario 1: Rapid Cluster Setup for Practice","text":"<p>Objective: Create disposable cluster for practice scenario</p> <pre><code># Create kind cluster\ncat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n- role: worker\n- role: worker\nEOF\n\n# Verify cluster ready\nkubectl get nodes\n\n# Practice exam task\nkubectl run nginx --image=nginx\nkubectl expose pod nginx --port=80 --type=NodePort\n\n# Clean up when done\nkind delete cluster\n</code></pre> <p>Time: 30-60 seconds for cluster creation</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#scenario-2-multi-cluster-context-switching","title":"Scenario 2: Multi-Cluster Context Switching","text":"<p>Objective: Practice switching between exam cluster contexts</p> <pre><code># Assume exam provides contexts: cluster1-context, cluster2-context\n\n# List available contexts\nkubectl config get-contexts\n\n# Switch to cluster1\nkubectl config use-context cluster1-context\n\n# Verify current context\nkubectl config current-context\n\n# Deploy to cluster1\nkubectl run web --image=nginx\n\n# Switch to cluster2\nkubectl config use-context cluster2-context\n\n# Deploy to cluster2\nkubectl run database --image=postgres\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#scenario-3-troubleshoot-cni-issues","title":"Scenario 3: Troubleshoot CNI Issues","text":"<p>Objective: Fix cluster networking</p> <pre><code># Check node status\nkubectl get nodes\n# NAME     STATUS     ROLES           AGE   VERSION\n# node-1   NotReady   control-plane   2m    v1.34.0\n\n# Check system pods\nkubectl -n kube-system get pods\n\n# Verify CNI config exists\nkubectl exec -n kube-system &lt;any-pod&gt; -- ls /etc/cni/net.d/\n\n# If empty, install CNI\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml\n\n# Verify nodes transition to Ready\nkubectl get nodes --watch\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#cni-network-communication","title":"CNI Network Communication","text":"<p>Understanding how CNI plugins enable pod-to-pod communication is critical for troubleshooting.</p> <pre><code>sequenceDiagram\n    participant Pod1 as Pod 1&lt;br/&gt;10.244.1.5&lt;br/&gt;Node1\n    participant CNI1 as CNI Plugin&lt;br/&gt;Node1\n    participant CNI2 as CNI Plugin&lt;br/&gt;Node2\n    participant Pod2 as Pod 2&lt;br/&gt;10.244.2.8&lt;br/&gt;Node2\n\n    Note over Pod1,Pod2: Pod-to-Pod Communication Across Nodes\n\n    Pod1-&gt;&gt;CNI1: Send packet to 10.244.2.8\n    CNI1-&gt;&gt;CNI1: Check routing table&lt;br/&gt;Pod CIDR 10.244.0.0/16\n    CNI1-&gt;&gt;CNI2: Forward via overlay network&lt;br/&gt;(VXLAN/BGP)\n    CNI2-&gt;&gt;CNI2: Lookup destination pod&lt;br/&gt;10.244.2.8 on Node2\n    CNI2-&gt;&gt;Pod2: Deliver packet\n    Pod2-&gt;&gt;CNI2: Send response\n    CNI2-&gt;&gt;CNI1: Return via overlay\n    CNI1-&gt;&gt;Pod1: Deliver response\n\n    Note over CNI1,CNI2: CNI ensures IP reachability&lt;br/&gt;without NodePort/Service</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-1-complete-kubeadm-cluster-setup-60-minutes","title":"Exercise 1: Complete kubeadm Cluster Setup (60 minutes)","text":"<p>Objective: Build production-like 3-node cluster</p> <p>Tasks: 1. Prepare 3 VMs (1 control plane, 2 workers) 2. Disable swap on all nodes 3. Install container runtime 4. Install kubeadm, kubelet, kubectl 5. Initialize control plane with Calico pod CIDR 6. Install Calico CNI 7. Join worker nodes 8. Verify all nodes Ready 9. Deploy test workload</p> <p>Success Criteria: - All 3 nodes show Ready status - CNI pods running in calico-system namespace - Test pod can communicate across nodes</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-2-kind-multi-node-lab-20-minutes","title":"Exercise 2: kind Multi-Node Lab (20 minutes)","text":"<p>Objective: Create multi-node cluster for rapid testing</p> <p>Tasks: 1. Create kind config for 3-node cluster 2. Map NodePort 30000 to localhost:8080 3. Create cluster 4. Deploy nginx with NodePort 30000 5. Access from host browser 6. Test pod scheduling across workers 7. Delete and recreate cluster</p> <p>Success Criteria: - Cluster creation completes in &lt;60 seconds - Nginx accessible on localhost:8080 - Can repeat cycle 5 times in 10 minutes</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-3-kubeconfig-context-mastery-30-minutes","title":"Exercise 3: kubeconfig Context Mastery (30 minutes)","text":"<p>Objective: Manage multiple clusters efficiently</p> <p>Tasks: 1. Create 3 different clusters (kubeadm, kind, minikube) 2. Export kubeconfig from each 3. Merge into single kubeconfig 4. Rename contexts meaningfully 5. Set default namespace per context 6. Practice rapid context switching 7. Deploy workload to specific context without switching</p> <p>Success Criteria: - Switch contexts in &lt;5 seconds - Deploy to specific cluster without errors - Verify deployment in correct cluster</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-4-shell-completion-and-aliases-15-minutes","title":"Exercise 4: Shell Completion and Aliases (15 minutes)","text":"<p>Objective: Optimize kubectl workflow for exam speed</p> <p>Tasks: 1. Install bash-completion 2. Configure kubectl completion 3. Test tab completion 4. Create aliases: k, kg, kd, kl 5. Configure alias completion 6. Time yourself: deploy nginx with and without aliases</p> <p>Success Criteria: - Tab completion works for resources - Aliases reduce command length by 50%+ - Muscle memory for common patterns</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-5-troubleshooting-simulation-45-minutes","title":"Exercise 5: Troubleshooting Simulation (45 minutes)","text":"<p>Objective: Diagnose and fix common cluster issues</p> <p>Tasks: 1. Initialize cluster WITHOUT CNI 2. Observe NotReady nodes 3. Check kubelet logs for errors 4. Identify CNI-related messages 5. Install CNI plugin 6. Verify nodes transition to Ready 7. Test pod communication 8. Break and fix: stop kubelet, observe effects 9. Break and fix: remove CNI config, restore</p> <p>Success Criteria: - Can identify CNI issues from logs - Successfully install and verify CNI - Understand node status transitions - Troubleshooting workflow muscle memory</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#lab-environment-comparison","title":"Lab Environment Comparison","text":"<pre><code>graph TD\n    subgraph \"Setup Method Comparison\"\n        Feature[Feature]\n        kubeadm[kubeadm]\n        minikube[Minikube]\n        kind[kind]\n    end\n\n    Feature --&gt; |Startup Time| Time\n    Time --&gt; kubeadm_time[5-10 min&lt;br/&gt;Multi-machine]\n    Time --&gt; minikube_time[2-5 min&lt;br/&gt;VM boot]\n    Time --&gt; kind_time[30-60 sec&lt;br/&gt;Containers]\n\n    Feature --&gt; |Resource Usage| Resources\n    Resources --&gt; kubeadm_res[High&lt;br/&gt;Real VMs]\n    Resources --&gt; minikube_res[Medium&lt;br/&gt;Single VM]\n    Resources --&gt; kind_res[Low&lt;br/&gt;Docker only]\n\n    Feature --&gt; |Multi-Node| MultiNode\n    MultiNode --&gt; kubeadm_mn[Native&lt;br/&gt;Production-like]\n    MultiNode --&gt; minikube_mn[Experimental&lt;br/&gt;Limited]\n    MultiNode --&gt; kind_mn[Native&lt;br/&gt;Config-based]\n\n    Feature --&gt; |CKA Relevance| CKA\n    CKA --&gt; kubeadm_cka[Essential&lt;br/&gt;Exam environment]\n    CKA --&gt; minikube_cka[Supplemental&lt;br/&gt;Learning]\n    CKA --&gt; kind_cka[High&lt;br/&gt;Practice speed]\n\n    style kind_time fill:#99ff99\n    style kind_res fill:#99ff99\n    style kubeadm_mn fill:#ff9999\n    style kubeadm_cka fill:#ff9999</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#quick-reference-commands","title":"Quick Reference Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeadm-cluster-lifecycle","title":"kubeadm Cluster Lifecycle","text":"<pre><code># Initialize control plane\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n\n# Configure kubectl\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n# Install CNI (Calico)\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml\n\n# Generate join command\nkubeadm token create --print-join-command\n\n# Reset cluster\nsudo kubeadm reset\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kind-quick-start","title":"kind Quick Start","text":"<pre><code># Create cluster\nkind create cluster --name cka-lab\n\n# Create multi-node cluster\ncat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n- role: worker\n- role: worker\nEOF\n\n# Delete cluster\nkind delete cluster --name cka-lab\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#minikube-quick-start","title":"Minikube Quick Start","text":"<pre><code># Start with resource spec\nminikube start --driver=docker --cpus=4 --memory=8192\n\n# Enable addons\nminikube addons enable metrics-server\nminikube addons enable ingress\n\n# Access service\nminikube service &lt;service-name&gt;\n\n# Clean up\nminikube delete\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubectl-context-management","title":"kubectl Context Management","text":"<pre><code># View contexts\nkubectl config get-contexts\n\n# Switch context\nkubectl config use-context &lt;context-name&gt;\n\n# Set namespace\nkubectl config set-context --current --namespace=&lt;namespace&gt;\n\n# Context-specific command\nkubectl --context=&lt;context&gt; get pods\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<pre><code># Check cluster health\nkubectl get nodes -o wide\nkubectl -n kube-system get pods\n\n# View kubelet logs (on node)\nsudo journalctl -u kubelet -f\n\n# Check CNI config\nls -l /etc/cni/net.d/\n\n# View events\nkubectl get events --sort-by='.lastTimestamp'\n\n# Check etcd health\nsudo ETCDCTL_API=3 etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  endpoint health\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 kubeadm is essential for CKA - The exam environment uses kubeadm clusters</p> <p>\u2705 kind enables rapid iteration - 30-second cluster creation for practice</p> <p>\u2705 kubectl proficiency is critical - Shell completion and aliases save exam minutes</p> <p>\u2705 kubeconfig mastery matters - Context switching is a core exam skill</p> <p>\u2705 CNI is non-negotiable - Clusters are non-functional without CNI plugins</p> <p>\u2705 Swap must be disabled - Kubernetes does not support swap memory</p> <p>\u2705 Version compatibility awareness - kubectl and cluster versions must align</p> <p>\u2705 Hands-on practice wins - Deploy clusters repeatedly until muscle memory forms</p> <p>\u2705 Troubleshooting is 30% of CKA - Practice breaking and fixing clusters</p> <p>\u2705 Speed through preparation - Optimize workflow before exam day</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#next-steps","title":"Next Steps","text":"<p>After mastering lab setup, continue with:</p> <p>Post 3: kubectl Essentials and Resource Management - Master the command-line tool for all Kubernetes operations</p> <p>Related Posts: - Kubernetes Architecture Fundamentals - Understanding cluster components - Kubernetes CKA Mastery - Complete Learning Path - Full exam preparation series</p> <p>External Resources: - kubeadm Official Documentation - kind Quick Start Guide - Minikube Documentation - kubectl Cheat Sheet - CKA Exam Curriculum</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/kubernetes-network-policies/","title":"Network Policies: Securing Pod Communication","text":"<p>In a world where security breaches make headlines daily, protecting your Kubernetes cluster isn't optional\u2014it's mission-critical. Network Policies are your first line of defense, implementing a zero-trust security model within your cluster. For CKA candidates, mastering Network Policies isn't just about passing the exam (though they represent a significant portion of the 20% Services &amp; Networking domain)\u2014it's about understanding how to build production-grade, secure Kubernetes environments.</p> <p>This guide will transform you from someone who knows Network Policies exist to someone who can design, implement, and troubleshoot complex network security scenarios. We'll explore not just the \"what\" and \"how,\" but the critical \"why\" behind each pattern. Whether you're preparing for the CKA exam or hardening your production clusters, this comprehensive exploration of Network Policies will equip you with battle-tested knowledge.</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#why-network-policies-matter","title":"Why Network Policies Matter","text":"<p>By default, Kubernetes operates on a permissive networking model: any pod can communicate with any other pod, regardless of namespace, labels, or purpose. While this \"allow-all\" approach simplifies initial development, it creates a massive security vulnerability in production environments.</p> <p>Consider a typical scenario: a compromised frontend pod could directly access your database, exfiltrate sensitive data, or pivot to other services. Without Network Policies, your cluster operates like a medieval castle with all the interior doors unlocked\u2014breach the outer wall, and everything is accessible.</p> <p>Network Policies implement the zero-trust principle: deny everything by default, then explicitly allow only the necessary communication paths. This security model assumes no implicit trust, even within your cluster.</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#the-cka-perspective","title":"The CKA Perspective","text":"<p>The CKA exam tests your ability to:</p> <ul> <li>Define Network Policies from scratch using YAML (no imperative commands available!)</li> <li>Implement namespace isolation and pod-to-pod restrictions</li> <li>Troubleshoot connectivity issues caused by restrictive policies</li> <li>Debug blocked traffic using testing containers like netshoot or busybox</li> <li>Understand CNI plugin requirements (not all plugins support Network Policies!)</li> </ul> <p>Expect exam scenarios like: \"Create a policy that allows only pods labeled <code>role=frontend</code> to access <code>role=backend</code> pods on port 8080, while denying all other traffic.\" You'll need to work quickly, referencing the official Kubernetes documentation efficiently.</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#real-world-impact","title":"Real-World Impact","text":"<p>Network Policies provide:</p> <ul> <li>Compliance: Meet regulatory requirements (PCI-DSS, HIPAA, SOC 2) requiring network segmentation</li> <li>Blast radius reduction: Limit damage from compromised workloads</li> <li>Multi-tenancy security: Isolate different teams or applications sharing the same cluster</li> <li>Defense in depth: Layer security beyond RBAC and Pod Security Standards</li> </ul>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#network-policy-fundamentals","title":"Network Policy Fundamentals","text":"","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#what-are-network-policies","title":"What Are Network Policies?","text":"<p>Network Policies are Kubernetes resources that control network traffic flow at the IP address or port level (OSI layers 3 and 4). Think of them as distributed firewalls that define rules for:</p> <ul> <li>Ingress traffic: Incoming connections to selected pods</li> <li>Egress traffic: Outgoing connections from selected pods</li> </ul> <p>Here's the crucial part: Network Policies are namespace-scoped resources that use label selectors to identify pods, creating a flexible, dynamic security model that adapts as your workloads scale.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: example-policy\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#default-allow-all-behavior","title":"Default Allow-All Behavior","text":"<p>Critical concept: Kubernetes defaults to allowing all traffic. Network Policies operate on a deny-by-default principle when applied, but only for selected pods.</p> <pre><code>flowchart TB\n    subgraph \"Before Network Policy\"\n        P1[Pod A] -.-&gt;|All traffic allowed| P2[Pod B]\n        P2 -.-&gt;|All traffic allowed| P3[Pod C]\n        P1 -.-&gt;|All traffic allowed| P3\n    end\n\n    subgraph \"After Network Policy Applied to Pod B\"\n        P4[Pod A] --&gt;|Allowed by policy| P5[Pod B - Protected]\n        P5 --&gt;|Allowed by policy| P6[Pod C]\n        P4 -.-&gt;|Still unrestricted| P6\n    end\n\n    style P5 fill:#f96,stroke:#333,stroke-width:3px</code></pre> <p>Key insight: If no Network Policy selects a pod, that pod remains completely open. Always start with a default-deny policy!</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#cni-plugin-requirements","title":"CNI Plugin Requirements","text":"<p>Not all Container Network Interface (CNI) plugins support Network Policies. This is a critical exam gotcha and production consideration.</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#plugins-with-network-policy-support","title":"Plugins with Network Policy Support","text":"CNI Plugin Policy Support Features CKA Relevance Calico \u2705 Full Traditional iptables-based, BGP support, GlobalNetworkPolicy Common exam choice Cilium \u2705 Full + Enhanced eBPF-based, L7 policies, DNS-aware, identity-based Advanced features Weave Net \u2705 Full Simple setup, works across network partitions Straightforward Canal \u2705 Full Combines Flannel + Calico policies Best of both Antrea \u2705 Full OVS-based, Layer 3/4 focus Enterprise-grade Flannel \u274c None Overlay networking only, no policy enforcement Exam trap!","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#calico-vs-cilium-quick-comparison","title":"Calico vs Cilium: Quick Comparison","text":"<p>Calico (Exam-friendly): <pre><code># Traditional Linux networking with iptables\n# Pros: Mature, stable, extensive documentation\n# Cons: iptables can bottleneck at high scale\n# Best for: Standard Kubernetes policies, production stability\n</code></pre></p> <p>Cilium (Advanced): <pre><code># Modern eBPF-based data plane\n# Pros: Layer 7 policies, DNS filtering, superior performance\n# Cons: Steeper learning curve, newer technology\n# Best for: Advanced security, observability, service mesh use cases\n</code></pre></p> <p>Exam tip: If the exam scenario doesn't specify the CNI, assume it supports Network Policies. Focus on YAML correctness, not CNI implementation details.</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#policy-enforcement-model","title":"Policy Enforcement Model","text":"<p>Network Policies are additive: multiple policies selecting the same pod combine via logical OR. If any policy allows traffic, it's permitted.</p> <pre><code>flowchart LR\n    subgraph \"Multiple Policies\"\n        NP1[Policy 1:&lt;br/&gt;Allow port 80]\n        NP2[Policy 2:&lt;br/&gt;Allow port 443]\n    end\n\n    Pod[Target Pod] --&gt; Result[Result:&lt;br/&gt;Ports 80 AND 443 allowed]\n\n    NP1 -.-&gt;|OR| Result\n    NP2 -.-&gt;|OR| Result\n\n    style Result fill:#9f6,stroke:#333,stroke-width:2px</code></pre> <p>However, within a single policy, rules use AND logic between different selector types:</p> <pre><code># This requires BOTH conditions to be true simultaneously\ningress:\n- from:\n  - namespaceSelector:      # Condition 1: Specific namespace\n      matchLabels:\n        env: production\n    podSelector:            # AND Condition 2: Specific pod label\n      matchLabels:\n        role: frontend\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#policy-structure-deep-dive","title":"Policy Structure Deep Dive","text":"<p>Understanding the anatomy of a Network Policy is essential for CKA success. Let's dissect each component with precision.</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#core-components","title":"Core Components","text":"<pre><code>apiVersion: networking.k8s.io/v1  # Always this API version\nkind: NetworkPolicy\nmetadata:\n  name: detailed-policy           # Descriptive name\n  namespace: default              # Policies are namespace-scoped\nspec:\n  podSelector:                    # Which pods this policy applies to\n    matchLabels:\n      app: api\n  policyTypes:                    # Explicit declaration required!\n  - Ingress                       # Controls incoming traffic\n  - Egress                        # Controls outgoing traffic\n  ingress:                        # Ingress rules array\n  - from:                         # Source specifications\n    - podSelector: {}\n    ports:                        # Destination port restrictions\n    - protocol: TCP\n      port: 8080\n  egress:                         # Egress rules array\n  - to:                           # Destination specifications\n    - ipBlock:\n        cidr: 0.0.0.0/0\n    ports:                        # Destination port restrictions\n    - protocol: TCP\n      port: 443\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#podselector-targeting-pods","title":"podSelector: Targeting Pods","text":"<p>The <code>podSelector</code> determines which pods the policy protects:</p> <pre><code># Select all pods in the namespace\npodSelector: {}\n\n# Select specific pods by label\npodSelector:\n  matchLabels:\n    app: database\n    tier: backend\n\n# Advanced matching with expressions\npodSelector:\n  matchExpressions:\n  - key: environment\n    operator: In\n    values: [production, staging]\n</code></pre> <p>Exam trap: An empty <code>podSelector: {}</code> selects all pods in the namespace, not zero pods!</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#policytypes-explicit-intent","title":"policyTypes: Explicit Intent","text":"<p>The <code>policyTypes</code> field declares whether the policy affects ingress, egress, or both:</p> <pre><code># Ingress only - egress remains unrestricted\npolicyTypes:\n- Ingress\n\n# Egress only - ingress remains unrestricted\npolicyTypes:\n- Egress\n\n# Both - full control\npolicyTypes:\n- Ingress\n- Egress\n</code></pre> <p>Critical exam point: If you omit <code>policyTypes</code>, Kubernetes infers it from the presence of <code>ingress</code> or <code>egress</code> rules. However, explicit is always better for clarity and avoiding subtle bugs.</p> <pre><code># This policy denies all egress but allows all ingress!\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  # No egress rules = deny all egress\n  # No ingress in policyTypes = allow all ingress\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#selector-types-and-vs-or-logic","title":"Selector Types: AND vs OR Logic","text":"<p>This is where many candidates fail the exam. Understanding AND/OR logic is absolutely critical.</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#the-three-selector-types","title":"The Three Selector Types","text":"<ol> <li>podSelector: Targets pods by labels (in the same namespace by default)</li> <li>namespaceSelector: Targets entire namespaces by labels</li> <li>ipBlock: Targets IP CIDR ranges (for external traffic or pod IPs)</li> </ol>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#and-logic-same-array-entry","title":"AND Logic (Same Array Entry)","text":"<pre><code># REQUIRES: Namespace labeled env=prod AND Pod labeled role=api\ningress:\n- from:\n  - namespaceSelector:      # Both must be true\n      matchLabels:\n        env: prod\n    podSelector:\n      matchLabels:\n        role: api\n</code></pre> <pre><code>flowchart TB\n    subgraph \"AND Logic (Single Entry)\"\n        NS[namespaceSelector:&lt;br/&gt;env=prod] --&gt; AND{AND}\n        PS[podSelector:&lt;br/&gt;role=api] --&gt; AND\n        AND --&gt; Match[\u2705 Match if BOTH true]\n    end\n\n    style AND fill:#f96\n    style Match fill:#9f6</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#or-logic-separate-array-entries","title":"OR Logic (Separate Array Entries)","text":"<pre><code># ALLOWS: Namespace labeled env=prod OR Pod labeled role=api\ningress:\n- from:\n  - namespaceSelector:      # Separate entry = OR\n      matchLabels:\n        env: prod\n  - podSelector:            # Separate entry = OR\n      matchLabels:\n        role: api\n</code></pre> <pre><code>flowchart TB\n    subgraph \"OR Logic (Multiple Entries)\"\n        NS[Entry 1:&lt;br/&gt;namespaceSelector] --&gt; OR{OR}\n        PS[Entry 2:&lt;br/&gt;podSelector] --&gt; OR\n        OR --&gt; Match[\u2705 Match if EITHER true]\n    end\n\n    style OR fill:#69f\n    style Match fill:#9f6</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#cross-namespace-communication","title":"Cross-Namespace Communication","text":"<pre><code># Allow ONLY pods from the monitoring namespace labeled app=prometheus\ningress:\n- from:\n  - namespaceSelector:\n      matchLabels:\n        name: monitoring\n    podSelector:              # AND logic: both must be true\n      matchLabels:\n        app: prometheus\n</code></pre> <p>Exam wisdom: Draw a quick AND/OR diagram on your scratch paper during the exam. It saves critical time!</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#port-specifications","title":"Port Specifications","text":"<p>Define which ports and protocols are allowed:</p> <pre><code>ports:\n- protocol: TCP              # TCP, UDP, or SCTP\n  port: 8080                 # Container port (not Service port!)\n- protocol: UDP\n  port: 53                   # DNS (crucial for egress!)\n  endPort: 54                # Port range (Kubernetes 1.25+)\n</code></pre> <p>Important: The <code>port</code> field refers to the pod's container port, not the Service port. This trips up many candidates!</p> <pre><code># Wrong - uses Service port\nports:\n- protocol: TCP\n  port: 80                   # Service port\n\n# Correct - uses container port\nports:\n- protocol: TCP\n  port: 8080                 # Actual container port\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#complete-example-with-annotations","title":"Complete Example with Annotations","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-policy\n  namespace: production\n  annotations:\n    description: \"Restricts backend API access to frontend and monitoring only\"\nspec:\n  # Apply to all backend pods\n  podSelector:\n    matchLabels:\n      tier: backend\n\n  # Control both directions\n  policyTypes:\n  - Ingress\n  - Egress\n\n  ingress:\n  # Rule 1: Allow frontend pods (same namespace)\n  - from:\n    - podSelector:\n        matchLabels:\n          tier: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n\n  # Rule 2: Allow monitoring namespace (any pod)\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring\n    ports:\n    - protocol: TCP\n      port: 9090              # Metrics endpoint\n\n  egress:\n  # Rule 1: Allow database access\n  - to:\n    - podSelector:\n        matchLabels:\n          tier: database\n    ports:\n    - protocol: TCP\n      port: 5432\n\n  # Rule 2: Allow DNS (critical!)\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n\n  # Rule 3: Allow external HTTPS (for API calls)\n  - to:\n    - ipBlock:\n        cidr: 0.0.0.0/0\n        except:\n        - 169.254.169.254/32  # Block cloud metadata API\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#common-patterns-for-production-and-exams","title":"Common Patterns for Production and Exams","text":"<p>These battle-tested patterns appear frequently in CKA exam scenarios and production environments. Master them!</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#pattern-1-deny-all-traffic-foundation","title":"Pattern 1: Deny All Traffic (Foundation)","text":"<p>Always start here. Create a default-deny policy before adding specific allow rules.</p> <pre><code># Deny all ingress and egress to all pods in the namespace\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: production\nspec:\n  podSelector: {}            # Selects all pods\n  policyTypes:\n  - Ingress\n  - Egress\n  # Empty rules = deny all\n</code></pre> <pre><code>flowchart TB\n    subgraph \"Default Deny Pattern\"\n        Internet[Internet] -.-&gt;|\u274c Blocked| NS[Namespace:&lt;br/&gt;production]\n        NS -.-&gt;|\u274c Blocked| Internet\n\n        subgraph NS\n            P1[Pod A] -.-&gt;|\u274c Blocked| P2[Pod B]\n            P2 -.-&gt;|\u274c Blocked| P3[Pod C]\n        end\n    end\n\n    style NS fill:#f96,stroke:#333,stroke-width:3px</code></pre> <p>Exam trick: After creating deny-all, verify pods can't communicate, then add specific allow policies incrementally.</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#pattern-2-allow-specific-ingress-only","title":"Pattern 2: Allow Specific Ingress Only","text":"<pre><code># Allow only frontend pods to access backend on port 8080\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-allow-frontend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#pattern-3-namespace-isolation","title":"Pattern 3: Namespace Isolation","text":"<p>Exam favorite! Isolate an entire namespace from external communication.</p> <pre><code># Allow pods in 'development' namespace to talk only to each other\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: namespace-isolation\n  namespace: development\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector: {}        # Any pod in same namespace\n  egress:\n  - to:\n    - podSelector: {}        # Any pod in same namespace\n  # Don't forget DNS!\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n</code></pre> <pre><code>flowchart TB\n    subgraph \"Namespace Isolation\"\n        subgraph dev[\"development namespace \"]\n            D1[Pod 1] &lt;--&gt;|\u2705 Allowed| D2[Pod 2]\n            D2 &lt;--&gt;|\u2705 Allowed| D3[Pod 3]\n        end\n\n        subgraph prod[\"production namespace\"]\n            P1[Pod A]\n        end\n\n        subgraph kube[\"kube-system\"]\n            DNS[CoreDNS]\n        end\n\n        D1 -.-&gt;|\u274c Blocked| P1\n        P1 -.-&gt;|\u274c Blocked| D1\n        D1 --&gt;|\u2705 DNS only| DNS\n    end\n\n    style dev fill:#9f6,stroke:#333,stroke-width:3px</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#pattern-4-allow-dns-critical-for-egress","title":"Pattern 4: Allow DNS (Critical for Egress!)","text":"<p>Most forgotten rule in exams: Pods need DNS to resolve service names!</p> <pre><code># Allow DNS queries to kube-dns/CoreDNS\negress:\n- to:\n  - namespaceSelector:\n      matchLabels:\n        kubernetes.io/metadata.name: kube-system\n    podSelector:\n      matchLabels:\n        k8s-app: kube-dns\n  ports:\n  - protocol: UDP\n    port: 53\n  - protocol: TCP            # Some DNS queries use TCP\n    port: 53\n</code></pre> <p>Exam shortcut: If you create an egress policy and pods can't resolve service names, you forgot DNS!</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#pattern-5-multi-tier-application-security","title":"Pattern 5: Multi-Tier Application Security","text":"<p>Real-world scenario: Frontend \u2192 Backend \u2192 Database architecture.</p> <pre><code>---\n# Frontend: Allow ingress from LoadBalancer, egress to backend\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: frontend-policy\n  namespace: app\nspec:\n  podSelector:\n    matchLabels:\n      tier: frontend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 0.0.0.0/0      # Allow internet (behind LB)\n    ports:\n    - protocol: TCP\n      port: 80\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          tier: backend\n    ports:\n    - protocol: TCP\n      port: 8080\n  - to:                      # DNS\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n\n---\n# Backend: Allow ingress from frontend, egress to database\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-policy\n  namespace: app\nspec:\n  podSelector:\n    matchLabels:\n      tier: backend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          tier: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          tier: database\n    ports:\n    - protocol: TCP\n      port: 5432\n  - to:                      # DNS\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n\n---\n# Database: Allow ingress from backend only, no egress\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: database-policy\n  namespace: app\nspec:\n  podSelector:\n    matchLabels:\n      tier: database\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          tier: backend\n    ports:\n    - protocol: TCP\n      port: 5432\n  egress:\n  - to:                      # DNS only (no external access!)\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n</code></pre> <pre><code>flowchart LR\n    Internet[Internet] --&gt;|:80| Frontend\n\n    subgraph app[\"app namespace\"]\n        Frontend[Frontend&lt;br/&gt;tier=frontend] --&gt;|:8080| Backend[Backend&lt;br/&gt;tier=backend]\n        Backend --&gt;|:5432| DB[(Database&lt;br/&gt;tier=database)]\n    end\n\n    subgraph kube[\"kube-system\"]\n        DNS[CoreDNS]\n    end\n\n    Frontend -.-&gt;|UDP:53| DNS\n    Backend -.-&gt;|UDP:53| DNS\n    DB -.-&gt;|UDP:53| DNS\n\n    DB -.-&gt;|\u274c No egress| Internet\n\n    style Frontend fill:#69f\n    style Backend fill:#9f6\n    style DB fill:#f96</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#pattern-6-allow-external-https-common-egress","title":"Pattern 6: Allow External HTTPS (Common Egress)","text":"<pre><code># Allow pods to make outbound HTTPS calls (APIs, registries)\negress:\n- to:\n  - ipBlock:\n      cidr: 0.0.0.0/0\n      except:\n      - 169.254.169.254/32   # Block AWS metadata\n      - 169.254.169.123/32   # Block GCP metadata\n      - 10.0.0.0/8           # Block private networks\n      - 172.16.0.0/12\n      - 192.168.0.0/16\n  ports:\n  - protocol: TCP\n    port: 443\n</code></pre> <p>Security tip: Always block cloud metadata endpoints to prevent credential theft!</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#advanced-scenarios-and-edge-cases","title":"Advanced Scenarios and Edge Cases","text":"","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#combining-multiple-policies-additive-behavior","title":"Combining Multiple Policies (Additive Behavior)","text":"<p>Multiple policies selecting the same pod merge via OR logic:</p> <pre><code>---\n# Policy 1: Allow port 80\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-http\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  ingress:\n  - ports:\n    - protocol: TCP\n      port: 80\n\n---\n# Policy 2: Allow port 443\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-https\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  ingress:\n  - ports:\n    - protocol: TCP\n      port: 443\n\n# Result: Pods labeled app=web can receive on BOTH ports 80 and 443\n</code></pre> <pre><code>flowchart TB\n    subgraph \"Policy Merging\"\n        NP1[Policy 1:&lt;br/&gt;Allow :80] -.-&gt;|OR| Result\n        NP2[Policy 2:&lt;br/&gt;Allow :443] -.-&gt;|OR| Result\n\n        Result[Final Result:&lt;br/&gt;Allow :80 AND :443]\n    end\n\n    Pod[app=web pod] --&gt; Result\n\n    style Result fill:#9f6,stroke:#333,stroke-width:3px</code></pre> <p>Exam scenario: \"Add HTTPS support without breaking HTTP\" \u2192 Create a second policy, don't modify the first!</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#troubleshooting-blocked-traffic","title":"Troubleshooting Blocked Traffic","text":"<p>When connectivity breaks, follow this systematic approach:</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#1-verify-pod-labels","title":"1. Verify Pod Labels","text":"<pre><code># Check pod labels match policy selectors\nkubectl get pods -n production --show-labels\n\n# Describe policy to see selectors\nkubectl describe networkpolicy backend-policy -n production\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#2-test-connectivity-with-netshoot","title":"2. Test Connectivity with netshoot","text":"<pre><code># Launch debugging pod\nkubectl run netshoot --rm -it --image=nicolaka/netshoot -- /bin/bash\n\n# Inside the pod, test connectivity\ncurl -v backend-service:8080\nnc -zv backend-pod-ip 8080\nnslookup backend-service      # Check DNS works!\n</code></pre> <p>Exam tip: The exam environment includes <code>busybox</code> and often <code>netshoot</code> images. Know how to use them!</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#3-check-policy-selection","title":"3. Check Policy Selection","text":"<pre><code># See which policies affect a pod\nkubectl get networkpolicies -n production -o wide\n\n# Detailed policy examination\nkubectl describe networkpolicy my-policy -n production\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#4-common-mistakes-checklist","title":"4. Common Mistakes Checklist","text":"<ul> <li> Forgot to include DNS egress rules</li> <li> Used Service port instead of container port</li> <li> Mixed up AND vs OR selector logic</li> <li> Applied policy to wrong namespace</li> <li> Typo in label selectors</li> <li> Forgot <code>policyTypes</code> field</li> <li> Selected zero pods accidentally</li> </ul>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#5-debugging-tool-policy-visualizer","title":"5. Debugging Tool: Policy Visualizer","text":"<p>During preparation (not in exam), use the Network Policy Editor to visualize and test policies.</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#cross-namespace-communication-patterns","title":"Cross-Namespace Communication Patterns","text":"<pre><code># Allow monitoring namespace to scrape metrics from all namespaces\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-monitoring-ingress\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      metrics: \"true\"        # Only pods exposing metrics\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring   # Must label the namespace!\n    ports:\n    - protocol: TCP\n      port: 9090\n</code></pre> <p>Critical step: Label the namespace first!</p> <pre><code>kubectl label namespace monitoring name=monitoring\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#ingress-vs-egress-the-complete-picture","title":"Ingress vs Egress: The Complete Picture","text":"<p>Understanding the difference is crucial for exam success:</p> <pre><code>flowchart LR\n    subgraph \"Network Policy Perspective\"\n        subgraph Client[\"Client Pod\"]\n            C[Egress Rule:&lt;br/&gt;Outgoing traffic]\n        end\n\n        subgraph Server[\"Server Pod\"]\n            S[Ingress Rule:&lt;br/&gt;Incoming traffic]\n        end\n\n        C --&gt;|Traffic flow| S\n    end\n\n    Note[Both must allow&lt;br/&gt;for traffic to flow!]\n\n    style C fill:#69f\n    style S fill:#9f6\n    style Note fill:#ff6</code></pre> <p>Key principle: For traffic to flow, both the source's egress policy and the destination's ingress policy must allow it!</p> <pre><code># Source pod needs egress rule\negress:\n- to:\n  - podSelector:\n      matchLabels:\n        app: api\n\n# Destination pod needs ingress rule\ningress:\n- from:\n  - podSelector:\n      matchLabels:\n        app: client\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#exam-scenario-deny-all-except-dns-and-metrics","title":"Exam Scenario: Deny All Except DNS and Metrics","text":"<p>A common exam task:</p> <pre><code># Deny all traffic except DNS and Prometheus metrics scraping\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: secure-app\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: secure-service\n  policyTypes:\n  - Ingress\n  - Egress\n\n  # Allow metrics scraping\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring\n    ports:\n    - protocol: TCP\n      port: 9090\n\n  # Allow only DNS egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n    - protocol: TCP\n      port: 53\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#cka-exam-skills-and-strategies","title":"CKA Exam Skills and Strategies","text":"","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#no-imperative-commands-available","title":"No Imperative Commands Available","text":"<p>Unlike many Kubernetes resources, there is no <code>kubectl create networkpolicy</code> command. You must:</p> <ol> <li>Navigate to the official documentation during the exam</li> <li>Copy and modify existing examples</li> <li>Understand YAML structure deeply (no shortcuts!)</li> </ol> <p>Exam strategy: <pre><code># Bookmark this in the exam browser\nhttps://kubernetes.io/docs/concepts/services-networking/network-policies/\n\n# Practice finding examples quickly\n# Search for: \"NetworkPolicy example\"\n# Ctrl+F for specific patterns like \"egress\" or \"namespaceSelector\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#testing-with-netshoot-container","title":"Testing with netshoot Container","text":"<p>The <code>netshoot</code> container is your best friend for validation:</p> <pre><code># Launch netshoot with specific labels\nkubectl run netshoot --rm -it \\\n  --labels=\"app=frontend,tier=web\" \\\n  --image=nicolaka/netshoot -- /bin/bash\n\n# Inside netshoot, test connectivity\ncurl -v http://backend-service:8080/health\nnc -zv database-service 5432\nnslookup backend-service\n\n# Test specific IPs\nping 10.244.1.5\n\n# Check DNS resolution\ndig +short backend-service.production.svc.cluster.local\n</code></pre> <p>Busybox alternative (lighter weight):</p> <pre><code>kubectl run busybox --rm -it \\\n  --labels=\"access=true\" \\\n  --image=busybox:1.28 -- /bin/sh\n\n# Inside busybox\nwget -O- http://nginx-service:80\nnc -zv backend 8080\nnslookup backend-service\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#debugging-connectivity-issues","title":"Debugging Connectivity Issues","text":"<p>Systematic approach for exam scenarios:</p> <pre><code># Step 1: Verify pod is running\nkubectl get pods -n production -l app=backend\n\n# Step 2: Check pod IP and labels\nkubectl get pods -n production -l app=backend -o wide --show-labels\n\n# Step 3: List all network policies in namespace\nkubectl get networkpolicies -n production\n\n# Step 4: Describe the relevant policy\nkubectl describe networkpolicy backend-policy -n production\n\n# Step 5: Check if DNS is working (if egress is blocked)\nkubectl run test --rm -it --image=busybox:1.28 -- nslookup kubernetes.default\n\n# Step 6: Test from a labeled pod\nkubectl run test --rm -it --labels=\"app=frontend\" --image=busybox:1.28 -- \\\n  wget --spider --timeout=1 backend-service:8080\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#common-exam-scenarios","title":"Common Exam Scenarios","text":"<p>Based on actual CKA exam reports:</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#scenario-1-implement-namespace-isolation","title":"Scenario 1: Implement Namespace Isolation","text":"<p>\"Create a network policy in namespace <code>dev</code> that allows pods to communicate only with other pods in the same namespace and DNS.\"</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: namespace-isolation\n  namespace: dev\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector: {}\n  egress:\n  - to:\n    - podSelector: {}\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#scenario-2-secure-database-access","title":"Scenario 2: Secure Database Access","text":"<p>\"Create a policy allowing only pods labeled <code>role=api</code> to access pods labeled <code>role=db</code> on port 5432.\"</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: db-access-control\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: api\n    ports:\n    - protocol: TCP\n      port: 5432\n</code></pre>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#scenario-3-default-deny-with-exceptions","title":"Scenario 3: Default Deny with Exceptions","text":"<p>\"Create a default deny-all policy in namespace <code>sensitive</code>, then allow ingress to pods labeled <code>app=web</code> from pods in namespace <code>proxy</code>.\"</p> <pre><code>---\n# Default deny\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: sensitive\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n\n---\n# Allow from proxy namespace\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-proxy-ingress\n  namespace: sensitive\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: proxy\n</code></pre> <p>Don't forget: Label the proxy namespace! <pre><code>kubectl label namespace proxy name=proxy\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#time-saving-exam-tips","title":"Time-Saving Exam Tips","text":"<ol> <li>Prepare YAML templates in notepad before exam starts (during tutorial time)</li> <li>Bookmark key documentation pages in the exam browser</li> <li>Use kubectl explain for quick field references:    <pre><code>kubectl explain networkpolicy.spec.ingress\nkubectl explain networkpolicy.spec.egress.to\n</code></pre></li> <li>Test with busybox immediately after creating policies</li> <li>Remember the DNS egress rule - write it once, copy/paste everywhere</li> <li>Use describe, not get for debugging - more details!</li> </ol>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#verification-checklist","title":"Verification Checklist","text":"<p>Before moving to the next exam question:</p> <ul> <li> Policy exists: <code>kubectl get networkpolicy -n &lt;namespace&gt;</code></li> <li> Policy selects correct pods: <code>kubectl describe networkpolicy ...</code></li> <li> Test allowed traffic: <code>kubectl run test ... -- wget ...</code></li> <li> Test blocked traffic: <code>kubectl run test ... -- wget ...</code> (should timeout)</li> <li> DNS works if egress policy exists: <code>nslookup kubernetes.default</code></li> </ul>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>Network Policies are your cluster's immune system\u2014they prevent unauthorized communication and limit blast radius during incidents. For the CKA exam, mastery means:</p> <p>\u2705 Understanding default-deny principles and starting with restrictive policies \u2705 Mastering AND vs OR selector logic (draw diagrams!) \u2705 Remembering DNS egress rules (the most common mistake) \u2705 Knowing CNI plugin requirements (Flannel doesn't support policies!) \u2705 Testing systematically with netshoot/busybox containers \u2705 Navigating official documentation efficiently under time pressure</p> <p>In production, Network Policies provide: - Defense in depth beyond RBAC and Pod Security - Compliance enablement for regulated industries - Multi-tenancy security in shared clusters - Incident containment when workloads are compromised</p> <p>Final exam wisdom: Network Policies are additive (multiple policies combine via OR), but selectors within a policy use AND logic when in the same array entry. Draw it out, test it twice, move forward confidently.</p> <p>Now go forth and secure those pods\u2014with care and love! \ud83d\udd12</p>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-network-policies/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official Kubernetes Network Policies Documentation</li> <li>Network Policy Editor (Visualizer)</li> <li>Calico Network Policy Tutorial</li> <li>Cilium Network Policy Guide</li> <li>CKA Curriculum - Services &amp; Networking</li> </ul>","tags":["kubernetes","k8s","cka-prep","network-policies","networking","security","isolation"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/","title":"Understanding Kubernetes Objects and YAML Manifests","text":"<p>Master the foundation of Kubernetes declarative configuration. Learn object anatomy, YAML syntax, labels, selectors, and annotations for CKA exam success and production deployments.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#overview","title":"Overview","text":"<p>Kubernetes objects are persistent entities in the Kubernetes system that represent the desired state of your cluster. Understanding object structure and YAML manifests is fundamental to the CKA exam and real-world Kubernetes administration.</p> <p>CKA Exam Domain: All domains (objects are used everywhere)</p> <p>Key Insight: Every Kubernetes resource you create, modify, or delete is an object with a consistent structure. Mastering this structure enables you to work with any Kubernetes resource confidently.</p> <p>What You'll Learn: - Kubernetes API object model and resource types - YAML syntax fundamentals and best practices - Object anatomy: metadata, spec, status structure - Labels and selectors for resource organization - Annotations for non-identifying metadata - Field validation and troubleshooting strategies</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#kubernetes-object-model","title":"Kubernetes Object Model","text":"<p>Kubernetes uses a declarative model where you describe the desired state, and the control plane works continuously to maintain that state.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#what-is-a-kubernetes-object","title":"What is a Kubernetes Object?","text":"<p>Definition: A Kubernetes object is a persistent entity that represents: - What applications are running (and on which nodes) - How many replicas should exist - Which resources are available to applications - Policies around behavior (restart, upgrades, fault-tolerance)</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#object-categories","title":"Object Categories","text":"<pre><code>graph TB\n    subgraph \"Workload Resources\"\n        POD[Pod]\n        DEPLOY[Deployment]\n        RS[ReplicaSet]\n        SS[StatefulSet]\n        DS[DaemonSet]\n        JOB[Job]\n        CRON[CronJob]\n    end\n\n    subgraph \"Service &amp; Networking\"\n        SVC[Service]\n        ING[Ingress]\n        NP[NetworkPolicy]\n        EP[Endpoints]\n    end\n\n    subgraph \"Configuration &amp; Storage\"\n        CM[ConfigMap]\n        SECRET[Secret]\n        PV[PersistentVolume]\n        PVC[PersistentVolumeClaim]\n        SC[StorageClass]\n    end\n\n    subgraph \"Cluster Resources\"\n        NS[Namespace]\n        NODE[Node]\n        SA[ServiceAccount]\n        ROLE[Role/ClusterRole]\n    end\n\n    DEPLOY --&gt; RS\n    RS --&gt; POD\n    SS --&gt; POD\n    DS --&gt; POD\n    SVC --&gt; POD\n    ING --&gt; SVC\n\n    style POD fill:#e1f5ff\n    style DEPLOY fill:#e8f5e8\n    style SVC fill:#fff4e1\n    style CM fill:#f5e1ff</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#object-lifecycle","title":"Object Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Desired: User creates manifest\n\n    Desired --&gt; Submitted: kubectl apply\n    Submitted --&gt; Validated: API server validates\n\n    Validated --&gt; Rejected: Validation fails\n    Rejected --&gt; [*]\n\n    Validated --&gt; Persisted: Write to etcd\n    Persisted --&gt; Scheduled: Controller processes\n    Scheduled --&gt; Running: Kubelet executes\n\n    Running --&gt; Updated: User modifies\n    Updated --&gt; Validated\n\n    Running --&gt; Deleted: User deletes\n    Deleted --&gt; [*]\n\n    Running --&gt; Running: System reconciles</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#object-anatomy-the-four-essential-fields","title":"Object Anatomy: The Four Essential Fields","text":"<p>Every Kubernetes object manifest contains four top-level fields:</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#complete-object-structure","title":"Complete Object Structure","text":"<pre><code>apiVersion: apps/v1              # 1. API version\nkind: Deployment                 # 2. Object type\nmetadata:                        # 3. Identifying metadata\n  name: nginx-deployment\n  namespace: production\n  labels:\n    app: nginx\n    tier: frontend\n  annotations:\n    description: \"Production web server\"\nspec:                            # 4. Desired state\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\nstatus:                          # System-managed current state\n  availableReplicas: 3\n  readyReplicas: 3\n  observedGeneration: 1\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#1-apiversion","title":"1. apiVersion","text":"<p>Purpose: Specifies the API group and version for the object type.</p> <p>Format: <code>&lt;group&gt;/&lt;version&gt;</code> or just <code>&lt;version&gt;</code> for core API</p> <p>Common API Versions: <pre><code># Core API (no group)\nv1                              # Pod, Service, ConfigMap, Secret, Namespace\n\n# Apps API\napps/v1                         # Deployment, StatefulSet, DaemonSet, ReplicaSet\n\n# Batch API\nbatch/v1                        # Job, CronJob\n\n# Networking API\nnetworking.k8s.io/v1           # Ingress, NetworkPolicy\n\n# RBAC API\nrbac.authorization.k8s.io/v1   # Role, ClusterRole, RoleBinding\n\n# Storage API\nstorage.k8s.io/v1              # StorageClass, VolumeAttachment\n</code></pre></p> <p>Finding API Versions: <pre><code># List all API resources and versions\nkubectl api-resources\n\n# Check specific resource\nkubectl explain deployment | head -5\n# KIND:       Deployment\n# VERSION:    apps/v1\n\n# List all API versions\nkubectl api-versions\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#2-kind","title":"2. kind","text":"<p>Purpose: Identifies the type of object being created.</p> <p>Common Object Kinds: - Workloads: Pod, Deployment, StatefulSet, DaemonSet, Job, CronJob - Services: Service, Ingress, Endpoints - Configuration: ConfigMap, Secret - Storage: PersistentVolume, PersistentVolumeClaim, StorageClass - Cluster: Namespace, Node, ServiceAccount - Access: Role, ClusterRole, RoleBinding, ClusterRoleBinding</p> <p>Case Sensitivity: Kind names are case-sensitive (must be exact capitalization).</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#3-metadata","title":"3. metadata","text":"<p>Purpose: Data that uniquely identifies the object.</p> <p>Required Fields: - <code>name</code> - Unique within namespace and object type - <code>namespace</code> - (optional for cluster-scoped resources)</p> <p>Common Optional Fields: - <code>labels</code> - Key-value pairs for organization and selection - <code>annotations</code> - Non-identifying metadata - <code>uid</code> - System-generated unique identifier - <code>resourceVersion</code> - Internal version for optimistic concurrency - <code>creationTimestamp</code> - When object was created</p> <p>Metadata Example: <pre><code>metadata:\n  name: webapp                           # Required: object name\n  namespace: production                  # Namespace (required for namespaced resources)\n  labels:                                # Labels for selection\n    app: webapp\n    tier: frontend\n    version: \"2.0\"\n    environment: production\n  annotations:                           # Annotations for metadata\n    description: \"Main production web application\"\n    owner: \"platform-team@company.com\"\n    version: \"2.0.1\"\n    deployment-date: \"2024-01-15\"\n  uid: 12345678-1234-1234-1234-123456789012    # System-generated\n  resourceVersion: \"1234567\"             # System-managed version\n  creationTimestamp: \"2024-01-15T10:30:00Z\"  # System-generated\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#4-spec","title":"4. spec","text":"<p>Purpose: Describes the desired state of the object.</p> <p>Characteristics: - Varies by object <code>kind</code> - User-defined and user-managed - Controller reads spec to understand desired state - Declarative: describes \"what\", not \"how\"</p> <p>Pod Spec Example: <pre><code>spec:\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 200m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 80\n      initialDelaySeconds: 30\n      periodSeconds: 10\n  restartPolicy: Always\n  nodeSelector:\n    disk: ssd\n</code></pre></p> <p>Deployment Spec Example: <pre><code>spec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:                    # Pod template\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#5-status-system-managed","title":"5. status (System-Managed)","text":"<p>Purpose: Describes the current observed state of the object.</p> <p>Characteristics: - Read-only for users - Managed by Kubernetes controllers - Updated continuously by the control plane - Represents actual state vs desired state (spec)</p> <p>Pod Status Example: <pre><code>status:\n  phase: Running               # Pod lifecycle phase\n  conditions:                  # Detailed status conditions\n  - type: Initialized\n    status: \"True\"\n    lastTransitionTime: \"2024-01-15T10:30:05Z\"\n  - type: Ready\n    status: \"True\"\n    lastTransitionTime: \"2024-01-15T10:30:10Z\"\n  - type: ContainersReady\n    status: \"True\"\n    lastTransitionTime: \"2024-01-15T10:30:10Z\"\n  - type: PodScheduled\n    status: \"True\"\n    lastTransitionTime: \"2024-01-15T10:30:00Z\"\n  containerStatuses:\n  - name: nginx\n    ready: true\n    restartCount: 0\n    state:\n      running:\n        startedAt: \"2024-01-15T10:30:08Z\"\n  hostIP: 192.168.1.100\n  podIP: 10.244.1.50\n  startTime: \"2024-01-15T10:30:00Z\"\n</code></pre></p> <p>Deployment Status Example: <pre><code>status:\n  availableReplicas: 3\n  readyReplicas: 3\n  replicas: 3\n  updatedReplicas: 3\n  observedGeneration: 5\n  conditions:\n  - type: Available\n    status: \"True\"\n    reason: MinimumReplicasAvailable\n  - type: Progressing\n    status: \"True\"\n    reason: NewReplicaSetAvailable\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#yaml-syntax-fundamentals","title":"YAML Syntax Fundamentals","text":"<p>YAML (YAML Ain't Markup Language) is the standard format for Kubernetes manifests.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#yaml-basics","title":"YAML Basics","text":"<p>Data Types: <pre><code># Strings\nname: nginx-deployment\ndescription: \"Multi-word string in quotes\"\nmultiline: |\n  This is a multi-line string\n  that preserves newlines\n\n# Numbers\nreplicas: 3\nport: 80\ncpu: 0.5\n\n# Booleans\nenabled: true\ndebug: false\n\n# Lists (arrays)\nargs:\n- \"arg1\"\n- \"arg2\"\n- \"arg3\"\n\n# Or inline\nargs: [\"arg1\", \"arg2\", \"arg3\"]\n\n# Maps (key-value pairs)\nlabels:\n  app: nginx\n  tier: frontend\n\n# Nested structures\nmetadata:\n  labels:\n    app: nginx\n  annotations:\n    description: \"Production app\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#indentation-rules","title":"Indentation Rules","text":"<p>Critical: YAML uses spaces only (no tabs) for indentation.</p> <pre><code># CORRECT - 2 spaces per level\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    tier: frontend\n\n# CORRECT - 4 spaces also works (be consistent)\nmetadata:\n    name: nginx\n    labels:\n        app: nginx\n\n# WRONG - mixing tabs and spaces\nmetadata:\n    name: nginx        # Tab used (will fail)\n  labels:\n    app: nginx       # Spaces used\n</code></pre> <p>Best Practice: Use 2-space indentation consistently.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-yaml-patterns","title":"Common YAML Patterns","text":"<p>Lists of Objects: <pre><code>containers:\n- name: nginx                   # First container\n  image: nginx:1.21\n  ports:\n  - containerPort: 80\n- name: sidecar                 # Second container\n  image: busybox:latest\n  command: [\"sh\", \"-c\", \"sleep 3600\"]\n</code></pre></p> <p>Multi-line Strings: <pre><code># Literal block (preserves newlines)\nscript: |\n  #!/bin/bash\n  echo \"Line 1\"\n  echo \"Line 2\"\n\n# Folded block (newlines become spaces)\ndescription: &gt;\n  This is a very long description\n  that spans multiple lines but\n  will be folded into a single line.\n\n# Result: \"This is a very long description that spans multiple lines...\"\n</code></pre></p> <p>Environment Variables: <pre><code>env:\n- name: DATABASE_HOST\n  value: \"mysql.default.svc.cluster.local\"\n- name: DATABASE_PORT\n  value: \"3306\"\n- name: DATABASE_PASSWORD\n  valueFrom:\n    secretKeyRef:\n      name: db-secret\n      key: password\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#yaml-validation","title":"YAML Validation","text":"<p>Common Errors:</p> <pre><code># ERROR: Missing colon\nmetadata\n  name: nginx         # Should be \"metadata:\"\n\n# ERROR: Wrong indentation\nmetadata:\n  name: nginx\n labels:              # Misaligned (should be 2 spaces)\n   app: nginx\n\n# ERROR: Missing quotes for special characters\nannotation: \"true\"   # Boolean - needs quotes to be string\nannotation: true     # Boolean value\n\n# ERROR: List item indentation\ncontainers:\n  - name: nginx      # Correct\n- name: sidecar      # Wrong (should align with first item)\n</code></pre> <p>Validation Tools: <pre><code># kubectl validates before applying\nkubectl apply --dry-run=client -f manifest.yaml\n\n# Check syntax only\nkubectl apply --dry-run=server -f manifest.yaml\n\n# Use yamllint for detailed validation\nyamllint manifest.yaml\n\n# Python YAML validation\npython -c 'import yaml, sys; yaml.safe_load(sys.stdin)' &lt; manifest.yaml\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#labels-organizing-and-selecting-resources","title":"Labels: Organizing and Selecting Resources","text":"<p>Labels are key-value pairs attached to objects for identification and grouping.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#label-syntax","title":"Label Syntax","text":"<p>Format: <code>key: value</code></p> <p>Key Syntax: - Optional prefix: <code>&lt;prefix&gt;/&lt;name&gt;</code> (prefix \u2264 253 chars, name \u2264 63 chars) - Name: alphanumeric, <code>-</code>, <code>_</code>, <code>.</code> (must start/end with alphanumeric)</p> <p>Value Syntax: - \u2264 63 characters - Alphanumeric, <code>-</code>, <code>_</code>, <code>.</code> (can be empty)</p> <p>Examples: <pre><code>labels:\n  # Simple labels\n  app: nginx\n  tier: frontend\n  environment: production\n\n  # Prefixed labels (for third-party tools)\n  example.com/team: platform\n  app.kubernetes.io/name: nginx\n  app.kubernetes.io/version: \"1.21\"\n\n  # Valid special characters\n  app.version: \"2.0.1\"\n  team_name: platform-team\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#recommended-labels","title":"Recommended Labels","text":"<p>Kubernetes recommends a standard set of labels for consistency:</p> <pre><code>metadata:\n  labels:\n    # Application identity\n    app.kubernetes.io/name: nginx                    # Application name\n    app.kubernetes.io/instance: nginx-prod           # Unique instance\n    app.kubernetes.io/version: \"1.21.0\"             # Application version\n    app.kubernetes.io/component: webserver           # Component role\n    app.kubernetes.io/part-of: ecommerce-platform   # Parent application\n\n    # Management metadata\n    app.kubernetes.io/managed-by: helm              # Management tool\n\n    # Custom organizational labels\n    team: platform\n    cost-center: engineering\n    environment: production\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#label-use-cases","title":"Label Use Cases","text":"<p>1. Resource Organization: <pre><code># Development pods\nmetadata:\n  labels:\n    environment: dev\n    team: backend\n\n# Production pods\nmetadata:\n  labels:\n    environment: production\n    team: backend\n</code></pre></p> <p>2. Service Selection: <pre><code># Service selects pods with matching labels\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx           # Selects all pods with app=nginx\n    tier: frontend\n  ports:\n  - port: 80\n</code></pre></p> <p>3. Deployment Management: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx        # Deployment manages pods with this label\n  template:\n    metadata:\n      labels:\n        app: nginx      # Pods get this label\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#label-operations","title":"Label Operations","text":"<p>View Labels: <pre><code># Show labels column\nkubectl get pods --show-labels\n\n# Show specific labels as columns\nkubectl get pods -L app,tier,environment\n\n# Filter output\nkubectl get pods -l app=nginx\nkubectl get pods -l 'environment in (dev,staging)'\nkubectl get pods -l app=nginx,tier!=backend\n</code></pre></p> <p>Add/Modify Labels: <pre><code># Add label\nkubectl label pod nginx-pod version=1.0\n\n# Modify label (requires --overwrite)\nkubectl label pod nginx-pod version=2.0 --overwrite\n\n# Add label to all pods\nkubectl label pods --all environment=production\n\n# Remove label\nkubectl label pod nginx-pod version-\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#selectors-matching-resources","title":"Selectors: Matching Resources","text":"<p>Selectors use labels to identify sets of objects.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#selector-types","title":"Selector Types","text":"<pre><code>graph TB\n    SELECTOR[Label Selectors] --&gt; EQUALITY[Equality-Based]\n    SELECTOR --&gt; SET[Set-Based]\n\n    EQUALITY --&gt; EQ[Equal: =, ==]\n    EQUALITY --&gt; NEQ[Not Equal: !=]\n\n    SET --&gt; IN[In: in (...)]\n    SET --&gt; NOTIN[Not In: notin (...)]\n    SET --&gt; EXISTS[Exists: key]\n    SET --&gt; NOTEXISTS[Not Exists: !key]\n\n    style EQUALITY fill:#e1f5ff\n    style SET fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#equality-based-selectors","title":"Equality-Based Selectors","text":"<p>Syntax: <code>key=value</code> or <code>key!=value</code></p> <p>Command-Line Examples: <pre><code># Single equality\nkubectl get pods -l app=nginx\n\n# Multiple conditions (AND logic)\nkubectl get pods -l app=nginx,tier=frontend\n\n# Not equal\nkubectl get pods -l app=nginx,environment!=production\n</code></pre></p> <p>Manifest Examples: <pre><code># Service selector (equality-based only)\nselector:\n  app: nginx\n  tier: frontend\n\n# Equivalent to:\n# app=nginx AND tier=frontend\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#set-based-selectors","title":"Set-Based Selectors","text":"<p>Syntax: More expressive matching with <code>in</code>, <code>notin</code>, <code>exists</code></p> <p>Command-Line Examples: <pre><code># In set\nkubectl get pods -l 'environment in (dev,staging)'\n\n# Not in set\nkubectl get pods -l 'tier notin (cache,db)'\n\n# Label exists\nkubectl get pods -l app\n\n# Label does not exist\nkubectl get pods -l '!app'\n\n# Complex combination\nkubectl get pods -l 'environment in (prod),tier notin (cache),app'\n</code></pre></p> <p>Manifest Examples: <pre><code># Deployment selector (supports both)\nselector:\n  matchLabels:                    # Equality-based\n    app: nginx\n  matchExpressions:               # Set-based\n  - key: tier\n    operator: In\n    values:\n    - frontend\n    - api\n  - key: environment\n    operator: NotIn\n    values:\n    - testing\n  - key: critical\n    operator: Exists\n  - key: deprecated\n    operator: DoesNotExist\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#selector-operators","title":"Selector Operators","text":"Operator Description Example <code>In</code> Value in set <code>tier in (frontend, api)</code> <code>NotIn</code> Value not in set <code>env notin (test, dev)</code> <code>Exists</code> Label exists <code>critical</code> (key exists) <code>DoesNotExist</code> Label doesn't exist <code>!deprecated</code> (key absent)","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#selector-matching-logic","title":"Selector Matching Logic","text":"<pre><code>flowchart TD\n    START([Resource with Labels]) --&gt; CHECK1{matchLabels&lt;br/&gt;satisfied?}\n\n    CHECK1 --&gt;|No| NOMATCH[No Match]\n    CHECK1 --&gt;|Yes| CHECK2{matchExpressions&lt;br/&gt;all true?}\n\n    CHECK2 --&gt;|No| NOMATCH\n    CHECK2 --&gt;|Yes| MATCH[Match Found]\n\n    MATCH --&gt; SELECTED[Resource Selected]\n    NOMATCH --&gt; IGNORED[Resource Ignored]\n\n    style MATCH fill:#e8f5e8\n    style NOMATCH fill:#ffe5e5</code></pre> <p>Example: <pre><code># Pod labels\nmetadata:\n  labels:\n    app: nginx\n    tier: frontend\n    environment: production\n    version: \"2.0\"\n\n# Selector\nselector:\n  matchLabels:\n    app: nginx                    # \u2705 Match\n  matchExpressions:\n  - key: tier\n    operator: In\n    values: [frontend, api]       # \u2705 Match (tier=frontend)\n  - key: environment\n    operator: NotIn\n    values: [dev, test]           # \u2705 Match (production not in list)\n  - key: version\n    operator: Exists              # \u2705 Match (version label exists)\n\n# Result: Pod matches selector\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#annotations-non-identifying-metadata","title":"Annotations: Non-Identifying Metadata","text":"<p>Annotations store arbitrary metadata that doesn't identify or select objects.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#annotations-vs-labels","title":"Annotations vs Labels","text":"Aspect Labels Annotations Purpose Identify and select Store metadata Selectable Yes (with selectors) No Size Limit 63 chars (value) 256 KB (total) Structure Simple key-value Can store JSON, YAML Use Case Grouping, selection Documentation, config","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#annotation-syntax","title":"Annotation Syntax","text":"<p>Format: Same as labels but values can be larger and more complex</p> <pre><code>metadata:\n  annotations:\n    # Documentation\n    description: \"Production Nginx deployment with 3 replicas\"\n    owner: \"platform-team@company.com\"\n    documentation: \"https://wiki.company.com/nginx-deployment\"\n\n    # Build information\n    build-version: \"2.0.1\"\n    git-commit: \"a3f2b1c\"\n    ci-pipeline: \"https://jenkins.company.com/job/nginx/123\"\n\n    # Operational metadata\n    deployment-date: \"2024-01-15T10:30:00Z\"\n    last-updated-by: \"john.doe@company.com\"\n\n    # Tool-specific configuration\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\n    prometheus.io/path: \"/metrics\"\n\n    # JSON configuration\n    custom-config: '{\"timeout\": 30, \"retries\": 3}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-annotation-use-cases","title":"Common Annotation Use Cases","text":"<p>1. Tool Integration: <pre><code>annotations:\n  # Prometheus monitoring\n  prometheus.io/scrape: \"true\"\n  prometheus.io/port: \"8080\"\n\n  # Nginx Ingress\n  nginx.ingress.kubernetes.io/rewrite-target: \"/\"\n  nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n\n  # Cert-manager\n  cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n\n  # Istio service mesh\n  sidecar.istio.io/inject: \"true\"\n</code></pre></p> <p>2. Change Tracking: <pre><code>annotations:\n  kubernetes.io/change-cause: \"Update nginx to version 1.21\"\n  deployment.kubernetes.io/revision: \"5\"\n</code></pre></p> <p>3. Documentation: <pre><code>annotations:\n  description: |\n    Main production web server deployment.\n\n    Handles customer-facing traffic with:\n    - 3 replicas for high availability\n    - Rolling update strategy\n    - Health checks configured\n\n    Contact: platform-team@company.com\n\n  runbook: \"https://wiki.company.com/runbooks/nginx-troubleshooting\"\n  oncall: \"https://pagerduty.com/schedules/nginx-team\"\n</code></pre></p> <p>4. Configuration Storage: <pre><code>annotations:\n  # Complex JSON configuration\n  fluentd-config: |\n    {\n      \"outputs\": [\n        {\"type\": \"elasticsearch\", \"host\": \"es.logging.svc\"},\n        {\"type\": \"s3\", \"bucket\": \"logs-backup\"}\n      ]\n    }\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#annotation-operations","title":"Annotation Operations","text":"<pre><code># View annotations\nkubectl describe pod nginx-pod | grep -A 10 \"Annotations:\"\n\n# Add annotation\nkubectl annotate pod nginx-pod description=\"Production web server\"\n\n# Update annotation\nkubectl annotate pod nginx-pod description=\"Updated description\" --overwrite\n\n# Remove annotation\nkubectl annotate pod nginx-pod description-\n\n# Annotate all resources of type\nkubectl annotate deployments --all team=platform\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#field-validation-and-troubleshooting","title":"Field Validation and Troubleshooting","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#required-vs-optional-fields","title":"Required vs Optional Fields","text":"<p>Required Fields (most common): <pre><code># All objects\napiVersion: apps/v1              # Required\nkind: Deployment                 # Required\nmetadata:\n  name: nginx                    # Required\nspec:                            # Required\n\n# Pod spec\nspec:\n  containers:                    # Required\n  - name: nginx                  # Required\n    image: nginx:1.21            # Required\n\n# Service spec\nspec:\n  selector:                      # Required\n    app: nginx\n  ports:                         # Required\n  - port: 80\n</code></pre></p> <p>Determining Required Fields: <pre><code># Use kubectl explain\nkubectl explain pod.spec\n# FIELDS:\n#   containers    &lt;[]Container&gt; -required-\n#   volumes       &lt;[]Volume&gt;\n\nkubectl explain pod.spec.containers\n# FIELDS:\n#   name          &lt;string&gt; -required-\n#   image         &lt;string&gt; -required-\n#   command       &lt;[]string&gt;        # Optional (no -required-)\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-validation-errors","title":"Common Validation Errors","text":"<p>1. Missing Required Field: <pre><code># ERROR: Missing image\nspec:\n  containers:\n  - name: nginx\n    # image: nginx:1.21    # Missing required field\n\n# Error message:\n# error: error validating \"pod.yaml\": error validating data:\n# ValidationError(Pod.spec.containers[0]): missing required field \"image\"\n</code></pre></p> <p>2. Invalid Field Name: <pre><code># ERROR: Typo in field name\nmetadata:\n  name: nginx\n  lables:              # Should be \"labels\"\n    app: nginx\n\n# Error message:\n# error: error validating \"pod.yaml\": error validating data:\n# ValidationError(Pod.metadata): unknown field \"lables\"\n</code></pre></p> <p>3. Wrong Data Type: <pre><code># ERROR: String instead of integer\nspec:\n  replicas: \"3\"        # Should be: replicas: 3\n\n# Error message:\n# error: error validating data: ValidationError(Deployment.spec.replicas):\n# invalid type for io.k8s.api.apps.v1.DeploymentSpec.replicas: got \"string\", expected \"integer\"\n</code></pre></p> <p>4. Invalid Value: <pre><code># ERROR: Invalid restart policy\nspec:\n  restartPolicy: OnError    # Valid: Always, OnFailure, Never\n\n# Error message:\n# error: error validating data: ValidationError(Pod.spec.restartPolicy):\n# unsupported value: \"OnError\": supported values: \"Always\", \"OnFailure\", \"Never\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#validation-workflow","title":"Validation Workflow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant kubectl\n    participant API\n    participant Validation\n    participant etcd\n\n    User-&gt;&gt;kubectl: kubectl apply -f manifest.yaml\n    kubectl-&gt;&gt;kubectl: Parse YAML syntax\n\n    alt YAML syntax error\n        kubectl--&gt;&gt;User: Syntax error (invalid YAML)\n    end\n\n    kubectl-&gt;&gt;API: Send parsed object\n    API-&gt;&gt;Validation: Validate object\n\n    Validation-&gt;&gt;Validation: Check required fields\n    Validation-&gt;&gt;Validation: Validate field types\n    Validation-&gt;&gt;Validation: Validate field values\n    Validation-&gt;&gt;Validation: Run admission controllers\n\n    alt Validation fails\n        Validation--&gt;&gt;API: Validation error\n        API--&gt;&gt;User: Error message with details\n    else Validation succeeds\n        Validation-&gt;&gt;etcd: Persist object\n        etcd--&gt;&gt;User: Created/Updated confirmation\n    end</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<pre><code># Validate YAML without creating\nkubectl apply --dry-run=client -f manifest.yaml\n\n# Server-side validation (includes admission controllers)\nkubectl apply --dry-run=server -f manifest.yaml\n\n# Explain field structure\nkubectl explain deployment.spec.template.spec.containers\n\n# Get field path for specific property\nkubectl explain deployment --recursive | grep -A 5 \"replicas\"\n\n# Validate all manifests in directory\nkubectl apply --dry-run=client -f ./manifests/\n\n# Check API resource availability\nkubectl api-resources | grep -i deployment\n\n# View object in YAML (for comparison)\nkubectl get deployment nginx -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#complete-object-examples","title":"Complete Object Examples","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#example-1-multi-container-pod-with-full-metadata","title":"Example 1: Multi-Container Pod with Full Metadata","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-container-pod\n  namespace: production\n  labels:\n    app: webapp\n    tier: frontend\n    environment: production\n    version: \"2.0\"\n  annotations:\n    description: \"Production web application with logging sidecar\"\n    owner: \"platform-team@company.com\"\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\nspec:\n  containers:\n  - name: webapp\n    image: nginx:1.21\n    ports:\n    - name: http\n      containerPort: 80\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 200m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 80\n      initialDelaySeconds: 30\n      periodSeconds: 10\n    env:\n    - name: ENVIRONMENT\n      value: \"production\"\n  - name: log-forwarder\n    image: fluent/fluent-bit:1.9\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n  volumes:\n  - name: logs\n    emptyDir: {}\n  restartPolicy: Always\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#example-2-deployment-with-complete-selectors","title":"Example 2: Deployment with Complete Selectors","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp-deployment\n  namespace: production\n  labels:\n    app.kubernetes.io/name: webapp\n    app.kubernetes.io/version: \"2.0.1\"\n    app.kubernetes.io/component: frontend\n    app.kubernetes.io/part-of: ecommerce-platform\n    app.kubernetes.io/managed-by: kubectl\n  annotations:\n    deployment.kubernetes.io/revision: \"3\"\n    kubernetes.io/change-cause: \"Update to version 2.0.1\"\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: webapp\n      tier: frontend\n    matchExpressions:\n    - key: environment\n      operator: In\n      values:\n      - production\n      - staging\n    - key: deprecated\n      operator: DoesNotExist\n  template:\n    metadata:\n      labels:\n        app: webapp\n        tier: frontend\n        environment: production\n        version: \"2.0.1\"\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"8080\"\n    spec:\n      containers:\n      - name: webapp\n        image: webapp:2.0.1\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 200m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#example-3-service-with-label-selector","title":"Example 3: Service with Label Selector","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: webapp-service\n  namespace: production\n  labels:\n    app: webapp\n    tier: frontend\n  annotations:\n    service.kubernetes.io/topology-aware-hints: \"auto\"\nspec:\n  type: ClusterIP\n  selector:\n    app: webapp           # Selects pods with app=webapp\n    tier: frontend        # AND tier=frontend\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n  sessionAffinity: ClientIP\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-exam-tasks","title":"Common Exam Tasks","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-1-create-object-with-labels","title":"Scenario 1: Create Object with Labels","text":"<p>Task: Create a pod named <code>web</code> with nginx image, labels <code>app=web</code> and <code>tier=frontend</code></p> <pre><code># Imperative with labels\nkubectl run web --image=nginx --labels=\"app=web,tier=frontend\"\n\n# Or generate and modify\nkubectl run web --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n# Edit pod.yaml to add labels\nkubectl apply -f pod.yaml\n\n# Verify\nkubectl get pod web --show-labels\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-2-select-resources-by-labels","title":"Scenario 2: Select Resources by Labels","text":"<p>Task: Get all pods with label <code>environment=production</code> and <code>tier!=backend</code></p> <pre><code># Single label\nkubectl get pods -l environment=production\n\n# Multiple labels (AND)\nkubectl get pods -l environment=production,tier!=backend\n\n# Set-based\nkubectl get pods -l 'environment in (production,staging),tier!=backend'\n\n# Show labels\nkubectl get pods -l environment=production --show-labels\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-3-addmodify-annotations","title":"Scenario 3: Add/Modify Annotations","text":"<p>Task: Add annotation <code>description=\"Production web server\"</code> to deployment <code>webapp</code></p> <pre><code># Add annotation\nkubectl annotate deployment webapp description=\"Production web server\"\n\n# Modify existing (requires --overwrite)\nkubectl annotate deployment webapp description=\"Updated description\" --overwrite\n\n# Verify\nkubectl describe deployment webapp | grep -A 5 \"Annotations:\"\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-4-validate-manifest-before-apply","title":"Scenario 4: Validate Manifest Before Apply","text":"<p>Task: Check if <code>deployment.yaml</code> is valid without creating it</p> <pre><code># Client-side validation (YAML syntax + basic structure)\nkubectl apply --dry-run=client -f deployment.yaml\n\n# Server-side validation (includes admission controllers)\nkubectl apply --dry-run=server -f deployment.yaml\n\n# Explain specific fields\nkubectl explain deployment.spec.template.spec.containers.resources\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-5-extract-object-yaml","title":"Scenario 5: Extract Object YAML","text":"<p>Task: Get running pod's YAML to create template</p> <pre><code># Get full YAML (includes status)\nkubectl get pod nginx -o yaml\n\n# Get YAML without system fields (for template)\nkubectl get pod nginx -o yaml | kubectl neat\n\n# Or manually clean\nkubectl get pod nginx -o yaml &gt; template.yaml\n# Remove status, uid, resourceVersion, creationTimestamp, etc.\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-1-object-creation-10-minutes","title":"Exercise 1: Object Creation (10 minutes)","text":"<p>Objective: Create objects with proper metadata structure</p> <p>Tasks: 1. Create pod <code>frontend-pod</code> with nginx:1.21 image 2. Add labels: <code>app=frontend</code>, <code>tier=web</code>, <code>env=dev</code> 3. Add annotation: <code>description=\"Development frontend pod\"</code> 4. Verify labels and annotations</p> <p>Solution: <pre><code># Generate template\nkubectl run frontend-pod --image=nginx:1.21 --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit pod.yaml\nvim pod.yaml\n# Add to metadata:\n#   labels:\n#     app: frontend\n#     tier: web\n#     env: dev\n#   annotations:\n#     description: \"Development frontend pod\"\n\n# Apply\nkubectl apply -f pod.yaml\n\n# Verify\nkubectl get pod frontend-pod --show-labels\nkubectl describe pod frontend-pod | grep -A 5 \"Annotations:\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-2-label-selection-15-minutes","title":"Exercise 2: Label Selection (15 minutes)","text":"<p>Objective: Practice label selectors</p> <p>Tasks: 1. Create 5 pods with various labels 2. Select pods with <code>environment=production</code> 3. Select pods with <code>tier</code> in (frontend, api) 4. Select pods with <code>environment=production</code> AND <code>tier!=backend</code> 5. Count pods matching each selector</p> <p>Solution: <pre><code># Create pods with different labels\nkubectl run pod1 --image=nginx --labels=\"app=web,environment=production,tier=frontend\"\nkubectl run pod2 --image=nginx --labels=\"app=api,environment=production,tier=api\"\nkubectl run pod3 --image=nginx --labels=\"app=db,environment=production,tier=backend\"\nkubectl run pod4 --image=nginx --labels=\"app=web,environment=staging,tier=frontend\"\nkubectl run pod5 --image=nginx --labels=\"app=cache,environment=dev,tier=cache\"\n\n# Select by environment\nkubectl get pods -l environment=production\n# Expected: pod1, pod2, pod3\n\n# Select by tier (set-based)\nkubectl get pods -l 'tier in (frontend,api)'\n# Expected: pod1, pod2, pod4\n\n# Complex selector\nkubectl get pods -l environment=production,tier!=backend\n# Expected: pod1, pod2\n\n# Count matches\nkubectl get pods -l environment=production --no-headers | wc -l\n# Expected: 3\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-3-deployment-with-selectors-20-minutes","title":"Exercise 3: Deployment with Selectors (20 minutes)","text":"<p>Objective: Create deployment with proper label selectors</p> <p>Tasks: 1. Create deployment <code>webapp</code> with 3 replicas 2. Use nginx:1.21 image 3. Add deployment labels: <code>app=webapp</code>, <code>version=v1</code> 4. Pod template labels: <code>app=webapp</code>, <code>tier=frontend</code>, <code>version=v1</code> 5. Configure selector to match pod labels 6. Verify pods are created with correct labels</p> <p>Solution: <pre><code># Generate template\nkubectl create deployment webapp --image=nginx:1.21 --replicas=3 --dry-run=client -o yaml &gt; deployment.yaml\n\n# Edit deployment.yaml\nvim deployment.yaml\n</code></pre></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp\n  labels:\n    app: webapp\n    version: v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: webapp\n      tier: frontend\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: webapp\n        tier: frontend\n        version: v1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n</code></pre> <pre><code># Apply\nkubectl apply -f deployment.yaml\n\n# Verify deployment\nkubectl get deployment webapp\n\n# Verify pods have correct labels\nkubectl get pods --show-labels -l app=webapp\n\n# Verify selector works\nkubectl get pods -l app=webapp,tier=frontend,version=v1\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-4-annotations-and-documentation-15-minutes","title":"Exercise 4: Annotations and Documentation (15 minutes)","text":"<p>Objective: Use annotations for metadata</p> <p>Tasks: 1. Create deployment with comprehensive annotations 2. Add build info, owner, documentation links 3. Add tool-specific annotations (e.g., Prometheus) 4. Extract and view annotations</p> <p>Solution: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: documented-app\n  annotations:\n    description: \"Production application with comprehensive documentation\"\n    owner: \"platform-team@company.com\"\n    documentation: \"https://wiki.company.com/apps/documented-app\"\n    runbook: \"https://wiki.company.com/runbooks/documented-app\"\n    build-version: \"2.0.1\"\n    git-commit: \"abc123def456\"\n    ci-pipeline: \"https://jenkins.company.com/job/app/123\"\n    deployment-date: \"2024-01-15T10:30:00Z\"\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: documented-app\n  template:\n    metadata:\n      labels:\n        app: documented-app\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9090\"\n    spec:\n      containers:\n      - name: app\n        image: nginx:1.21\n</code></pre></p> <pre><code># Apply\nkubectl apply -f documented-app.yaml\n\n# View annotations\nkubectl describe deployment documented-app | grep -A 15 \"Annotations:\"\n\n# Get specific annotation\nkubectl get deployment documented-app -o jsonpath='{.metadata.annotations.owner}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-5-validation-and-troubleshooting-20-minutes","title":"Exercise 5: Validation and Troubleshooting (20 minutes)","text":"<p>Objective: Practice validation and error fixing</p> <p>Tasks: 1. Create manifest with intentional errors 2. Use validation to identify errors 3. Fix each error 4. Successfully create object</p> <p>Broken Manifest: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: broken-deploy\n  lables:                          # ERROR: Typo\n    app: broken\nspec:\n  replicas: \"3\"                    # ERROR: String instead of int\n  selector:\n    matchLabels:\n      app: broken\n  template:\n    metadata:\n      labels:\n        app: broken\n    spec:\n      containers:\n      - name: nginx\n        # image: nginx:1.21         # ERROR: Missing required field\n        ports:\n        - containerPort: 80\n        restartPolicy: OnError      # ERROR: Invalid value (wrong location too)\n</code></pre></p> <p>Solution: <pre><code># Try to validate\nkubectl apply --dry-run=client -f broken.yaml\n# See errors for each issue\n\n# Fixed version:\n</code></pre></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fixed-deploy\n  labels:                          # Fixed: labels (not lables)\n    app: fixed\nspec:\n  replicas: 3                      # Fixed: integer (not string)\n  selector:\n    matchLabels:\n      app: fixed\n  template:\n    metadata:\n      labels:\n        app: fixed\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21          # Fixed: added required field\n        ports:\n        - containerPort: 80\n      restartPolicy: Always        # Fixed: moved to pod spec, valid value\n</code></pre> <pre><code># Validate fixed version\nkubectl apply --dry-run=client -f fixed.yaml\n# Should succeed\n\n# Apply\nkubectl apply -f fixed.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#quick-reference","title":"Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#object-structure-template","title":"Object Structure Template","text":"<pre><code>apiVersion: &lt;group&gt;/&lt;version&gt;\nkind: &lt;ObjectKind&gt;\nmetadata:\n  name: &lt;object-name&gt;\n  namespace: &lt;namespace&gt;\n  labels:\n    &lt;key&gt;: &lt;value&gt;\n  annotations:\n    &lt;key&gt;: &lt;value&gt;\nspec:\n  # Object-specific desired state\nstatus:\n  # System-managed current state (read-only)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-apiversion-values","title":"Common apiVersion Values","text":"<pre><code>v1                              # Core: Pod, Service, ConfigMap\napps/v1                         # Deployment, StatefulSet, DaemonSet\nbatch/v1                        # Job, CronJob\nnetworking.k8s.io/v1           # Ingress, NetworkPolicy\nrbac.authorization.k8s.io/v1   # Role, ClusterRole, RoleBinding\nstorage.k8s.io/v1              # StorageClass\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#label-selector-syntax","title":"Label Selector Syntax","text":"<pre><code># Equality-based (command-line)\nkubectl get pods -l app=nginx                    # Single label\nkubectl get pods -l app=nginx,tier=frontend      # Multiple (AND)\nkubectl get pods -l app=nginx,tier!=backend      # Not equal\n\n# Set-based (command-line)\nkubectl get pods -l 'environment in (prod,staging)'\nkubectl get pods -l 'tier notin (cache,db)'\nkubectl get pods -l app                          # Label exists\nkubectl get pods -l '!app'                       # Label doesn't exist\n\n# Manifest (YAML)\nselector:\n  matchLabels:                   # Equality-based\n    app: nginx\n  matchExpressions:              # Set-based\n  - key: tier\n    operator: In                 # In, NotIn, Exists, DoesNotExist\n    values: [frontend, api]\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#essential-commands","title":"Essential Commands","text":"<pre><code># Object creation\nkubectl apply -f manifest.yaml\nkubectl create -f manifest.yaml\n\n# Label operations\nkubectl label pod nginx app=web                  # Add\nkubectl label pod nginx app=web --overwrite      # Modify\nkubectl label pod nginx app-                     # Remove\nkubectl get pods --show-labels                   # View\nkubectl get pods -L app,tier                     # Show specific labels\nkubectl get pods -l app=nginx                    # Filter by labels\n\n# Annotation operations\nkubectl annotate pod nginx description=\"Web server\"    # Add\nkubectl annotate pod nginx description- # Remove\nkubectl describe pod nginx                       # View\n\n# Validation\nkubectl apply --dry-run=client -f manifest.yaml  # Client-side\nkubectl apply --dry-run=server -f manifest.yaml  # Server-side\nkubectl explain deployment.spec                  # Field documentation\n\n# Extraction\nkubectl get pod nginx -o yaml                    # Full YAML\nkubectl get pod nginx -o json                    # Full JSON\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Every Kubernetes resource is an object with consistent structure: apiVersion, kind, metadata, spec</p> <p>\u2705 YAML is the standard format - master 2-space indentation and common patterns</p> <p>\u2705 Four essential fields define objects - apiVersion, kind, metadata (name/namespace/labels), spec</p> <p>\u2705 Labels enable selection - use for grouping, Services, Deployments, and queries</p> <p>\u2705 Selectors match labels - equality-based (simple) and set-based (complex matching)</p> <p>\u2705 Annotations store metadata - documentation, configuration, tool integration (no selection)</p> <p>\u2705 Use kubectl explain extensively - fastest way to discover field structure during exam</p> <p>\u2705 Validation catches errors early - always use --dry-run before applying</p> <p>\u2705 Recommended labels maintain consistency - app.kubernetes.io/* prefix for standard labels</p> <p>\u2705 Master label/annotation operations - critical for exam speed and production work</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#next-steps","title":"Next Steps","text":"<p>Continue your CKA journey with:</p> <p>Post 5: Namespaces and Resource Quotas - Learn cluster resource organization and multi-tenancy</p> <p>Related Posts: - Kubernetes Architecture Fundamentals - Understanding cluster components - kubectl Essentials - Command-line mastery - Kubernetes CKA Mastery - Complete Learning Path - Full exam preparation series</p> <p>External Resources: - Kubernetes Objects (Official Docs) - Labels and Selectors - Annotations - Recommended Labels - Understanding Kubernetes Objects</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/","title":"Pods: The Atomic Unit of Kubernetes","text":"<p>Master Kubernetes Pods - the fundamental building block of container orchestration. Learn multi-container patterns, lifecycle management, health probes, and essential troubleshooting techniques for the CKA exam.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#introduction","title":"Introduction","text":"<p>In the vast ecosystem of Kubernetes, pods stand as the fundamental building block - the smallest deployable unit you can create and manage. Understanding pods is not just important for the Certified Kubernetes Administrator (CKA) exam; it's absolutely essential for anyone working with Kubernetes in production environments.</p> <p>Think of a pod as a logical host for your containers. Just as traditional applications might run multiple processes on a single server that share resources and communicate via localhost, pods enable containers to share networking, storage, and a common context. This design pattern enables powerful architectural patterns while maintaining the benefits of container isolation.</p> <p>In this comprehensive guide, you'll master:</p> <ul> <li>Pod fundamentals: What pods are, why they're atomic, and how they work</li> <li>Lifecycle management: Understanding pod phases and state transitions</li> <li>Multi-container patterns: Sidecar, init containers, adapter, and ambassador patterns</li> <li>Configuration mastery: Resource management, health probes, and environment configuration</li> <li>Troubleshooting skills: Debugging common pod issues with kubectl</li> <li>CKA exam techniques: Fast pod creation, debugging strategies, and time-saving tips</li> </ul> <p>CKA Exam Relevance: Pods are covered extensively in the \"Workloads &amp; Scheduling\" domain (15% of the exam). You'll need to demonstrate proficiency in creating, configuring, and troubleshooting pods under time pressure. This guide provides the knowledge and hands-on skills you need to excel.</p> <p>Whether you're preparing for the CKA exam or working to deepen your Kubernetes expertise, mastering pods is your foundation for success. Let's dive in.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#pod-fundamentals","title":"Pod Fundamentals","text":"","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#what-is-a-pod","title":"What is a Pod?","text":"<p>A pod is the smallest deployable unit in Kubernetes. It represents a single instance of a running process in your cluster. While pods can contain one or more containers, they're designed to run a single instance of an application.</p> <pre><code>graph TB\n    subgraph \"Pod: web-app\"\n        subgraph \"Shared Network Namespace\"\n            C1[App Container&lt;br/&gt;nginx:1.21&lt;br/&gt;localhost:80]\n            C2[Logging Sidecar&lt;br/&gt;fluentd&lt;br/&gt;localhost:24224]\n        end\n\n        subgraph \"Shared Storage\"\n            V1[Volume: logs&lt;br/&gt;/var/log/nginx]\n            V2[Volume: config&lt;br/&gt;/etc/nginx]\n        end\n\n        NET[Pod IP: 10.244.1.5&lt;br/&gt;Shared Network]\n    end\n\n    C1 -.-&gt;|reads/writes| V1\n    C1 -.-&gt;|reads| V2\n    C2 -.-&gt;|reads| V1\n    C1 -.-&gt;|localhost| C2\n\n    style C1 fill:#e1f5ff\n    style C2 fill:#fff4e1\n    style V1 fill:#e8f5e8\n    style V2 fill:#e8f5e8\n    style NET fill:#ffe5e5</code></pre> <p>Key Characteristics:</p> <ol> <li>Atomic Unit: Pods are created, scheduled, and managed as a single entity</li> <li>Shared Network: All containers share the same network namespace and IP address</li> <li>Shared Storage: Containers can share volumes mounted to the pod</li> <li>Co-located: Containers in a pod are always scheduled on the same node</li> <li>Co-managed: Containers in a pod have the same lifecycle</li> </ol>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#why-are-pods-atomic","title":"Why Are Pods Atomic?","text":"<p>Kubernetes treats pods as the smallest unit of deployment because:</p> <ul> <li>Scheduling: The scheduler places entire pods on nodes, not individual containers</li> <li>Scaling: Replicas create complete pod copies, not individual containers</li> <li>Lifecycle: All containers in a pod start and stop together</li> <li>Resource allocation: Resources are allocated at the pod level</li> </ul> <p>This design enables tight coupling between containers that need to work together while maintaining loose coupling between pods.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#pod-lifecycle-phases","title":"Pod Lifecycle Phases","text":"<p>Pods move through distinct phases during their lifetime:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Pending: Pod created\n    Pending --&gt; Running: Containers starting\n    Running --&gt; Succeeded: All containers exit 0\n    Running --&gt; Failed: Container exits non-zero\n    Pending --&gt; Failed: Cannot schedule\n    Running --&gt; Unknown: Node communication lost\n    Unknown --&gt; Running: Communication restored\n    Unknown --&gt; Failed: Timeout exceeded\n    Succeeded --&gt; [*]\n    Failed --&gt; [*]\n\n    note right of Pending\n        Waiting for scheduling\n        Pulling images\n        Starting containers\n    end note\n\n    note right of Running\n        At least one container\n        is executing\n    end note\n\n    note right of Succeeded\n        All containers\n        terminated successfully\n    end note\n\n    note right of Failed\n        At least one container\n        failed (non-zero exit)\n    end note</code></pre> Phase Description Common Reasons Pending Pod accepted but not running Scheduling delays, image pulling, insufficient resources Running Pod bound to node, containers created Normal operation Succeeded All containers terminated successfully Batch jobs, init containers completed Failed All containers terminated, at least one failed Application errors, configuration issues Unknown Pod state cannot be determined Node communication failure","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#single-vs-multi-container-pods","title":"Single vs Multi-Container Pods","text":"<p>Single-Container Pods (most common):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: web\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n</code></pre> <p>Multi-Container Pods (for tightly coupled components):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-with-logging\nspec:\n  containers:\n  # Main application container\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n\n  # Sidecar container for log processing\n  - name: log-processor\n    image: fluentd:latest\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n      readOnly: true\n\n  volumes:\n  - name: logs\n    emptyDir: {}\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#shared-networking","title":"Shared Networking","text":"<p>All containers in a pod share the same network namespace:</p> <ul> <li>Same IP address: All containers share the pod's IP</li> <li>localhost communication: Containers can reach each other on <code>localhost</code></li> <li>Port space: Containers must use different ports (no conflicts)</li> <li>Network policies: Applied at the pod level, not container level</li> </ul> <pre><code># Container 1 listens on port 80\nnginx -g 'daemon off;'\n\n# Container 2 can access it via localhost\ncurl localhost:80\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#shared-storage","title":"Shared Storage","text":"<p>Pods can define volumes that containers share:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: shared-storage-example\nspec:\n  containers:\n  - name: writer\n    image: busybox\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - while true; do\n        echo \"$(date): Writing data\" &gt;&gt; /data/log.txt;\n        sleep 10;\n      done\n    volumeMounts:\n    - name: shared-data\n      mountPath: /data\n\n  - name: reader\n    image: busybox\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - tail -f /data/log.txt\n    volumeMounts:\n    - name: shared-data\n      mountPath: /data\n      readOnly: true\n\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n</code></pre> <p>Common Volume Types:</p> <ul> <li><code>emptyDir</code>: Temporary storage, deleted with pod</li> <li><code>hostPath</code>: Mount from host node (use sparingly)</li> <li><code>configMap</code>: Configuration files</li> <li><code>secret</code>: Sensitive data</li> <li><code>persistentVolumeClaim</code>: Persistent storage</li> </ul>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#multi-container-patterns","title":"Multi-Container Patterns","text":"<p>Multi-container pods enable powerful design patterns. Here are the four most common patterns you'll encounter in production and the CKA exam.</p> <pre><code>graph TB\n    subgraph \"Multi-Container Patterns\"\n        subgraph \"Sidecar Pattern\"\n            S1[Main App]\n            S2[Sidecar&lt;br/&gt;Enhancement]\n            S1 &lt;-.-&gt; S2\n        end\n\n        subgraph \"Init Container Pattern\"\n            I1[Init Container&lt;br/&gt;Runs First]\n            I2[Main App&lt;br/&gt;Starts After]\n            I1 --&gt; I2\n        end\n\n        subgraph \"Adapter Pattern\"\n            A1[Main App&lt;br/&gt;Custom Format]\n            A2[Adapter&lt;br/&gt;Standardizes]\n            A3[Monitoring&lt;br/&gt;System]\n            A1 --&gt; A2 --&gt; A3\n        end\n\n        subgraph \"Ambassador Pattern\"\n            AM1[Main App&lt;br/&gt;Simple Client]\n            AM2[Ambassador&lt;br/&gt;Proxy/Router]\n            AM3[External&lt;br/&gt;Services]\n            AM1 --&gt; AM2 --&gt; AM3\n        end\n    end\n\n    style S1 fill:#e1f5ff\n    style I1 fill:#fff4e1\n    style A2 fill:#e8f5e8\n    style AM2 fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#1-sidecar-pattern","title":"1. Sidecar Pattern","text":"<p>Purpose: Extend or enhance the main container's functionality without modifying it.</p> <p>Common Use Cases: - Log collection and shipping - Configuration synchronization - Monitoring and metrics collection - Security proxies (mTLS, auth)</p> <p>Example: Log Collection Sidecar</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app-with-logging\nspec:\n  containers:\n  # Main application\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n\n  # Sidecar: Log collector\n  - name: fluentd\n    image: fluent/fluentd:latest\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd/fluentd.conf\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n      readOnly: true\n    - name: fluentd-config\n      mountPath: /etc/fluentd\n\n  volumes:\n  - name: logs\n    emptyDir: {}\n  - name: fluentd-config\n    configMap:\n      name: fluentd-config\n</code></pre> <p>Why It Works: - Main app (nginx) is unmodified and focused on serving requests - Sidecar (fluentd) handles log shipping independently - Both containers share the logs volume - Sidecar can be updated without touching the main app</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#2-init-container-pattern","title":"2. Init Container Pattern","text":"<p>Purpose: Run initialization tasks before the main container starts.</p> <p>Characteristics: - Init containers run sequentially (one at a time) - Main containers don't start until all init containers succeed - Init containers can use different images and tools - Perfect for setup tasks, migrations, waiting for dependencies</p> <p>Example: Database Migration Init Container</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app-with-init\nspec:\n  # Init containers run first\n  initContainers:\n  # Wait for database to be ready\n  - name: wait-for-db\n    image: busybox:1.35\n    command:\n    - sh\n    - -c\n    - |\n      until nc -z postgres-service 5432; do\n        echo \"Waiting for database...\"\n        sleep 2\n      done\n      echo \"Database is ready!\"\n\n  # Run database migrations\n  - name: run-migrations\n    image: myapp:migrations\n    command: [\"/app/migrate.sh\"]\n    env:\n    - name: DATABASE_URL\n      valueFrom:\n        secretKeyRef:\n          name: db-secret\n          key: url\n\n  # Download configuration\n  - name: fetch-config\n    image: curlimages/curl:latest\n    command:\n    - sh\n    - -c\n    - curl -o /config/app.json https://config-server/api/config\n    volumeMounts:\n    - name: config\n      mountPath: /config\n\n  # Main container starts only after all init containers succeed\n  containers:\n  - name: web-app\n    image: myapp:latest\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: config\n      mountPath: /etc/app\n\n  volumes:\n  - name: config\n    emptyDir: {}\n</code></pre> <p>CKA Tip: Init containers are heavily tested. Know how to troubleshoot them with:</p> <pre><code># Check init container status\nkubectl describe pod web-app-with-init\n\n# View init container logs\nkubectl logs web-app-with-init -c wait-for-db\nkubectl logs web-app-with-init -c run-migrations\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#3-adapter-pattern","title":"3. Adapter Pattern","text":"<p>Purpose: Transform the main container's output to a standard format.</p> <p>Common Use Cases: - Standardizing log formats for centralized logging - Converting metrics to Prometheus format - API response transformation - Protocol translation</p> <p>Example: Log Format Adapter</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-with-adapter\nspec:\n  containers:\n  # Main app produces custom log format\n  - name: legacy-app\n    image: legacy-app:1.0\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/app\n\n  # Adapter converts logs to JSON format\n  - name: log-adapter\n    image: log-transformer:latest\n    command:\n    - sh\n    - -c\n    - |\n      tail -f /var/log/app/app.log | while read line; do\n        # Transform: \"2025-01-01 ERROR message\"\n        # To: {\"timestamp\":\"2025-01-01\",\"level\":\"ERROR\",\"message\":\"message\"}\n        echo \"$line\" | /app/transform-to-json.sh\n      done\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/app\n      readOnly: true\n\n  volumes:\n  - name: logs\n    emptyDir: {}\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#4-ambassador-pattern","title":"4. Ambassador Pattern","text":"<p>Purpose: Proxy network connections, hiding complexity from the main container.</p> <p>Common Use Cases: - Service mesh proxies (Istio, Linkerd) - Database connection pooling - Rate limiting and circuit breaking - Load balancing to multiple backends</p> <p>Example: Database Proxy Ambassador</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-with-ambassador\nspec:\n  containers:\n  # Main app connects to localhost:5432\n  - name: web-app\n    image: myapp:latest\n    env:\n    - name: DATABASE_HOST\n      value: \"localhost\"  # Connects to ambassador\n    - name: DATABASE_PORT\n      value: \"5432\"\n\n  # Ambassador handles connection pooling and routing\n  - name: db-proxy\n    image: pgbouncer:latest\n    ports:\n    - containerPort: 5432\n    env:\n    - name: DB_HOST\n      value: \"postgres-primary.database.svc.cluster.local\"\n    - name: DB_PORT\n      value: \"5432\"\n    - name: POOL_MODE\n      value: \"transaction\"\n    - name: MAX_CLIENT_CONN\n      value: \"100\"\n    volumeMounts:\n    - name: proxy-config\n      mountPath: /etc/pgbouncer\n\n  volumes:\n  - name: proxy-config\n    configMap:\n      name: pgbouncer-config\n</code></pre> <p>Benefits: - Main app uses simple <code>localhost:5432</code> connection - Ambassador handles connection pooling, failover, load balancing - Can swap ambassador without changing main app - Easier testing (mock ambassador in dev/test)</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#real-world-multi-container-scenarios","title":"Real-World Multi-Container Scenarios","text":"<p>Scenario 1: Microservice with Service Mesh</p> <pre><code># Istio automatically injects envoy sidecar\napiVersion: v1\nkind: Pod\nmetadata:\n  name: order-service\n  annotations:\n    sidecar.istio.io/inject: \"true\"\nspec:\n  containers:\n  - name: order-service\n    image: order-service:v2.1\n    ports:\n    - containerPort: 8080\n\n  # Istio injects this automatically:\n  # - name: istio-proxy\n  #   image: istio/proxyv2:1.18.0\n  #   # Handles mTLS, traffic routing, telemetry\n</code></pre> <p>Scenario 2: Application with Config Sync</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-with-config-sync\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    volumeMounts:\n    - name: config\n      mountPath: /etc/app\n\n  # Sidecar syncs config from Git every 60s\n  - name: config-sync\n    image: git-sync:latest\n    env:\n    - name: GIT_SYNC_REPO\n      value: \"https://github.com/myorg/config\"\n    - name: GIT_SYNC_BRANCH\n      value: \"main\"\n    - name: GIT_SYNC_WAIT\n      value: \"60\"\n    volumeMounts:\n    - name: config\n      mountPath: /config\n\n  volumes:\n  - name: config\n    emptyDir: {}\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#pod-configuration","title":"Pod Configuration","text":"<p>Effective pod configuration is critical for running reliable, secure, and efficient applications in Kubernetes. Let's explore essential configuration patterns.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#resource-requests-and-limits","title":"Resource Requests and Limits","text":"<p>Resource management determines pod scheduling and runtime behavior.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: resource-demo\nspec:\n  containers:\n  - name: app\n    image: nginx:1.21\n    resources:\n      # Requests: Minimum guaranteed resources\n      requests:\n        memory: \"64Mi\"   # 64 mebibytes\n        cpu: \"250m\"      # 250 millicores (0.25 CPU)\n\n      # Limits: Maximum allowed resources\n      limits:\n        memory: \"128Mi\"  # Pod killed (OOMKilled) if exceeded\n        cpu: \"500m\"      # Throttled if exceeded\n</code></pre> <p>How It Works:</p> <pre><code>graph TB\n    subgraph \"Resource Management\"\n        REQ[Resource Request&lt;br/&gt;CPU: 250m&lt;br/&gt;Memory: 64Mi]\n        LIM[Resource Limit&lt;br/&gt;CPU: 500m&lt;br/&gt;Memory: 128Mi]\n\n        REQ --&gt;|Guarantees| SCHED[Scheduler&lt;br/&gt;Finds Node]\n        LIM --&gt;|Enforces| RUNTIME[Container Runtime&lt;br/&gt;cgroups]\n\n        SCHED --&gt;|Sufficient&lt;br/&gt;resources?| NODE[Node Placement]\n        RUNTIME --&gt;|CPU| THROTTLE[CPU Throttling]\n        RUNTIME --&gt;|Memory| OOM[OOMKilled if&lt;br/&gt;limit exceeded]\n    end\n\n    style REQ fill:#e8f5e8\n    style LIM fill:#ffe5e5\n    style SCHED fill:#e1f5ff\n    style OOM fill:#ffcccc</code></pre> <p>Best Practices:</p> <ol> <li>Always set requests: Required for proper scheduling</li> <li>Set limits carefully: Prevents resource starvation</li> <li>Memory limits = OOMKilled: Exceeding memory limit kills the pod</li> <li>CPU limits = Throttling: Exceeding CPU limit slows the container</li> <li>Requests \u2264 Limits: Requests should never exceed limits</li> </ol> <pre><code># Check resource usage\nkubectl top pod resource-demo\n\n# Common OOMKilled scenario\nkubectl describe pod resource-demo\n# Last State: Terminated\n#   Reason: OOMKilled\n#   Exit Code: 137\n</code></pre> <p>Quality of Service (QoS) Classes:</p> QoS Class Condition Behavior Guaranteed Requests = Limits for all resources Last to be evicted Burstable Requests &lt; Limits or only requests set Evicted before Guaranteed BestEffort No requests or limits First to be evicted","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#environment-variables-and-secrets","title":"Environment Variables and Secrets","text":"<p>Environment Variables:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: env-demo\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    env:\n    # Static value\n    - name: ENVIRONMENT\n      value: \"production\"\n\n    # From ConfigMap\n    - name: APP_CONFIG\n      valueFrom:\n        configMapKeyRef:\n          name: app-config\n          key: config.json\n\n    # From Secret\n    - name: DATABASE_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: db-secret\n          key: password\n\n    # Pod metadata\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n\n    # Resource limits\n    - name: MEMORY_LIMIT\n      valueFrom:\n        resourceFieldRef:\n          containerName: app\n          resource: limits.memory\n</code></pre> <p>Load All Keys from ConfigMap/Secret:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: env-from-demo\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    envFrom:\n    # All keys from ConfigMap as environment variables\n    - configMapRef:\n        name: app-config\n\n    # All keys from Secret\n    - secretRef:\n        name: app-secrets\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#health-probes","title":"Health Probes","text":"<p>Health probes determine container health and availability.</p> <pre><code>graph TB\n    START[Container Starts] --&gt; STARTUP{Startup Probe&lt;br/&gt;Configured?}\n\n    STARTUP --&gt;|Yes| SP[Startup Probe&lt;br/&gt;Runs First]\n    STARTUP --&gt;|No| LP[Liveness Probe&lt;br/&gt;Begins]\n\n    SP --&gt;|Success| LP\n    SP --&gt;|Failure| WAIT[Wait failureThreshold&lt;br/&gt;\u00d7 periodSeconds]\n    WAIT --&gt;|Still Failing| RESTART1[Restart Container]\n    RESTART1 --&gt; START\n\n    LP --&gt;|Running| LIVE{Liveness&lt;br/&gt;Check}\n    LIVE --&gt;|Healthy| RP{Readiness&lt;br/&gt;Check}\n    LIVE --&gt;|Unhealthy| RESTART2[Restart Container]\n    RESTART2 --&gt; START\n\n    RP --&gt;|Ready| TRAFFIC[Receive Traffic&lt;br/&gt;Added to Service]\n    RP --&gt;|Not Ready| NOTRAFFIC[No Traffic&lt;br/&gt;Removed from Service]\n    NOTRAFFIC --&gt;|Recovers| RP\n    TRAFFIC --&gt; LP\n\n    style START fill:#e8f5e8\n    style TRAFFIC fill:#e1f5ff\n    style RESTART1 fill:#ffcccc\n    style RESTART2 fill:#ffcccc\n    style NOTRAFFIC fill:#fff4e1</code></pre> <p>Complete Health Probe Example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: health-demo\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    ports:\n    - containerPort: 8080\n\n    # Startup Probe: For slow-starting containers\n    startupProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 0\n      periodSeconds: 10        # Check every 10s\n      failureThreshold: 30     # Allow 300s (5min) to start\n      successThreshold: 1\n      timeoutSeconds: 5\n\n    # Liveness Probe: Restart if unhealthy\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: X-Custom-Header\n          value: Liveness\n      initialDelaySeconds: 15\n      periodSeconds: 10\n      failureThreshold: 3      # Restart after 3 failures\n      successThreshold: 1\n      timeoutSeconds: 5\n\n    # Readiness Probe: Remove from Service if not ready\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n      initialDelaySeconds: 5\n      periodSeconds: 5\n      failureThreshold: 3\n      successThreshold: 1\n      timeoutSeconds: 3\n</code></pre> <p>Probe Types:</p> <ol> <li> <p>HTTP GET: <pre><code>httpGet:\n  path: /healthz\n  port: 8080\n  scheme: HTTP  # or HTTPS\n</code></pre></p> </li> <li> <p>TCP Socket: <pre><code>tcpSocket:\n  port: 8080\n</code></pre></p> </li> <li> <p>Exec Command: <pre><code>exec:\n  command:\n  - cat\n  - /tmp/healthy\n</code></pre></p> </li> </ol> <p>Common Patterns:</p> Scenario Startup Liveness Readiness Simple web app Not needed HTTP <code>/healthz</code> HTTP <code>/ready</code> Slow startup (&gt;60s) HTTP <code>/healthz</code> HTTP <code>/healthz</code> HTTP <code>/ready</code> Database TCP socket TCP socket Exec query Batch job Not needed Not needed Not needed <p>CKA Tip: Know the difference!</p> <ul> <li>Startup Probe: Protects slow-starting containers from being killed by liveness</li> <li>Liveness Probe: Restarts unhealthy containers</li> <li>Readiness Probe: Controls traffic routing, doesn't restart</li> </ul>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#restart-policies","title":"Restart Policies","text":"<p>Control what happens when containers exit.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: restart-demo\nspec:\n  restartPolicy: Always  # Always | OnFailure | Never\n  containers:\n  - name: app\n    image: myapp:latest\n</code></pre> Policy Behavior Use Case Always (default) Restart on any termination Long-running services OnFailure Restart only on non-zero exit Batch jobs, retryable tasks Never Never restart One-time tasks, debugging <p>Exit Code Meanings:</p> <ul> <li><code>0</code>: Success (clean exit)</li> <li><code>1</code>: General error</li> <li><code>137</code>: OOMKilled (128 + 9 SIGKILL)</li> <li><code>143</code>: Terminated gracefully (128 + 15 SIGTERM)</li> </ul>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#pod-affinity-and-anti-affinity-basics","title":"Pod Affinity and Anti-Affinity Basics","text":"<p>Control pod placement relative to other pods.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-server\nspec:\n  # Prefer to run on nodes where cache pods are running\n  affinity:\n    podAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchLabels:\n              app: cache\n          topologyKey: kubernetes.io/hostname\n\n    # Never run on nodes where other web-server pods are running\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchLabels:\n            app: web-server\n        topologyKey: kubernetes.io/hostname\n\n  containers:\n  - name: nginx\n    image: nginx:1.21\n</code></pre> <p>Common Use Cases:</p> <ul> <li>High Availability: Spread replicas across nodes (anti-affinity)</li> <li>Performance: Co-locate related services (affinity)</li> <li>Data Locality: Place compute near storage</li> </ul>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#cka-exam-skills","title":"CKA Exam Skills","text":"<p>Speed and accuracy are critical for the CKA exam. Master these imperative commands and troubleshooting techniques.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#imperative-pod-creation","title":"Imperative Pod Creation","text":"<p>Fastest Pod Creation:</p> <pre><code># Basic pod\nkubectl run nginx --image=nginx\n\n# Pod with port\nkubectl run nginx --image=nginx --port=80\n\n# Pod with labels\nkubectl run nginx --image=nginx --labels=\"app=web,tier=frontend\"\n\n# Pod with environment variable\nkubectl run nginx --image=nginx --env=\"ENV=prod\"\n\n# Pod with resource limits\nkubectl run nginx --image=nginx --requests='cpu=100m,memory=256Mi' --limits='cpu=200m,memory=512Mi'\n\n# Dry run to generate YAML (don't create)\nkubectl run nginx --image=nginx --dry-run=client -o yaml\n\n# Save to file for editing\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n</code></pre> <p>CKA Time-Saving Technique:</p> <pre><code># Generate base YAML quickly, then edit\nkubectl run mypod --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit with vi/vim\nvi pod.yaml\n\n# Apply the edited version\nkubectl apply -f pod.yaml\n</code></pre> <p>Multi-Container Pod (manual YAML):</p> <pre><code># Start with single container\nkubectl run webapp --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit to add second container\nvi pod.yaml\n# Add another container to the containers array\n\nkubectl apply -f pod.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<pre><code>graph TB\n    START[kubectl get pods] --&gt; STATUS{Pod Status?}\n\n    STATUS --&gt;|Pending| PEND[Check Events]\n    PEND --&gt; SCHED{Scheduling&lt;br/&gt;Issue?}\n    SCHED --&gt;|Insufficient resources| RESOURCES[Scale cluster&lt;br/&gt;or reduce requests]\n    SCHED --&gt;|Node selector| SELECTOR[Fix node selector&lt;br/&gt;or labels]\n\n    STATUS --&gt;|ImagePullBackOff| IMG[Image Issue]\n    IMG --&gt; IMGFIX{Fix:}\n    IMGFIX --&gt; IMGNAME[Correct image name]\n    IMGFIX --&gt; IMGREG[Fix registry auth]\n    IMGFIX --&gt; IMGTAG[Verify tag exists]\n\n    STATUS --&gt;|CrashLoopBackOff| CRASH[Container Crashing]\n    CRASH --&gt; LOGS[Check Logs]\n    LOGS --&gt; LOGSFIX{Common Issues:}\n    LOGSFIX --&gt; APPBUG[Application bug]\n    LOGSFIX --&gt; MISCONFIG[Missing config]\n    LOGSFIX --&gt; DEPS[Missing dependencies]\n\n    STATUS --&gt;|Running| READY{Ready?}\n    READY --&gt;|0/1| PROBE[Readiness Probe&lt;br/&gt;Failing]\n    PROBE --&gt; PROBEFIX[Check probe&lt;br/&gt;endpoint]\n\n    style START fill:#e1f5ff\n    style RESOURCES fill:#ffcccc\n    style IMGFIX fill:#fff4e1\n    style LOGSFIX fill:#e8f5e8</code></pre> <p>1. CrashLoopBackOff</p> <p>Container starts but crashes immediately.</p> <pre><code># Check pod status\nkubectl get pod mypod\n\n# View recent logs\nkubectl logs mypod\n\n# View logs from previous container (after crash)\nkubectl logs mypod --previous\n\n# Describe pod for events\nkubectl describe pod mypod\n\n# Common causes in events:\n# - Error: failed to start container\n# - Back-off restarting failed container\n</code></pre> <p>Quick Fixes:</p> <pre><code># Wrong command\nkubectl run mypod --image=busybox --command -- /bin/sh -c \"invalid-command\"\n\n# Fix: Use valid command\nkubectl run mypod --image=busybox --command -- /bin/sh -c \"sleep 3600\"\n\n# Missing environment variable\nkubectl set env pod/mypod DATABASE_URL=postgres://db:5432\n\n# Check container configuration\nkubectl get pod mypod -o yaml | grep -A 10 command\n</code></pre> <p>2. ImagePullBackOff</p> <p>Cannot pull container image.</p> <pre><code># Describe pod to see error\nkubectl describe pod mypod\n# Events:\n#   Failed to pull image \"ngin:1.21\": rpc error: code = NotFound\n\n# Common causes:\n# 1. Typo in image name (ngin vs nginx)\n# 2. Tag doesn't exist (nginx:999)\n# 3. Private registry without auth\n# 4. Network issues\n\n# Fix: Update image\nkubectl set image pod/mypod container-name=nginx:1.21\n\n# For private registries, create secret\nkubectl create secret docker-registry regcred \\\n  --docker-server=myregistry.com \\\n  --docker-username=myuser \\\n  --docker-password=mypass\n\n# Reference in pod\nkubectl patch pod mypod -p '{\"spec\":{\"imagePullSecrets\":[{\"name\":\"regcred\"}]}}'\n</code></pre> <p>3. Pending Status</p> <p>Pod cannot be scheduled.</p> <pre><code># Check events\nkubectl describe pod mypod\n# Events:\n#   0/3 nodes are available: insufficient memory\n\n# Check node resources\nkubectl describe nodes\n\n# Common causes:\n# 1. Insufficient CPU/memory\n# 2. Node selector doesn't match\n# 3. Taints/tolerations mismatch\n# 4. PVC cannot be bound\n\n# Fix: Reduce resource requests\nkubectl set resources pod mypod -c=container-name --requests=cpu=100m,memory=128Mi\n\n# Or add nodes to cluster\n</code></pre> <p>4. Pod Running but Not Ready</p> <pre><code># Check readiness probe\nkubectl get pod mypod\n# NAME    READY   STATUS    RESTARTS   AGE\n# mypod   0/1     Running   0          2m\n\n# Describe to see probe failures\nkubectl describe pod mypod\n# Readiness probe failed: HTTP probe failed with statuscode: 500\n\n# Fix readiness probe endpoint\n# Edit pod or deployment\nkubectl edit pod mypod\n\n# Or exec into container to debug\nkubectl exec -it mypod -- /bin/sh\ncurl localhost:8080/ready\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#essential-kubectl-commands","title":"Essential kubectl Commands","text":"<pre><code># View pods\nkubectl get pods\nkubectl get pods -o wide              # Show IP and node\nkubectl get pods -l app=nginx         # Filter by label\nkubectl get pods --all-namespaces     # All namespaces\n\n# Detailed information\nkubectl describe pod mypod\n\n# Logs\nkubectl logs mypod                    # Current logs\nkubectl logs mypod -f                 # Follow logs\nkubectl logs mypod --previous         # Previous container\nkubectl logs mypod -c container-name  # Specific container\n\n# Execute commands\nkubectl exec mypod -- ls /app\nkubectl exec -it mypod -- /bin/bash   # Interactive shell\n\n# Port forwarding (access pod from local machine)\nkubectl port-forward mypod 8080:80\n\n# Copy files\nkubectl cp mypod:/app/config.json ./config.json\nkubectl cp ./config.json mypod:/app/config.json\n\n# Delete pods\nkubectl delete pod mypod\nkubectl delete pod -l app=nginx       # Delete by label\nkubectl delete pod mypod --force --grace-period=0  # Force delete\n\n# Edit running pod\nkubectl edit pod mypod                # Opens in editor\n\n# Replace pod\nkubectl replace -f pod.yaml --force   # Delete and recreate\n\n# Show resource usage\nkubectl top pod mypod\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#fast-debugging-workflow","title":"Fast Debugging Workflow","text":"<p>CKA Exam Strategy:</p> <ol> <li> <p>Understand the problem (30 seconds)    <pre><code>kubectl get pods\n</code></pre></p> </li> <li> <p>Gather information (1 minute)    <pre><code>kubectl describe pod &lt;name&gt;\nkubectl logs &lt;name&gt;\n</code></pre></p> </li> <li> <p>Identify root cause (1 minute)</p> </li> <li>Check events in <code>describe</code> output</li> <li>Look for error messages in logs</li> <li> <p>Verify configuration (image, env vars, probes)</p> </li> <li> <p>Apply fix (2 minutes)    <pre><code># Quick fixes\nkubectl delete pod &lt;name&gt;  # Let controller recreate\nkubectl edit pod &lt;name&gt;    # Edit configuration\nkubectl set image pod/&lt;name&gt; container=new-image\n</code></pre></p> </li> <li> <p>Verify solution (30 seconds)    <pre><code>kubectl get pod &lt;name&gt;\nkubectl logs &lt;name&gt;\n</code></pre></p> </li> </ol> <p>CKA Time-Savers:</p> <pre><code># Alias kubectl to k\nalias k=kubectl\n\n# Set default namespace\nkubectl config set-context --current --namespace=mynamespace\n\n# Use -o wide for extra info\nk get pods -o wide\n\n# Combine commands\nk describe pod mypod | grep -A 5 Events\nk logs mypod | tail -20\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#practice-exercises","title":"Practice Exercises","text":"<p>Hands-on practice is essential for CKA success. Complete these exercises to reinforce your pod mastery.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-1-create-multi-container-pod","title":"Exercise 1: Create Multi-Container Pod","text":"<p>Task: Create a pod with nginx and busybox containers sharing a volume.</p> <p>Solution:</p> <pre><code># Generate base YAML\nkubectl run multi --image=nginx --dry-run=client -o yaml &gt; multi.yaml\n\n# Edit to add second container\ncat &lt;&lt;EOF &gt; multi.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: multi\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    volumeMounts:\n    - name: shared\n      mountPath: /usr/share/nginx/html\n\n  - name: busybox\n    image: busybox:1.35\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - while true; do\n        echo \"Hello from busybox - $(date)\" &gt; /html/index.html;\n        sleep 10;\n      done\n    volumeMounts:\n    - name: shared\n      mountPath: /html\n\n  volumes:\n  - name: shared\n    emptyDir: {}\nEOF\n\n# Apply\nkubectl apply -f multi.yaml\n\n# Verify\nkubectl exec multi -c nginx -- cat /usr/share/nginx/html/index.html\nkubectl exec multi -c busybox -- cat /html/index.html\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-2-configure-health-probes","title":"Exercise 2: Configure Health Probes","text":"<p>Task: Add startup, liveness, and readiness probes to a pod.</p> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: probes\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n\n    startupProbe:\n      httpGet:\n        path: /\n        port: 80\n      periodSeconds: 5\n      failureThreshold: 12\n\n    livenessProbe:\n      httpGet:\n        path: /\n        port: 80\n      periodSeconds: 10\n      failureThreshold: 3\n\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      periodSeconds: 5\n      failureThreshold: 3\nEOF\n\n# Watch pod startup\nkubectl get pod probes -w\n\n# Verify probes in pod spec\nkubectl describe pod probes | grep -A 10 Liveness\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-3-troubleshoot-crashloopbackoff","title":"Exercise 3: Troubleshoot CrashLoopBackOff","text":"<p>Task: Fix a pod stuck in CrashLoopBackOff.</p> <p>Setup: <pre><code>kubectl run crash --image=busybox --command -- /bin/sh -c \"exit 1\"\n</code></pre></p> <p>Solution:</p> <pre><code># Check status\nkubectl get pod crash\n# STATUS: CrashLoopBackOff\n\n# Check logs\nkubectl logs crash\nkubectl logs crash --previous\n\n# Describe to see crash reason\nkubectl describe pod crash\n# Last State: Terminated\n#   Reason: Error\n#   Exit Code: 1\n\n# Fix: Use valid command\nkubectl delete pod crash\nkubectl run crash --image=busybox --command -- /bin/sh -c \"sleep 3600\"\n\n# Verify\nkubectl get pod crash\n# STATUS: Running\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-4-set-resource-requests-and-limits","title":"Exercise 4: Set Resource Requests and Limits","text":"<p>Task: Create a pod with CPU and memory constraints.</p> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: resources\nspec:\n  containers:\n  - name: stress\n    image: polinux/stress\n    command: [\"stress\"]\n    args: [\"--cpu\", \"1\", \"--vm\", \"1\", \"--vm-bytes\", \"128M\"]\n    resources:\n      requests:\n        cpu: \"250m\"\n        memory: \"128Mi\"\n      limits:\n        cpu: \"500m\"\n        memory: \"256Mi\"\nEOF\n\n# Monitor resource usage\nkubectl top pod resources\n\n# Check QoS class\nkubectl get pod resources -o jsonpath='{.status.qosClass}'\n# Output: Burstable\n\n# Test memory limit (cause OOMKilled)\nkubectl exec resources -- stress --vm 1 --vm-bytes 300M --timeout 10s\nkubectl describe pod resources | grep -A 5 \"Last State\"\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-5-use-init-containers","title":"Exercise 5: Use Init Containers","text":"<p>Task: Create a pod with an init container that downloads configuration.</p> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: init-demo\nspec:\n  initContainers:\n  - name: download-config\n    image: busybox:1.35\n    command:\n    - sh\n    - -c\n    - |\n      echo \"Downloading configuration...\"\n      echo '{\"app\":\"demo\",\"version\":\"1.0\"}' &gt; /config/app.json\n      echo \"Configuration ready!\"\n    volumeMounts:\n    - name: config\n      mountPath: /config\n\n  containers:\n  - name: app\n    image: busybox:1.35\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - |\n      echo \"Starting application...\"\n      cat /etc/app/app.json\n      sleep 3600\n    volumeMounts:\n    - name: config\n      mountPath: /etc/app\n\n  volumes:\n  - name: config\n    emptyDir: {}\nEOF\n\n# Watch init container execute\nkubectl get pod init-demo -w\n\n# Check init container logs\nkubectl logs init-demo -c download-config\n\n# Check main container logs\nkubectl logs init-demo -c app\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#time-saving-tips-for-exercises","title":"Time-Saving Tips for Exercises","text":"<ol> <li>Use imperative commands for base YAML generation</li> <li>Master kubectl edit for quick modifications</li> <li>Practice with --dry-run=client -o yaml to generate templates</li> <li>Use kubectl explain for field documentation:    <pre><code>kubectl explain pod.spec.containers.livenessProbe\n</code></pre></li> <li>Create reusable snippets for common patterns</li> <li>Use tab completion in bash/zsh:    <pre><code>source &lt;(kubectl completion bash)\n</code></pre></li> </ol>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#conclusion","title":"Conclusion","text":"<p>Pods are the foundation of Kubernetes - master them and you've built a solid base for everything else. In this guide, you've learned:</p> <ul> <li>Pod fundamentals: The atomic unit of Kubernetes with shared networking and storage</li> <li>Lifecycle management: Understanding pod phases and state transitions</li> <li>Multi-container patterns: Sidecar, init, adapter, and ambassador patterns for real-world scenarios</li> <li>Configuration mastery: Resource management, health probes, environment variables, and affinity</li> <li>Troubleshooting skills: Debugging CrashLoopBackOff, ImagePullBackOff, and Pending pods</li> <li>CKA exam techniques: Fast imperative commands and efficient debugging workflows</li> </ul> <p>Next Steps for CKA Preparation:</p> <ol> <li>Practice daily: Create 5-10 pods with different configurations</li> <li>Break things: Intentionally create failing pods and fix them</li> <li>Time yourself: Practice pod creation and troubleshooting under time pressure</li> <li>Explore workload controllers: Deployments, StatefulSets, DaemonSets (build on pods)</li> <li>Master kubectl: Speed and accuracy with imperative commands</li> </ol> <p>Related CKA Topics:</p> <ul> <li>Deployments: Manage pod replicas and rolling updates</li> <li>Services: Expose pods via stable network endpoints</li> <li>ConfigMaps and Secrets: External configuration for pods</li> <li>Persistent Volumes: Durable storage for stateful pods</li> <li>Network Policies: Control pod-to-pod communication</li> <li>RBAC: Secure pod access to Kubernetes API</li> </ul> <p>Resources for Continued Learning:</p> <ul> <li>Kubernetes Documentation: Pods</li> <li>Kubernetes Documentation: Pod Lifecycle</li> <li>kubectl Cheat Sheet</li> </ul> <p>Final CKA Tip: The exam tests your ability to solve problems quickly under pressure. Focus on:</p> <ul> <li>Speed: Imperative commands over writing YAML from scratch</li> <li>Troubleshooting: Most questions involve fixing broken resources</li> <li>Documentation: Know how to navigate Kubernetes docs efficiently</li> <li>Practice: Hands-on experience beats reading theory</li> </ul> <p>Good luck with your CKA preparation! Remember: every complex Kubernetes application starts with understanding pods. Master this fundamental building block, and you're well on your way to Kubernetes expertise.</p> <p>About the CKA Exam Series:</p> <p>This post is part of a comprehensive Certified Kubernetes Administrator (CKA) exam preparation series. Each article focuses on a specific exam domain with hands-on exercises, real-world examples, and exam-specific tips.</p> <p>Related Posts:</p> <ul> <li>Kubernetes Architecture Fundamentals</li> <li>Kubernetes Objects &amp; YAML Mastery</li> <li>kubectl Essentials for CKA</li> <li>Namespaces &amp; Resource Quotas</li> </ul> <p>Stay tuned for upcoming posts on Deployments, Services, Persistent Storage, and Security!</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/","title":"RBAC: Role-Based Access Control","text":"<p>Master Kubernetes authorization with Roles, ClusterRoles, and the principle of least privilege</p> <p>Role-Based Access Control (RBAC) is Kubernetes' primary authorization mechanism, controlling who can perform which actions on what resources. For the CKA exam, RBAC is critical\u2014you'll need to create ServiceAccounts, configure Roles and RoleBindings, troubleshoot permission issues, and apply least privilege principles. This comprehensive guide covers everything from basic concepts to advanced aggregation patterns, giving you the skills to secure any Kubernetes cluster.</p> <p>CKA Exam Relevance: Cluster Architecture, Installation &amp; Configuration (25% of exam weight)</p>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li>RBAC Architecture: Roles, ClusterRoles, Bindings, and ServiceAccounts</li> <li>Permission Model: API groups, resources, verbs, and resource names</li> <li>Scope Control: Namespace vs cluster-wide permissions</li> <li>Default Roles: admin, edit, view, cluster-admin</li> <li>ServiceAccount Patterns: Pod authentication and authorization</li> <li>Aggregation: Composing roles from multiple sources</li> <li>Troubleshooting: Permission denied errors and audit logs</li> <li>CKA Strategies: Fast role creation and verification techniques</li> </ul>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#rbac-architecture-overview","title":"\ud83c\udfaf RBAC Architecture Overview","text":"<p>RBAC in Kubernetes uses four core resource types to implement authorization:</p> <pre><code>graph TD\n    subgraph \"Authorization Request\"\n        User[User/ServiceAccount]\n    end\n\n    subgraph \"Bindings\"\n        RB[RoleBinding]\n        CRB[ClusterRoleBinding]\n    end\n\n    subgraph \"Permissions\"\n        Role[Role]\n        CR[ClusterRole]\n    end\n\n    subgraph \"Scope\"\n        NS[Namespace Resources]\n        Cluster[Cluster Resources]\n    end\n\n    User --&gt;|namespace-scoped| RB\n    User --&gt;|cluster-scoped| CRB\n\n    RB --&gt;|references| Role\n    RB --&gt;|can reference| CR\n    CRB --&gt;|references| CR\n\n    Role --&gt;|grants access to| NS\n    CR --&gt;|grants access to| Cluster\n    CR --&gt;|can grant access to| NS\n\n    style User fill:#e1f5ff\n    style RB fill:#fff4e1\n    style CRB fill:#fff4e1\n    style Role fill:#e8f5e8\n    style CR fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#core-components","title":"Core Components","text":"<p>Roles and ClusterRoles (Define permissions): - Role: Namespace-scoped permission definitions - ClusterRole: Cluster-wide permission definitions</p> <p>RoleBindings and ClusterRoleBindings (Grant permissions): - RoleBinding: Grants Role or ClusterRole within a namespace - ClusterRoleBinding: Grants ClusterRole cluster-wide</p> <p>Subjects (Who gets permissions): - User: Human users authenticated by the cluster - Group: Collections of users (e.g., <code>system:authenticated</code>) - ServiceAccount: Pod-level authentication identities</p>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#roles-and-clusterroles","title":"\ud83d\udd11 Roles and ClusterRoles","text":"","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#role-namespace-scoped","title":"Role (Namespace-Scoped)","text":"<p>A Role defines permissions within a specific namespace:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: dev\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]  # \"\" indicates core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\"]\n</code></pre> <pre><code># Create role imperatively (faster for CKA)\nkubectl create role pod-reader \\\n  --verb=get,list,watch \\\n  --resource=pods \\\n  --namespace=dev\n\n# Verify role\nkubectl get role pod-reader -n dev -o yaml\n\n# Describe role permissions\nkubectl describe role pod-reader -n dev\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#clusterrole-cluster-wide","title":"ClusterRole (Cluster-Wide)","text":"<p>A ClusterRole can: 1. Grant permissions to cluster-scoped resources (nodes, namespaces) 2. Grant permissions to namespaced resources across all namespaces 3. Be referenced by RoleBinding for namespace-scoped access</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: secret-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <pre><code># Create ClusterRole imperatively\nkubectl create clusterrole secret-reader \\\n  --verb=get,list,watch \\\n  --resource=secrets\n\n# ClusterRole for cluster resources (nodes)\nkubectl create clusterrole node-reader \\\n  --verb=get,list \\\n  --resource=nodes\n\n# Verify ClusterRole\nkubectl get clusterrole secret-reader -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#permission-rules-breakdown","title":"Permission Rules Breakdown","text":"<pre><code>graph LR\n    subgraph \"RBAC Rule Structure\"\n        A[apiGroups] --&gt; B[resources]\n        B --&gt; C[verbs]\n        C --&gt; D[resourceNames&lt;br/&gt;optional]\n    end\n\n    subgraph \"API Groups\"\n        A1[\"''&lt;br/&gt;core\"]\n        A2[apps]\n        A3[batch]\n        A4[networking.k8s.io]\n    end\n\n    subgraph \"Resources\"\n        B1[pods]\n        B2[deployments]\n        B3[services]\n        B4[secrets]\n    end\n\n    subgraph \"Verbs\"\n        C1[get, list, watch]\n        C2[create, update, patch]\n        C3[delete, deletecollection]\n        C4['*'&lt;br/&gt;all verbs]\n    end\n\n    A --&gt; A1\n    A --&gt; A2\n    B --&gt; B1\n    B --&gt; B2\n    C --&gt; C1\n    C --&gt; C2\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e8\n    style D fill:#f3e5f5</code></pre> <p>Common API Groups: - <code>\"\"</code> (empty string): Core API group (pods, services, configmaps, secrets) - <code>apps</code>: Deployments, StatefulSets, DaemonSets, ReplicaSets - <code>batch</code>: Jobs, CronJobs - <code>networking.k8s.io</code>: NetworkPolicies, Ingress - <code>rbac.authorization.k8s.io</code>: Roles, RoleBindings, ClusterRoles - <code>storage.k8s.io</code>: StorageClasses, VolumeAttachments</p> <p>Common Verbs: - Read: <code>get</code>, <code>list</code>, <code>watch</code> - Write: <code>create</code>, <code>update</code>, <code>patch</code> - Delete: <code>delete</code>, <code>deletecollection</code> - Special: <code>*</code> (all verbs)</p> <p>Resource Names (optional specificity): <pre><code>rules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"app-config\", \"db-config\"]  # Only these ConfigMaps\n  verbs: [\"get\", \"update\"]\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#rolebindings-and-clusterrolebindings","title":"\ud83d\udd17 RoleBindings and ClusterRoleBindings","text":"","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#rolebinding-namespace-scoped","title":"RoleBinding (Namespace-Scoped)","text":"<p>A RoleBinding grants permissions defined in a Role or ClusterRole to subjects within a namespace:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: dev\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\n- kind: ServiceAccount\n  name: app-sa\n  namespace: dev\nroleRef:\n  kind: Role        # or ClusterRole\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <pre><code># Bind Role to user\nkubectl create rolebinding read-pods \\\n  --role=pod-reader \\\n  --user=jane \\\n  --namespace=dev\n\n# Bind ClusterRole to ServiceAccount (namespace-scoped)\nkubectl create rolebinding app-view \\\n  --clusterrole=view \\\n  --serviceaccount=dev:app-sa \\\n  --namespace=dev\n\n# Verify RoleBinding\nkubectl get rolebinding read-pods -n dev -o yaml\nkubectl describe rolebinding read-pods -n dev\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#clusterrolebinding-cluster-wide","title":"ClusterRoleBinding (Cluster-Wide)","text":"<p>A ClusterRoleBinding grants ClusterRole permissions across the entire cluster:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: read-secrets-global\nsubjects:\n- kind: Group\n  name: security-team\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <pre><code># Bind ClusterRole to user cluster-wide\nkubectl create clusterrolebinding cluster-admin-user \\\n  --clusterrole=cluster-admin \\\n  --user=admin\n\n# Bind ClusterRole to group\nkubectl create clusterrolebinding view-all \\\n  --clusterrole=view \\\n  --group=developers\n\n# Verify ClusterRoleBinding\nkubectl get clusterrolebinding cluster-admin-user -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#binding-scope-decision-matrix","title":"Binding Scope Decision Matrix","text":"<pre><code>graph TD\n    Start{What permission&lt;br/&gt;scope needed?}\n\n    Start --&gt;|Namespace-specific| NS[Use RoleBinding]\n    Start --&gt;|Cluster-wide| CL[Use ClusterRoleBinding]\n\n    NS --&gt; NSRole{What defines&lt;br/&gt;permissions?}\n    NSRole --&gt;|Namespace resources only| UseRole[Use Role]\n    NSRole --&gt;|Reuse cluster definition| UseCR1[Reference ClusterRole&lt;br/&gt;in RoleBinding]\n\n    CL --&gt; CLRole{What resources?}\n    CLRole --&gt;|Cluster resources&lt;br/&gt;nodes, namespaces| UseCR2[Use ClusterRole +&lt;br/&gt;ClusterRoleBinding]\n    CLRole --&gt;|All namespace resources&lt;br/&gt;cluster-wide| UseCR3[Use ClusterRole +&lt;br/&gt;ClusterRoleBinding]\n\n    UseRole --&gt; Example1[\"Example: dev-pods Role&lt;br/&gt;in dev namespace\"]\n    UseCR1 --&gt; Example2[\"Example: view ClusterRole&lt;br/&gt;in dev namespace\"]\n    UseCR2 --&gt; Example3[\"Example: node-reader&lt;br/&gt;for all nodes\"]\n    UseCR3 --&gt; Example4[\"Example: secret-reader&lt;br/&gt;in all namespaces\"]\n\n    style Start fill:#e1f5ff\n    style NS fill:#fff4e1\n    style CL fill:#e8f5e8\n    style UseRole fill:#f3e5f5\n    style UseCR1 fill:#f3e5f5\n    style UseCR2 fill:#ffe5e5\n    style UseCR3 fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#serviceaccounts","title":"\ud83d\udc64 ServiceAccounts","text":"<p>ServiceAccounts provide an identity for processes running in Pods.</p>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#creating-and-using-serviceaccounts","title":"Creating and Using ServiceAccounts","text":"<pre><code># Create ServiceAccount\nkubectl create serviceaccount app-sa -n dev\n\n# Get ServiceAccount details (shows auto-created secret)\nkubectl get serviceaccount app-sa -n dev -o yaml\n\n# Describe to see token secret\nkubectl describe serviceaccount app-sa -n dev\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: app-sa\n  namespace: dev\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\n  namespace: dev\nspec:\n  serviceAccountName: app-sa  # Use custom ServiceAccount\n  containers:\n  - name: app\n    image: nginx:1.27\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#default-serviceaccount-behavior","title":"Default ServiceAccount Behavior","text":"<p>Every namespace has a default ServiceAccount automatically:</p> <pre><code># Check default ServiceAccount\nkubectl get sa default -n dev -o yaml\n\n# Every pod uses 'default' unless specified\n# Default has NO permissions beyond discovery\n</code></pre> <p>Best Practice: Always create dedicated ServiceAccounts for applications, never use <code>default</code>.</p>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#default-clusterroles","title":"\ud83c\udfe2 Default ClusterRoles","text":"<p>Kubernetes provides built-in ClusterRoles for common use cases:</p> ClusterRole Description Use Case cluster-admin Super-user access to all resources Cluster administrators only admin Namespace admin (can create Roles/RoleBindings) Namespace owners edit Read/write access to most resources Developers, CI/CD view Read-only access (no secrets) Monitoring, auditing <pre><code># View built-in roles\nkubectl get clusterrole | grep -E \"^(cluster-admin|admin|edit|view)\"\n\n# Examine 'view' ClusterRole\nkubectl describe clusterrole view\n\n# Grant namespace admin to user\nkubectl create rolebinding bob-admin \\\n  --clusterrole=admin \\\n  --user=bob \\\n  --namespace=dev\n\n# Grant cluster-wide view to group\nkubectl create clusterrolebinding developers-view \\\n  --clusterrole=view \\\n  --group=developers\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#default-role-comparison","title":"Default Role Comparison","text":"<pre><code>graph TD\n    subgraph \"cluster-admin\"\n        CA1[All resources]\n        CA2[All namespaces]\n        CA3[Cluster resources]\n        CA4[Create/Delete anything]\n    end\n\n    subgraph \"admin\"\n        A1[Most namespace resources]\n        A2[Create Roles/RoleBindings]\n        A3[View/Edit Secrets]\n        A4[NOT namespace itself]\n    end\n\n    subgraph \"edit\"\n        E1[Most namespace resources]\n        E2[View/Edit Secrets]\n        E3[NOT Roles/RoleBindings]\n        E4[Can exec into pods]\n    end\n\n    subgraph \"view\"\n        V1[Read most resources]\n        V2[NOT Secrets]\n        V3[NOT Roles/RoleBindings]\n        V4[Read-only everything else]\n    end\n\n    style CA1 fill:#ff6b6b\n    style A1 fill:#ffd93d\n    style E1 fill:#6bcf7f\n    style V1 fill:#4d96ff</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#role-aggregation","title":"\ud83e\udde9 Role Aggregation","text":"<p>ClusterRole aggregation composes roles from multiple sources using label selectors:</p> <pre><code># Parent ClusterRole (aggregates others)\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring\naggregationRule:\n  clusterRoleSelectors:\n  - matchLabels:\n      rbac.example.com/aggregate-to-monitoring: \"true\"\nrules: []  # Rules automatically filled from aggregated roles\n---\n# Child ClusterRole (aggregated into 'monitoring')\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring-endpoints\n  labels:\n    rbac.example.com/aggregate-to-monitoring: \"true\"\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\", \"endpoints\", \"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>Default aggregation targets (extend built-in roles): <pre><code>metadata:\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-view: \"true\"\n</code></pre></p> <pre><code># Check aggregated rules (automatically combined)\nkubectl get clusterrole monitoring -o yaml\n\n# View default 'admin' aggregation\nkubectl get clusterrole admin -o yaml | grep -A 5 aggregationRule\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#least-privilege-principle","title":"\ud83d\udd12 Least Privilege Principle","text":"<p>Always grant the minimum permissions required for a workload to function:</p>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#anti-pattern-overly-permissive","title":"Anti-Pattern: Overly Permissive","text":"<pre><code># \u274c BAD: Grants cluster-admin to application\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: app-admin\nsubjects:\n- kind: ServiceAccount\n  name: app-sa\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin  # WAY TOO MUCH ACCESS\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#best-practice-scoped-and-specific","title":"Best Practice: Scoped and Specific","text":"<pre><code># \u2705 GOOD: Specific permissions for application needs\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: app-role\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"app-config\"]  # Only this ConfigMap\n  verbs: [\"get\"]\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"app-secret\"]  # Only this Secret\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: app-sa\n  namespace: default\nroleRef:\n  kind: Role\n  name: app-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#principle-application-steps","title":"Principle Application Steps","text":"<pre><code>graph TD\n    Start[Identify Workload&lt;br/&gt;Requirements]\n\n    Start --&gt; Q1{Cluster resources&lt;br/&gt;needed?}\n    Q1 --&gt;|Yes| ClusterRole[Use ClusterRole]\n    Q1 --&gt;|No| Role[Use Role]\n\n    ClusterRole --&gt; Q2{Access needed&lt;br/&gt;cluster-wide?}\n    Q2 --&gt;|Yes| CRB[ClusterRoleBinding]\n    Q2 --&gt;|No| RB1[RoleBinding per namespace]\n\n    Role --&gt; RB2[RoleBinding in namespace]\n\n    RB1 --&gt; Specific[Add resourceNames&lt;br/&gt;if specific resources]\n    RB2 --&gt; Specific\n    CRB --&gt; Specific\n\n    Specific --&gt; Verbs[Limit verbs to&lt;br/&gt;minimum needed]\n\n    Verbs --&gt; Test[Test with&lt;br/&gt;auth can-i]\n\n    Test --&gt; Done[Document and&lt;br/&gt;Audit Regularly]\n\n    style Start fill:#e1f5ff\n    style ClusterRole fill:#fff4e1\n    style Role fill:#fff4e1\n    style Specific fill:#e8f5e8\n    style Verbs fill:#f3e5f5\n    style Done fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#testing-and-troubleshooting","title":"\ud83d\udee0\ufe0f Testing and Troubleshooting","text":"","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#testing-permissions-with-kubectl-auth-can-i","title":"Testing Permissions with <code>kubectl auth can-i</code>","text":"<pre><code># Test current user permissions\nkubectl auth can-i create pods -n dev\nkubectl auth can-i delete deployments -n prod\nkubectl auth can-i get nodes  # Cluster resource\nkubectl auth can-i '*' '*'    # Check super-user\n\n# Test as specific user\nkubectl auth can-i list secrets --as=jane -n dev\nkubectl auth can-i create pods --as=system:serviceaccount:dev:app-sa -n dev\n\n# Test as group\nkubectl auth can-i get pods --as=jane --as-group=developers -n dev\n\n# List all permissions for user (CKA helpful)\nkubectl auth can-i --list --as=jane -n dev\nkubectl auth can-i --list --as=system:serviceaccount:dev:app-sa -n dev\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#common-permission-issues","title":"Common Permission Issues","text":"<p>Issue 1: ServiceAccount can't access resources</p> <pre><code># Check ServiceAccount exists\nkubectl get sa app-sa -n dev\n\n# Check RoleBinding exists\nkubectl get rolebinding -n dev | grep app\n\n# Verify RoleBinding references correct SA\nkubectl describe rolebinding app-binding -n dev\n\n# Test permissions\nkubectl auth can-i get pods \\\n  --as=system:serviceaccount:dev:app-sa \\\n  -n dev\n</code></pre> <p>Issue 2: User denied access despite Role</p> <pre><code># Check RoleBinding subjects\nkubectl get rolebinding -n dev -o yaml | grep -A 5 subjects\n\n# Verify user/group name matches exactly (case-sensitive)\nkubectl describe rolebinding user-binding -n dev\n\n# Check if Role has required verbs\nkubectl describe role user-role -n dev\n</code></pre> <p>Issue 3: Permission denied on cluster resources</p> <pre><code># Cluster resources require ClusterRole + ClusterRoleBinding\nkubectl auth can-i get nodes  # Requires cluster-wide access\n\n# Check ClusterRoleBindings for user\nkubectl get clusterrolebinding -o yaml | grep -B 5 \"name: jane\"\n\n# Verify ClusterRole includes cluster resources\nkubectl describe clusterrole node-reader\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#debugging-authorization-flow","title":"Debugging Authorization Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant APIServer\n    participant Authenticator\n    participant Authorizer\n    participant Admission\n    participant etcd\n\n    User-&gt;&gt;APIServer: kubectl get pods -n dev\n    APIServer-&gt;&gt;Authenticator: Who is this user?\n    Authenticator--&gt;&gt;APIServer: User: jane, Groups: [developers]\n\n    APIServer-&gt;&gt;Authorizer: Can jane GET pods in dev?\n\n    Note over Authorizer: Check RoleBindings in 'dev'&lt;br/&gt;Check ClusterRoleBindings&lt;br/&gt;Evaluate all matching Roles/ClusterRoles\n\n    alt Permission Granted\n        Authorizer--&gt;&gt;APIServer: ALLOW (via dev-reader RoleBinding)\n        APIServer-&gt;&gt;Admission: Validate request\n        Admission--&gt;&gt;APIServer: OK\n        APIServer-&gt;&gt;etcd: Fetch pods\n        etcd--&gt;&gt;APIServer: Pod list\n        APIServer--&gt;&gt;User: [pod1, pod2, pod3]\n    else Permission Denied\n        Authorizer--&gt;&gt;APIServer: DENY (no matching permissions)\n        APIServer--&gt;&gt;User: Error: pods is forbidden&lt;br/&gt;User \"jane\" cannot list resource \"pods\"&lt;br/&gt;in API group \"\" in namespace \"dev\"\n    end</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#cka-exam-practice-exercises","title":"\ud83d\udcdd CKA Exam Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#exercise-1-create-role-and-rolebinding-for-pod-management","title":"Exercise 1: Create Role and RoleBinding for Pod Management","text":"<p>Scenario: Create a Role and RoleBinding for user <code>alice</code> to manage pods and services in the <code>production</code> namespace.</p> Solution <pre><code># Create Role\nkubectl create role pod-manager \\\n  --verb=get,list,watch,create,update,patch,delete \\\n  --resource=pods,services \\\n  --namespace=production\n\n# Create RoleBinding\nkubectl create rolebinding alice-pod-manager \\\n  --role=pod-manager \\\n  --user=alice \\\n  --namespace=production\n\n# Verify\nkubectl auth can-i create pods --as=alice -n production  # Should return 'yes'\nkubectl auth can-i delete services --as=alice -n production  # Should return 'yes'\nkubectl auth can-i get deployments --as=alice -n production  # Should return 'no'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#exercise-2-serviceaccount-with-specific-secret-access","title":"Exercise 2: ServiceAccount with Specific Secret Access","text":"<p>Scenario: Create a ServiceAccount <code>backup-sa</code> in the <code>ops</code> namespace that can only read the secret named <code>backup-credentials</code>.</p> Solution <pre><code># Create namespace and ServiceAccount\nkubectl create namespace ops\nkubectl create serviceaccount backup-sa -n ops\n\n# Create Role with resourceNames restriction\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: backup-secret-reader\n  namespace: ops\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"backup-credentials\"]  # ONLY this secret\n  verbs: [\"get\"]\nEOF\n\n# Create RoleBinding\nkubectl create rolebinding backup-sa-binding \\\n  --role=backup-secret-reader \\\n  --serviceaccount=ops:backup-sa \\\n  --namespace=ops\n\n# Create test secret\nkubectl create secret generic backup-credentials \\\n  --from-literal=password=secret123 \\\n  -n ops\n\nkubectl create secret generic other-secret \\\n  --from-literal=data=value \\\n  -n ops\n\n# Verify\nkubectl auth can-i get secret backup-credentials \\\n  --as=system:serviceaccount:ops:backup-sa \\\n  -n ops  # Should return 'yes'\n\nkubectl auth can-i get secret other-secret \\\n  --as=system:serviceaccount:ops:backup-sa \\\n  -n ops  # Should return 'no'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#exercise-3-cluster-wide-read-only-access","title":"Exercise 3: Cluster-Wide Read-Only Access","text":"<p>Scenario: Grant user <code>monitor</code> read-only access to pods, services, and deployments across ALL namespaces.</p> Solution <pre><code># Create ClusterRole\nkubectl create clusterrole cluster-reader \\\n  --verb=get,list,watch \\\n  --resource=pods,services,deployments\n\n# Create ClusterRoleBinding\nkubectl create clusterrolebinding monitor-cluster-reader \\\n  --clusterrole=cluster-reader \\\n  --user=monitor\n\n# Verify across namespaces\nkubectl auth can-i get pods --as=monitor -n default  # yes\nkubectl auth can-i get pods --as=monitor -n kube-system  # yes\nkubectl auth can-i get pods --as=monitor --all-namespaces  # yes\nkubectl auth can-i delete pods --as=monitor -n default  # no\nkubectl auth can-i get secrets --as=monitor -n default  # no\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#exercise-4-debug-permission-denied-error","title":"Exercise 4: Debug Permission Denied Error","text":"<p>Scenario: User <code>dev-user</code> reports: <code>Error: pods is forbidden: User \"dev-user\" cannot list resource \"pods\" in namespace \"development\"</code>. Investigate and fix.</p> Solution <pre><code># Step 1: Check if RoleBinding exists for user\nkubectl get rolebinding -n development -o yaml | grep -A 5 dev-user\n# Output: No RoleBinding found\n\n# Step 2: Check if ClusterRoleBinding grants access\nkubectl get clusterrolebinding -o yaml | grep -B 5 dev-user\n# Output: No ClusterRoleBinding found\n\n# Step 3: Create appropriate Role and RoleBinding\nkubectl create role pod-reader \\\n  --verb=get,list,watch \\\n  --resource=pods \\\n  --namespace=development\n\nkubectl create rolebinding dev-user-pods \\\n  --role=pod-reader \\\n  --user=dev-user \\\n  --namespace=development\n\n# Step 4: Verify fix\nkubectl auth can-i list pods --as=dev-user -n development  # Now returns 'yes'\n\n# Alternative: Use existing 'view' ClusterRole\nkubectl create rolebinding dev-user-view \\\n  --clusterrole=view \\\n  --user=dev-user \\\n  --namespace=development\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#exercise-5-least-privilege-for-cicd-pipeline","title":"Exercise 5: Least Privilege for CI/CD Pipeline","text":"<p>Scenario: A CI/CD pipeline needs to: - Create/update Deployments and Services in <code>staging</code> namespace - Read ConfigMaps and Secrets in <code>staging</code> namespace - NOT delete any resources - NOT access other namespaces</p> Solution <pre><code># Create ServiceAccount\nkubectl create namespace staging\nkubectl create serviceaccount cicd-sa -n staging\n\n# Create Role with minimal permissions\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cicd-deployer\n  namespace: staging\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]  # NO delete\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]  # NO delete\n- apiGroups: [\"\"]\n  resources: [\"configmaps\", \"secrets\"]\n  verbs: [\"get\", \"list\"]  # Read-only\nEOF\n\n# Create RoleBinding\nkubectl create rolebinding cicd-sa-deployer \\\n  --role=cicd-deployer \\\n  --serviceaccount=staging:cicd-sa \\\n  --namespace=staging\n\n# Verify permissions\nkubectl auth can-i create deployments \\\n  --as=system:serviceaccount:staging:cicd-sa \\\n  -n staging  # yes\n\nkubectl auth can-i delete deployments \\\n  --as=system:serviceaccount:staging:cicd-sa \\\n  -n staging  # no\n\nkubectl auth can-i get secrets \\\n  --as=system:serviceaccount:staging:cicd-sa \\\n  -n staging  # yes\n\nkubectl auth can-i create secrets \\\n  --as=system:serviceaccount:staging:cicd-sa \\\n  -n staging  # no\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#cka-exam-tips","title":"\ud83c\udfaf CKA Exam Tips","text":"","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#fast-role-creation-strategies","title":"Fast Role Creation Strategies","text":"<pre><code># Use imperative commands for speed\nkubectl create role &lt;name&gt; --verb=&lt;verbs&gt; --resource=&lt;resources&gt; -n &lt;namespace&gt;\nkubectl create rolebinding &lt;name&gt; --role=&lt;role&gt; --user=&lt;user&gt; -n &lt;namespace&gt;\n\n# Dry-run to generate YAML quickly\nkubectl create role test --verb=get --resource=pods --dry-run=client -o yaml\n\n# Copy and modify existing roles\nkubectl get clusterrole view -o yaml &gt; custom-view.yaml\n# Edit and apply\n\n# Use --list to see all verbs/resources quickly\nkubectl api-resources  # List all resource types\nkubectl api-versions   # List all API groups\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#time-saving-commands","title":"Time-Saving Commands","text":"<pre><code># Test permissions without applying\nkubectl auth can-i &lt;verb&gt; &lt;resource&gt; --as=&lt;user&gt; -n &lt;namespace&gt;\n\n# List all permissions for ServiceAccount\nkubectl auth can-i --list \\\n  --as=system:serviceaccount:&lt;namespace&gt;:&lt;sa-name&gt; \\\n  -n &lt;namespace&gt;\n\n# Find all RoleBindings for a user\nkubectl get rolebinding,clusterrolebinding -A -o json | \\\n  jq '.items[] | select(.subjects[]?.name == \"username\")'\n\n# Quick describe for troubleshooting\nkubectl describe rolebinding &lt;name&gt; -n &lt;namespace&gt;\nkubectl describe clusterrole &lt;name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#common-exam-patterns","title":"Common Exam Patterns","text":"<ol> <li>Grant namespace admin: <code>--clusterrole=admin</code></li> <li>Grant read-only: <code>--clusterrole=view</code></li> <li>ServiceAccount pattern: <code>--serviceaccount=&lt;namespace&gt;:&lt;sa-name&gt;</code></li> <li>Always test: <code>kubectl auth can-i</code> after creation</li> </ol>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#quick-reference","title":"\ud83d\udcda Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#common-kubectl-rbac-commands","title":"Common kubectl RBAC Commands","text":"<pre><code># Create resources\nkubectl create role &lt;name&gt; --verb=&lt;verbs&gt; --resource=&lt;resources&gt; -n &lt;ns&gt;\nkubectl create clusterrole &lt;name&gt; --verb=&lt;verbs&gt; --resource=&lt;resources&gt;\nkubectl create rolebinding &lt;name&gt; --role=&lt;role&gt; --user=&lt;user&gt; -n &lt;ns&gt;\nkubectl create clusterrolebinding &lt;name&gt; --clusterrole=&lt;cr&gt; --user=&lt;user&gt;\nkubectl create serviceaccount &lt;name&gt; -n &lt;ns&gt;\n\n# View resources\nkubectl get role,rolebinding -n &lt;ns&gt;\nkubectl get clusterrole,clusterrolebinding\nkubectl get sa -n &lt;ns&gt;\n\n# Describe for details\nkubectl describe role &lt;name&gt; -n &lt;ns&gt;\nkubectl describe rolebinding &lt;name&gt; -n &lt;ns&gt;\n\n# Test permissions\nkubectl auth can-i &lt;verb&gt; &lt;resource&gt; --as=&lt;user&gt; -n &lt;ns&gt;\nkubectl auth can-i --list --as=&lt;user&gt; -n &lt;ns&gt;\n\n# Impersonate user for testing\nkubectl get pods --as=&lt;user&gt; -n &lt;ns&gt;\nkubectl get pods --as=&lt;user&gt; --as-group=&lt;group&gt; -n &lt;ns&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#rbac-decision-tree","title":"RBAC Decision Tree","text":"<pre><code>Need to grant permissions?\n\u2502\n\u251c\u2500 Namespace-specific access?\n\u2502  \u251c\u2500 YES \u2192 Use Role + RoleBinding\n\u2502  \u2502  \u2514\u2500 (or RoleBinding \u2192 ClusterRole for reuse)\n\u2502  \u2502\n\u2502  \u2514\u2500 NO \u2192 Use ClusterRole + ClusterRoleBinding\n\u2502\n\u251c\u2500 What type of subject?\n\u2502  \u251c\u2500 Human \u2192 User\n\u2502  \u251c\u2500 Pod \u2192 ServiceAccount\n\u2502  \u2514\u2500 Collection \u2192 Group\n\u2502\n\u2514\u2500 Test with: kubectl auth can-i &lt;verb&gt; &lt;resource&gt; --as=&lt;subject&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Previous: Post 14 - ConfigMaps and Secrets</li> <li>Next: Post 16 - Security Contexts and Pod Security Standards</li> <li>Reference: Kubernetes RBAC Documentation</li> <li>Series: Kubernetes CKA Mastery</li> </ul>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/rbac-role-based-access-control/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>RBAC is mandatory for securing Kubernetes clusters</li> <li>Roles are namespace-scoped, ClusterRoles are cluster-wide</li> <li>RoleBindings grant permissions by linking subjects to roles</li> <li>ServiceAccounts provide pod-level authentication</li> <li>Least privilege principle: Grant minimum permissions required</li> <li>Use <code>kubectl auth can-i</code> to test permissions before applying</li> <li>Default roles (admin, edit, view) cover common use cases</li> <li>Aggregation allows composing complex roles from simple components</li> <li>CKA exam tip: Master imperative commands for speed</li> </ul> <p>Next Steps: Apply security contexts and pod security standards to harden workloads at the pod level.</p>","tags":["kubernetes","k8s","cka-prep","rbac","security"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/","title":"Advanced Scheduling: Taints, Tolerations, and Affinity","text":"<p>Master Kubernetes pod scheduling with taints, tolerations, node affinity, and pod affinity/anti-affinity. Essential knowledge for CKA exam success and production-grade workload placement.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#overview","title":"Overview","text":"<p>The Kubernetes scheduler is responsible for assigning pods to nodes, making intelligent decisions based on resource requirements, constraints, and policies. While the default scheduler works well for simple deployments, production environments require fine-grained control over pod placement.</p> <p>CKA Exam Domain: Workloads &amp; Scheduling (15% of exam)</p> <p>Key Insight: Advanced scheduling isn't just about passing the CKA exam\u2014it's essential for high availability, resource optimization, cost management, and regulatory compliance in production Kubernetes clusters.</p> <p>What You'll Learn: - How the Kubernetes scheduler makes placement decisions - Node selectors for simple node selection - Taints and tolerations for node restriction patterns - Node affinity for complex node selection rules - Pod affinity and anti-affinity for inter-pod placement - Priority classes and preemption mechanisms - CKA exam speed techniques for scheduling tasks</p> <p>Understanding these scheduling mechanisms enables you to: - Optimize costs by directing workloads to specific node types - Improve reliability through strategic pod distribution - Meet compliance requirements with dedicated node pools - Enhance performance by co-locating related services - Manage maintenance without disrupting critical workloads</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#how-the-scheduler-works","title":"How the Scheduler Works","text":"<p>The Kubernetes scheduler is a control plane component that watches for newly created pods with no assigned node and selects an optimal node for them to run on.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#scheduling-process","title":"Scheduling Process","text":"<pre><code>graph TB\n    START[New Pod Created] --&gt; FILTER[Filter Phase]\n    FILTER --&gt; |Remove unsuitable nodes| SCORE[Score Phase]\n    SCORE --&gt; |Rank remaining nodes| SELECT[Select Best Node]\n    SELECT --&gt; BIND[Bind Pod to Node]\n\n    FILTER --&gt; CHECK1{Sufficient Resources?}\n    FILTER --&gt; CHECK2{Node Selectors Match?}\n    FILTER --&gt; CHECK3{Taints Tolerated?}\n    FILTER --&gt; CHECK4{Affinity Rules Met?}\n\n    CHECK1 --&gt; |No| REJECT1[Remove Node]\n    CHECK2 --&gt; |No| REJECT2[Remove Node]\n    CHECK3 --&gt; |No| REJECT3[Remove Node]\n    CHECK4 --&gt; |No| REJECT4[Remove Node]\n\n    CHECK1 --&gt; |Yes| PASS1[Keep Node]\n    CHECK2 --&gt; |Yes| PASS2[Keep Node]\n    CHECK3 --&gt; |Yes| PASS3[Keep Node]\n    CHECK4 --&gt; |Yes| PASS4[Keep Node]\n\n    PASS1 --&gt; SCORE\n    PASS2 --&gt; SCORE\n    PASS3 --&gt; SCORE\n    PASS4 --&gt; SCORE</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#two-phase-algorithm","title":"Two-Phase Algorithm","text":"<p>Phase 1: Filtering (Feasibility) The scheduler eliminates nodes that don't meet pod requirements:</p> <ul> <li>Resource availability: Does the node have enough CPU, memory, and ephemeral storage?</li> <li>Node selector constraints: Does the node have required labels?</li> <li>Taints and tolerations: Can the pod tolerate node taints?</li> <li>Affinity/anti-affinity rules: Do inter-pod placement rules allow this node?</li> <li>Port availability: Are required host ports available?</li> <li>Volume constraints: Can required volumes be mounted?</li> </ul> <p>Phase 2: Scoring (Prioritization) For remaining nodes, the scheduler assigns scores based on:</p> <ul> <li>Resource balance: Prefer nodes with balanced resource utilization</li> <li>Image locality: Favor nodes with container images already pulled</li> <li>Spreading: Distribute pods across zones/nodes for high availability</li> <li>Affinity preferences: Honor preferred (not required) affinity rules</li> <li>Custom priorities: Apply user-defined priority functions</li> </ul> <p>The node with the highest score wins. Ties are broken randomly to ensure even distribution.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#default-behavior","title":"Default Behavior","text":"<p>Without any scheduling constraints, the scheduler: 1. Filters nodes with sufficient resources 2. Scores nodes based on resource balance and spreading 3. Selects the optimal node 4. Binds the pod to that node</p> <p>This works well for stateless applications but isn't sufficient for: - Specialized hardware requirements (GPUs, SSDs, specific CPU types) - Regulatory compliance (data locality, dedicated infrastructure) - Performance optimization (co-location, anti-affinity) - Cost optimization (spot instances, cheaper node pools)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#node-selectors-simple-label-based-selection","title":"Node Selectors: Simple Label-Based Selection","text":"<p>Node selectors provide the simplest way to constrain pods to nodes with specific labels.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#how-node-selectors-work","title":"How Node Selectors Work","text":"<p>Node selectors use key-value pairs to match node labels. A pod will only be scheduled on nodes that have all specified labels.</p> <p>Labeling a Node: <pre><code># Add label to node\nkubectl label nodes node01 disktype=ssd\n\n# Add multiple labels\nkubectl label nodes node02 disktype=hdd environment=production\n\n# Verify labels\nkubectl get nodes --show-labels\nkubectl describe node node01 | grep -A5 Labels\n</code></pre></p> <p>Using Node Selectors in Pods: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-ssd\nspec:\n  nodeSelector:\n    disktype: ssd  # Only schedule on nodes with this label\n  containers:\n  - name: nginx\n    image: nginx:1.24\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#multiple-node-selectors","title":"Multiple Node Selectors","text":"<p>When you specify multiple node selectors, all labels must match (AND logic):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: database\nspec:\n  nodeSelector:\n    disktype: ssd\n    environment: production\n    region: us-west\n  containers:\n  - name: postgres\n    image: postgres:15\n</code></pre> <p>This pod will only schedule on nodes that have all three labels.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#built-in-node-labels","title":"Built-In Node Labels","text":"<p>Kubernetes automatically applies several useful labels to nodes:</p> <pre><code># Architecture and OS\nkubernetes.io/arch: amd64\nkubernetes.io/os: linux\n\n# Cloud provider metadata (on cloud-managed clusters)\ntopology.kubernetes.io/region: us-west-2\ntopology.kubernetes.io/zone: us-west-2a\nnode.kubernetes.io/instance-type: m5.xlarge\n\n# Hostname\nkubernetes.io/hostname: node01\n</code></pre> <p>Example - Zone-Specific Scheduling: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-west\nspec:\n  nodeSelector:\n    topology.kubernetes.io/zone: us-west-2a\n  containers:\n  - name: app\n    image: myapp:v1\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#limitations-of-node-selectors","title":"Limitations of Node Selectors","text":"<p>While simple and fast, node selectors have significant limitations:</p> <ol> <li>No OR logic: Cannot express \"disktype=ssd OR disktype=nvme\"</li> <li>No NOT logic: Cannot express \"NOT environment=development\"</li> <li>No numeric comparisons: Cannot express \"cpu-count &gt; 8\"</li> <li>Hard requirements only: Cannot express preferences (e.g., \"prefer SSD but allow HDD\")</li> <li>Single level: Cannot express complex nested logic</li> </ol> <p>When to Use Node Selectors: - \u2705 Simple label matching requirements - \u2705 Quick temporary constraints - \u2705 CKA exam speed (fastest to type)</p> <p>When to Upgrade to Node Affinity: - \u274c Need OR/NOT logic - \u274c Need soft preferences - \u274c Need numeric comparisons - \u274c Complex multi-condition rules</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#taints-and-tolerations-node-restriction-pattern","title":"Taints and Tolerations: Node Restriction Pattern","text":"<p>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. Think of taints as a \"repellent\" on nodes, and tolerations as a pod's \"immunity\" to specific taints.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#conceptual-model","title":"Conceptual Model","text":"<p>Taints are applied to nodes to mark them as unsuitable for most pods: <pre><code># Syntax: key=value:effect\nkubectl taint nodes node01 gpu=true:NoSchedule\n</code></pre></p> <p>Tolerations are specified in pod specs to allow scheduling on tainted nodes: <pre><code>tolerations:\n- key: \"gpu\"\n  operator: \"Equal\"\n  value: \"true\"\n  effect: \"NoSchedule\"\n</code></pre></p> <p>Key Concept: Taints repel pods by default. Only pods with matching tolerations can be scheduled on tainted nodes. This is the opposite of affinity (which attracts pods to nodes).</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#taint-effects","title":"Taint Effects","text":"<p>Kubernetes supports three taint effects that control scheduling and eviction behavior:</p> <pre><code>graph TB\n    subgraph \"Taint Effects\"\n        NOSCHEDULE[NoSchedule]\n        PREFERNOSCHEDULE[PreferNoSchedule]\n        NOEXECUTE[NoExecute]\n    end\n\n    NOSCHEDULE --&gt; |Behavior| NS_BEHAVIOR[Hard constraint&lt;br/&gt;No new pods scheduled&lt;br/&gt;Existing pods stay]\n    PREFERNOSCHEDULE --&gt; |Behavior| PNS_BEHAVIOR[Soft constraint&lt;br/&gt;Avoid if possible&lt;br/&gt;Schedule if necessary]\n    NOEXECUTE --&gt; |Behavior| NE_BEHAVIOR[Hard constraint + eviction&lt;br/&gt;No new pods scheduled&lt;br/&gt;Existing pods evicted]\n\n    subgraph \"Use Cases\"\n        NS_USE[Dedicated nodes&lt;br/&gt;Special hardware&lt;br/&gt;Maintenance prep]\n        PNS_USE[Spot instances&lt;br/&gt;Lower priority nodes&lt;br/&gt;Cost optimization]\n        NE_USE[Node draining&lt;br/&gt;Emergency eviction&lt;br/&gt;Hardware failures]\n    end\n\n    NOSCHEDULE -.-&gt; NS_USE\n    PREFERNOSCHEDULE -.-&gt; PNS_USE\n    NOEXECUTE -.-&gt; NE_USE</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#noschedule-hard-constraint","title":"NoSchedule (Hard Constraint)","text":"<p>Prevents new pods from being scheduled on the node. Existing pods remain running.</p> <pre><code># Apply NoSchedule taint\nkubectl taint nodes node01 dedicated=ml:NoSchedule\n\n# Verify taint\nkubectl describe node node01 | grep Taints\n# Output: Taints: dedicated=ml:NoSchedule\n</code></pre> <p>Pod Toleration: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ml-training\nspec:\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Equal\"\n    value: \"ml\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: trainer\n    image: tensorflow:latest\n</code></pre></p> <p>Use Cases: - Dedicated node pools (GPU nodes, high-memory nodes) - Isolating production workloads from development - Reserving nodes for specific teams or projects</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#prefernoschedule-soft-constraint","title":"PreferNoSchedule (Soft Constraint)","text":"<p>Scheduler tries to avoid placing pods on the node but will schedule them if no other nodes are available.</p> <pre><code># Apply PreferNoSchedule taint\nkubectl taint nodes node02 spot-instance=true:PreferNoSchedule\n</code></pre> <p>Behavior: - Scheduler prefers untainted nodes - Falls back to tainted nodes under resource pressure - No eviction of existing pods</p> <p>Use Cases: - Spot/preemptible instances (prefer stable nodes but allow spot) - Cost optimization (prefer cheaper nodes) - Gradual migration (discourage but don't prevent scheduling)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#noexecute-hard-constraint-eviction","title":"NoExecute (Hard Constraint + Eviction)","text":"<p>Prevents scheduling and evicts existing pods that don't tolerate the taint.</p> <pre><code># Apply NoExecute taint\nkubectl taint nodes node03 maintenance=true:NoExecute\n</code></pre> <p>Immediate Effects: 1. No new pods scheduled on the node 2. Existing pods without tolerations are evicted immediately 3. Pods with tolerations remain (unless <code>tolerationSeconds</code> expires)</p> <p>Toleration with Grace Period: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  tolerations:\n  - key: \"maintenance\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 300  # Evict after 5 minutes\n  containers:\n  - name: nginx\n    image: nginx:1.24\n</code></pre></p> <p>Use Cases: - Node draining: Gracefully migrate workloads before maintenance - Failure handling: Automatically evict pods from degraded nodes - Time-bounded tolerations: Allow temporary execution (e.g., batch jobs)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#toleration-operators","title":"Toleration Operators","text":"<p>Kubernetes supports two toleration operators:</p> <p>Equal Operator (exact match): <pre><code>tolerations:\n- key: \"gpu\"\n  operator: \"Equal\"\n  value: \"nvidia-a100\"\n  effect: \"NoSchedule\"\n</code></pre> Matches taint: <code>gpu=nvidia-a100:NoSchedule</code></p> <p>Exists Operator (key-only match): <pre><code>tolerations:\n- key: \"gpu\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n</code></pre> Matches any taint with key <code>gpu</code> and effect <code>NoSchedule</code>, regardless of value.</p> <p>Wildcard Tolerations (tolerate all taints): <pre><code>tolerations:\n- operator: \"Exists\"  # Tolerates ALL taints (dangerous!)\n</code></pre></p> <p>Effect Wildcard (tolerate all effects for a key): <pre><code>tolerations:\n- key: \"gpu\"\n  operator: \"Exists\"  # Tolerates gpu=* with any effect\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#control-plane-taints","title":"Control Plane Taints","text":"<p>By default, Kubernetes taints control plane nodes to prevent workload pods from being scheduled there:</p> <pre><code># View control plane taints\nkubectl describe node controlplane | grep Taints\n\n# Typical output:\n# Taints: node-role.kubernetes.io/control-plane:NoSchedule\n</code></pre> <p>Scheduling on Control Plane (not recommended for production): <pre><code>tolerations:\n- key: \"node-role.kubernetes.io/control-plane\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n</code></pre></p> <p>CKA Exam Note: You may need to schedule pods on control plane nodes in exam clusters. Use the toleration above.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#taint-management-commands","title":"Taint Management Commands","text":"<pre><code># Add taint\nkubectl taint nodes node01 key=value:NoSchedule\n\n# Remove taint (note the trailing dash)\nkubectl taint nodes node01 key=value:NoSchedule-\n\n# Remove taint by key only\nkubectl taint nodes node01 key-\n\n# Update taint value\nkubectl taint nodes node01 key=newvalue:NoSchedule --overwrite\n\n# View all node taints\nkubectl get nodes -o json | jq '.items[].spec.taints'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#practical-taint-scenarios","title":"Practical Taint Scenarios","text":"<p>Scenario 1: Dedicated GPU Nodes <pre><code># Taint GPU nodes\nkubectl taint nodes gpu-node-01 hardware=gpu:NoSchedule\nkubectl label nodes gpu-node-01 accelerator=nvidia-a100\n\n# Deploy GPU workload\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-training\nspec:\n  nodeSelector:\n    accelerator: nvidia-a100\n  tolerations:\n  - key: \"hardware\"\n    operator: \"Equal\"\n    value: \"gpu\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: trainer\n    image: tensorflow/tensorflow:latest-gpu\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre></p> <p>Scenario 2: Node Maintenance Preparation <pre><code># Step 1: Prevent new pods (NoSchedule)\nkubectl taint nodes node02 maintenance=scheduled:NoSchedule\n\n# Step 2: After workloads migrated, evict remaining pods\nkubectl taint nodes node02 maintenance=scheduled:NoExecute --overwrite\n\n# Step 3: Perform maintenance...\n\n# Step 4: Remove taint and return to service\nkubectl taint nodes node02 maintenance-\n</code></pre></p> <p>Scenario 3: Spot Instance Node Pool <pre><code># Taint spot instances as less preferred\nkubectl taint nodes spot-node-01 spot-instance=true:PreferNoSchedule\n\n# Workload that can tolerate spot instances\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: batch-processor\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: batch\n  template:\n    metadata:\n      labels:\n        app: batch\n    spec:\n      tolerations:\n      - key: \"spot-instance\"\n        operator: \"Exists\"\n      containers:\n      - name: processor\n        image: batch-app:v1\nEOF\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#node-affinity-advanced-node-selection","title":"Node Affinity: Advanced Node Selection","text":"<p>Node affinity is a powerful evolution of node selectors that provides expressive label-based selection with support for operators, soft preferences, and complex logic.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#node-affinity-vs-node-selectors","title":"Node Affinity vs Node Selectors","text":"Feature nodeSelector Node Affinity Label matching \u2705 Exact match only \u2705 Multiple operators OR logic \u274c Not supported \u2705 <code>In</code> operator NOT logic \u274c Not supported \u2705 <code>NotIn</code>, <code>DoesNotExist</code> Numeric comparison \u274c Not supported \u2705 <code>Gt</code>, <code>Lt</code> operators Soft preferences \u274c Hard only \u2705 <code>preferred</code> rules Multiple rules \u274c AND only \u2705 AND + OR combinations","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#affinity-rule-types","title":"Affinity Rule Types","text":"<pre><code>graph TB\n    subgraph \"Node Affinity Types\"\n        REQUIRED[requiredDuringSchedulingIgnoredDuringExecution]\n        PREFERRED[preferredDuringSchedulingIgnoredDuringExecution]\n    end\n\n    REQUIRED --&gt; |Behavior| REQ_BEHAVIOR[Hard constraint&lt;br/&gt;Must match or pod pending&lt;br/&gt;Scheduling phase only]\n    PREFERRED --&gt; |Behavior| PREF_BEHAVIOR[Soft constraint&lt;br/&gt;Prefer but not required&lt;br/&gt;Weighted scoring]\n\n    subgraph \"Naming Convention\"\n        SCHEDULING[DuringScheduling&lt;br/&gt;Rules apply when scheduling]\n        EXECUTION[DuringExecution&lt;br/&gt;IgnoredDuringExecution = no eviction]\n    end\n\n    REQUIRED -.-&gt; SCHEDULING\n    REQUIRED -.-&gt; EXECUTION\n    PREFERRED -.-&gt; SCHEDULING\n    PREFERRED -.-&gt; EXECUTION</code></pre> <p>Naming Convention Explained: - <code>requiredDuringScheduling</code>: Pod must match rules to be scheduled - <code>preferredDuringScheduling</code>: Scheduler prefers matching nodes but allows others - <code>IgnoredDuringExecution</code>: Rules are not re-evaluated after scheduling (no eviction)</p> <p>Future (not yet implemented): - <code>requiredDuringExecution</code>: Would evict pods if node labels change (similar to NoExecute taint)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#required-node-affinity","title":"Required Node Affinity","text":"<p>Required affinity creates hard constraints that must be satisfied for scheduling.</p> <p>Basic Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: database\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - ssd\n            - nvme\n  containers:\n  - name: postgres\n    image: postgres:15\n</code></pre></p> <p>Logic: Schedule on nodes with <code>disktype=ssd</code> OR <code>disktype=nvme</code>.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#all-node-affinity-operators","title":"All Node Affinity Operators","text":"<p>In - Label value in list (OR logic): <pre><code>- key: environment\n  operator: In\n  values:\n  - production\n  - staging\n</code></pre></p> <p>NotIn - Label value not in list: <pre><code>- key: environment\n  operator: NotIn\n  values:\n  - development\n  - test\n</code></pre></p> <p>Exists - Label key exists (value doesn't matter): <pre><code>- key: ssd\n  operator: Exists\n</code></pre></p> <p>DoesNotExist - Label key doesn't exist: <pre><code>- key: spot-instance\n  operator: DoesNotExist\n</code></pre></p> <p>Gt - Numeric greater than: <pre><code>- key: cpu-count\n  operator: Gt\n  values:\n  - \"8\"  # Note: values are always strings\n</code></pre></p> <p>Lt - Numeric less than: <pre><code>- key: age-days\n  operator: Lt\n  values:\n  - \"30\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#multiple-node-selector-terms-or-logic","title":"Multiple Node Selector Terms (OR Logic)","text":"<p>Multiple <code>nodeSelectorTerms</code> are ORed together. Each term's <code>matchExpressions</code> are ANDed.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: flexible-app\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        # Term 1: High-performance nodes\n        - matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - nvme\n          - key: cpu-count\n            operator: Gt\n            values:\n            - \"16\"\n        # OR\n        # Term 2: GPU nodes (even if slower disk)\n        - matchExpressions:\n          - key: accelerator\n            operator: Exists\n  containers:\n  - name: app\n    image: myapp:v1\n</code></pre> <p>Logic: Schedule on nodes that match: - (disktype=nvme AND cpu-count&gt;16) OR (accelerator exists)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#preferred-node-affinity","title":"Preferred Node Affinity","text":"<p>Preferred affinity creates soft preferences with configurable weights.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  affinity:\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 80  # Higher weight = stronger preference\n        preference:\n          matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - ssd\n      - weight: 20\n        preference:\n          matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - us-west-2a\n  containers:\n  - name: nginx\n    image: nginx:1.24\n</code></pre> <p>Behavior: - Scheduler calculates scores for each node - Node score += weight if preference matches - Highest total score wins - Pod will still schedule even if no preferences match</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#combining-required-and-preferred","title":"Combining Required and Preferred","text":"<p>Best practice: Use required for must-have constraints, preferred for nice-to-have optimizations.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: critical-db\nspec:\n  affinity:\n    nodeAffinity:\n      # MUST be in production environment\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: environment\n            operator: In\n            values:\n            - production\n      # PREFER high-performance and specific zone\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        preference:\n          matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - nvme\n      - weight: 50\n        preference:\n          matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - us-west-2a\n  containers:\n  - name: postgres\n    image: postgres:15\n</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#node-affinity-evaluation-flow","title":"Node Affinity Evaluation Flow","text":"<pre><code>graph TB\n    START[Evaluate Node] --&gt; REQUIRED{Required Affinity&lt;br/&gt;Rules Exist?}\n    REQUIRED --&gt; |Yes| EVAL_REQ[Evaluate Required Rules]\n    REQUIRED --&gt; |No| PREFERRED{Preferred Affinity&lt;br/&gt;Rules Exist?}\n\n    EVAL_REQ --&gt; REQ_MATCH{All Required&lt;br/&gt;Rules Match?}\n    REQ_MATCH --&gt; |No| REJECT[Remove Node from Consideration]\n    REQ_MATCH --&gt; |Yes| PREFERRED\n\n    PREFERRED --&gt; |Yes| EVAL_PREF[Calculate Preference Score]\n    PREFERRED --&gt; |No| BASE_SCORE[Use Base Score]\n\n    EVAL_PREF --&gt; SCORE[Add to Node Score]\n    BASE_SCORE --&gt; SCORE\n    SCORE --&gt; NEXT[Continue to Next Node]</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#cka-exam-speed-techniques","title":"CKA Exam Speed Techniques","text":"<p>Fast YAML generation: <pre><code># Generate pod with dry-run, then edit\nkubectl run myapp --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit to add node affinity\nvim pod.yaml\n</code></pre></p> <p>Quick affinity template (memorize this structure): <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: KEY\n          operator: In\n          values:\n          - VALUE\n</code></pre></p> <p>Common exam pattern: <pre><code># 1. Label node\nkubectl label nodes node01 tier=frontend\n\n# 2. Create pod with affinity\nkubectl run web --image=nginx --dry-run=client -o yaml | \\\nkubectl set selector --local -f - 'app=web' -o yaml | \\\n# ... add affinity in vim ...\nkubectl apply -f -\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-affinity-and-anti-affinity","title":"Pod Affinity and Anti-Affinity","text":"<p>Pod affinity and anti-affinity allow you to constrain pod scheduling based on labels of other pods already running on nodes, rather than node labels.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#use-cases","title":"Use Cases","text":"<p>Pod Affinity (co-location): - Deploy web tier pods on same nodes as cache pods - Co-locate related microservices for low-latency communication - Place pods near data for performance</p> <p>Pod Anti-Affinity (spreading): - Distribute replicas across nodes for high availability - Prevent multiple critical services on same node - Spread pods across failure domains (zones, racks)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#topology-keys","title":"Topology Keys","text":"<p>Pod affinity rules use topology keys to define the scope of \"co-location\":</p> <pre><code>topologyKey: kubernetes.io/hostname  # Same node\ntopologyKey: topology.kubernetes.io/zone  # Same zone\ntopologyKey: topology.kubernetes.io/region  # Same region\ntopologyKey: kubernetes.io/os  # Same OS (all nodes)\n</code></pre> <p>Topology Key Behavior: - Scheduler groups nodes by the topology key value - Affinity/anti-affinity rules apply within each group - <code>kubernetes.io/hostname</code> = node-level granularity (most common)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-affinity-visualization","title":"Pod Affinity Visualization","text":"<pre><code>graph TB\n    subgraph \"Zone us-west-2a\"\n        subgraph \"Node 1\"\n            CACHE1[Cache Pod&lt;br/&gt;app=redis]\n            WEB1[Web Pod&lt;br/&gt;affinity to redis]\n        end\n        subgraph \"Node 2\"\n            CACHE2[Cache Pod&lt;br/&gt;app=redis]\n            WEB2[Web Pod&lt;br/&gt;affinity to redis]\n        end\n    end\n\n    subgraph \"Zone us-west-2b\"\n        subgraph \"Node 3\"\n            DB1[Database Pod&lt;br/&gt;anti-affinity]\n        end\n        subgraph \"Node 4\"\n            DB2[Database Pod&lt;br/&gt;anti-affinity]\n        end\n    end\n\n    WEB1 -.-&gt;|Pod Affinity| CACHE1\n    WEB2 -.-&gt;|Pod Affinity| CACHE2\n    DB1 -.-&gt;|Anti-Affinity| DB2</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-affinity-examples","title":"Pod Affinity Examples","text":"<p>Required Pod Affinity (hard constraint): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-frontend\n  labels:\n    app: web\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - cache\n        topologyKey: kubernetes.io/hostname\n  containers:\n  - name: nginx\n    image: nginx:1.24\n</code></pre></p> <p>Logic: This web pod must be scheduled on a node that already has a pod with <code>app=cache</code>.</p> <p>Preferred Pod Affinity (soft constraint): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-server\nspec:\n  affinity:\n    podAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: component\n              operator: In\n              values:\n              - database\n          topologyKey: kubernetes.io/hostname\n  containers:\n  - name: api\n    image: api:v1\n</code></pre></p> <p>Logic: Prefer to schedule this API pod on nodes with database pods, but allow other nodes if necessary.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-anti-affinity-examples","title":"Pod Anti-Affinity Examples","text":"<p>High Availability Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: critical-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: critical\n  template:\n    metadata:\n      labels:\n        app: critical\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - critical\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: app\n        image: critical-app:v1\n</code></pre></p> <p>Logic: No two <code>app=critical</code> pods can run on the same node. Each replica must be on a different node.</p> <p>Zone-Level Anti-Affinity: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: database-replica\n  labels:\n    app: database\nspec:\n  affinity:\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: app\n              operator: In\n              values:\n              - database\n          topologyKey: topology.kubernetes.io/zone\n  containers:\n  - name: postgres\n    image: postgres:15\n</code></pre></p> <p>Logic: Prefer to spread database pods across different availability zones for disaster recovery.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#symmetric-vs-asymmetric-affinity","title":"Symmetric vs Asymmetric Affinity","text":"<p>Symmetric (mutual attraction/repulsion): <pre><code># Pod A has affinity to Pod B\n# Pod B has affinity to Pod A\n# Result: Co-located pairs\n</code></pre></p> <p>Asymmetric (one-way): <pre><code># Pod A (web) has affinity to Pod B (cache)\n# Pod B (cache) has NO affinity rules\n# Result: Web pods follow cache pods, but cache pods schedule independently\n</code></pre></p> <p>Best Practice: Use asymmetric affinity where one component (cache, database) should schedule freely, and dependent components (web, API) follow them.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#performance-considerations","title":"Performance Considerations","text":"<p>Pod affinity/anti-affinity has performance costs:</p> <p>Scheduling Latency: - Required rules: Moderate impact (must check all nodes) - Preferred rules: Higher impact (calculate scores for all nodes) - Large clusters (&gt;500 nodes): Significant delay</p> <p>Recommendations: - Use affinity sparingly (only where truly needed) - Prefer node affinity over pod affinity when possible - Use preferred over required when acceptable - Limit label selector complexity</p> <p>Exam Note: Pod affinity can cause pods to remain Pending if no nodes satisfy the constraint. Always verify with <code>kubectl describe pod</code> to check scheduling events.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#advanced-scheduling-concepts","title":"Advanced Scheduling Concepts","text":"","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#priority-classes-and-preemption","title":"Priority Classes and Preemption","text":"<p>Priority classes allow you to define the relative importance of pods. The scheduler can evict (preempt) lower-priority pods to make room for higher-priority ones.</p> <p>Creating a Priority Class: <pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000  # Higher = more important\nglobalDefault: false\ndescription: \"High-priority workloads\"\n</code></pre></p> <pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: low-priority\nvalue: 100\nglobalDefault: false\ndescription: \"Best-effort workloads\"\n</code></pre> <p>Using Priority Classes: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: critical-app\nspec:\n  priorityClassName: high-priority\n  containers:\n  - name: app\n    image: critical:v1\n</code></pre></p> <p>Preemption Flow: <pre><code>graph TB\n    START[High-Priority Pod Created] --&gt; FILTER[Filter Feasible Nodes]\n    FILTER --&gt; SPACE{Sufficient&lt;br/&gt;Resources?}\n    SPACE --&gt; |Yes| SCHEDULE[Schedule Normally]\n    SPACE --&gt; |No| PREEMPT{Can Preempt&lt;br/&gt;Lower-Priority Pods?}\n    PREEMPT --&gt; |Yes| EVICT[Evict Lower-Priority Pods]\n    EVICT --&gt; SCHEDULE2[Schedule High-Priority Pod]\n    PREEMPT --&gt; |No| PENDING[Pod Remains Pending]\n\n    EVICT --&gt; GRACE[Respect PodDisruptionBudget]\n    GRACE --&gt; TERM[Graceful Termination]</code></pre></p> <p>Built-In Priority Classes: <pre><code>kubectl get priorityclasses\n\n# Typical output:\n# NAME                      VALUE        GLOBAL-DEFAULT   AGE\n# system-cluster-critical   2000000000   false            1d\n# system-node-critical      2000001000   false            1d\n</code></pre></p> <p>Use Cases: - Critical infrastructure: Cluster DNS, monitoring, logging - Production vs development: Preempt dev pods for production - Batch processing: Low-priority batch jobs yield to interactive workloads</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-topology-spread-constraints","title":"Pod Topology Spread Constraints","text":"<p>Topology spread constraints provide fine-grained control over pod distribution across failure domains.</p> <p>Basic Spread Constraint: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1  # Max difference between zones\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule  # Hard constraint\n        labelSelector:\n          matchLabels:\n            app: web\n      containers:\n      - name: nginx\n        image: nginx:1.24\n</code></pre></p> <p>Behavior: - With 3 zones and 6 replicas: Each zone gets 2 pods - <code>maxSkew: 1</code> means max difference of 1 pod between zones - If a zone is unavailable, pods remain Pending (<code>DoNotSchedule</code>)</p> <p>Soft Constraint (use <code>ScheduleAnyway</code>): <pre><code>topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: topology.kubernetes.io/zone\n  whenUnsatisfiable: ScheduleAnyway  # Soft: prefer but allow skew\n  labelSelector:\n    matchLabels:\n      app: web\n</code></pre></p> <p>Multi-Level Spreading (zones and nodes): <pre><code>topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: topology.kubernetes.io/zone\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchLabels:\n      app: web\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: ScheduleAnyway\n  labelSelector:\n    matchLabels:\n      app: web\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#resource-requests-and-scheduling","title":"Resource Requests and Scheduling","text":"<p>While not a scheduling constraint per se, resource requests heavily influence scheduling decisions.</p> <p>Resource-Aware Scheduling: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: resource-demo\nspec:\n  containers:\n  - name: app\n    image: myapp:v1\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n</code></pre></p> <p>Scheduler Behavior: - Filters out nodes with insufficient allocatable resources - Considers requests (not limits) for scheduling - Spreads pods to balance resource utilization</p> <p>QoS Classes and Scheduling: - Guaranteed: requests = limits (highest priority, never preempted) - Burstable: requests &lt; limits (medium priority) - BestEffort: no requests/limits (lowest priority, first to be evicted)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#custom-schedulers","title":"Custom Schedulers","text":"<p>Kubernetes allows custom schedulers for specialized scheduling logic.</p> <p>Specifying a Custom Scheduler: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: custom-scheduled\nspec:\n  schedulerName: my-custom-scheduler  # Default: \"default-scheduler\"\n  containers:\n  - name: app\n    image: myapp:v1\n</code></pre></p> <p>Use Cases (rare in practice): - Machine learning job scheduling (gang scheduling) - Highly specialized hardware placement - Custom cost optimization algorithms</p> <p>CKA Exam: Custom schedulers are not required knowledge for the exam.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#cka-exam-skills-fast-scheduling-commands","title":"CKA Exam Skills: Fast Scheduling Commands","text":"","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#speed-techniques-for-taints-and-tolerations","title":"Speed Techniques for Taints and Tolerations","text":"<p>Fast Taint Application: <pre><code># Standard taint\nkubectl taint nodes node01 key=value:NoSchedule\n\n# Multiple taints at once\nkubectl taint nodes node01 gpu=true:NoSchedule dedicated=ml:NoSchedule\n\n# Remove taint (trailing dash)\nkubectl taint nodes node01 key:NoSchedule-\nkubectl taint nodes node01 key-  # Remove all effects for key\n\n# List taints on all nodes\nkubectl describe nodes | grep -i taint\n</code></pre></p> <p>Generating Tolerations: <pre><code># Start with pod template\nkubectl run mypod --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit to add toleration (use vim macros or snippets)\nvim pod.yaml\n</code></pre></p> <p>Toleration YAML snippet (memorize): <pre><code>tolerations:\n- key: \"KEY\"\n  operator: \"Equal\"\n  value: \"VALUE\"\n  effect: \"NoSchedule\"\n</code></pre></p> <p>Exists operator (faster typing): <pre><code>tolerations:\n- key: \"KEY\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#speed-techniques-for-node-affinity","title":"Speed Techniques for Node Affinity","text":"<p>Quick nodeSelector (use when possible): <pre><code># Generate pod with dry-run\nkubectl run web --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Add nodeSelector inline\ncat &lt;&lt;EOF &gt;&gt; pod.yaml\n  nodeSelector:\n    disktype: ssd\nEOF\n\nkubectl apply -f pod.yaml\n</code></pre></p> <p>Affinity template (for complex rules): <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: KEY\n          operator: In\n          values:\n          - VALUE1\n          - VALUE2\n</code></pre></p> <p>Vim snippet (create in <code>~/.vimrc</code>): <pre><code>iabbrev affin affinity:&lt;CR&gt;  nodeAffinity:&lt;CR&gt;    requiredDuringSchedulingIgnoredDuringExecution:&lt;CR&gt;      nodeSelectorTerms:&lt;CR&gt;      - matchExpressions:&lt;CR&gt;        - key: &lt;CR&gt;          operator: In&lt;CR&gt;          values:&lt;CR&gt;          -\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#troubleshooting-pending-pods","title":"Troubleshooting Pending Pods","text":"<p>Fast Diagnosis: <pre><code># Check pod status\nkubectl get pods\n\n# Describe pod for events\nkubectl describe pod &lt;pod-name&gt;\n\n# Look for scheduling failures\nkubectl describe pod &lt;pod-name&gt; | grep -A10 Events\n\n# Common issues:\n# - \"0/3 nodes are available: 3 node(s) didn't match node selector.\"\n# - \"0/3 nodes are available: 3 node(s) had taints that the pod didn't tolerate.\"\n# - \"0/3 nodes are available: 3 Insufficient cpu.\"\n</code></pre></p> <p>Check Node Resources: <pre><code># View allocatable resources\nkubectl describe nodes | grep -A5 \"Allocatable\"\n\n# View node labels\nkubectl get nodes --show-labels\n\n# View node taints\nkubectl describe nodes | grep -i taint\n</code></pre></p> <p>Debugging Workflow: <pre><code>graph TB\n    PENDING[Pod Status: Pending] --&gt; DESCRIBE[kubectl describe pod]\n    DESCRIBE --&gt; EVENTS{Check Events&lt;br/&gt;Section}\n\n    EVENTS --&gt; SELECTOR{Node selector&lt;br/&gt;mismatch?}\n    SELECTOR --&gt; |Yes| FIX1[Fix labels or selector]\n\n    EVENTS --&gt; TAINT{Taint not&lt;br/&gt;tolerated?}\n    TAINT --&gt; |Yes| FIX2[Add toleration]\n\n    EVENTS --&gt; RESOURCES{Insufficient&lt;br/&gt;resources?}\n    RESOURCES --&gt; |Yes| FIX3[Reduce requests or add nodes]\n\n    EVENTS --&gt; AFFINITY{Affinity not&lt;br/&gt;satisfied?}\n    AFFINITY --&gt; |Yes| FIX4[Fix affinity rules or labels]\n\n    FIX1 --&gt; VERIFY[Verify pod schedules]\n    FIX2 --&gt; VERIFY\n    FIX3 --&gt; VERIFY\n    FIX4 --&gt; VERIFY</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exam-time-savers","title":"Exam Time-Savers","text":"<p>1. Use <code>--dry-run=client -o yaml</code> aggressively: <pre><code>kubectl run mypod --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n# Edit pod.yaml to add scheduling constraints\nkubectl apply -f pod.yaml\n</code></pre></p> <p>2. Label nodes before creating pods: <pre><code># Label first\nkubectl label nodes node01 tier=frontend\n\n# Then create pod with nodeSelector\n</code></pre></p> <p>3. Use vim search/replace for bulk edits: <pre><code># In vim, replace all instances\n:%s/old-value/new-value/g\n</code></pre></p> <p>4. Keep common YAML snippets handy: Create <code>~/snippets/</code> directory with common patterns: - <code>nodeSelector.yaml</code> - <code>toleration.yaml</code> - <code>affinity.yaml</code> - <code>anti-affinity.yaml</code></p> <p>5. Practice <code>kubectl set</code> commands (where applicable): <pre><code># Set image\nkubectl set image deployment/web nginx=nginx:1.24\n\n# Unfortunately, no `kubectl set` for affinity/taints (use edit)\n</code></pre></p> <p>6. Use <code>kubectl edit</code> for quick fixes: <pre><code># Edit pod in-place (creates new pod for immutable fields)\nkubectl edit pod mypod\n\n# Edit deployment (updates in-place)\nkubectl edit deployment web\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-1-dedicated-node-pool","title":"Exercise 1: Dedicated Node Pool","text":"<p>Scenario: Create a dedicated node pool for database workloads.</p> <p>Tasks: 1. Taint node <code>node01</code> with <code>workload=database:NoSchedule</code> 2. Label node <code>node01</code> with <code>tier=database</code> and <code>disktype=ssd</code> 3. Create a pod named <code>postgres</code> with image <code>postgres:15</code> that:    - Tolerates the <code>workload=database</code> taint    - Uses nodeSelector for <code>tier=database</code>    - Requests 2Gi memory and 1 CPU</p> <p>Solution: <pre><code># 1. Taint and label node\nkubectl taint nodes node01 workload=database:NoSchedule\nkubectl label nodes node01 tier=database disktype=ssd\n\n# 2. Create pod\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\nspec:\n  nodeSelector:\n    tier: database\n  tolerations:\n  - key: \"workload\"\n    operator: \"Equal\"\n    value: \"database\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: postgres\n    image: postgres:15\n    resources:\n      requests:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n    env:\n    - name: POSTGRES_PASSWORD\n      value: example\nEOF\n\n# 3. Verify scheduling\nkubectl get pods -o wide\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-2-high-availability-web-deployment","title":"Exercise 2: High-Availability Web Deployment","text":"<p>Scenario: Deploy a web application with high availability across nodes and zones.</p> <p>Tasks: 1. Create a deployment <code>web-ha</code> with 6 replicas using <code>nginx:1.24</code> 2. Configure pod anti-affinity to prevent multiple replicas on the same node 3. Add preferred affinity to spread across zones (weight: 100)</p> <p>Solution: <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ha\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - web\n            topologyKey: kubernetes.io/hostname\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n              - key: topology.kubernetes.io/zone\n                operator: Exists\n      containers:\n      - name: nginx\n        image: nginx:1.24\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\nEOF\n\n# Verify spreading\nkubectl get pods -o wide -l app=web\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-3-cache-co-location","title":"Exercise 3: Cache Co-Location","text":"<p>Scenario: Deploy an application tier that must run on the same nodes as cache pods.</p> <p>Tasks: 1. Create a deployment <code>redis-cache</code> with 3 replicas using <code>redis:7</code> 2. Label the pods with <code>component=cache</code> 3. Create a deployment <code>web-app</code> with 6 replicas using <code>nginx:1.24</code> 4. Configure <code>web-app</code> with required pod affinity to <code>component=cache</code> pods</p> <p>Solution: <pre><code># 1. Deploy cache\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-cache\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: cache\n  template:\n    metadata:\n      labels:\n        component: cache\n    spec:\n      containers:\n      - name: redis\n        image: redis:7\nEOF\n\n# 2. Wait for cache pods to schedule\nkubectl wait --for=condition=ready pod -l component=cache --timeout=60s\n\n# 3. Deploy web-app with affinity\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - cache\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nginx\n        image: nginx:1.24\nEOF\n\n# 4. Verify co-location\nkubectl get pods -o wide -l app=web\nkubectl get pods -o wide -l component=cache\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-4-spot-instance-workloads","title":"Exercise 4: Spot Instance Workloads","text":"<p>Scenario: Configure a node pool for spot instances and deploy tolerant workloads.</p> <p>Tasks: 1. Taint node <code>node02</code> with <code>instance-type=spot:PreferNoSchedule</code> 2. Create a deployment <code>batch-processor</code> with 5 replicas using <code>busybox</code> that:    - Tolerates the spot instance taint    - Sleeps for 3600 seconds (simulating batch work)</p> <p>Solution: <pre><code># 1. Taint spot node\nkubectl taint nodes node02 instance-type=spot:PreferNoSchedule\n\n# 2. Deploy batch workload\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: batch-processor\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: batch\n  template:\n    metadata:\n      labels:\n        app: batch\n    spec:\n      tolerations:\n      - key: \"instance-type\"\n        operator: \"Equal\"\n        value: \"spot\"\n        effect: \"PreferNoSchedule\"\n      containers:\n      - name: processor\n        image: busybox\n        command: [\"sleep\", \"3600\"]\nEOF\n\n# 3. Observe scheduling\nkubectl get pods -o wide -l app=batch\n# Some pods may schedule on node02 (spot), others on regular nodes\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-5-node-maintenance-workflow","title":"Exercise 5: Node Maintenance Workflow","text":"<p>Scenario: Prepare a node for maintenance without disrupting workloads.</p> <p>Tasks: 1. Apply <code>NoSchedule</code> taint to <code>node03</code> with key <code>maintenance=pending</code> 2. Verify no new pods schedule on <code>node03</code> 3. After workloads migrate, change taint to <code>NoExecute</code> 4. Verify existing pods are evicted 5. After maintenance, remove taint</p> <p>Solution: <pre><code># 1. Apply NoSchedule taint\nkubectl taint nodes node03 maintenance=pending:NoSchedule\n\n# 2. Create test pod (should not schedule on node03)\nkubectl run test-pod --image=nginx\nkubectl get pod test-pod -o wide\n# Should show node != node03\n\n# 3. Check existing pods on node03\nkubectl get pods -A -o wide --field-selector spec.nodeName=node03\n\n# 4. Upgrade to NoExecute (evict existing pods)\nkubectl taint nodes node03 maintenance=pending:NoExecute --overwrite\n\n# 5. Verify pods evicted\nkubectl get pods -A -o wide --field-selector spec.nodeName=node03\n# Should show no pods (or only those with tolerations)\n\n# 6. Perform maintenance...\n# (simulated)\n\n# 7. Remove taint and return to service\nkubectl taint nodes node03 maintenance-\n\n# 8. Verify node accepts workloads\nkubectl get nodes\nkubectl describe node node03 | grep Taints\n# Should show: Taints: &lt;none&gt;\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#summary","title":"Summary","text":"<p>Advanced scheduling is a critical skill for both the CKA exam and production Kubernetes operations.</p> <p>Key Takeaways:</p> <ol> <li>Scheduler Basics:</li> <li>Filter phase removes unsuitable nodes</li> <li>Score phase ranks remaining nodes</li> <li> <p>Highest score wins (ties broken randomly)</p> </li> <li> <p>Node Selectors:</p> </li> <li>Simple label-based matching</li> <li>Fast to type (ideal for CKA exam speed)</li> <li> <p>Limited to exact equality matching</p> </li> <li> <p>Taints and Tolerations:</p> </li> <li>Taints repel pods from nodes</li> <li>Tolerations allow pods to tolerate taints</li> <li>Three effects: NoSchedule, PreferNoSchedule, NoExecute</li> <li>NoExecute evicts existing pods</li> <li> <p>Critical for dedicated nodes and maintenance</p> </li> <li> <p>Node Affinity:</p> </li> <li>Advanced label-based node selection</li> <li>Supports In, NotIn, Exists, DoesNotExist, Gt, Lt operators</li> <li>Required vs preferred (hard vs soft constraints)</li> <li> <p>Multiple selector terms for OR logic</p> </li> <li> <p>Pod Affinity/Anti-Affinity:</p> </li> <li>Schedule based on other pods' labels</li> <li>Affinity = co-location</li> <li>Anti-affinity = spreading</li> <li>Topology keys define scope (node, zone, region)</li> <li> <p>Performance costs in large clusters</p> </li> <li> <p>Advanced Concepts:</p> </li> <li>Priority classes enable preemption</li> <li>Topology spread constraints for even distribution</li> <li>Resource requests influence scheduling</li> <li> <p>Custom schedulers for specialized logic</p> </li> <li> <p>CKA Exam Skills:</p> </li> <li>Use <code>--dry-run=client -o yaml</code> for YAML generation</li> <li>Memorize affinity/toleration YAML structure</li> <li>Practice <code>kubectl describe</code> for troubleshooting</li> <li>Label nodes before creating pods</li> <li>Keep common snippets handy</li> </ol> <p>Production Best Practices: - Use node selectors for simple cases (fastest, clearest) - Use taints for dedicated node pools and maintenance - Combine required + preferred affinity for optimal placement - Always test anti-affinity with realistic replica counts - Monitor scheduling latency in large clusters - Document scheduling decisions for team knowledge sharing</p> <p>Next Steps: - Practice all five exercises until comfortable - Set up a multi-node cluster (kubeadm or kind) - Experiment with topology spread constraints - Review official Kubernetes scheduling docs - Time yourself on exam-style tasks</p> <p>Mastering these scheduling techniques will not only help you pass the CKA exam but also enable you to build resilient, optimized, and cost-effective Kubernetes deployments in production.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/","title":"Kubernetes Services: Exposing Your Applications","text":"<p>Kubernetes Services are the cornerstone of networking in modern containerized applications. They provide stable, reliable access to dynamic sets of Pods, abstracting away the complexity of Pod IP management and enabling seamless communication between application components. For anyone preparing for the Certified Kubernetes Administrator (CKA) exam, mastering Services is essential\u2014they represent 20% of the exam's Services &amp; Networking domain.</p> <p>In this comprehensive guide, we'll explore how Services work, when to use each type, and the practical kubectl commands you need to succeed in both the exam and production environments.</p>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#why-services-matter","title":"Why Services Matter","text":"<p>Imagine deploying a microservices application where frontend Pods need to communicate with backend Pods. Pods are ephemeral\u2014they get created, destroyed, and rescheduled constantly. Their IP addresses change with each recreation. How do you maintain reliable communication in this dynamic environment?</p> <p>This is precisely the problem Services solve:</p> <ul> <li>Stable Endpoint: Services provide a consistent DNS name and IP address that remains constant regardless of Pod lifecycle changes</li> <li>Load Balancing: Automatically distribute traffic across healthy Pod replicas</li> <li>Service Discovery: Enable applications to find each other using DNS names rather than hard-coded IP addresses</li> <li>External Access: Expose applications to clients outside the cluster in various ways</li> </ul> <p>Services decouple application components, enabling independent scaling, rolling updates, and resilient architectures. They're not just a networking abstraction\u2014they're fundamental to building cloud-native applications.</p>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#service-architecture-overview","title":"Service Architecture Overview","text":"<p>Before diving into specific Service types, let's understand the core architecture:</p> <pre><code>graph TB\n    subgraph \"Service Types\"\n        ClusterIP[ClusterIP&lt;br/&gt;Internal Only]\n        NodePort[NodePort&lt;br/&gt;Node Port Access]\n        LoadBalancer[LoadBalancer&lt;br/&gt;Cloud Integration]\n        ExternalName[ExternalName&lt;br/&gt;DNS CNAME]\n    end\n\n    subgraph \"Components\"\n        DNS[CoreDNS]\n        Proxy[kube-proxy]\n        Endpoints[Endpoints /&lt;br/&gt;EndpointSlices]\n    end\n\n    subgraph \"Traffic Flow\"\n        Client[Client Request]\n        Service[Service IP]\n        Pods[Pod Replicas]\n    end\n\n    Client --&gt; Service\n    Service --&gt; Endpoints\n    Endpoints --&gt; Pods\n    DNS -.-&gt; Service\n    Proxy -.-&gt; Service\n\n    style ClusterIP fill:#e1f5ff\n    style NodePort fill:#fff4e1\n    style LoadBalancer fill:#e8f5e9\n    style ExternalName fill:#fce4ec</code></pre> <p>Key Components:</p> <ul> <li>Service: The abstraction layer with a stable ClusterIP and DNS name</li> <li>Endpoints/EndpointSlices: Dynamic lists of Pod IPs backing the Service (2025: EndpointSlices are the modern, scalable replacement)</li> <li>kube-proxy: Runs on each node, implementing the network rules that route traffic to Pods</li> <li>CoreDNS: Provides DNS-based service discovery within the cluster</li> </ul>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#service-fundamentals","title":"Service Fundamentals","text":"","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#the-service-object","title":"The Service Object","text":"<p>A Service is a Kubernetes resource that groups Pods using label selectors and provides a unified access point:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\nspec:\n  selector:\n    app: backend\n    tier: api\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80        # Service port\n      targetPort: 8080 # Pod port\n</code></pre> <p>Port Mapping Explained:</p> <ul> <li><code>port</code>: The port exposed by the Service (what clients connect to)</li> <li><code>targetPort</code>: The port on the Pod where the application listens</li> <li><code>nodePort</code>: (NodePort type only) The port exposed on each node</li> </ul>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#service-discovery-mechanisms","title":"Service Discovery Mechanisms","text":"<p>Kubernetes provides two primary service discovery methods:</p> <p>1. DNS (Recommended)</p> <p>CoreDNS creates DNS records for each Service:</p> <pre><code># Full DNS name format\n&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local\n\n# Examples\nbackend-service.default.svc.cluster.local\ndatabase.production.svc.cluster.local\n\n# Within same namespace, short form works\ncurl http://backend-service/api\n</code></pre> <p>2. Environment Variables</p> <p>When a Pod starts, Kubernetes injects environment variables for Services that existed at creation time:</p> <pre><code>BACKEND_SERVICE_SERVICE_HOST=10.96.100.200\nBACKEND_SERVICE_SERVICE_PORT=80\n</code></pre> <p>Environment Variable Limitations</p> <p>Environment variables are only populated for Services that existed when the Pod was created. This creates ordering dependencies and is less flexible than DNS. Always prefer DNS-based discovery.</p>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#kube-proxy-modes","title":"kube-proxy Modes","text":"<p>kube-proxy implements Service routing using different backends:</p> Mode Description Use Case iptables Uses iptables rules (default in most clusters) Standard deployments IPVS IP Virtual Server, better performance at scale Large clusters (&gt;1000 Services) nftables Modern replacement for iptables (Kubernetes 1.29+) Future-proof deployments <p>You can check the current mode:</p> <pre><code>kubectl logs -n kube-system kube-proxy-xxxxx | grep \"Using\"\n# Output: Using iptables Proxier\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#clusterip-services-internal-communication","title":"ClusterIP Services: Internal Communication","text":"<p>ClusterIP is the default Service type, providing cluster-internal access only. It's perfect for microservices communication where services don't need external exposure.</p>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#standard-clusterip","title":"Standard ClusterIP","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-api\n  namespace: production\nspec:\n  type: ClusterIP  # Optional, this is the default\n  selector:\n    app: backend\n    version: v2\n  ports:\n    - name: http\n      port: 80\n      targetPort: 8080\n      protocol: TCP\n    - name: metrics\n      port: 9090\n      targetPort: 9090\n      protocol: TCP\n  sessionAffinity: ClientIP  # Optional: sticky sessions\n</code></pre> <p>Creating with kubectl (imperative):</p> <pre><code># Expose a deployment\nkubectl expose deployment backend \\\n  --name=backend-api \\\n  --port=80 \\\n  --target-port=8080 \\\n  --type=ClusterIP\n\n# Verify the Service\nkubectl get svc backend-api\n# NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\n# backend-api   ClusterIP   10.96.100.200   &lt;none&gt;        80/TCP    5s\n\n# Test DNS resolution\nkubectl run test-pod --rm -it --image=busybox -- sh\n/ # nslookup backend-api\n# Server:    10.96.0.10\n# Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n#\n# Name:      backend-api\n# Address 1: 10.96.100.200 backend-api.production.svc.cluster.local\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#traffic-flow-diagram","title":"Traffic Flow Diagram","text":"<pre><code>sequenceDiagram\n    participant Client as Client Pod\n    participant DNS as CoreDNS\n    participant Service as ClusterIP Service&lt;br/&gt;10.96.100.200\n    participant KP as kube-proxy\n    participant Pod1 as Backend Pod 1&lt;br/&gt;10.244.1.10\n    participant Pod2 as Backend Pod 2&lt;br/&gt;10.244.2.15\n\n    Client-&gt;&gt;DNS: Resolve backend-api\n    DNS--&gt;&gt;Client: 10.96.100.200\n    Client-&gt;&gt;Service: HTTP GET :80\n    Service-&gt;&gt;KP: iptables/IPVS rules\n    KP-&gt;&gt;Pod1: Forward to :8080\n    Pod1--&gt;&gt;Client: Response\n\n    Note over Client,Pod2: Next request load balanced\n    Client-&gt;&gt;Service: HTTP GET :80\n    KP-&gt;&gt;Pod2: Forward to :8080\n    Pod2--&gt;&gt;Client: Response</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#headless-services-clusterip-none","title":"Headless Services (clusterIP: None)","text":"<p>Headless Services don't get a ClusterIP. Instead, DNS returns the individual Pod IPs directly. This is crucial for stateful applications that need direct Pod addressing.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: database-headless\nspec:\n  clusterIP: None  # This makes it headless\n  selector:\n    app: postgres\n    role: primary\n  ports:\n    - port: 5432\n      targetPort: 5432\n</code></pre> <p>DNS behavior with headless Services:</p> <pre><code># Normal Service (ClusterIP)\nnslookup backend-api\n# Returns: 10.96.100.200 (single Service IP)\n\n# Headless Service\nnslookup database-headless\n# Returns: 10.244.1.20, 10.244.1.21, 10.244.1.22 (all Pod IPs)\n</code></pre> <p>Use cases for headless Services:</p> <ul> <li>StatefulSets with stable network identities</li> <li>Database replication (primary/replica identification)</li> <li>Custom load balancing logic in the application</li> <li>Service mesh data planes (Istio, Linkerd)</li> </ul> <pre><code># Create headless Service imperatively\nkubectl create service clusterip db-headless \\\n  --clusterip=\"None\" \\\n  --tcp=5432:5432\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#session-affinity","title":"Session Affinity","text":"<p>By default, Services load balance each request randomly. Session affinity ensures requests from the same client go to the same Pod:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: sticky-service\nspec:\n  selector:\n    app: stateful-app\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 10800  # 3 hours\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#nodeport-loadbalancer-external-access","title":"NodePort &amp; LoadBalancer: External Access","text":"<p>While ClusterIP Services work for internal communication, NodePort and LoadBalancer Services enable external access.</p>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#nodeport-services","title":"NodePort Services","text":"<p>NodePort opens a static port on every node in the cluster (range: 30000-32767 by default). External traffic sent to <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code> gets routed to the Service.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-frontend\nspec:\n  type: NodePort\n  selector:\n    app: nginx\n    tier: frontend\n  ports:\n    - port: 80         # ClusterIP port\n      targetPort: 8080  # Pod port\n      nodePort: 30080   # External node port (optional)\n      protocol: TCP\n</code></pre> <p>Traffic path:</p> <pre><code>graph LR\n    Internet[Internet Client] --&gt;|http://NodeIP:30080| Node1[Node 1&lt;br/&gt;:30080]\n    Internet --&gt;|http://NodeIP:30080| Node2[Node 2&lt;br/&gt;:30080]\n    Node1 --&gt; Service[Service&lt;br/&gt;10.96.100.50:80]\n    Node2 --&gt; Service\n    Service --&gt; Pod1[Pod 1&lt;br/&gt;10.244.1.30:8080]\n    Service --&gt; Pod2[Pod 2&lt;br/&gt;10.244.2.40:8080]\n    Service --&gt; Pod3[Pod 3&lt;br/&gt;10.244.3.50:8080]\n\n    style Internet fill:#ffcdd2\n    style Service fill:#c5e1a5</code></pre> <p>Creating and accessing NodePort Services:</p> <pre><code># Create NodePort Service\nkubectl expose deployment nginx \\\n  --type=NodePort \\\n  --name=web-frontend \\\n  --port=80 \\\n  --target-port=8080\n\n# Get the assigned NodePort\nkubectl get svc web-frontend\n# NAME           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n# web-frontend   NodePort   10.96.100.50    &lt;none&gt;        80:31234/TCP   10s\n\n# Access from outside cluster\n# Get node IPs\nkubectl get nodes -o wide\n# NAME     STATUS   ROLES    INTERNAL-IP    EXTERNAL-IP\n# node-1   Ready    &lt;none&gt;   192.168.1.10   203.0.113.10\n# node-2   Ready    &lt;none&gt;   192.168.1.11   203.0.113.11\n\n# Access via any node\ncurl http://203.0.113.10:31234\ncurl http://203.0.113.11:31234  # Both work!\n</code></pre> <p>NodePort considerations:</p> <ul> <li>\u2705 Simple, works on any cluster (no cloud provider needed)</li> <li>\u26a0\ufe0f Port range limited (30000-32767)</li> <li>\u26a0\ufe0f Non-standard ports (requires clients to know the NodePort)</li> <li>\u26a0\ufe0f Security: Exposes ports on all nodes</li> </ul>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#loadbalancer-services","title":"LoadBalancer Services","text":"<p>LoadBalancer type integrates with cloud provider load balancers (AWS ELB/ALB, GCP Load Balancer, Azure Load Balancer) to provision external access with standard ports.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: production-app\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"  # AWS-specific\nspec:\n  type: LoadBalancer\n  selector:\n    app: production\n    tier: frontend\n  ports:\n    - name: http\n      port: 80\n      targetPort: 8080\n    - name: https\n      port: 443\n      targetPort: 8443\n  loadBalancerSourceRanges:  # Optional: restrict source IPs\n    - 203.0.113.0/24\n  externalTrafficPolicy: Local  # Preserve source IP\n</code></pre> <p>LoadBalancer architecture:</p> <pre><code>graph TB\n    subgraph Cloud Provider\n        LB[Cloud Load Balancer&lt;br/&gt;203.0.113.100]\n    end\n\n    subgraph Kubernetes Cluster\n        subgraph Node1[Node 1]\n            NP1[NodePort&lt;br/&gt;:30080]\n            Pod1[Pod 1&lt;br/&gt;:8080]\n        end\n        subgraph Node2[Node 2]\n            NP2[NodePort&lt;br/&gt;:30080]\n            Pod2[Pod 2&lt;br/&gt;:8080]\n        end\n        Service[LoadBalancer Service&lt;br/&gt;ClusterIP: 10.96.100.75]\n    end\n\n    Internet[Internet Clients] --&gt;|http://203.0.113.100:80| LB\n    LB --&gt;|Health checks| NP1\n    LB --&gt;|Health checks| NP2\n    LB --&gt; NP1\n    LB --&gt; NP2\n    NP1 --&gt; Service\n    NP2 --&gt; Service\n    Service --&gt; Pod1\n    Service --&gt; Pod2\n\n    style Internet fill:#ffcdd2\n    style LB fill:#fff9c4\n    style Service fill:#c5e1a5</code></pre> <p>Creating and managing LoadBalancer Services:</p> <pre><code># Create LoadBalancer Service\nkubectl expose deployment app \\\n  --type=LoadBalancer \\\n  --name=production-app \\\n  --port=80 \\\n  --target-port=8080\n\n# Watch for external IP provisioning (takes 1-3 minutes)\nkubectl get svc production-app -w\n# NAME             TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE\n# production-app   LoadBalancer   10.96.100.75    &lt;pending&gt;       80:31456/TCP   5s\n# production-app   LoadBalancer   10.96.100.75    203.0.113.100   80:31456/TCP   45s\n\n# Access via load balancer\ncurl http://203.0.113.100\n</code></pre> <p>Cloud provider annotations (examples):</p> <pre><code># AWS\nannotations:\n  service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"  # Network Load Balancer\n  service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:...\"\n  service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n\n# GCP\nannotations:\n  cloud.google.com/load-balancer-type: \"Internal\"\n  networking.gke.io/load-balancer-type: \"Internal\"\n\n# Azure\nannotations:\n  service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#metallb-loadbalancer-for-bare-metal","title":"MetalLB: LoadBalancer for Bare Metal","text":"<p>On-premises or bare-metal clusters don't have cloud load balancers. MetalLB provides LoadBalancer functionality using Layer 2 (ARP) or BGP.</p> <p>Install MetalLB:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml\n\n# Create IP address pool\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: production-pool\n  namespace: metallb-system\nspec:\n  addresses:\n    - 192.168.1.240-192.168.1.250  # Available IPs on your network\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: production-l2\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n    - production-pool\nEOF\n</code></pre> <p>Now LoadBalancer Services automatically get IPs from this pool.</p>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#service-configuration-deep-dive","title":"Service Configuration Deep Dive","text":"","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#selector-and-port-mapping","title":"Selector and Port Mapping","text":"<p>Services use label selectors to find their target Pods:</p> <pre><code># Service selector\nspec:\n  selector:\n    app: backend\n    version: v2\n\n# Matching Pods must have these labels\nmetadata:\n  labels:\n    app: backend\n    version: v2\n    team: platform\n</code></pre> <p>Multi-port Services:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: multi-port-service\nspec:\n  selector:\n    app: fullstack\n  ports:\n    - name: http        # Named ports are required for multi-port\n      port: 80\n      targetPort: web   # Can reference Pod's named port\n    - name: grpc\n      port: 9090\n      targetPort: 9090\n    - name: metrics\n      port: 9091\n      targetPort: prometheus\n</code></pre> <p>Pod with named ports:</p> <pre><code>containers:\n  - name: app\n    ports:\n      - name: web\n        containerPort: 8080\n      - name: prometheus\n        containerPort: 9091\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#externaltrafficpolicy-source-ip-preservation","title":"externalTrafficPolicy: Source IP Preservation","text":"<p>By default, NodePort and LoadBalancer Services perform source NAT, replacing the client IP with a node IP. This breaks IP-based access control and logging.</p> <pre><code>spec:\n  type: LoadBalancer\n  externalTrafficPolicy: Cluster  # Default: performs SNAT\n</code></pre> <p>Problems with <code>Cluster</code> policy:</p> <ul> <li>Client IP is lost (replaced with node IP)</li> <li>Application logs show internal node IPs</li> <li>IP-based rate limiting doesn't work</li> <li>Geographic routing breaks</li> </ul> <p>Solution: <code>externalTrafficPolicy: Local</code>:</p> <pre><code>spec:\n  type: LoadBalancer\n  externalTrafficPolicy: Local  # Preserve source IP\n</code></pre> <pre><code>graph TB\n    subgraph \"externalTrafficPolicy: Cluster (Default)\"\n        Client1[Client&lt;br/&gt;203.0.113.50]\n        Node1[Node 1]\n        Node2[Node 2]\n        Pod1[Pod on Node 2]\n\n        Client1 --&gt;|1. Request| Node1\n        Node1 --&gt;|2. SNAT to Node1 IP| Node2\n        Node2 --&gt;|3. Forward| Pod1\n        Pod1 -.-&gt;|Sees Node1 IP| Pod1\n    end\n\n    subgraph \"externalTrafficPolicy: Local\"\n        Client2[Client&lt;br/&gt;203.0.113.50]\n        Node3[Node 1&lt;br/&gt;Has Pod]\n        Pod2[Local Pod]\n\n        Client2 --&gt;|1. Request| Node3\n        Node3 --&gt;|2. Direct forward&lt;br/&gt;No SNAT| Pod2\n        Pod2 -.-&gt;|Sees Real Client IP| Pod2\n    end\n\n    style Pod1 fill:#ffcdd2\n    style Pod2 fill:#c5e1a5</code></pre> <p>Trade-offs:</p> Policy Source IP Load Distribution Health Checks <code>Cluster</code> Lost (SNAT) Even across all Pods All nodes healthy <code>Local</code> Preserved Only Pods on receiving node Only nodes with Pods <p>Testing source IP preservation:</p> <pre><code># Deploy test application that logs client IP\nkubectl run source-ip-app \\\n  --image=registry.k8s.io/echoserver:1.4 \\\n  --port=8080\n\nkubectl expose pod source-ip-app \\\n  --type=LoadBalancer \\\n  --name=source-test \\\n  --port=80 \\\n  --target-port=8080\n\n# Get external IP\nEXTERNAL_IP=$(kubectl get svc source-test -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n\n# Test with Cluster policy (default)\ncurl http://$EXTERNAL_IP/\n# client_address=10.244.1.1  &lt;-- Node IP, not your real IP\n\n# Change to Local policy\nkubectl patch svc source-test -p '{\"spec\":{\"externalTrafficPolicy\":\"Local\"}}'\n\n# Test again\ncurl http://$EXTERNAL_IP/\n# client_address=203.0.113.50  &lt;-- Your real IP!\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#endpoints-and-endpointslices-2025-update","title":"Endpoints and EndpointSlices (2025 Update)","text":"<p>When you create a Service, Kubernetes creates a corresponding Endpoints or EndpointSlices object that lists the IPs of matching Pods.</p> <p>Legacy Endpoints API (deprecated in v1.33):</p> <pre><code>kubectl get endpoints backend-service\n# NAME              ENDPOINTS                          AGE\n# backend-service   10.244.1.10:8080,10.244.2.15:8080  5m\n</code></pre> <p>Modern EndpointSlices (recommended):</p> <pre><code>kubectl get endpointslices\n# NAME                        ADDRESSTYPE   PORTS   ENDPOINTS                          AGE\n# backend-service-abc123      IPv4          8080    10.244.1.10,10.244.2.15           5m\n\nkubectl describe endpointslice backend-service-abc123\n# Name:         backend-service-abc123\n# Namespace:    default\n# AddressType:  IPv4\n# Ports:\n#   Name     Port  Protocol\n#   ----     ----  --------\n#   http     8080  TCP\n# Endpoints:\n#   - Addresses:  10.244.1.10\n#     Conditions:\n#       Ready:    true\n#     Hostname:   backend-6b8f9-abc12\n#     TargetRef:  Pod/backend-6b8f9-abc12\n</code></pre> <p>Why EndpointSlices are better (Kubernetes 1.33+):</p> Feature Endpoints EndpointSlices Scalability Monolithic (all IPs in one object) Distributed (100 endpoints per slice) Watch efficiency Full update on any change Incremental updates Dual-stack Limited support Native IPv4/IPv6 Topology No awareness Topology hints for routing Performance Degrades at scale Up to 50% reduction in watch payload <p>Creating Services without selectors (manual Endpoints):</p> <pre><code># Service without selector\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-database\nspec:\n  ports:\n    - port: 5432\n      targetPort: 5432\n---\n# Manually created Endpoints\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: external-database  # Must match Service name\nsubsets:\n  - addresses:\n      - ip: 192.168.1.100  # External database IP\n    ports:\n      - port: 5432\n</code></pre> <p>This allows Services to point to external resources outside the cluster while maintaining the same DNS-based discovery.</p>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#cka-exam-skills-imperative-service-management","title":"CKA Exam Skills: Imperative Service Management","text":"<p>The CKA exam emphasizes speed and efficiency. Master these imperative commands to save time.</p>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#quick-service-creation","title":"Quick Service Creation","text":"<pre><code># ClusterIP (default)\nkubectl expose deployment nginx --port=80 --target-port=8080\n\n# NodePort\nkubectl expose deployment nginx \\\n  --type=NodePort \\\n  --port=80 \\\n  --target-port=8080 \\\n  --name=nginx-nodeport\n\n# LoadBalancer\nkubectl expose deployment nginx \\\n  --type=LoadBalancer \\\n  --port=80 \\\n  --target-port=8080 \\\n  --name=nginx-lb\n\n# Generate YAML without creating (for exam prep)\nkubectl expose deployment nginx --port=80 --dry-run=client -o yaml &gt; service.yaml\n\n# Create Service directly (not from deployment)\nkubectl create service clusterip my-svc --tcp=5678:8080\nkubectl create service nodeport my-svc --tcp=5678:8080 --node-port=30080\nkubectl create service loadbalancer my-svc --tcp=5678:8080\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#dns-troubleshooting","title":"DNS Troubleshooting","text":"<p>CoreDNS verification:</p> <pre><code># Check CoreDNS pods are running\nkubectl get pods -n kube-system -l k8s-app=kube-dns\n# NAME                       READY   STATUS    RESTARTS   AGE\n# coredns-5d78c9869d-abc12   1/1     Running   0          10d\n# coredns-5d78c9869d-def34   1/1     Running   0          10d\n\n# Test DNS resolution from a pod\nkubectl run dns-test --rm -it --image=busybox:1.28 -- sh\n/ # nslookup kubernetes\n# Server:    10.96.0.10\n# Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n#\n# Name:      kubernetes\n# Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local\n\n/ # nslookup backend-service\n# Returns IP if Service exists, error if not\n\n# Test external DNS\n/ # nslookup google.com\n# Should resolve if CoreDNS is configured for external lookups\n</code></pre> <p>Common DNS issues:</p> <pre><code># Issue: DNS not resolving\n# Check: CoreDNS ConfigMap\nkubectl get configmap -n kube-system coredns -o yaml\n\n# Issue: Pods can't resolve Service names\n# Check: Pod DNS configuration\nkubectl get pod test-pod -o jsonpath='{.spec.dnsPolicy}'\n# Should return: ClusterFirst\n\nkubectl get pod test-pod -o jsonpath='{.spec.dnsConfig}'\n# Should show nameservers: [10.96.0.10]\n\n# Issue: Service exists but DNS doesn't resolve\n# Check: Service has endpoints\nkubectl get endpoints backend-service\n# If empty, check Pod selectors match\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#port-forward-for-debugging","title":"Port-Forward for Debugging","text":"<p>Port-forward creates a tunnel from your local machine to a Service or Pod, bypassing Service networking. Essential for debugging.</p> <pre><code># Forward to a Service\nkubectl port-forward service/backend-service 8080:80\n# Access on localhost:8080\n\n# Forward to a specific Pod\nkubectl port-forward pod/backend-abc123 8080:8080\n\n# Forward to a Deployment (auto-selects pod)\nkubectl port-forward deployment/backend 8080:8080\n\n# Forward multiple ports\nkubectl port-forward service/backend 8080:80 9090:9090\n\n# Bind to all interfaces (not just localhost)\nkubectl port-forward --address 0.0.0.0 service/backend 8080:80\n</code></pre> <p>Use cases:</p> <ul> <li>Test Services before exposing externally</li> <li>Debug Pod connectivity issues</li> <li>Access cluster services from local development</li> <li>Bypass authentication for troubleshooting</li> </ul>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#service-debugging-workflow","title":"Service Debugging Workflow","text":"<pre><code>graph TB\n    Start[Service Not Working] --&gt; DNS{DNS Resolving?}\n\n    DNS --&gt;|No| CheckDNS[Check CoreDNS pods&lt;br/&gt;kubectl get pods -n kube-system]\n    CheckDNS --&gt; CheckConfig[Check DNS config&lt;br/&gt;kubectl describe svc kube-dns -n kube-system]\n\n    DNS --&gt;|Yes| Endpoints{Endpoints exist?}\n\n    Endpoints --&gt;|No| CheckSelector[Verify selectors&lt;br/&gt;kubectl get svc -o yaml]\n    CheckSelector --&gt; CheckLabels[Check Pod labels&lt;br/&gt;kubectl get pods --show-labels]\n    CheckLabels --&gt; Fix1[Fix selector mismatch]\n\n    Endpoints --&gt;|Yes| Connectivity{Can reach Pods?}\n\n    Connectivity --&gt;|No| CheckNetwork[Check network policy&lt;br/&gt;kubectl get netpol]\n    CheckNetwork --&gt; CheckKubeProxy[Check kube-proxy logs&lt;br/&gt;kubectl logs -n kube-system kube-proxy-xxx]\n\n    Connectivity --&gt;|Yes| CheckApp[Check application logs&lt;br/&gt;kubectl logs pod-name]\n    CheckApp --&gt; Fix2[Fix application issue]\n\n    Fix1 --&gt; Verify[Verify fix&lt;br/&gt;kubectl port-forward&lt;br/&gt;curl localhost:8080]\n    Fix2 --&gt; Verify\n\n    style Start fill:#ffcdd2\n    style Verify fill:#c5e1a5</code></pre> <p>Step-by-step debugging:</p> <pre><code># 1. Verify Service exists and has correct configuration\nkubectl get svc backend-service\nkubectl describe svc backend-service\n\n# 2. Check if Endpoints are populated\nkubectl get endpoints backend-service\n# If empty, selectors don't match any Pods\n\n# 3. Verify Pod labels match Service selector\nkubectl get pods --show-labels\nkubectl get svc backend-service -o jsonpath='{.spec.selector}'\n\n# 4. Test connectivity directly to Pod\nPOD_IP=$(kubectl get pod backend-abc123 -o jsonpath='{.status.podIP}')\nkubectl run test --rm -it --image=busybox -- wget -O- http://$POD_IP:8080\n\n# 5. Test Service connectivity\nkubectl run test --rm -it --image=busybox -- wget -O- http://backend-service:80\n\n# 6. Check kube-proxy is creating iptables rules\nkubectl get pods -n kube-system -l k8s-app=kube-proxy\nkubectl logs -n kube-system kube-proxy-xxxxx | grep backend-service\n\n# 7. Verify DNS resolution\nkubectl run test --rm -it --image=busybox -- nslookup backend-service\n\n# 8. Use port-forward as final test\nkubectl port-forward svc/backend-service 8080:80\ncurl localhost:8080\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#service-discovery-best-practices","title":"Service Discovery Best Practices","text":"<pre><code># \u2705 GOOD: Use DNS names, works across namespaces\napiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\nspec:\n  containers:\n    - name: app\n      image: frontend:v1\n      env:\n        - name: BACKEND_URL\n          value: \"http://backend-service.production.svc.cluster.local\"\n\n# \u274c BAD: Hard-coded IPs break when Services change\n        - name: BACKEND_URL\n          value: \"http://10.96.100.200\"\n\n# \u274c BAD: Environment variables have ordering issues\n        - name: BACKEND_HOST\n          value: \"$(BACKEND_SERVICE_SERVICE_HOST)\"\n</code></pre> <p>Cross-namespace Service access:</p> <pre><code># Same namespace\ncurl http://backend-service:80\n\n# Different namespace\ncurl http://backend-service.production:80\n\n# Fully qualified (always works)\ncurl http://backend-service.production.svc.cluster.local:80\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#security-considerations","title":"Security Considerations","text":"<p>Limit Service exposure:</p> <pre><code># Restrict LoadBalancer source IPs\nspec:\n  type: LoadBalancer\n  loadBalancerSourceRanges:\n    - 203.0.113.0/24    # Only allow specific CIDR\n    - 198.51.100.0/24\n\n# Use NetworkPolicies to control Pod-to-Service traffic\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-netpol\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              app: frontend\n      ports:\n        - protocol: TCP\n          port: 8080\n</code></pre> <p>Disable NodePort allocation for LoadBalancer (Kubernetes 1.24+):</p> <pre><code># Don't allocate NodePorts unnecessarily\nspec:\n  type: LoadBalancer\n  allocateLoadBalancerNodePorts: false  # Save port space\n</code></pre>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#quick-reference-service-types-comparison","title":"Quick Reference: Service Types Comparison","text":"Feature ClusterIP NodePort LoadBalancer ExternalName Cluster-internal \u2705 \u2705 \u2705 \u2705 External access \u274c \u2705 (via NodePort) \u2705 (via LB) N/A Requires cloud \u274c \u274c \u2705 \u274c Standard ports \u2705 \u274c (30000-32767) \u2705 N/A Cost Free Free Paid (cloud) Free Use case Microservices Development/Testing Production External DNS alias","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#cka-exam-tips","title":"CKA Exam Tips","text":"<ol> <li>Speed matters: Use imperative commands for creation, YAML only when needed</li> <li>DNS is your friend: Always test with <code>nslookup</code> and <code>kubectl run test --rm -it --image=busybox</code></li> <li>Port-forward for debugging: Fastest way to verify Pod/Service connectivity</li> <li>Know the differences: Be ready to explain ClusterIP vs NodePort vs LoadBalancer</li> <li>EndpointSlices: Understand the migration from Endpoints (exam may cover both)</li> <li>externalTrafficPolicy: Know when and why to use <code>Local</code> vs <code>Cluster</code></li> </ol>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/kubernetes-services-exposing-applications/#summary","title":"Summary","text":"<p>Kubernetes Services are the glue that holds distributed applications together. They provide:</p> <ul> <li>Abstraction: Stable endpoints for dynamic Pod sets</li> <li>Discovery: DNS-based service location</li> <li>Load balancing: Traffic distribution across replicas</li> <li>Flexibility: Multiple exposure methods (ClusterIP, NodePort, LoadBalancer)</li> </ul> <p>For the CKA exam, focus on:</p> <ul> <li>Imperative Service creation with <code>kubectl expose</code> and <code>kubectl create service</code></li> <li>DNS troubleshooting with <code>nslookup</code> and <code>kubectl run</code></li> <li>Understanding EndpointSlices and their advantages</li> <li>Debugging workflow: DNS \u2192 Endpoints \u2192 Connectivity \u2192 Application</li> </ul> <p>Master these concepts, and you'll not only pass the exam but also build resilient, scalable Kubernetes applications in production. Services aren't just networking primitives\u2014they're the foundation of cloud-native architecture.</p> <p>Now get out there and expose your applications with confidence! \ud83d\ude80</p>","tags":["kubernetes","k8s","cka-prep","services","networking","service-discovery","endpoints"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/","title":"Kubernetes Monitoring, Metrics, and Resource Management","text":"<p>Master Metrics Server, resource requests/limits, Horizontal Pod Autoscaler (HPA), Vertical Pod Autoscaler (VPA), and kubectl top - essential skills for resource management and monitoring in Kubernetes.</p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#overview","title":"Overview","text":"<p>Resource management and monitoring are critical for cluster efficiency - proper configuration prevents resource starvation, enables autoscaling, and provides observability into cluster health. This guide covers the Kubernetes metrics pipeline and resource management best practices for the CKA exam.</p> <pre><code>graph TD\n    A[Metrics Pipeline] --&gt; B[kubelet/cAdvisor]\n    B --&gt; C[Metrics Server]\n    C --&gt; D[metrics.k8s.io API]\n\n    D --&gt; E[kubectl top]\n    D --&gt; F[HPA]\n    D --&gt; G[VPA]\n    D --&gt; H[Scheduler]\n\n    I[Resource Requests/Limits] --&gt; H\n    I --&gt; F\n    I --&gt; G\n\n    style A fill:#ff9999\n    style C fill:#99ccff\n    style D fill:#ffcc99\n    style I fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#metrics-server","title":"Metrics Server","text":"<p>Metrics Server is a cluster-wide aggregator of resource usage data - required for <code>kubectl top</code> and Horizontal Pod Autoscaler.</p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#architecture","title":"Architecture","text":"<pre><code>sequenceDiagram\n    participant KB as Kubelet (cAdvisor)\n    participant MS as Metrics Server\n    participant API as metrics.k8s.io API\n    participant KT as kubectl top\n    participant HPA as HorizontalPodAutoscaler\n\n    KB-&gt;&gt;KB: Collect container metrics\n    MS-&gt;&gt;KB: Query metrics (port 10250)\n    KB-&gt;&gt;MS: Return CPU/memory data\n    MS-&gt;&gt;MS: Aggregate metrics\n    MS-&gt;&gt;API: Expose via API\n\n    KT-&gt;&gt;API: GET /apis/metrics.k8s.io/v1beta1/nodes\n    API-&gt;&gt;KT: Return node metrics\n\n    HPA-&gt;&gt;API: GET pod metrics\n    API-&gt;&gt;HPA: Return metrics\n    HPA-&gt;&gt;HPA: Calculate desired replicas</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#install-metrics-server","title":"Install Metrics Server","text":"<p>Prerequisites: Cluster with kubelet configured to expose metrics endpoint (default in most clusters).</p> <pre><code># Install Metrics Server using kubectl\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n# Verify Metrics Server deployment\nkubectl get deployment metrics-server -n kube-system\n\n# Check Metrics Server pods\nkubectl get pods -n kube-system -l k8s-app=metrics-server\n\n# Verify Metrics API is available\nkubectl get apiservices | grep metrics\n# Output should show:\n# v1beta1.metrics.k8s.io         kube-system/metrics-server   True\n</code></pre> <p>Troubleshoot Metrics Server: <pre><code># Check Metrics Server logs\nkubectl logs -n kube-system deployment/metrics-server\n\n# Common issues:\n# 1. TLS certificate verification (dev/test clusters)\n# Edit deployment to add: --kubelet-insecure-tls\n\nkubectl edit deployment metrics-server -n kube-system\n# Add to container args:\n#   - --kubelet-insecure-tls\n\n# 2. Kubelet hostname resolution\n# Add: --kubelet-preferred-address-types=InternalIP\n\n# Restart Metrics Server\nkubectl rollout restart deployment metrics-server -n kube-system\n\n# Verify metrics collection (wait ~1 minute for data)\nkubectl top nodes\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#kubectl-top-command","title":"kubectl top Command","text":"<p>kubectl top displays resource usage for nodes and pods - requires Metrics Server.</p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#node-metrics","title":"Node Metrics","text":"<pre><code># Display resource usage for all nodes\nkubectl top nodes\n\n# Output example:\n# NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\n# master     250m         12%    1024Mi          30%\n# worker-1   450m         22%    2048Mi          50%\n# worker-2   380m         19%    1536Mi          38%\n\n# Show node metrics sorted by CPU\nkubectl top nodes --sort-by=cpu\n\n# Show node metrics sorted by memory\nkubectl top nodes --sort-by=memory\n\n# Show specific node\nkubectl top node worker-1\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#pod-metrics","title":"Pod Metrics","text":"<pre><code># Display resource usage for all pods\nkubectl top pods\n\n# Display for specific namespace\nkubectl top pods -n kube-system\n\n# Display all namespaces\nkubectl top pods -A\n\n# Show pod metrics sorted by CPU\nkubectl top pods --sort-by=cpu\n\n# Show pod metrics sorted by memory\nkubectl top pods --sort-by=memory\n\n# Show containers in pods\nkubectl top pods --containers\n\n# Filter by label\nkubectl top pods -l app=nginx\n\n# Output example:\n# NAME                    CPU(cores)   MEMORY(bytes)\n# nginx-7c6d8f5b8d-abc    10m          32Mi\n# nginx-7c6d8f5b8d-xyz    12m          34Mi\n# nginx-7c6d8f5b8d-def    11m          33Mi\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#resource-requests-and-limits","title":"Resource Requests and Limits","text":"<p>Resource requests and limits define container resource requirements and constraints - critical for scheduling and QoS.</p> <pre><code>graph TD\n    A[Pod Submitted] --&gt; B{Has Resource Requests?}\n    B --&gt;|Yes| C[Scheduler Finds Node]\n    B --&gt;|No| D[Assumes 0 CPU, 0 Memory]\n\n    C --&gt; E{Node Has Capacity?}\n    E --&gt;|Yes| F[Schedule Pod]\n    E --&gt;|No| G[Pending - Insufficient Resources]\n\n    F --&gt; H{Pod Exceeds Limits?}\n    H --&gt;|Memory Limit| I[OOMKilled]\n    H --&gt;|CPU Limit| J[Throttled]\n    H --&gt;|Within Limits| K[Running Normally]\n\n    style B fill:#ff9999\n    style E fill:#ffcc99\n    style H fill:#99ccff\n    style I fill:#ff6666</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#resource-types","title":"Resource Types","text":"Resource Description Units cpu CPU time Cores (1000m = 1 core), millicores (m) memory RAM Bytes (Ki, Mi, Gi, Ti) ephemeral-storage Temporary disk space Bytes (Ki, Mi, Gi) hugepages- Huge pages Bytes","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#requests-vs-limits","title":"Requests vs Limits","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: resource-demo\nspec:\n  containers:\n  - name: app\n    image: nginx:1.27\n    resources:\n      requests:\n        memory: \"128Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"256Mi\"\n        cpu: \"500m\"\n</code></pre> <p>Requests: - Minimum guaranteed resources - Used by scheduler for pod placement - Node must have available capacity &gt;= requests - If exceeded: No immediate action (except CPU gets throttled at limit)</p> <p>Limits: - Maximum allowed resources - Enforced by kubelet - Memory limit exceeded \u2192 OOMKilled - CPU limit exceeded \u2192 Throttled (not killed)</p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#qos-classes","title":"QoS Classes","text":"<p>Kubernetes assigns Quality of Service (QoS) class based on requests/limits:</p> <pre><code>graph TD\n    A[Pod QoS Class] --&gt; B{Requests == Limits?}\n    B --&gt;|Yes, All Containers| C[Guaranteed]\n    B --&gt;|No| D{Has Requests or Limits?}\n\n    D --&gt;|Has Some| E[Burstable]\n    D --&gt;|None| F[BestEffort]\n\n    C --&gt; G[Lowest Eviction Priority]\n    E --&gt; H[Medium Eviction Priority]\n    F --&gt; I[Highest Eviction Priority]\n\n    style C fill:#99ff99\n    style E fill:#ffcc99\n    style F fill:#ff9999</code></pre> <p>QoS Classes:</p> Class Criteria Behavior Guaranteed All containers have requests == limits for CPU and memory Lowest eviction priority, most stable Burstable At least one container has request or limit Medium eviction priority BestEffort No requests or limits set Highest eviction priority, first to be evicted <p>Check Pod QoS Class: <pre><code>kubectl get pod &lt;pod-name&gt; -o jsonpath='{.status.qosClass}'\n\n# Example outputs:\n# Guaranteed\n# Burstable\n# BestEffort\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#horizontal-pod-autoscaler-hpa","title":"Horizontal Pod Autoscaler (HPA)","text":"<p>HPA automatically scales the number of pods based on observed metrics (CPU, memory, custom metrics).</p> <pre><code>graph LR\n    A[HPA Controller] --&gt; B[Query Metrics API]\n    B --&gt; C[Calculate Desired Replicas]\n    C --&gt; D{Current != Desired?}\n    D --&gt;|Yes| E[Scale Deployment]\n    D --&gt;|No| F[No Action]\n\n    E --&gt; G[Deployment Controller]\n    G --&gt; H[ReplicaSet]\n    H --&gt; I[Create/Delete Pods]\n\n    style A fill:#99ccff\n    style C fill:#ffcc99\n    style E fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#hpa-calculation-formula","title":"HPA Calculation Formula","text":"<pre><code>desiredReplicas = ceil[currentReplicas * (currentMetricValue / targetMetricValue)]\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#create-hpa-with-kubectl-autoscale","title":"Create HPA with kubectl autoscale","text":"<pre><code># Create HPA for deployment (CPU-based)\nkubectl autoscale deployment nginx --cpu-percent=50 --min=2 --max=10\n\n# Verify HPA\nkubectl get hpa\n\n# Output:\n# NAME    REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\n# nginx   Deployment/nginx   20%/50%   2         10        2          1m\n\n# Describe HPA\nkubectl describe hpa nginx\n\n# Watch HPA in real-time\nkubectl get hpa nginx --watch\n\n# Delete HPA\nkubectl delete hpa nginx\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#hpa-yaml-manifest-resource-metrics","title":"HPA YAML Manifest (Resource Metrics)","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nginx-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50  # Target 50% CPU\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 70  # Target 70% memory\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#hpa-with-containerresource-metrics-v127","title":"HPA with ContainerResource Metrics (v1.27+)","text":"<p>Scale based on specific container (useful for multi-container pods):</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: ContainerResource\n    containerResource:\n      name: cpu\n      container: application  # Target specific container\n      target:\n        type: Utilization\n        averageUtilization: 60\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#hpa-with-custom-metrics","title":"HPA with Custom Metrics","text":"<p>Object metrics (from Ingress, Service, etc.): <pre><code>metrics:\n- type: Object\n  object:\n    metric:\n      name: requests-per-second\n    describedObject:\n      apiVersion: networking.k8s.io/v1\n      kind: Ingress\n      name: main-route\n    target:\n      type: Value\n      value: \"2k\"\n</code></pre></p> <p>Pod metrics (custom application metrics): <pre><code>metrics:\n- type: Pods\n  pods:\n    metric:\n      name: packets-per-second\n    target:\n      type: AverageValue\n      averageValue: \"1k\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#hpa-scaling-behavior-v123","title":"HPA Scaling Behavior (v1.23+)","text":"<p>Control scale-up/scale-down rates: <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nginx-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300  # 5 min cooldown before scaling down\n      policies:\n      - type: Percent\n        value: 10  # Max 10% pods removed per period\n        periodSeconds: 60\n      - type: Pods\n        value: 5  # Max 5 pods removed per period\n        periodSeconds: 60\n      selectPolicy: Min  # Use minimum of policies\n    scaleUp:\n      stabilizationWindowSeconds: 0  # Immediate scale-up\n      policies:\n      - type: Percent\n        value: 100  # Max 100% pods added per period (double)\n        periodSeconds: 15\n      - type: Pods\n        value: 4  # Max 4 pods added per period\n        periodSeconds: 15\n      selectPolicy: Max  # Use maximum of policies\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#vertical-pod-autoscaler-vpa","title":"Vertical Pod Autoscaler (VPA)","text":"<p>VPA automatically adjusts resource requests/limits based on historical usage - complementary to HPA.</p> <pre><code>graph TD\n    A[VPA Controller] --&gt; B[Monitor Pod Resource Usage]\n    B --&gt; C[Calculate Recommendations]\n    C --&gt; D[VPA UpdatePolicy]\n\n    D --&gt; E{Update Mode?}\n    E --&gt;|Off| F[Recommend Only]\n    E --&gt;|Initial| G[Set on Creation Only]\n    E --&gt;|Recreate| H[Evict Pod, Update]\n    E --&gt;|Auto| I[Evict Pod, Update]\n\n    H --&gt; J[Pod Recreated with New Requests]\n    I --&gt; J\n\n    style A fill:#99ccff\n    style C fill:#ffcc99\n    style J fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#vpa-modes","title":"VPA Modes","text":"Mode Behavior Use Case Off Only calculate recommendations, don't apply Observation, manual review Initial Apply recommendations only on pod creation One-time sizing Recreate Apply recommendations by evicting pods Compatible with stateless apps Auto Apply recommendations without eviction (future) Seamless updates","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#vpa-installation","title":"VPA Installation","text":"<pre><code># Clone VPA repository\ngit clone https://github.com/kubernetes/autoscaler.git\ncd autoscaler/vertical-pod-autoscaler\n\n# Install VPA (CRDs, controllers)\n./hack/vpa-up.sh\n\n# Verify VPA components\nkubectl get pods -n kube-system | grep vpa\n# vpa-admission-controller\n# vpa-recommender\n# vpa-updater\n\n# Verify VPA CRDs\nkubectl get crd | grep verticalpodautoscaler\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#vpa-example","title":"VPA Example","text":"<pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: nginx-vpa\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx\n  updatePolicy:\n    updateMode: \"Auto\"  # Options: Off, Initial, Recreate, Auto\n  resourcePolicy:\n    containerPolicies:\n    - containerName: \"*\"\n      minAllowed:\n        cpu: \"100m\"\n        memory: \"100Mi\"\n      maxAllowed:\n        cpu: \"2\"\n        memory: \"2Gi\"\n      controlledResources: [\"cpu\", \"memory\"]\n</code></pre> <p>Check VPA Recommendations: <pre><code># Get VPA status\nkubectl get vpa nginx-vpa\n\n# Describe VPA (shows recommendations)\nkubectl describe vpa nginx-vpa\n\n# Output includes:\n# Recommendation:\n#   Container Recommendations:\n#     Container Name:  nginx\n#     Lower Bound:     (minimum safe resources)\n#       Cpu:     50m\n#       Memory:  128Mi\n#     Target:          (recommended resources)\n#       Cpu:     100m\n#       Memory:  256Mi\n#     Upper Bound:     (maximum expected)\n#       Cpu:     200m\n#       Memory:  512Mi\n</code></pre></p> <p>Note: Do NOT use VPA and HPA together on the same metric (CPU/memory) - they will conflict. Use VPA for requests/limits sizing, HPA for replica scaling.</p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#resource-quotas","title":"Resource Quotas","text":"<p>ResourceQuotas limit aggregate resource consumption per namespace.</p> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"20\"           # Total CPU requests\n    requests.memory: \"40Gi\"      # Total memory requests\n    limits.cpu: \"40\"             # Total CPU limits\n    limits.memory: \"80Gi\"        # Total memory limits\n    pods: \"50\"                   # Max number of pods\n    persistentvolumeclaims: \"10\" # Max PVCs\n</code></pre> <pre><code># Create ResourceQuota\nkubectl apply -f resource-quota.yaml -n production\n\n# View quotas\nkubectl get resourcequota -n production\nkubectl describe resourcequota compute-quota -n production\n\n# Output shows:\n# Name:            compute-quota\n# Namespace:       production\n# Resource         Used   Hard\n# --------         ----   ----\n# limits.cpu       5      40\n# limits.memory    10Gi   80Gi\n# pods             12     50\n# requests.cpu     2.5    20\n# requests.memory  5Gi    40Gi\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#limitrange","title":"LimitRange","text":"<p>LimitRange sets default requests/limits and enforces min/max per container/pod.</p> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-cpu-limit-range\n  namespace: default\nspec:\n  limits:\n  - max:\n      cpu: \"2\"\n      memory: \"2Gi\"\n    min:\n      cpu: \"100m\"\n      memory: \"128Mi\"\n    default:\n      cpu: \"500m\"\n      memory: \"512Mi\"\n    defaultRequest:\n      cpu: \"250m\"\n      memory: \"256Mi\"\n    type: Container\n  - max:\n      cpu: \"4\"\n      memory: \"4Gi\"\n    type: Pod\n</code></pre> <pre><code># Create LimitRange\nkubectl apply -f limit-range.yaml\n\n# View LimitRange\nkubectl get limitrange\nkubectl describe limitrange mem-cpu-limit-range\n\n# New pods without requests/limits will get defaults automatically\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#cka-exam-practice-exercises","title":"CKA Exam Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#exercise-1-install-metrics-server-and-use-kubectl-top","title":"Exercise 1: Install Metrics Server and Use kubectl top","text":"<p>Scenario: Install Metrics Server in your cluster and use <code>kubectl top</code> to monitor resource usage.</p> Solution <pre><code># 1. Install Metrics Server\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n# 2. Verify deployment\nkubectl get deployment metrics-server -n kube-system\n# NAME             READY   UP-TO-DATE   AVAILABLE   AGE\n# metrics-server   1/1     1            1           30s\n\n# 3. Check Metrics Server pods\nkubectl get pods -n kube-system -l k8s-app=metrics-server\n# NAME                             READY   STATUS    RESTARTS   AGE\n# metrics-server-5f9b8c7d4d-xyz    1/1     Running   0          40s\n\n# 4. Verify Metrics API is registered\nkubectl get apiservices | grep metrics\n# v1beta1.metrics.k8s.io   kube-system/metrics-server   True   1m\n\n# 5. Wait for metrics collection (~1 minute)\nsleep 60\n\n# 6. Check node metrics\nkubectl top nodes\n# NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\n# master     250m         12%    1024Mi          30%\n# worker-1   450m         22%    2048Mi          50%\n\n# 7. Check pod metrics\nkubectl top pods -A\n# NAMESPACE     NAME                              CPU(cores)   MEMORY(bytes)\n# kube-system   coredns-5d78c9869d-abc           3m           12Mi\n# kube-system   kube-proxy-xyz                   1m           16Mi\n# default       nginx-deployment-7c6d8f5b8d-123  10m          32Mi\n\n# 8. Check specific namespace\nkubectl top pods -n kube-system\n\n# 9. Show pod metrics with containers\nkubectl top pods --containers -n kube-system\n\n# 10. Sort by CPU\nkubectl top pods --sort-by=cpu -A\n</code></pre>  **Key Takeaways**: - Metrics Server is required for `kubectl top` and HPA - Metrics take ~1 minute to start appearing after installation - `kubectl top nodes` shows cluster-level resource usage - `kubectl top pods` shows pod-level usage","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#exercise-2-configure-resource-requests-and-limits","title":"Exercise 2: Configure Resource Requests and Limits","text":"<p>Scenario: Create a deployment with proper resource requests and limits, then check its QoS class.</p> Solution <pre><code># 1. Create deployment with resource configuration\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: resource-demo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: resource-demo\n  template:\n    metadata:\n      labels:\n        app: resource-demo\n    spec:\n      containers:\n      - name: app\n        image: nginx:1.27\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"500m\"\nEOF\n\n# 2. Verify deployment\nkubectl get deployment resource-demo\nkubectl get pods -l app=resource-demo\n\n# 3. Check QoS class of one pod\nPOD=$(kubectl get pods -l app=resource-demo -o jsonpath='{.items[0].metadata.name}')\nkubectl get pod $POD -o jsonpath='{.status.qosClass}'\n# Output: Burstable (because requests != limits)\n\n# 4. Check resource configuration\nkubectl describe pod $POD | grep -A 6 \"Requests:\"\n# Requests:\n#   cpu:     250m\n#   memory:  128Mi\n# Limits:\n#   cpu:     500m\n#   memory:  256Mi\n\n# 5. Check resource usage\nkubectl top pod $POD\n# NAME                            CPU(cores)   MEMORY(bytes)\n# resource-demo-xyz               12m          34Mi\n\n# 6. Create Guaranteed QoS pod (requests == limits)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: guaranteed-pod\nspec:\n  containers:\n  - name: app\n    image: nginx:1.27\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"256Mi\"\n        cpu: \"500m\"\nEOF\n\n# 7. Check QoS of guaranteed pod\nkubectl get pod guaranteed-pod -o jsonpath='{.status.qosClass}'\n# Output: Guaranteed\n\n# 8. Create BestEffort QoS pod (no resources)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: besteffort-pod\nspec:\n  containers:\n  - name: app\n    image: nginx:1.27\n    # No resource requests or limits\nEOF\n\n# 9. Check QoS of besteffort pod\nkubectl get pod besteffort-pod -o jsonpath='{.status.qosClass}'\n# Output: BestEffort\n\n# 10. Cleanup\nkubectl delete deployment resource-demo\nkubectl delete pod guaranteed-pod besteffort-pod\n</code></pre>  **Key Takeaways**: - Guaranteed QoS: requests == limits for all resources - Burstable QoS: has some requests or limits - BestEffort QoS: no requests or limits (evicted first) - Proper resource configuration prevents resource starvation","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#exercise-3-create-and-test-horizontal-pod-autoscaler","title":"Exercise 3: Create and Test Horizontal Pod Autoscaler","text":"<p>Scenario: Create an HPA for a deployment and observe it scale based on CPU load.</p> Solution <pre><code># 1. Create deployment with resource requests (HPA requires requests)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: php-apache\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: php-apache\n  template:\n    metadata:\n      labels:\n        app: php-apache\n    spec:\n      containers:\n      - name: php-apache\n        image: registry.k8s.io/hpa-example\n        resources:\n          requests:\n            cpu: \"200m\"\n          limits:\n            cpu: \"500m\"\nEOF\n\n# 2. Expose deployment as service\nkubectl expose deployment php-apache --port=80\n\n# 3. Create HPA using kubectl autoscale\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\n\n# 4. Check HPA status\nkubectl get hpa\n# NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\n# php-apache   Deployment/php-apache   0%/50%    1         10        1          10s\n\n# 5. Describe HPA\nkubectl describe hpa php-apache\n# Shows detailed scaling information\n\n# 6. Generate load (in separate terminal)\nkubectl run load-generator --image=busybox:1.28 --restart=Never -- /bin/sh -c \"while true; do wget -q -O- http://php-apache; done\"\n\n# 7. Watch HPA scale up (in original terminal)\nkubectl get hpa php-apache --watch\n# NAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE\n# php-apache   Deployment/php-apache   0%/50%     1         10        1          1m\n# php-apache   Deployment/php-apache   250%/50%   1         10        1          1m\n# php-apache   Deployment/php-apache   250%/50%   1         10        5          2m\n# php-apache   Deployment/php-apache   150%/50%   1         10        5          2m\n\n# 8. Check deployment replicas\nkubectl get deployment php-apache\n# NAME         READY   UP-TO-DATE   AVAILABLE   AGE\n# php-apache   5/5     5            5           3m\n\n# 9. Stop load generator\nkubectl delete pod load-generator\n\n# 10. Watch HPA scale down (takes ~5 minutes by default)\nkubectl get hpa php-apache --watch\n# NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\n# php-apache   Deployment/php-apache   0%/50%    1         10        5          5m\n# php-apache   Deployment/php-apache   0%/50%    1         10        1          10m\n\n# 11. View HPA YAML\nkubectl get hpa php-apache -o yaml\n\n# 12. Cleanup\nkubectl delete hpa php-apache\nkubectl delete deployment php-apache\nkubectl delete svc php-apache\n</code></pre>  **Key Takeaways**: - HPA requires resource requests to calculate utilization - CPU utilization = (current CPU usage / requested CPU) * 100 - Scale-up is fast, scale-down has cooldown (default 5 min) - HPA uses formula: desiredReplicas = ceil[currentReplicas * (current/target)]","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#exercise-4-create-resourcequota-and-limitrange","title":"Exercise 4: Create ResourceQuota and LimitRange","text":"<p>Scenario: Create a namespace with ResourceQuota and LimitRange to control resource allocation.</p> Solution <pre><code># 1. Create namespace\nkubectl create namespace resource-controlled\n\n# 2. Create ResourceQuota\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: resource-controlled\nspec:\n  hard:\n    requests.cpu: \"2\"\n    requests.memory: \"4Gi\"\n    limits.cpu: \"4\"\n    limits.memory: \"8Gi\"\n    pods: \"10\"\nEOF\n\n# 3. Verify ResourceQuota\nkubectl get resourcequota -n resource-controlled\nkubectl describe resourcequota compute-quota -n resource-controlled\n# Resource         Used  Hard\n# --------         ----  ----\n# limits.cpu       0     4\n# limits.memory    0     8Gi\n# pods             0     10\n# requests.cpu     0     2\n# requests.memory  0     4Gi\n\n# 4. Create LimitRange (default requests/limits)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-cpu-limit\n  namespace: resource-controlled\nspec:\n  limits:\n  - max:\n      cpu: \"1\"\n      memory: \"1Gi\"\n    min:\n      cpu: \"50m\"\n      memory: \"64Mi\"\n    default:\n      cpu: \"500m\"\n      memory: \"512Mi\"\n    defaultRequest:\n      cpu: \"250m\"\n      memory: \"256Mi\"\n    type: Container\nEOF\n\n# 5. Verify LimitRange\nkubectl get limitrange -n resource-controlled\nkubectl describe limitrange mem-cpu-limit -n resource-controlled\n\n# 6. Create pod WITHOUT specifying resources (should get defaults)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: default-resources\n  namespace: resource-controlled\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.27\nEOF\n\n# 7. Check applied resources\nkubectl describe pod default-resources -n resource-controlled | grep -A 6 \"Requests:\"\n# Requests:\n#   cpu:        250m  (from defaultRequest)\n#   memory:     256Mi (from defaultRequest)\n# Limits:\n#   cpu:        500m  (from default)\n#   memory:     512Mi (from default)\n\n# 8. Check ResourceQuota usage\nkubectl describe resourcequota compute-quota -n resource-controlled\n# Resource         Used    Hard\n# --------         ----    ----\n# limits.cpu       500m    4\n# limits.memory    512Mi   8Gi\n# pods             1       10\n# requests.cpu     250m    2\n# requests.memory  256Mi   4Gi\n\n# 9. Try to exceed quota (create 9 more pods)\nfor i in {1..9}; do\n  kubectl run pod-$i --image=nginx:1.27 -n resource-controlled\ndone\n\n# 10. Check quota after 8 pods\nkubectl describe resourcequota compute-quota -n resource-controlled\n# Resource         Used    Hard\n# --------         ----    ----\n# limits.cpu       4       4     (at limit!)\n# limits.memory    4Gi     8Gi\n# pods             9       10\n# requests.cpu     2       2     (at limit!)\n# requests.memory  2304Mi  4Gi\n\n# 11. Try to create 10th pod (should succeed - under pod limit)\nkubectl run pod-10 --image=nginx:1.27 -n resource-controlled\n\n# 12. Try to create 11th pod (should FAIL - exceeds pod limit)\nkubectl run pod-11 --image=nginx:1.27 -n resource-controlled\n# Error: pods \"pod-11\" is forbidden: exceeded quota: compute-quota\n\n# 13. Cleanup\nkubectl delete namespace resource-controlled\n</code></pre>  **Key Takeaways**: - ResourceQuota limits total namespace resource consumption - LimitRange sets defaults and enforces min/max per container - Pods without resources get defaults from LimitRange - Quota prevents pod creation when limits exceeded","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#exercise-5-debug-hpa-not-scaling","title":"Exercise 5: Debug HPA Not Scaling","text":"<p>Scenario: An HPA is created but not scaling. Troubleshoot and fix the issue.</p> Solution <pre><code># 1. Create deployment WITHOUT resource requests (common mistake)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: broken-hpa-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: broken-hpa-demo\n  template:\n    metadata:\n      labels:\n        app: broken-hpa-demo\n    spec:\n      containers:\n      - name: app\n        image: nginx:1.27\n        # Missing: resource requests!\nEOF\n\n# 2. Create HPA\nkubectl autoscale deployment broken-hpa-demo --cpu-percent=50 --min=1 --max=5\n\n# 3. Check HPA status\nkubectl get hpa broken-hpa-demo\n# NAME              REFERENCE                    TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\n# broken-hpa-demo   Deployment/broken-hpa-demo   &lt;unknown&gt;/50%   1         5         1          30s\n\n# TARGETS shows \"&lt;unknown&gt;\" - HPA cannot calculate!\n\n# 4. Describe HPA to see error\nkubectl describe hpa broken-hpa-demo\n# Events:\n#   Warning  FailedGetResourceMetric  unable to get metrics for resource cpu: no metrics returned from resource metrics API\n\n# 5. Check pod for resource requests\nkubectl get deployment broken-hpa-demo -o yaml | grep -A 10 \"resources:\"\n# No resources section found!\n\n# 6. Fix: Add resource requests\nkubectl patch deployment broken-hpa-demo --type='json' -p='[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/containers/0/resources\",\n    \"value\": {\n      \"requests\": {\n        \"cpu\": \"200m\",\n        \"memory\": \"128Mi\"\n      },\n      \"limits\": {\n        \"cpu\": \"500m\",\n        \"memory\": \"256Mi\"\n      }\n    }\n  }\n]'\n\n# 7. Verify patch applied\nkubectl get deployment broken-hpa-demo -o yaml | grep -A 6 \"resources:\"\n# resources:\n#   requests:\n#     cpu: 200m\n#     memory: 128Mi\n#   limits:\n#     cpu: 500m\n#     memory: 256Mi\n\n# 8. Wait for metrics (new pod needs to collect metrics)\nsleep 60\n\n# 9. Check HPA again\nkubectl get hpa broken-hpa-demo\n# NAME              REFERENCE                    TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\n# broken-hpa-demo   Deployment/broken-hpa-demo   0%/50%    1         5         1          5m\n\n# Now shows actual CPU utilization!\n\n# 10. Describe HPA (should show metrics now)\nkubectl describe hpa broken-hpa-demo\n# Metrics:\n#   resource cpu on pods (as a percentage of request):  0% (0) / 50%\n# Conditions:\n#   Type            Status\n#   ----            ------\n#   AbleToScale     True\n#   ScalingActive   True\n\n# 11. Cleanup\nkubectl delete hpa broken-hpa-demo\nkubectl delete deployment broken-hpa-demo\n</code></pre>  **Key Takeaways**: - HPA **requires** resource requests to calculate utilization - `` targets indicate missing requests or metrics - Wait ~1 minute after fixing for metrics to appear - Common HPA issues: no requests, Metrics Server not installed, no load","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#quick-reference-commands","title":"Quick Reference Commands","text":"","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#metrics-server_1","title":"Metrics Server","text":"<pre><code>kubectl top nodes                           # Node resource usage\nkubectl top pods -A                         # All pods resource usage\nkubectl top pods --containers               # Show container breakdown\nkubectl top pods --sort-by=cpu              # Sort by CPU\nkubectl top pods --sort-by=memory           # Sort by memory\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#hpa-management","title":"HPA Management","text":"<pre><code>kubectl autoscale deployment &lt;name&gt; --cpu-percent=&lt;pct&gt; --min=&lt;n&gt; --max=&lt;m&gt;  # Create HPA\nkubectl get hpa                             # List HPAs\nkubectl describe hpa &lt;name&gt;                 # Detailed HPA info\nkubectl get hpa &lt;name&gt; --watch              # Watch HPA scaling\nkubectl delete hpa &lt;name&gt;                   # Delete HPA\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#resource-inspection","title":"Resource Inspection","text":"<pre><code>kubectl describe pod &lt;pod&gt; | grep -A 6 \"Requests:\"  # Check requests/limits\nkubectl get pod &lt;pod&gt; -o jsonpath='{.status.qosClass}'  # Check QoS\nkubectl get resourcequota -n &lt;ns&gt;           # List quotas\nkubectl describe limitrange -n &lt;ns&gt;         # Show LimitRange\n</code></pre>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#related-resources","title":"Related Resources","text":"<ul> <li>Troubleshooting Clusters, Nodes, and Components - Cluster-level debugging</li> <li>Application Troubleshooting and Log Analysis - Pod-level debugging</li> <li>Resource Quotas and LimitRanges - Namespace resource management</li> </ul>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#summary","title":"Summary","text":"<p>Monitoring and resource management ensure cluster efficiency and application stability - proper configuration of requests/limits, ResourceQuotas, and autoscaling enables reliable, self-healing Kubernetes deployments. Master these concepts for the CKA exam's Troubleshooting (30%) and Workloads (15%) domains.</p> <p>Key Takeaways: - \u2705 Metrics Server \u2192 Required for <code>kubectl top</code> and HPA, collects CPU/memory metrics - \u2705 Resource Requests \u2192 Used for scheduling, minimum guaranteed resources - \u2705 Resource Limits \u2192 Maximum allowed resources, memory limit \u2192 OOMKilled, CPU limit \u2192 throttled - \u2705 QoS Classes \u2192 Guaranteed (requests==limits) &gt; Burstable (some resources) &gt; BestEffort (none) - \u2705 HPA \u2192 Scales replicas based on metrics, requires resource requests, formula: ceil[current * (actual/target)] - \u2705 VPA \u2192 Adjusts requests/limits based on usage, don't use with HPA on same metric - \u2705 ResourceQuota \u2192 Limits total namespace consumption - \u2705 LimitRange \u2192 Sets defaults and enforces min/max per container</p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/monitoring-metrics-resource-management/#series-completion","title":"Series Completion","text":"<p>\ud83c\udf89 Congratulations! You've completed all 22 posts in the Kubernetes CKA Mastery series. You now have comprehensive knowledge covering:</p> <ul> <li>Phase 1: Foundations (Architecture, Setup, kubectl, YAML, Namespaces)</li> <li>Phase 2: Workloads &amp; Scheduling (Pods, Deployments, Scheduling)</li> <li>Phase 3: Services &amp; Networking (Services, Ingress, Network Policies, DNS)</li> <li>Phase 4: Storage (PV/PVC, ConfigMaps/Secrets)</li> <li>Phase 5: Security (RBAC, Security Contexts, CRDs)</li> <li>Phase 6: Advanced Configuration (Helm, Kustomize)</li> <li>Phase 7: Troubleshooting &amp; Monitoring (Cluster/App Troubleshooting, Monitoring)</li> </ul> <p>Next Steps: 1. Practice all exercises in this series 2. Set up a lab environment and recreate scenarios 3. Review CKA exam domains and map to posts 4. Take practice exams 5. Schedule your CKA certification exam</p> <p>Good luck with your CKA certification journey! \ud83d\ude80</p> <p>Back to: Kubernetes CKA Mastery Index</p>","tags":["kubernetes","k8s","cka-prep","monitoring","metrics","autoscaling"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/","title":"Kubernetes Namespaces and Resource Quotas","text":"<p>Master namespace isolation and resource management for CKA exam success. Learn how to partition clusters, enforce resource limits, and prevent resource exhaustion in multi-tenant environments.</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#overview","title":"Overview","text":"<p>Namespaces provide virtual cluster partitioning within a physical Kubernetes cluster, enabling multi-tenancy, resource isolation, and access control. Resource quotas and limit ranges ensure fair resource distribution and prevent resource starvation.</p> <p>CKA Exam Domain: Workloads &amp; Scheduling (15%), Services &amp; Networking (20%)</p> <p>Key Insight: CKA exam scenarios frequently test namespace-aware operations and resource constraint troubleshooting. Understanding namespace scope and quota enforcement is critical for multi-tenant cluster management.</p> <p>What You'll Learn: - Namespace fundamentals and scope boundaries - Resource quota design and enforcement - Limit ranges for default resource constraints - Multi-tenant isolation strategies - Troubleshooting resource quota issues - CKA exam patterns and time-saving workflows</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-fundamentals","title":"Namespace Fundamentals","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#what-are-namespaces","title":"What Are Namespaces?","text":"<p>Definition: Namespaces are logical partitions within a Kubernetes cluster that provide scope for resource names and enable resource isolation.</p> <p>Core Concepts: - Resource names must be unique within a namespace, not across cluster - Most Kubernetes resources are namespace-scoped (pods, services, deployments) - Some resources are cluster-scoped (nodes, persistent volumes, namespaces) - Namespaces enable RBAC policies, network policies, and resource quotas</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-architecture","title":"Namespace Architecture","text":"<pre><code>graph TB\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"default Namespace\"\n            POD1[app-pod-1]\n            SVC1[app-service]\n            DEP1[app-deployment]\n        end\n\n        subgraph \"kube-system Namespace\"\n            POD2[coredns-xxx]\n            SVC2[kube-dns]\n            DEP2[coredns]\n        end\n\n        subgraph \"production Namespace\"\n            POD3[web-pod-1]\n            SVC3[web-service]\n            DEP3[web-deployment]\n            QUOTA1[ResourceQuota]\n            LIMIT1[LimitRange]\n        end\n\n        subgraph \"development Namespace\"\n            POD4[test-pod-1]\n            SVC4[test-service]\n            DEP4[test-deployment]\n            QUOTA2[ResourceQuota]\n            LIMIT2[LimitRange]\n        end\n\n        NODE1[Node: worker-1]\n        NODE2[Node: worker-2]\n        PV[(PersistentVolume)]\n    end\n\n    POD1 -.-&gt; NODE1\n    POD2 -.-&gt; NODE2\n    POD3 -.-&gt; NODE1\n    POD4 -.-&gt; NODE2\n\n    QUOTA1 -.-&gt;|enforces limits| POD3\n    QUOTA2 -.-&gt;|enforces limits| POD4\n\n    PV -.-&gt;|cluster-scoped| POD3\n    PV -.-&gt;|cluster-scoped| POD4\n\n    style default fill:#e1f5ff\n    style kube-system fill:#ffe5e5\n    style production fill:#e8f5e8\n    style development fill:#fff4e1\n    style NODE1 fill:#f5e1ff\n    style PV fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#default-namespaces","title":"Default Namespaces","text":"<p>Kubernetes creates several namespaces automatically:</p> Namespace Purpose Default Resources default Default namespace for resources without explicit namespace User workloads kube-system Kubernetes system components API server, scheduler, controller manager, DNS kube-public Publicly readable resources Cluster information kube-node-lease Node heartbeat objects for performance Node lease objects","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-operations","title":"Namespace Operations","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#creating-namespaces","title":"Creating Namespaces","text":"<p>Imperative Method (Fast for CKA): <pre><code># Create namespace\nkubectl create namespace production\nkubectl create ns development  # Short form\n\n# Create with labels\nkubectl create namespace staging --dry-run=client -o yaml | \\\n  kubectl label -f - --local environment=staging -o yaml | \\\n  kubectl apply -f -\n</code></pre></p> <p>Declarative Method: <pre><code># namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    environment: production\n    team: backend\n</code></pre></p> <pre><code>kubectl apply -f namespace.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#listing-and-inspecting-namespaces","title":"Listing and Inspecting Namespaces","text":"<pre><code># List all namespaces\nkubectl get namespaces\nkubectl get ns\n\n# Show labels\nkubectl get ns --show-labels\n\n# Describe namespace (shows quota and limits)\nkubectl describe namespace production\n\n# Get namespace details in YAML\nkubectl get ns production -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#working-with-namespaced-resources","title":"Working with Namespaced Resources","text":"<pre><code># Create resources in specific namespace\nkubectl run nginx --image=nginx -n production\nkubectl create deployment webapp --image=nginx --replicas=3 -n development\n\n# Get resources from specific namespace\nkubectl get pods -n production\nkubectl get all -n development\n\n# Get resources from all namespaces\nkubectl get pods -A\nkubectl get pods --all-namespaces\n\n# Set default namespace for current context\nkubectl config set-context --current --namespace=production\n\n# Verify current namespace\nkubectl config view --minify | grep namespace\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#deleting-namespaces","title":"Deleting Namespaces","text":"<p>\u26a0\ufe0f WARNING: Deleting a namespace deletes ALL resources within it.</p> <pre><code># Delete namespace (deletes all resources)\nkubectl delete namespace development\n\n# Delete with confirmation\nkubectl delete ns development --force --grace-period=0\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resource-quotas","title":"Resource Quotas","text":"<p>Resource quotas provide constraints that limit aggregate resource consumption per namespace, preventing resource exhaustion and ensuring fair allocation.</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resourcequota-types","title":"ResourceQuota Types","text":"<p>Compute Resource Quotas: - <code>requests.cpu</code> - Sum of CPU requests - <code>requests.memory</code> - Sum of memory requests - <code>limits.cpu</code> - Sum of CPU limits - <code>limits.memory</code> - Sum of memory limits - <code>requests.storage</code> - Sum of storage requests</p> <p>Object Count Quotas: - <code>count/pods</code> - Maximum pod count - <code>count/services</code> - Maximum service count - <code>count/configmaps</code> - Maximum ConfigMap count - <code>count/secrets</code> - Maximum secret count - <code>count/persistentvolumeclaims</code> - Maximum PVC count - <code>count/deployments.apps</code> - Maximum deployment count - <code>count/replicasets.apps</code> - Maximum ReplicaSet count</p> <p>Storage Class Quotas: - <code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/requests.storage</code> - <code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/persistentvolumeclaims</code></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#creating-resource-quotas","title":"Creating Resource Quotas","text":"<p>Basic Compute Quota: <pre><code># quota-compute.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"10\"           # Max 10 CPU cores requested\n    requests.memory: 20Gi        # Max 20Gi memory requested\n    limits.cpu: \"20\"             # Max 20 CPU cores limit\n    limits.memory: 40Gi          # Max 40Gi memory limit\n</code></pre></p> <pre><code>kubectl apply -f quota-compute.yaml\n</code></pre> <p>Object Count Quota: <pre><code># quota-objects.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: object-quota\n  namespace: production\nspec:\n  hard:\n    count/pods: \"100\"\n    count/services: \"50\"\n    count/configmaps: \"20\"\n    count/secrets: \"30\"\n    count/persistentvolumeclaims: \"10\"\n    count/deployments.apps: \"30\"\n    count/replicasets.apps: \"50\"\n</code></pre></p> <p>Combined Quota (CKA Exam Pattern): <pre><code># quota-comprehensive.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: production-quota\n  namespace: production\nspec:\n  hard:\n    # Compute resources\n    requests.cpu: \"50\"\n    requests.memory: 100Gi\n    limits.cpu: \"100\"\n    limits.memory: 200Gi\n\n    # Storage\n    requests.storage: 500Gi\n    persistentvolumeclaims: \"20\"\n\n    # Object counts\n    count/pods: \"200\"\n    count/services: \"50\"\n    count/configmaps: \"50\"\n    count/secrets: \"50\"\n\n    # Workloads\n    count/deployments.apps: \"50\"\n    count/statefulsets.apps: \"10\"\n    count/jobs.batch: \"20\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#checking-quota-usage","title":"Checking Quota Usage","text":"<pre><code># View quota details\nkubectl get resourcequota -n production\nkubectl describe resourcequota production-quota -n production\n\n# Output shows:\n# Name:                   production-quota\n# Namespace:              production\n# Resource                Used    Hard\n# --------                ----    ----\n# requests.cpu            5       50\n# requests.memory         10Gi    100Gi\n# limits.cpu              10      100\n# limits.memory           20Gi    200Gi\n# count/pods              15      200\n\n# Get quota in YAML format\nkubectl get quota production-quota -n production -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resourcequota-enforcement-flow","title":"ResourceQuota Enforcement Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant API as API Server\n    participant Admission as Admission Controller\n    participant Quota as ResourceQuota\n    participant Scheduler\n    participant Kubelet\n\n    User-&gt;&gt;API: kubectl apply -f pod.yaml\n    API-&gt;&gt;Admission: Validate request\n    Admission-&gt;&gt;Quota: Check quota limits\n\n    alt Quota Available\n        Quota--&gt;&gt;Admission: \u2705 Within limits\n        Admission--&gt;&gt;API: Approve\n        API-&gt;&gt;Scheduler: Schedule pod\n        Scheduler-&gt;&gt;Kubelet: Assign to node\n        Kubelet--&gt;&gt;User: Pod created\n        Quota-&gt;&gt;Quota: Update used resources\n    else Quota Exceeded\n        Quota--&gt;&gt;Admission: \u274c Quota exceeded\n        Admission--&gt;&gt;API: Reject\n        API--&gt;&gt;User: Error: exceeded quota\n        Note over User,Quota: Pod creation fails\n    end</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limit-ranges","title":"Limit Ranges","text":"<p>LimitRange objects set default resource requests/limits and enforce min/max constraints for containers and pods within a namespace.</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limitrange-components","title":"LimitRange Components","text":"<p>Container-Level Constraints: - <code>defaultRequest</code> - Default resource requests if not specified - <code>default</code> - Default resource limits if not specified - <code>min</code> - Minimum allowed resource values - <code>max</code> - Maximum allowed resource values - <code>maxLimitRequestRatio</code> - Max ratio of limit to request</p> <p>Pod-Level Constraints: - Total resource consumption across all containers</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#creating-limit-ranges","title":"Creating Limit Ranges","text":"<p>Container Defaults and Constraints: <pre><code># limitrange-container.yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: container-limits\n  namespace: production\nspec:\n  limits:\n  - type: Container\n    default:                      # Default limits\n      cpu: 500m\n      memory: 512Mi\n    defaultRequest:               # Default requests\n      cpu: 100m\n      memory: 128Mi\n    min:                          # Minimum allowed\n      cpu: 50m\n      memory: 64Mi\n    max:                          # Maximum allowed\n      cpu: 2\n      memory: 2Gi\n    maxLimitRequestRatio:         # Max limit/request ratio\n      cpu: 4\n      memory: 4\n</code></pre></p> <p>Pod-Level Constraints: <pre><code># limitrange-pod.yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: pod-limits\n  namespace: production\nspec:\n  limits:\n  - type: Pod\n    max:\n      cpu: \"4\"\n      memory: 8Gi\n    min:\n      cpu: 100m\n      memory: 128Mi\n</code></pre></p> <p>PersistentVolumeClaim Constraints: <pre><code># limitrange-pvc.yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: pvc-limits\n  namespace: production\nspec:\n  limits:\n  - type: PersistentVolumeClaim\n    max:\n      storage: 100Gi\n    min:\n      storage: 1Gi\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#checking-limit-ranges","title":"Checking Limit Ranges","text":"<pre><code># View limit ranges\nkubectl get limitrange -n production\nkubectl describe limitrange container-limits -n production\n\n# Output shows:\n# Name:       container-limits\n# Namespace:  production\n# Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio\n# ----        --------  ---   ---   ---------------  -------------  -----------------------\n# Container   cpu       50m   2     100m             500m           4\n# Container   memory    64Mi  2Gi   128Mi            512Mi          4\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limitrange-application-flow","title":"LimitRange Application Flow","text":"<pre><code>flowchart TD\n    Start([Pod Creation]) --&gt; HasLimits{Has resource&lt;br/&gt;limits/requests?}\n\n    HasLimits --&gt;|Yes| ValidateRange[Validate against LimitRange]\n    HasLimits --&gt;|No| ApplyDefaults[Apply LimitRange defaults]\n\n    ApplyDefaults --&gt; ValidateRange\n\n    ValidateRange --&gt; InRange{Within&lt;br/&gt;min/max?}\n\n    InRange --&gt;|Yes| RatioCheck{Limit/Request&lt;br/&gt;ratio OK?}\n    InRange --&gt;|No| Reject1[\u274c Reject: Out of range]\n\n    RatioCheck --&gt;|Yes| CheckQuota[Check ResourceQuota]\n    RatioCheck --&gt;|No| Reject2[\u274c Reject: Ratio exceeded]\n\n    CheckQuota --&gt; QuotaOK{Quota&lt;br/&gt;available?}\n\n    QuotaOK --&gt;|Yes| Accept[\u2705 Accept pod]\n    QuotaOK --&gt;|No| Reject3[\u274c Reject: Quota exceeded]\n\n    Accept --&gt; Schedule[Schedule pod]\n\n    style Accept fill:#99ff99\n    style Reject1 fill:#ff9999\n    style Reject2 fill:#ff9999\n    style Reject3 fill:#ff9999</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#multi-tenant-resource-management","title":"Multi-Tenant Resource Management","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#design-patterns","title":"Design Patterns","text":"<p>Pattern 1: Environment-Based Namespaces <pre><code># production namespace - Strict quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    environment: production\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: production-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"100\"\n    requests.memory: 200Gi\n    limits.cpu: \"200\"\n    limits.memory: 400Gi\n    count/pods: \"500\"\n</code></pre></p> <pre><code># development namespace - Relaxed quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: development\n  labels:\n    environment: development\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: development-quota\n  namespace: development\nspec:\n  hard:\n    requests.cpu: \"20\"\n    requests.memory: 40Gi\n    limits.cpu: \"40\"\n    limits.memory: 80Gi\n    count/pods: \"100\"\n</code></pre> <p>Pattern 2: Team-Based Namespaces <pre><code># backend-team namespace\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: backend-team\n  labels:\n    team: backend\n    cost-center: \"1234\"\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: backend-quota\n  namespace: backend-team\nspec:\n  hard:\n    requests.cpu: \"50\"\n    requests.memory: 100Gi\n    count/deployments.apps: \"30\"\n</code></pre></p> <p>Pattern 3: Application-Based Namespaces <pre><code># ecommerce-app namespace\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ecommerce-app\n  labels:\n    app: ecommerce\n    tier: frontend\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: ecommerce-quota\n  namespace: ecommerce-app\nspec:\n  hard:\n    requests.cpu: \"30\"\n    requests.memory: 60Gi\n    count/services: \"20\"\n    count/configmaps: \"30\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#quota-scope-selectors","title":"Quota Scope Selectors","text":"<p>Priority Class-Based Quotas: <pre><code># quota-high-priority.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: high-priority-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"50\"\n    requests.memory: 100Gi\n    count/pods: \"100\"\n  scopeSelector:\n    matchExpressions:\n    - operator: In\n      scopeName: PriorityClass\n      values:\n      - high-priority\n</code></pre></p> <p>BestEffort/NotBestEffort Quotas: <pre><code># quota-besteffort.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: besteffort-quota\n  namespace: production\nspec:\n  hard:\n    count/pods: \"10\"  # Limit BestEffort pods\n  scopes:\n  - BestEffort\n---\n# quota-guaranteed.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: guaranteed-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"80\"\n    requests.memory: 160Gi\n  scopes:\n  - NotBestEffort\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#troubleshooting-quota-issues","title":"Troubleshooting Quota Issues","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#common-error-scenarios","title":"Common Error Scenarios","text":"<p>Error 1: Insufficient Quota <pre><code># Error message:\n# Error from server (Forbidden): pods \"nginx\" is forbidden:\n# exceeded quota: production-quota, requested: requests.cpu=1,requests.memory=1Gi,\n# used: requests.cpu=49,requests.memory=99Gi,\n# limited: requests.cpu=50,requests.memory=100Gi\n</code></pre></p> <p>Diagnosis and Fix: <pre><code># 1. Check current quota usage\nkubectl describe quota production-quota -n production\n\n# 2. Identify resource hogs\nkubectl top pods -n production --sort-by=cpu\nkubectl top pods -n production --sort-by=memory\n\n# 3. Options:\n# Option A: Increase quota\nkubectl edit quota production-quota -n production\n\n# Option B: Scale down workloads\nkubectl scale deployment high-cpu-app --replicas=2 -n production\n\n# Option C: Delete unused resources\nkubectl get pods -n production --field-selector=status.phase=Succeeded -o name | \\\n  xargs kubectl delete -n production\n</code></pre></p> <p>Error 2: Missing Resource Requests <pre><code># Error message:\n# Error from server (Forbidden): pods \"nginx\" is forbidden:\n# failed quota: production-quota: must specify requests.cpu,requests.memory\n</code></pre></p> <p>Diagnosis and Fix: <pre><code># When ResourceQuota exists, ALL pods must specify requests/limits\n# Option A: Add requests/limits to pod\nkubectl run nginx --image=nginx -n production \\\n  --requests='cpu=100m,memory=128Mi' \\\n  --limits='cpu=200m,memory=256Mi'\n\n# Option B: Create LimitRange to provide defaults\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: default-limits\n  namespace: production\nspec:\n  limits:\n  - type: Container\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    default:\n      cpu: 200m\n      memory: 256Mi\nEOF\n</code></pre></p> <p>Error 3: Limit/Request Ratio Exceeded <pre><code># Error message:\n# Error from server (Forbidden): pods \"nginx\" is forbidden:\n# maximum cpu limit to request ratio per Container is 4, but provided ratio is 10.000000\n</code></pre></p> <p>Diagnosis and Fix: <pre><code># Check LimitRange constraints\nkubectl describe limitrange -n production\n\n# Fix: Adjust pod resources to meet ratio\n# If request=100m and maxLimitRequestRatio=4, then limit cannot exceed 400m\nkubectl run nginx --image=nginx -n production \\\n  --requests='cpu=100m' \\\n  --limits='cpu=400m'  # 4:1 ratio\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#debugging-workflow","title":"Debugging Workflow","text":"<pre><code>flowchart TD\n    Error([Pod Creation Failed]) --&gt; CheckMsg{Error Message}\n\n    CheckMsg --&gt;|exceeded quota| QuotaDiag[Check Quota Status]\n    CheckMsg --&gt;|must specify| MissingRes[Add Resources or LimitRange]\n    CheckMsg --&gt;|ratio exceeded| RatioDiag[Check LimitRange Ratio]\n    CheckMsg --&gt;|insufficient| InsufficientRes[Increase Quota or Scale Down]\n\n    QuotaDiag --&gt; DescQuota[kubectl describe quota]\n    DescQuota --&gt; AnalyzeUsage{Quota Full?}\n\n    AnalyzeUsage --&gt;|Yes| Options[Choose Fix]\n    AnalyzeUsage --&gt;|No| CheckLR[Check LimitRange]\n\n    Options --&gt; OptA[Increase Quota]\n    Options --&gt; OptB[Delete Resources]\n    Options --&gt; OptC[Scale Down]\n\n    MissingRes --&gt; AddReq[Add requests/limits]\n    MissingRes --&gt; CreateLR[Create LimitRange]\n\n    RatioDiag --&gt; AdjustRatio[Adjust limit:request ratio]\n\n    CheckLR --&gt; DescLR[kubectl describe limitrange]\n\n    style Error fill:#ff9999\n    style OptA fill:#e1f5ff\n    style OptB fill:#fff4e1\n    style OptC fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#cka-exam-scenarios","title":"CKA Exam Scenarios","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-1-create-namespace-with-quota","title":"Scenario 1: Create Namespace with Quota","text":"<p>Task: Create namespace 'webapp' with quota: max 10 pods, 5 CPU cores, 10Gi memory.</p> <p>Solution: <pre><code># Create namespace\nkubectl create namespace webapp\n\n# Create quota (imperative)\nkubectl create quota webapp-quota \\\n  --hard=count/pods=10,requests.cpu=5,requests.memory=10Gi \\\n  -n webapp\n\n# Verify\nkubectl describe quota webapp-quota -n webapp\n</code></pre></p> <p>Alternative (Declarative): <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: webapp\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: webapp-quota\n  namespace: webapp\nspec:\n  hard:\n    count/pods: \"10\"\n    requests.cpu: \"5\"\n    requests.memory: 10Gi\nEOF\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-2-create-pod-in-quota-constrained-namespace","title":"Scenario 2: Create Pod in Quota-Constrained Namespace","text":"<p>Task: Create nginx pod in namespace with quota requiring resource specifications.</p> <p>Solution: <pre><code># Generate pod YAML with resources\nkubectl run nginx --image=nginx -n webapp \\\n  --requests='cpu=100m,memory=128Mi' \\\n  --limits='cpu=200m,memory=256Mi' \\\n  --dry-run=client -o yaml &gt; pod.yaml\n\n# Apply\nkubectl apply -f pod.yaml\n\n# Verify quota usage\nkubectl describe quota -n webapp\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-3-troubleshoot-quota-exceeded-error","title":"Scenario 3: Troubleshoot Quota Exceeded Error","text":"<p>Task: Deployment fails to scale due to quota. Identify issue and fix.</p> <p>Solution: <pre><code># 1. Check error\nkubectl get events -n webapp --sort-by='.lastTimestamp'\n\n# 2. Check quota usage\nkubectl describe quota -n webapp\n# Shows: requests.cpu used=4.8/5, requests.memory used=9.5Gi/10Gi\n\n# 3. Identify resource usage\nkubectl top pods -n webapp --sort-by=cpu\nkubectl top pods -n webapp --sort-by=memory\n\n# 4. Fix - Option A: Delete completed pods\nkubectl delete pod --field-selector=status.phase=Succeeded -n webapp\n\n# 4. Fix - Option B: Increase quota\nkubectl edit quota webapp-quota -n webapp\n# Increase limits\n\n# 5. Verify\nkubectl describe quota -n webapp\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-4-set-default-resource-constraints","title":"Scenario 4: Set Default Resource Constraints","text":"<p>Task: Create LimitRange in 'development' namespace with defaults: cpu=100m/200m, memory=128Mi/256Mi.</p> <p>Solution: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: dev-limits\n  namespace: development\nspec:\n  limits:\n  - type: Container\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    default:\n      cpu: 200m\n      memory: 256Mi\n    min:\n      cpu: 50m\n      memory: 64Mi\n    max:\n      cpu: 1\n      memory: 1Gi\nEOF\n\n# Verify\nkubectl describe limitrange dev-limits -n development\n\n# Test - create pod without resources (should get defaults)\nkubectl run test --image=nginx -n development\nkubectl get pod test -n development -o yaml | grep -A 10 resources:\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-5-multi-namespace-resource-isolation","title":"Scenario 5: Multi-Namespace Resource Isolation","text":"<p>Task: Create 3 namespaces (prod, staging, dev) with appropriate quotas.</p> <p>Solution: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\n# Production - Strict quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: prod\n  labels:\n    environment: production\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: prod-quota\n  namespace: prod\nspec:\n  hard:\n    requests.cpu: \"100\"\n    requests.memory: 200Gi\n    limits.cpu: \"200\"\n    limits.memory: 400Gi\n    count/pods: \"500\"\n    count/services: \"100\"\n---\n# Staging - Moderate quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: staging\n  labels:\n    environment: staging\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: staging-quota\n  namespace: staging\nspec:\n  hard:\n    requests.cpu: \"50\"\n    requests.memory: 100Gi\n    limits.cpu: \"100\"\n    limits.memory: 200Gi\n    count/pods: \"250\"\n---\n# Development - Relaxed quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n  labels:\n    environment: development\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: dev-quota\n  namespace: dev\nspec:\n  hard:\n    requests.cpu: \"20\"\n    requests.memory: 40Gi\n    limits.cpu: \"40\"\n    limits.memory: 80Gi\n    count/pods: \"100\"\nEOF\n\n# Verify all quotas\nkubectl get quota -A\nkubectl describe quota -n prod\nkubectl describe quota -n staging\nkubectl describe quota -n dev\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#best-practices","title":"Best Practices","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-design","title":"Namespace Design","text":"<p>\u2705 DO: - Use namespaces to separate environments (prod, staging, dev) - Use namespaces for team isolation (backend-team, frontend-team) - Use namespaces for application isolation (app-a, app-b) - Apply labels to namespaces for organization - Document namespace ownership and purpose</p> <p>\u274c DON'T: - Use namespaces for version separation (use labels instead) - Create excessive granularity (1 namespace per microservice) - Rely solely on namespaces for security (combine with RBAC, NetworkPolicy) - Use special characters in namespace names</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resource-quota-design","title":"Resource Quota Design","text":"<p>\u2705 DO: - Set both requests and limits quotas - Include object count quotas to prevent resource proliferation - Monitor quota usage regularly - Set quotas based on measured usage patterns - Use quota scope selectors for fine-grained control</p> <p>\u274c DON'T: - Set quotas too tight (causes frequent failures) - Set quotas too loose (defeats the purpose) - Forget to account for system pods in kube-system - Ignore quota usage metrics</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limitrange-design","title":"LimitRange Design","text":"<p>\u2705 DO: - Always create LimitRange when using ResourceQuota - Set reasonable defaults for requests and limits - Enforce min/max constraints to prevent extremes - Set maxLimitRequestRatio to prevent wasteful overcommit - Document LimitRange rationale</p> <p>\u274c DON'T: - Set defaults too high (wastes resources) - Set defaults too low (causes performance issues) - Set maxLimitRequestRatio too strict (prevents legitimate use cases) - Forget to test LimitRange impact on existing workloads</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#monitoring-and-metrics","title":"Monitoring and Metrics","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#quota-usage-monitoring","title":"Quota Usage Monitoring","text":"<pre><code># Check all quotas in cluster\nkubectl get quota -A\n\n# Monitor specific namespace quota\nkubectl describe quota -n production\n\n# Watch quota usage\nkubectl get quota -n production -w\n\n# Get quota usage metrics\nkubectl get quota -n production -o json | \\\n  jq '.items[].status.used'\n\n# Custom columns for quota overview\nkubectl get quota -A -o custom-columns=\\\nNAMESPACE:.metadata.namespace,\\\nNAME:.metadata.name,\\\nCPU_USED:.status.used.\\'requests\\.cpu\\',\\\nCPU_HARD:.status.hard.\\'requests\\.cpu\\',\\\nMEM_USED:.status.used.\\'requests\\.memory\\',\\\nMEM_HARD:.status.hard.\\'requests\\.memory\\'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#quota-utilization-metrics","title":"Quota Utilization Metrics","text":"<pre><code>graph LR\n    subgraph \"Namespace: production\"\n        Used[Used Resources&lt;br/&gt;CPU: 45/50&lt;br/&gt;Memory: 90Gi/100Gi&lt;br/&gt;Pods: 180/200]\n        Util[Utilization&lt;br/&gt;CPU: 90%&lt;br/&gt;Memory: 90%&lt;br/&gt;Pods: 90%]\n        Alert[\u26a0\ufe0f Alert Threshold&lt;br/&gt;\u2265 85%]\n    end\n\n    Used --&gt; Util\n    Util --&gt; Alert\n\n    Alert -.-&gt;|Trigger| Action1[Increase Quota]\n    Alert -.-&gt;|Trigger| Action2[Scale Down Workloads]\n    Alert -.-&gt;|Trigger| Action3[Optimize Resources]\n\n    style Used fill:#e1f5ff\n    style Util fill:#fff4e1\n    style Alert fill:#ff9999\n    style Action1 fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#exercise-1-basic-namespace-and-quota-10-minutes","title":"Exercise 1: Basic Namespace and Quota (10 minutes)","text":"<p>Tasks: 1. Create namespace 'test-ns' 2. Create ResourceQuota limiting to 5 pods, 2 CPU, 4Gi memory 3. Create 3 nginx pods with appropriate resources 4. Verify quota usage 5. Try creating 3 more pods (should fail) 6. Delete namespace</p> <p>Solution: <pre><code># 1. Create namespace\nkubectl create ns test-ns\n\n# 2. Create quota\nkubectl create quota test-quota \\\n  --hard=count/pods=5,requests.cpu=2,requests.memory=4Gi \\\n  -n test-ns\n\n# 3. Create pods\nfor i in {1..3}; do\n  kubectl run nginx-$i --image=nginx \\\n    --requests='cpu=200m,memory=512Mi' \\\n    --limits='cpu=400m,memory=1Gi' \\\n    -n test-ns\ndone\n\n# 4. Check quota\nkubectl describe quota test-quota -n test-ns\n\n# 5. Try more pods (will fail after 2 more)\nkubectl run nginx-4 --image=nginx \\\n  --requests='cpu=200m,memory=512Mi' -n test-ns\n\n# 6. Cleanup\nkubectl delete ns test-ns\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#exercise-2-limitrange-configuration-15-minutes","title":"Exercise 2: LimitRange Configuration (15 minutes)","text":"<p>Tasks: 1. Create namespace 'limit-test' 2. Create LimitRange with defaults and constraints 3. Create pod without resources (should get defaults) 4. Create pod with resources exceeding max (should fail) 5. Verify defaults applied</p> <p>Solution: <pre><code># 1. Create namespace\nkubectl create ns limit-test\n\n# 2. Create LimitRange\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limit-test-range\n  namespace: limit-test\nspec:\n  limits:\n  - type: Container\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    default:\n      cpu: 200m\n      memory: 256Mi\n    min:\n      cpu: 50m\n      memory: 64Mi\n    max:\n      cpu: 1\n      memory: 1Gi\n    maxLimitRequestRatio:\n      cpu: 4\n      memory: 4\nEOF\n\n# 3. Create pod without resources\nkubectl run test1 --image=nginx -n limit-test\n\n# Check applied defaults\nkubectl get pod test1 -n limit-test -o yaml | grep -A 10 resources:\n\n# 4. Try exceeding max (should fail)\nkubectl run test2 --image=nginx \\\n  --requests='cpu=2' -n limit-test\n# Error: maximum cpu usage per Container is 1, but limit is 2\n\n# 5. Cleanup\nkubectl delete ns limit-test\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#exercise-3-quota-troubleshooting-20-minutes","title":"Exercise 3: Quota Troubleshooting (20 minutes)","text":"<p>Tasks: 1. Create namespace with tight quota (2 pods max) 2. Create deployment with 5 replicas 3. Observe only 2 pods created 4. Diagnose and fix by increasing quota 5. Verify all 5 pods running</p> <p>Solution: <pre><code># 1. Create namespace and quota\nkubectl create ns tight-quota\nkubectl create quota tight-quota \\\n  --hard=count/pods=2,requests.cpu=1,requests.memory=1Gi \\\n  -n tight-quota\n\n# 2. Create deployment\nkubectl create deployment webapp --image=nginx --replicas=5 -n tight-quota\n\n# 3. Check pods (only 2 created)\nkubectl get pods -n tight-quota\nkubectl get rs -n tight-quota\n\n# View ReplicaSet events\nkubectl describe rs -n tight-quota\n# Shows: exceeded quota\n\n# 4. Diagnose\nkubectl describe quota tight-quota -n tight-quota\n# Shows: count/pods: 2/2\n\n# Fix - increase quota\nkubectl patch quota tight-quota -n tight-quota \\\n  --type='json' \\\n  -p='[{\"op\": \"replace\", \"path\": \"/spec/hard/count~1pods\", \"value\":\"10\"}]'\n\n# Or edit directly\nkubectl edit quota tight-quota -n tight-quota\n\n# 5. Verify\nkubectl get pods -n tight-quota\n# Should now show 5/5 pods running\n\n# Cleanup\nkubectl delete ns tight-quota\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#quick-reference","title":"Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-commands","title":"Namespace Commands","text":"<pre><code># Create\nkubectl create namespace &lt;name&gt;\nkubectl create ns &lt;name&gt;\n\n# List\nkubectl get namespaces\nkubectl get ns\n\n# Describe\nkubectl describe namespace &lt;name&gt;\n\n# Delete\nkubectl delete namespace &lt;name&gt;\n\n# Set default for context\nkubectl config set-context --current --namespace=&lt;name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resourcequota-commands","title":"ResourceQuota Commands","text":"<pre><code># Create quota\nkubectl create quota &lt;name&gt; --hard=&lt;key&gt;=&lt;value&gt; -n &lt;namespace&gt;\n\n# List quotas\nkubectl get quota -n &lt;namespace&gt;\nkubectl get quota -A\n\n# Describe quota\nkubectl describe quota &lt;name&gt; -n &lt;namespace&gt;\n\n# Edit quota\nkubectl edit quota &lt;name&gt; -n &lt;namespace&gt;\n\n# Delete quota\nkubectl delete quota &lt;name&gt; -n &lt;namespace&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limitrange-commands","title":"LimitRange Commands","text":"<pre><code># Create from file\nkubectl apply -f limitrange.yaml\n\n# List limit ranges\nkubectl get limitrange -n &lt;namespace&gt;\nkubectl get limits -n &lt;namespace&gt;\n\n# Describe\nkubectl describe limitrange &lt;name&gt; -n &lt;namespace&gt;\n\n# Delete\nkubectl delete limitrange &lt;name&gt; -n &lt;namespace&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#common-quota-keys","title":"Common Quota Keys","text":"<pre><code># Compute resources\nrequests.cpu\nrequests.memory\nlimits.cpu\nlimits.memory\n\n# Storage\nrequests.storage\npersistentvolumeclaims\n\n# Object counts\ncount/pods\ncount/services\ncount/configmaps\ncount/secrets\ncount/deployments.apps\ncount/replicasets.apps\ncount/statefulsets.apps\ncount/jobs.batch\ncount/cronjobs.batch\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Namespaces provide logical isolation - Not physical security boundaries</p> <p>\u2705 ResourceQuotas enforce aggregate limits - Prevent resource exhaustion per namespace</p> <p>\u2705 LimitRange provides defaults and constraints - Essential when using quotas</p> <p>\u2705 Always specify resource requests/limits - When quotas are active, required for admission</p> <p>\u2705 Monitor quota usage proactively - Avoid runtime failures from quota exhaustion</p> <p>\u2705 Combine quota scopes for flexibility - PriorityClass, BestEffort filters for fine-grained control</p> <p>\u2705 Delete namespace deletes all resources - Exercise caution with <code>kubectl delete ns</code></p> <p>\u2705 Exam context switching is critical - Always verify namespace before operations</p> <p>\u2705 Troubleshoot with describe - <code>kubectl describe quota/limitrange</code> shows usage and constraints</p> <p>\u2705 Plan quota capacity - Base on measured usage plus growth buffer</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#next-steps","title":"Next Steps","text":"<p>After mastering namespaces and resource quotas, continue with:</p> <p>Post 6: Services and Networking - Service discovery and cluster networking</p> <p>Related Posts: - kubectl Essentials - Command-line mastery for Kubernetes - Kubernetes Architecture Fundamentals - Understanding cluster components - Kubernetes CKA Mastery - Complete Learning Path - Full exam preparation series</p> <p>External Resources: - Namespaces Official Documentation - Resource Quotas - Limit Ranges - Configure Default CPU/Memory Requests and Limits - CKA Exam Curriculum</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/","title":"Persistent Volumes and Claims: Stateful Storage","text":"<p>Kubernetes excels at managing stateless applications, but production reality demands stateful workloads\u2014databases, message queues, file storage systems. PersistentVolumes (PV) and PersistentVolumeClaims (PVC) provide the abstraction layer that makes stateful storage portable, durable, and manageable across cluster infrastructure. For the CKA exam, mastering storage represents 10% of your score and is essential for real-world Kubernetes administration.</p> <p>The storage model in Kubernetes separates provisioning (administrator responsibility via PV) from consumption (developer responsibility via PVC). This decoupling enables platform teams to standardize storage offerings through StorageClasses while application teams request storage without infrastructure knowledge. Understanding the PV lifecycle\u2014binding, mounting, releasing, and reclaiming\u2014is critical for both exam success and production troubleshooting.</p> <p>Modern Kubernetes storage leverages the Container Storage Interface (CSI), which replaced legacy in-tree volume plugins. CSI drivers from AWS, Google Cloud, Azure, NetApp, and others provide production-grade dynamic provisioning. The 2025 Kubernetes landscape includes ReadWriteOncePod access mode (v1.29+), enhanced volume expansion capabilities, and topology-aware scheduling via WaitForFirstConsumer binding mode.</p>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#the-persistentvolume-lifecycle","title":"The PersistentVolume Lifecycle","text":"<p>Understanding the PV lifecycle is fundamental to storage operations and troubleshooting:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Available: PV Created\n    Available --&gt; Bound: PVC Claims PV\n    Bound --&gt; Released: PVC Deleted\n    Released --&gt; Available: Reclaim Policy: Retain\n    Released --&gt; [*]: Reclaim Policy: Delete\n    Released --&gt; Available: Reclaim Policy: Recycle (Deprecated)\n\n    note right of Available\n        PV exists but not claimed\n        Waiting for PVC to bind\n    end note\n\n    note right of Bound\n        PV claimed by PVC\n        Mounted in Pod\n        Data actively in use\n    end note\n\n    note right of Released\n        PVC deleted but data remains\n        Manual intervention required\n        Cannot be rebound automatically\n    end note</code></pre> <p>Lifecycle States Explained:</p> <ul> <li>Available: PV created by admin, waiting for PVC to claim it</li> <li>Bound: PVC successfully claimed PV (exclusive 1:1 relationship)</li> <li>Released: PVC deleted but PV retains data (manual cleanup needed)</li> <li>Failed: Automatic reclamation failed (requires admin intervention)</li> </ul> <p>The key insight: PV and PVC have a 1:1 binding relationship. Once bound, no other PVC can claim that PV until it's released and reclaimed.</p>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#persistentvolume-architecture","title":"PersistentVolume Architecture","text":"<pre><code>graph TB\n    subgraph \"Storage Layer\"\n        AWS[AWS EBS]\n        GCE[GCE Persistent Disk]\n        Azure[Azure Disk]\n        NFS[NFS Server]\n        Ceph[Ceph RBD]\n        Local[Local Storage]\n    end\n\n    subgraph \"Kubernetes Abstraction\"\n        PV1[PersistentVolume&lt;br/&gt;pv-database]\n        PV2[PersistentVolume&lt;br/&gt;pv-logs]\n        SC[StorageClass&lt;br/&gt;fast-ssd]\n    end\n\n    subgraph \"Application Layer\"\n        PVC1[PersistentVolumeClaim&lt;br/&gt;db-storage]\n        PVC2[PersistentVolumeClaim&lt;br/&gt;log-storage]\n        Pod1[Pod: postgres]\n        Pod2[Pod: fluentd]\n    end\n\n    AWS --&gt; PV1\n    GCE --&gt; PV2\n    SC -.-&gt;|Dynamic Provisioning| PV1\n    PV1 --&gt; PVC1\n    PV2 --&gt; PVC2\n    PVC1 --&gt; Pod1\n    PVC2 --&gt; Pod2\n\n    style SC fill:#e1f5ff\n    style PV1 fill:#fff4e1\n    style PVC1 fill:#e8f5e8</code></pre> <p>Static Provisioning (Manual PV Creation): <pre><code># Admin creates PV manually\nkubectl apply -f pv-database.yaml\n\n# Developer creates PVC (automatically binds)\nkubectl apply -f pvc-database.yaml\n</code></pre></p> <p>Dynamic Provisioning (Automated via StorageClass): <pre><code># Developer only creates PVC referencing StorageClass\nkubectl apply -f pvc-database.yaml\n\n# Kubernetes automatically provisions PV via CSI driver\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#creating-persistentvolumes-static-provisioning","title":"Creating PersistentVolumes (Static Provisioning)","text":"","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#basic-pv-with-nfs","title":"Basic PV with NFS","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-nfs-web\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: nfs-storage\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /exports/web\n    server: 192.168.1.100\n</code></pre> <p>Key Fields: - <code>capacity.storage</code>: Size of volume (cannot be reduced) - <code>accessModes</code>: How pods can access (see Access Modes section) - <code>persistentVolumeReclaimPolicy</code>: What happens when PVC is deleted - <code>storageClassName</code>: Groups PVs for PVC matching - <code>nfs</code>: Backend storage specification (CSI preferred for new deployments)</p>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#pv-with-local-storage-node-specific","title":"PV with Local Storage (Node-Specific)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-local-ssd\nspec:\n  capacity:\n    storage: 100Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: local-storage\n  local:\n    path: /mnt/disks/ssd1\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n        - matchExpressions:\n            - key: kubernetes.io/hostname\n              operator: In\n              values:\n                - node-01\n</code></pre> <p>Local Volume Critical Points: - Requires <code>nodeAffinity</code> (PV tied to specific node) - Pod scheduled to node with PV (not portable) - High performance (no network overhead) - No replication (data loss if node fails)</p>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#creating-pvs-via-kubectl","title":"Creating PVs via kubectl","text":"<pre><code># Create PV from YAML\nkubectl apply -f pv-database.yaml\n\n# List all PVs\nkubectl get pv\n\n# Detailed PV information\nkubectl describe pv pv-database\n\n# Watch PV status changes\nkubectl get pv -w\n\n# Check PV capacity and reclaim policy\nkubectl get pv -o custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage,RECLAIM:.spec.persistentVolumeReclaimPolicy,STATUS:.status.phase\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#creating-persistentvolumeclaims","title":"Creating PersistentVolumeClaims","text":"","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#basic-pvc-binds-to-existing-pv","title":"Basic PVC (Binds to Existing PV)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-postgres\n  namespace: production\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: nfs-storage\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <p>PVC Matching Logic: 1. StorageClass must match (or both empty) 2. AccessModes must be compatible 3. Capacity request \u2264 PV capacity 4. Selectors (if specified) must match PV labels</p>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#pvc-with-label-selector","title":"PVC with Label Selector","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-mysql\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi\n  storageClassName: fast-ssd\n  selector:\n    matchLabels:\n      environment: production\n      tier: database\n    matchExpressions:\n      - key: performance\n        operator: In\n        values: [high, ultra]\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#managing-pvcs","title":"Managing PVCs","text":"<pre><code># Create PVC\nkubectl apply -f pvc-database.yaml\n\n# List PVCs in namespace\nkubectl get pvc -n production\n\n# Check PVC binding status\nkubectl get pvc pvc-postgres\n\n# Describe PVC (shows bound PV)\nkubectl describe pvc pvc-postgres\n\n# Delete PVC (releases PV)\nkubectl delete pvc pvc-postgres\n\n# Check which PV is bound to PVC\nkubectl get pvc pvc-postgres -o jsonpath='{.spec.volumeName}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#access-modes-explained","title":"Access Modes Explained","text":"<pre><code>graph LR\n    subgraph \"Access Modes\"\n        RWO[ReadWriteOnce&lt;br/&gt;RWO]\n        ROX[ReadOnlyMany&lt;br/&gt;ROX]\n        RWX[ReadWriteMany&lt;br/&gt;RWX]\n        RWOP[ReadWriteOncePod&lt;br/&gt;RWOP&lt;br/&gt;&lt;i&gt;v1.29+&lt;/i&gt;]\n    end\n\n    subgraph \"RWO: Single Node\"\n        RWO --&gt; Node1[Node]\n        Node1 --&gt; Pod1[Pod 1]\n        Node1 --&gt; Pod2[Pod 2]\n    end\n\n    subgraph \"ROX: Multiple Nodes Read\"\n        ROX --&gt; NodeA[Node A]\n        ROX --&gt; NodeB[Node B]\n        NodeA --&gt; PodA[Pod - Read Only]\n        NodeB --&gt; PodB[Pod - Read Only]\n    end\n\n    subgraph \"RWX: Multiple Nodes Write\"\n        RWX --&gt; NodeX[Node X]\n        RWX --&gt; NodeY[Node Y]\n        NodeX --&gt; PodX[Pod - Read/Write]\n        NodeY --&gt; PodY[Pod - Read/Write]\n    end\n\n    subgraph \"RWOP: Single Pod\"\n        RWOP --&gt; SingleNode[Node]\n        SingleNode --&gt; OnlyPod[Single Pod Only]\n        SingleNode -.x.- Blocked[Other Pods Blocked]\n    end\n\n    style RWO fill:#e1f5ff\n    style RWOP fill:#fff4e1\n    style RWX fill:#e8f5e8\n    style ROX fill:#f3e5f5</code></pre> Access Mode Abbreviation Description Use Cases ReadWriteOnce RWO Volume mounted read-write by single node (multiple pods on same node OK) Databases, block storage, most workloads ReadOnlyMany ROX Volume mounted read-only by multiple nodes Shared configuration, static assets ReadWriteMany RWX Volume mounted read-write by multiple nodes Shared file systems (NFS, CephFS, GlusterFS) ReadWriteOncePod RWOP Volume mounted read-write by single pod (strictest) High-security databases, data isolation requirements <p>Volume Type Support Matrix:</p> Storage Type RWO ROX RWX RWOP AWS EBS \u2705 \u274c \u274c \u2705 GCE PD \u2705 \u2705 \u274c \u2705 Azure Disk \u2705 \u274c \u274c \u2705 NFS \u2705 \u2705 \u2705 \u2705 CephFS \u2705 \u2705 \u2705 \u2705 Local Volume \u2705 \u274c \u274c \u2705","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#reclaim-policies","title":"Reclaim Policies","text":"<pre><code>flowchart TD\n    PVC_Delete[PVC Deleted]\n\n    PVC_Delete --&gt; Retain{Reclaim Policy?}\n\n    Retain --&gt;|Retain| Manual[PV Status: Released&lt;br/&gt;Data Preserved&lt;br/&gt;Manual Cleanup Required]\n    Retain --&gt;|Delete| Auto[PV Automatically Deleted&lt;br/&gt;Data Destroyed&lt;br/&gt;Cloud Volume Deleted]\n    Retain --&gt;|Recycle| Deprecated[Deprecated&lt;br/&gt;Scrubs Data rm -rf&lt;br/&gt;PV Returns to Available]\n\n    Manual --&gt; Admin[Admin Actions:&lt;br/&gt;1. Backup data if needed&lt;br/&gt;2. Delete PV manually&lt;br/&gt;3. Recreate for reuse]\n\n    Auto --&gt; Dynamic[Typical for:&lt;br/&gt;- Dynamic Provisioning&lt;br/&gt;- Cloud Storage&lt;br/&gt;- Dev/Test Environments]\n\n    Deprecated --&gt; UseDelete[Use Delete instead&lt;br/&gt;Recycle removed in future]\n\n    style Retain fill:#e1f5ff\n    style Manual fill:#fff4e1\n    style Auto fill:#e8f5e8\n    style Deprecated fill:#ffcccc</code></pre> <p>Reclaim Policy Comparison:</p> Policy Behavior PV State After PVC Delete Use Case Retain Preserves data, requires manual cleanup Released (cannot auto-bind) Production databases, manual backup workflows Delete Automatically deletes PV and storage PV deleted, data destroyed Dynamic provisioning, ephemeral storage Recycle \u26a0\ufe0f Deprecated (Do not use) Replaced by dynamic provisioning <pre><code># Change reclaim policy on existing PV\nkubectl patch pv pv-database -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}'\n\n# Verify reclaim policy change\nkubectl get pv pv-database -o jsonpath='{.spec.persistentVolumeReclaimPolicy}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#storageclass-and-dynamic-provisioning","title":"StorageClass and Dynamic Provisioning","text":"<pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant PVC as PersistentVolumeClaim\n    participant SC as StorageClass\n    participant Provisioner as CSI Driver/Provisioner\n    participant Cloud as Cloud Provider API\n    participant PV as PersistentVolume\n\n    Dev-&gt;&gt;PVC: 1. Create PVC&lt;br/&gt;storageClassName: fast-ssd\n    PVC-&gt;&gt;SC: 2. Lookup StorageClass&lt;br/&gt;\"fast-ssd\"\n    SC-&gt;&gt;Provisioner: 3. Trigger Provisioner&lt;br/&gt;(e.g., ebs.csi.aws.com)\n    Provisioner-&gt;&gt;Cloud: 4. API Call&lt;br/&gt;Create EBS Volume (gp3, 50Gi)\n    Cloud--&gt;&gt;Provisioner: 5. Volume ID&lt;br/&gt;(vol-abc123)\n    Provisioner-&gt;&gt;PV: 6. Create PersistentVolume&lt;br/&gt;volumeHandle: vol-abc123\n    PV--&gt;&gt;PVC: 7. Bind PV to PVC\n    PVC--&gt;&gt;Dev: 8. PVC Ready (Bound)\n\n    Note over Dev,Cloud: Entire process automated&lt;br/&gt;No manual PV creation needed</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#aws-ebs-csi-storageclass","title":"AWS EBS CSI StorageClass","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-ssd\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3              # General Purpose SSD v3\n  iops: \"3000\"           # Provisioned IOPS\n  throughput: \"125\"      # MB/s\n  encrypted: \"true\"      # Encrypt at rest\n  kmsKeyId: arn:aws:kms:us-east-1:123456789012:key/abcd-1234\nvolumeBindingMode: WaitForFirstConsumer  # Topology-aware\nallowVolumeExpansion: true\nreclaimPolicy: Delete\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#google-cloud-persistent-disk-csi","title":"Google Cloud Persistent Disk CSI","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: pd-balanced\nprovisioner: pd.csi.storage.gke.io\nparameters:\n  type: pd-balanced      # Balanced performance/cost\n  replication-type: regional-pd  # Multi-zone replication\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nreclaimPolicy: Delete\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#nfs-dynamic-provisioning","title":"NFS Dynamic Provisioning","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs-client\nprovisioner: nfs.csi.k8s.io\nparameters:\n  server: 192.168.1.100\n  share: /exports/dynamic\n  mountOptions: \"nfsvers=4.1,hard\"\nvolumeBindingMode: Immediate\nallowVolumeExpansion: false\nreclaimPolicy: Retain\nmountOptions:\n  - hard\n  - nfsvers=4.1\n</code></pre> <p>StorageClass Key Fields: - <code>provisioner</code>: CSI driver name (e.g., ebs.csi.aws.com) - <code>parameters</code>: Provisioner-specific settings (type, IOPS, encryption) - <code>volumeBindingMode</code>: When to provision (Immediate vs WaitForFirstConsumer) - <code>allowVolumeExpansion</code>: Enable PVC size increases - <code>reclaimPolicy</code>: Default policy for dynamically provisioned PVs - <code>mountOptions</code>: Mount flags applied to volumes</p> <pre><code># List StorageClasses\nkubectl get storageclasses\nkubectl get sc\n\n# Describe StorageClass\nkubectl describe sc fast-ssd\n\n# Set default StorageClass\nkubectl patch storageclass fast-ssd -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n\n# Remove default annotation\nkubectl patch storageclass fast-ssd -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'\n\n# Check which StorageClass is default\nkubectl get sc -o jsonpath='{.items[?(@.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class==\"true\")].metadata.name}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#volume-binding-modes","title":"Volume Binding Modes","text":"<pre><code>graph TB\n    subgraph \"Immediate Binding\"\n        PVC1[PVC Created]\n        PVC1 --&gt; Provision1[Volume Provisioned&lt;br/&gt;Immediately]\n        Provision1 --&gt; Zone1[Random Zone Selected]\n        Zone1 --&gt; Bind1[PV Created &amp; Bound]\n        Bind1 --&gt; Schedule1[Pod Scheduled Later]\n        Schedule1 -.-&gt;|Problem| Mismatch1[Pod may land in&lt;br/&gt;different zone than volume]\n    end\n\n    subgraph \"WaitForFirstConsumer\"\n        PVC2[PVC Created]\n        PVC2 --&gt; Pending[PVC Status: Pending]\n        Pending --&gt; Pod2[Pod Created&lt;br/&gt;References PVC]\n        Pod2 --&gt; Scheduler[Scheduler Picks Node]\n        Scheduler --&gt; Topology[Check Node Topology&lt;br/&gt;Zone, Region]\n        Topology --&gt; ProvisionHere[Provision Volume&lt;br/&gt;in Same Zone as Node]\n        ProvisionHere --&gt; Bind2[PV Created &amp; Bound]\n        Bind2 --&gt; Mount[Pod Starts&lt;br/&gt;Volume Attached]\n    end\n\n    style Immediate Binding fill:#ffcccc\n    style WaitForFirstConsumer fill:#ccffcc\n    style Mismatch1 fill:#ff9999</code></pre> <p>Immediate (Legacy Default): - Volume provisioned as soon as PVC created - Pod may be scheduled to different zone/region - Risk: Volume and Pod in incompatible locations - Use case: Single-zone clusters, latency-insensitive workloads</p> <p>WaitForFirstConsumer (Recommended): - Volume provisioned only after Pod scheduled - Ensures volume created in same topology as Pod - Prevents zone mismatch issues - Use case: Multi-zone clusters, topology-aware applications</p> <pre><code># PVC with WaitForFirstConsumer\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-topology-aware\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: fast-ssd  # Must have volumeBindingMode: WaitForFirstConsumer\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#volume-expansion","title":"Volume Expansion","text":"<pre><code># Check if StorageClass allows expansion\nkubectl get sc fast-ssd -o jsonpath='{.allowVolumeExpansion}'\n\n# Edit PVC to request more storage\nkubectl edit pvc pvc-database\n\n# Modify spec.resources.requests.storage from 10Gi to 20Gi\n# Save and exit\n\n# Watch PVC for resize completion\nkubectl get pvc pvc-database -w\n\n# Check PVC conditions for expansion status\nkubectl describe pvc pvc-database | grep -A 5 Conditions\n\n# Some volume types require pod restart for filesystem expansion\nkubectl rollout restart deployment myapp\n</code></pre> <p>Volume Expansion Requirements: 1. StorageClass must have <code>allowVolumeExpansion: true</code> 2. CSI driver must support volume expansion 3. Cannot shrink volumes (only expand) 4. Some filesystems require pod restart (XFS, Ext4 online resize supported)</p>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#using-pvcs-in-pods","title":"Using PVCs in Pods","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\nspec:\n  containers:\n    - name: postgres\n      image: postgres:16\n      volumeMounts:\n        - name: db-storage\n          mountPath: /var/lib/postgresql/data\n          subPath: postgres  # Important: Avoids mounting to lost+found\n      env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: password\n  volumes:\n    - name: db-storage\n      persistentVolumeClaim:\n        claimName: pvc-postgres\n</code></pre> <p>subPath Use Cases: - Avoid mounting root of volume (e.g., lost+found directory) - Multiple containers sharing same PVC with different subdirectories - Prevents volume root permissions conflicts</p>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#troubleshooting-storage-issues","title":"Troubleshooting Storage Issues","text":"","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#pvc-stuck-in-pending","title":"PVC Stuck in Pending","text":"<pre><code># Check PVC status and events\nkubectl describe pvc pvc-database\n\n# Common causes and solutions:\n# 1. No matching PV available\nkubectl get pv | grep Available\n\n# 2. StorageClass doesn't exist\nkubectl get sc\n\n# 3. Insufficient capacity\nkubectl get pv -o custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage\n\n# 4. Access mode mismatch\nkubectl get pv pv-name -o jsonpath='{.spec.accessModes}'\n\n# 5. Dynamic provisioning failing (check provisioner logs)\nkubectl logs -n kube-system -l app=ebs-csi-controller\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#pvpvc-binding-issues","title":"PV/PVC Binding Issues","text":"<pre><code># Check PV \u2192 PVC binding\nkubectl get pv -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,CLAIM:.spec.claimRef.name\n\n# Check PVC \u2192 PV binding\nkubectl get pvc -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,VOLUME:.spec.volumeName\n\n# Force rebinding (edit PV claimRef)\nkubectl edit pv pv-database\n\n# Remove claimRef section to unbind\n# Then PVC can claim it again\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#volume-mount-failures-in-pods","title":"Volume Mount Failures in Pods","text":"<pre><code># Check pod events for mount errors\nkubectl describe pod postgres | grep -A 10 Events\n\n# Common errors:\n# - \"Volume not found\" \u2192 PVC deleted or doesn't exist\n# - \"Multi-Attach error\" \u2192 RWO volume already attached to different node\n# - \"Permission denied\" \u2192 Filesystem permissions issue\n\n# Check if volume attached to node\nkubectl get volumeattachment\n\n# Debug mount on node (requires node access)\nssh node-01\nsudo mount | grep pvc\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#orphaned-volumes","title":"Orphaned Volumes","text":"<pre><code># Find Released PVs (orphaned)\nkubectl get pv | grep Released\n\n# Option 1: Delete PV (if data not needed)\nkubectl delete pv pv-database\n\n# Option 2: Retain data and recreate PV\n# 1. Backup data\n# 2. Delete Released PV\nkubectl delete pv pv-old\n# 3. Create new PV pointing to same storage\nkubectl apply -f pv-recreated.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#cka-exam-practice-exercises","title":"CKA Exam Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#exercise-1-static-provisioning-with-nfs","title":"Exercise 1: Static Provisioning with NFS","text":"<p>Task: Create a PersistentVolume named <code>pv-web</code> with 5Gi capacity using NFS storage at <code>192.168.1.100:/exports/web</code>. Then create a PersistentVolumeClaim named <code>pvc-web</code> that binds to this PV. Deploy an nginx pod that mounts this PVC at <code>/usr/share/nginx/html</code>.</p> Solution <pre><code># Create PV\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-web\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: nfs\n  nfs:\n    path: /exports/web\n    server: 192.168.1.100\nEOF\n\n# Create PVC\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-web\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: nfs\n  resources:\n    requests:\n      storage: 5Gi\nEOF\n\n# Verify binding\nkubectl get pv,pvc\n\n# Create nginx pod\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-web\nspec:\n  containers:\n    - name: nginx\n      image: nginx:1.25\n      volumeMounts:\n        - name: web-storage\n          mountPath: /usr/share/nginx/html\n  volumes:\n    - name: web-storage\n      persistentVolumeClaim:\n        claimName: pvc-web\nEOF\n\n# Verify pod running\nkubectl get pod nginx-web\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#exercise-2-dynamic-provisioning-with-storageclass","title":"Exercise 2: Dynamic Provisioning with StorageClass","text":"<p>Task: Create a StorageClass named <code>fast-storage</code> using your cluster's CSI provisioner. Configure it with <code>WaitForFirstConsumer</code> volume binding mode and enable volume expansion. Create a PVC requesting 10Gi using this StorageClass, then deploy a MySQL StatefulSet using the PVC.</p> Solution <pre><code># Create StorageClass (adjust provisioner for your cluster)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-storage\nprovisioner: ebs.csi.aws.com  # Use your cluster's provisioner\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nreclaimPolicy: Delete\nparameters:\n  type: gp3\nEOF\n\n# Create PVC\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: fast-storage\n  resources:\n    requests:\n      storage: 10Gi\nEOF\n\n# Create MySQL pod\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql\nspec:\n  containers:\n    - name: mysql\n      image: mysql:8.0\n      env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"test123\"\n      volumeMounts:\n        - name: mysql-storage\n          mountPath: /var/lib/mysql\n          subPath: mysql\n  volumes:\n    - name: mysql-storage\n      persistentVolumeClaim:\n        claimName: mysql-pvc\nEOF\n\n# Check PVC bound after pod scheduled\nkubectl get pvc mysql-pvc\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#exercise-3-volume-expansion","title":"Exercise 3: Volume Expansion","text":"<p>Task: Expand the <code>mysql-pvc</code> from exercise 2 from 10Gi to 20Gi. Verify the expansion completed successfully.</p> Solution <pre><code># Edit PVC to increase size\nkubectl patch pvc mysql-pvc -p '{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"20Gi\"}}}}'\n\n# Watch for completion\nkubectl get pvc mysql-pvc -w\n\n# Check PVC conditions\nkubectl describe pvc mysql-pvc | grep -A 10 Conditions\n\n# Verify expanded size\nkubectl get pvc mysql-pvc -o jsonpath='{.status.capacity.storage}'\n\n# Some filesystems require pod restart\nkubectl delete pod mysql\n# Pod will restart and filesystem expansion will complete\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#exercise-4-troubleshoot-pending-pvc","title":"Exercise 4: Troubleshoot Pending PVC","text":"<p>Task: A PVC named <code>broken-pvc</code> in the <code>default</code> namespace has been stuck in Pending state for 5 minutes. Investigate and fix the issue.</p> Solution <pre><code># Describe PVC to see events\nkubectl describe pvc broken-pvc\n\n# Check if StorageClass exists\nSC=$(kubectl get pvc broken-pvc -o jsonpath='{.spec.storageClassName}')\nkubectl get sc $SC\n\n# Check for available PVs\nkubectl get pv | grep Available\n\n# Check PVC access modes\nkubectl get pvc broken-pvc -o jsonpath='{.spec.accessModes}'\n\n# Check capacity request\nkubectl get pvc broken-pvc -o jsonpath='{.spec.resources.requests.storage}'\n\n# Common fixes:\n# 1. If StorageClass missing:\n#    - Create missing StorageClass or use existing one\n\n# 2. If no matching PV:\n#    - Create PV with matching specs or enable dynamic provisioning\n\n# 3. If dynamic provisioning failing:\n#    - Check provisioner logs\nkubectl logs -n kube-system -l app=&lt;provisioner-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#exercise-5-change-reclaim-policy","title":"Exercise 5: Change Reclaim Policy","text":"<p>Task: You have a PV named <code>pv-important</code> with reclaim policy <code>Delete</code>. Change it to <code>Retain</code> to prevent data loss when the PVC is deleted. Verify the change.</p> Solution <pre><code># Check current reclaim policy\nkubectl get pv pv-important -o jsonpath='{.spec.persistentVolumeReclaimPolicy}'\n\n# Patch PV to change reclaim policy\nkubectl patch pv pv-important -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}'\n\n# Verify change\nkubectl get pv pv-important -o jsonpath='{.spec.persistentVolumeReclaimPolicy}'\n\n# Now when PVC is deleted, PV will move to Released state instead of being deleted\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#quick-reference","title":"Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#essential-kubectl-commands","title":"Essential kubectl Commands","text":"<pre><code># PersistentVolumes\nkubectl get pv                                    # List all PVs\nkubectl describe pv &lt;pv-name&gt;                     # Describe PV\nkubectl delete pv &lt;pv-name&gt;                       # Delete PV\nkubectl patch pv &lt;pv-name&gt; -p &lt;json-patch&gt;       # Patch PV\n\n# PersistentVolumeClaims\nkubectl get pvc                                   # List PVCs in current namespace\nkubectl get pvc -A                                # List PVCs across all namespaces\nkubectl describe pvc &lt;pvc-name&gt;                   # Describe PVC\nkubectl delete pvc &lt;pvc-name&gt;                     # Delete PVC\n\n# StorageClasses\nkubectl get sc                                    # List StorageClasses\nkubectl describe sc &lt;sc-name&gt;                     # Describe StorageClass\nkubectl get sc &lt;sc-name&gt; -o yaml                 # Get StorageClass YAML\n\n# Troubleshooting\nkubectl get events --sort-by='.lastTimestamp'    # Recent cluster events\nkubectl get volumeattachment                      # Check volume attachments\nkubectl get pv,pvc                               # Show PVs and PVCs together\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#access-modes-quick-reference","title":"Access Modes Quick Reference","text":"<pre><code>RWO  # ReadWriteOnce     - Single node read-write\nROX  # ReadOnlyMany      - Multiple nodes read-only\nRWX  # ReadWriteMany     - Multiple nodes read-write\nRWOP # ReadWriteOncePod  - Single pod exclusive (v1.29+)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/11/persistent-volumes-claims-stateful-storage/#related-posts","title":"Related Posts","text":"<ul> <li>ConfigMaps, Secrets, and Volume Mounts - Configuration management and secrets storage</li> <li>Kubernetes Pods: The Atomic Unit - Understanding pod volume mounts</li> <li>Deployments, ReplicaSets, and Rolling Updates - StatefulSets for persistent storage</li> </ul> <p>CKA Exam Domain: Storage (10%) Key Skills: PV/PVC creation, dynamic provisioning, troubleshooting storage issues Production Focus: StorageClass design, reclaim policies, backup strategies</p>","tags":["kubernetes","k8s","cka-prep","storage","persistent-volumes","stateful"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/","title":"High-Performance pNFS v4.2 Distributed Storage Architecture","text":"<p>A deep dive into building a clustered, high-availability parallel NFS storage system with load-balanced metadata servers, NVMe-backed storage nodes, and low-latency interconnects.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#architecture-overview","title":"Architecture Overview","text":"<p>This architecture implements a production-grade parallel NFS (pNFS) v4.2 deployment designed for GPU compute clusters requiring high-throughput, low-latency storage with built-in redundancy and horizontal scalability.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#key-design-goals","title":"Key Design Goals","text":"<ul> <li>Parallel I/O Performance: Direct client-to-storage data paths bypassing metadata bottlenecks</li> <li>Metadata High Availability: Clustered MDS with automatic failover</li> <li>Horizontal Scalability: Add storage nodes without downtime</li> <li>Low Latency: InfiniBand/RoCE interconnects for sub-microsecond latencies</li> <li>Fault Tolerance: No single points of failure in the architecture</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#system-topology","title":"System Topology","text":"<pre><code>sequenceDiagram\n    participant Client as Client&lt;br/&gt;(pNFS v4.2)\n    participant MDS as MDS Cluster&lt;br/&gt;(Active-Active via VIP)\n    participant S1 as Storage Node 1&lt;br/&gt;(NVMe)\n    participant S2 as Storage Node 2&lt;br/&gt;(NVMe)\n    participant S3 as Storage Node 3&lt;br/&gt;(NVMe)\n\n    Note over Client,S3: \u2501\u2501\u2501\u2501\u2501\u2501\u2501 PHASE 1: METADATA PATH \u2501\u2501\u2501\u2501\u2501\u2501\u2501\n    Note over MDS: Virtual IP load balances to any MDS&lt;br/&gt;All MDS nodes share distributed state\n\n    Client-&gt;&gt;+MDS: LAYOUTGET(file_handle)\n    Note right of MDS: MDS queries distributed&lt;br/&gt;backend for file layout\n    MDS--&gt;&gt;-Client: LAYOUT(stripe_pattern, DS_list)\n    Note left of Client: \u2713 Client caches layout&lt;br/&gt;Stripe unit: 1MB&lt;br/&gt;Stripe count: 3 nodes\n\n    Note over Client,S3: \u2501\u2501\u2501\u2501\u2501\u2501\u2501 PHASE 2: DATA PATH (MDS BYPASSED) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n    par Parallel Direct I/O over InfiniBand/RoCE\n        Client-&gt;&gt;+S1: WRITE Stripe 0\n        S1-&gt;&gt;S1: NVMe I/O\n        S1--&gt;&gt;-Client: ACK\n    and\n        Client-&gt;&gt;+S2: WRITE Stripe 1\n        S2-&gt;&gt;S2: NVMe I/O\n        S2--&gt;&gt;-Client: ACK\n    and\n        Client-&gt;&gt;+S3: WRITE Stripe 2\n        S3-&gt;&gt;S3: NVMe I/O\n        S3--&gt;&gt;-Client: ACK\n    end\n\n    Note over Client,S3: \u26a1 Aggregate: 3 \u00d7 7 GB/s = ~20 GB/s effective throughput</code></pre> <p>Key Architecture Points:</p> Layer Component Function Control Plane MDS Cluster (Active-Active) Virtual IP \u2192 Load balances metadata requestsDistributed backend \u2192 Shared state (GFS2/OCFS2)Co-located with storage nodes Data Plane Storage Nodes Direct parallel I/O bypasses MDS entirelyEach node: MDS service + Data service + NVMeHigh-speed fabric: InfiniBand or 100GbE RoCE Client pNFS v4.2 One-time layout fetch \u2192 caches stripe patternDirect parallel writes to multiple storage nodesNo metadata bottleneck on data path <p>Architecture Advantage</p> <p>Separation of Control and Data Planes: Client contacts MDS once to get file layout, then performs all subsequent I/O directly to storage nodes over high-speed network. MDS handles only metadata operations (LAYOUTGET, OPEN, CLOSE), while bulk data transfer happens in parallel across multiple storage nodes, eliminating the metadata server bottleneck.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#component-breakdown","title":"Component Breakdown","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#1-client-layer-pnfs-v42-clients","title":"1. Client Layer (pNFS v4.2 Clients)","text":"<p>Role: GPU compute nodes running pNFS-aware clients</p> <p>Characteristics: - Protocol: NFSv4.2 with pNFS layout extensions - Parallelism: Multiple concurrent I/O streams to storage nodes - Two-phase operations:     1. Metadata phase: Request file layout from MDS via VIP     2. Data phase: Direct parallel I/O to multiple storage nodes</p> <p>Advantages: - Metadata and data paths are separated - MDS only handles control plane; data plane scales independently - Clients cache layouts, reducing metadata round-trips</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#2-metadata-virtual-ip-vip-load-balancer","title":"2. Metadata Virtual IP (VIP) / Load Balancer","text":"<p>Role: Distribute metadata requests across clustered MDS instances</p> <p>Implementation Options:</p> Technology Use Case Pros Cons Keepalived + VRRP Simple HA Easy setup, fast failover Layer 3 only, single active HAProxy Advanced LB Health checks, stats, multi-algo Additional component Pacemaker + Corosync Enterprise HA Full cluster manager Complex configuration <p>Configuration Considerations: - Failover time: Target &lt;2 seconds for MDS failover - Session stickiness: Not required (stateless metadata operations) - Health checks: Monitor MDS service health on each node</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#3-mds-cluster-metadata-servers","title":"3. MDS Cluster (Metadata Servers)","text":"<p>Role: Manage namespace, permissions, file layouts, and client coordination</p> <p>Clustering Strategy:</p> <p>Active-Active Clustering</p> <p>All MDS instances are active simultaneously, sharing load via the VIP. This differs from traditional active-passive designs and requires:</p> <ul> <li>Shared backend: Distributed consensus or shared storage for metadata</li> <li>State synchronization: Real-time metadata replication</li> <li>Lock coordination: Distributed locking for file operations</li> </ul> <p>Backend Options:</p> <pre><code>Option 1: Shared Block Device (DRBD + GFS2/OCFS2)\n  pros:\n    - Battle-tested clustering\n    - POSIX semantics\n  cons:\n    - Block-level sync overhead\n    - Limited to 2-3 nodes typically\n\nOption 2: Distributed Database (etcd/Consul)\n  pros:\n    - Raft consensus built-in\n    - Horizontal scaling\n    - Cloud-native\n  cons:\n    - Additional latency\n    - More complex integration\n\nOption 3: Lustre MGS/MDT (if using Lustre as pNFS backend)\n  pros:\n    - Native high availability\n    - Proven at exascale\n  cons:\n    - Lustre-specific\n    - Complex deployment\n</code></pre> <p>Heartbeat Mechanism: - Interval: 500ms - 1s between nodes - Quorum: Majority voting prevents split-brain - Fencing: STONITH (Shoot The Other Node In The Head) for failed nodes</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#4-high-speed-network-fabric","title":"4. High-Speed Network Fabric","text":"<p>Role: Low-latency, high-bandwidth interconnect for storage traffic</p> <p>Technology Comparison:</p> Technology Bandwidth Latency Use Case InfiniBand HDR 200 Gbps &lt;1 \u03bcs HPC, AI training clusters 100GbE RoCE v2 100 Gbps &lt;5 \u03bcs Cost-effective alternative Omni-Path 100 Gbps &lt;1 \u03bcs Intel ecosystem <p>Network Design: <pre><code>- Dedicated storage VLAN/subnet\n- Jumbo frames (MTU 9000) for throughput\n- RDMA for zero-copy transfers\n- Lossless Ethernet (PFC) if using RoCE\n- Multiple paths for redundancy (LACP/MLAG)\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#5-storage-nodes","title":"5. Storage Nodes","text":"<p>Role: Serve actual file data via pNFS Data Service (DS)</p> <p>Node Architecture:</p> <pre><code>Each storage node runs:\n\u251c\u2500\u2500 MDS Service (part of cluster)\n\u251c\u2500\u2500 Data Service (DS) (serves pNFS I/O)\n\u2514\u2500\u2500 Physical Storage (NVMe SSDs)\n</code></pre> <p>NVMe Configuration: - Device: PCIe Gen4 NVMe SSDs (7000+ MB/s per device) - RAID: No RAID (rely on pNFS striping across nodes) - File System: XFS or ZFS for local storage - Tuning:     - <code>nvme.io_timeout=4294967295</code> (disable timeout)     - <code>elevator=none</code> (bypass I/O scheduler for NVMe)     - <code>vm.dirty_ratio=5</code> (aggressive writeback)</p> <p>Capacity Planning: <pre><code>Per-node capacity:\n  - 4x 4TB NVMe = 16TB raw per node\n  - 10 nodes = 160TB aggregate raw\n  - No RAID overhead (redundancy via replication)\n  - Effective capacity: ~140TB (accounting for metadata)\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#data-flow-read-operation","title":"Data Flow: Read Operation","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-1-layout-request-metadata-path","title":"Phase 1: Layout Request (Metadata Path)","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant VIP\n    participant MDS1\n    participant Backend\n\n    Client-&gt;&gt;VIP: LAYOUTGET (file handle)\n    VIP-&gt;&gt;MDS1: Forward request\n    MDS1-&gt;&gt;Backend: Query file layout\n    Backend--&gt;&gt;MDS1: Layout map\n    MDS1--&gt;&gt;Client: LAYOUT (stripe pattern, DS list)\n    Note over Client: Client caches layout</code></pre> <p>Layout Information Returned: <pre><code>{\n  \"layout_type\": \"LAYOUT4_NFSV4_1_FILES\",\n  \"stripe_unit\": 1048576,\n  \"stripe_count\": 4,\n  \"data_servers\": [\n    \"10.10.1.11:2049\",  // Storage Node 1\n    \"10.10.1.12:2049\",  // Storage Node 2\n    \"10.10.1.13:2049\",  // Storage Node 3\n    \"10.10.1.14:2049\"   // Storage Node 4\n  ]\n}\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-2-parallel-data-io-data-path","title":"Phase 2: Parallel Data I/O (Data Path)","text":"<pre><code>graph LR\n    Client --&gt;|Stripe 0| DS1[Storage Node 1]\n    Client --&gt;|Stripe 1| DS2[Storage Node 2]\n    Client --&gt;|Stripe 3| DS3[Storage Node 3]\n    Client --&gt;|Stripe 4| DS4[Storage Node 4]\n\n    DS1 --&gt; NVMe1[NVMe SSD]\n    DS2 --&gt; NVMe2[NVMe SSD]\n    DS3 --&gt; NVMe3[NVMe SSD]\n    DS4 --&gt; NVMe4[NVMe SSD]</code></pre> <p>Throughput Calculation: <pre><code>Single NVMe: 7 GB/s read\n4-way stripe: 7 GB/s \u00d7 4 = 28 GB/s aggregate\nOverhead (20%): ~22 GB/s effective client throughput\n</code></pre></p> <p>Key Advantage</p> <p>The MDS is completely bypassed during data I/O. Only initial layout fetch requires MDS contact, then client directly streams data from multiple storage nodes in parallel.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#write-operation-with-coherency","title":"Write Operation with Coherency","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#challenges","title":"Challenges","text":"<ul> <li>Cache coherency: Multiple clients may access same file</li> <li>Consistency: Must maintain POSIX semantics</li> <li>Layout revocation: MDS may recall layouts during conflicts</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#write-flow","title":"Write Flow","text":"<pre><code>sequenceDiagram\n    participant Client1\n    participant Client2\n    participant MDS\n    participant DS1\n\n    Client1-&gt;&gt;MDS: OPEN (file, WRITE)\n    MDS--&gt;&gt;Client1: LAYOUT (read-write)\n    Client1-&gt;&gt;DS1: WRITE data\n\n    Client2-&gt;&gt;MDS: OPEN (same file, WRITE)\n    MDS-&gt;&gt;Client1: CB_LAYOUTRECALL\n    Client1-&gt;&gt;DS1: COMMIT writes\n    Client1-&gt;&gt;MDS: LAYOUTRETURN\n    MDS--&gt;&gt;Client2: LAYOUT (read-write)</code></pre> <p>Layout Recall Scenarios: 1. Write-write conflict: Second writer needs exclusive layout 2. Read-write conflict: Writer needs to invalidate reader caches 3. Layout change: File being migrated or restriped</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#high-availability-scenarios","title":"High Availability Scenarios","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#scenario-1-mds-node-failure","title":"Scenario 1: MDS Node Failure","text":"<pre><code>Before:\n  VIP \u2192 MDS1 (active)\n      \u2192 MDS2 (active)\n      \u2192 MDS3 (active)  \u2190 fails\n\nAfter (within 2 seconds):\n  VIP \u2192 MDS1 (active)  \u2190 absorbs load\n      \u2192 MDS2 (active)  \u2190 absorbs load\n\n  MDS3: Fenced by cluster, removed from VIP pool\n  Client layouts: Still valid, no client disruption\n</code></pre> <p>Recovery Actions: - Quorum maintained (2/3 nodes) - Clients continue data I/O unaffected - New metadata requests distributed to healthy MDS nodes - Failed MDS auto-rejoins after recovery</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#scenario-2-storage-node-failure","title":"Scenario 2: Storage Node Failure","text":"<pre><code>pNFS File with 4-way striping across nodes 1-4:\n  Node 3 fails \u2192 Stripes 2 (stored on node 3) unavailable\n\nClient behavior:\n  1. Client detects I/O error on stripe 2\n  2. Client returns partial read/write to application\n  3. Application must handle EIO (or use replication)\n\nRecovery:\n  - Option A: File replicated (pNFS server-side replication)\n             \u2192 Automatic failover to replica stripe\n  - Option B: No replication \u2192 Data loss for affected stripes\n</code></pre> <p>Data Durability</p> <p>pNFS itself does NOT provide redundancy. You must implement:</p> <ul> <li>Server-side replication (e.g., Lustre OST pools)</li> <li>Client-side RAID (mdadm over pNFS)</li> <li>Application-level erasure coding</li> <li>Regular snapshots/backups</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#scenario-3-network-partition-split-brain-prevention","title":"Scenario 3: Network Partition (Split-Brain Prevention)","text":"<pre><code>Network partition splits cluster:\n  Partition A: MDS1, MDS2 (2 nodes)\n  Partition B: MDS3 (1 node)\n\nQuorum voting:\n  Partition A: 2/3 nodes = HAS QUORUM \u2192 continues operation\n  Partition B: 1/3 nodes = NO QUORUM \u2192 enters read-only mode\n\nPrevention:\n  - Fencing agent (IPMI, PDU) forcibly powers off minority partition\n  - Prevents conflicting writes to shared backend\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#performance-tuning","title":"Performance Tuning","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#client-side-tunables","title":"Client-Side Tunables","text":"<pre><code># /etc/nfsmount.conf or mount options\nmount -t nfs4 -o \\\n  vers=4.2,\\                      # Enable pNFS\n  pnfs,\\                          # Use parallel NFS layouts\n  rsize=1048576,\\                 # 1MB read size\n  wsize=1048576,\\                 # 1MB write size\n  timeo=600,\\                     # 60s timeout\n  retrans=2,\\                     # 2 retransmissions\n  hard,\\                          # Hard mount (don't give up)\n  async,\\                         # Asynchronous I/O\n  ac,\\                            # Attribute caching\n  actimeo=3600 \\                  # 1-hour attribute cache\n  10.10.1.100:/export /mnt/pnfs\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#server-side-tunables","title":"Server-Side Tunables","text":"<pre><code># NFS server threads (per-node)\necho 256 &gt; /proc/sys/sunrpc/nfsd_threads\n\n# Network receive buffers\nsysctl -w net.core.rmem_max=134217728\nsysctl -w net.core.wmem_max=134217728\n\n# NVMe queue depth\necho 1024 &gt; /sys/block/nvme0n1/queue/nr_requests\n\n# Disable CPU frequency scaling (performance mode)\ncpupower frequency-set -g performance\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#monitoring-metrics","title":"Monitoring Metrics","text":"<pre><code>Key metrics to track:\n  - MDS operations/sec (LAYOUTGET, OPEN, CLOSE)\n  - Data server throughput (GB/s per node)\n  - Latency percentiles (p50, p95, p99)\n  - Client cache hit rates\n  - Network utilization (per fabric)\n  - NVMe IOPS and latency\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#implementation-deployment-checklist","title":"Implementation: Deployment Checklist","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-1-network-setup","title":"Phase 1: Network Setup","text":"<ul> <li> Deploy InfiniBand/RoCE fabric</li> <li> Configure storage VLAN with jumbo frames</li> <li> Enable RDMA on all nodes</li> <li> Verify bandwidth with <code>ib_write_bw</code> / <code>ib_read_bw</code></li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-2-storage-node-provisioning","title":"Phase 2: Storage Node Provisioning","text":"<ul> <li> Install NVMe SSDs and verify <code>nvme list</code></li> <li> Create XFS/ZFS filesystems</li> <li> Apply NVMe performance tunings</li> <li> Install <code>nfs-kernel-server</code> with pNFS support</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-3-mds-cluster-setup","title":"Phase 3: MDS Cluster Setup","text":"<ul> <li> Choose clustering backend (DRBD, etcd, etc.)</li> <li> Configure Pacemaker/Corosync or equivalent</li> <li> Set up VIP with failover tests</li> <li> Deploy metadata synchronization</li> <li> Test quorum and fencing</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-4-pnfs-configuration","title":"Phase 4: pNFS Configuration","text":"<ul> <li> Configure pNFS layouts on each storage node</li> <li> Export file systems via NFS4 with pNFS enabled</li> <li> Register data servers with MDS</li> <li> Test layout distribution from clients</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-5-client-deployment","title":"Phase 5: Client Deployment","text":"<ul> <li> Mount pNFS export with optimized parameters</li> <li> Verify parallel I/O with <code>dd</code> or <code>fio</code></li> <li> Test layout recall and coherency</li> <li> Run application workload benchmarks</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-6-production-hardening","title":"Phase 6: Production Hardening","text":"<ul> <li> Set up monitoring (Prometheus + Grafana)</li> <li> Configure alerting for node failures</li> <li> Document failover procedures</li> <li> Schedule regular disaster recovery drills</li> <li> Implement backup strategy</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#real-world-performance","title":"Real-World Performance","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#benchmark-environment","title":"Benchmark Environment","text":"<pre><code>Hardware:\n  - 10x storage nodes (Dell R750)\n  - 4x 7.68TB NVMe per node (Samsung PM9A3)\n  - 100GbE RoCE network\n  - 2x AMD EPYC 7543 per node\n\nWorkload:\n  - FIO sequential read (4MB block size)\n  - 8 clients, 16 threads each\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#results","title":"Results","text":"Metric Value Notes Aggregate Throughput 82 GB/s 10 nodes \u00d7 ~8 GB/s each Per-Client Throughput 10.2 GB/s 82 GB/s / 8 clients Latency (p99) 3.2 ms Network + NVMe + pNFS overhead MDS Load 2,300 ops/s Only layout requests CPU Utilization 35% avg Plenty of headroom <p>Key Takeaway</p> <p>pNFS achieved near-linear scaling across 10 storage nodes. MDS remained under 10% CPU utilization, proving effective metadata/data path separation.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#troubleshooting-guide","title":"Troubleshooting Guide","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#problem-clients-not-using-pnfs-falling-back-to-standard-nfs","title":"Problem: Clients not using pNFS (falling back to standard NFS)","text":"<p>Symptoms: <pre><code># All I/O going through MDS node\nnfsstat -m | grep \"pnfs\"  # Shows \"pnfs: not in use\"\n</code></pre></p> <p>Diagnosis: <pre><code># Check server pNFS support\nnfsstat -s | grep pnfs\n\n# Check client kernel support\ngrep PNFS /boot/config-$(uname -r)  # Should show CONFIG_PNFS_FILE_LAYOUT=m\n</code></pre></p> <p>Solution: - Ensure server exports with <code>pnfs</code> option - Verify client kernel has <code>nfs_layout_nfsv41_files</code> module loaded - Check for layout request denials in <code>/var/log/messages</code></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#problem-high-mds-cpu-usage","title":"Problem: High MDS CPU usage","text":"<p>Symptoms: <pre><code># MDS nodes showing &gt;80% CPU\ntop  # nfsd threads consuming CPU\n</code></pre></p> <p>Diagnosis: <pre><code># Check for excessive LAYOUTGET operations\nnfsstat -s | grep LAYOUTGET\n</code></pre></p> <p>Possible Causes: - Clients not caching layouts (check <code>actimeo</code>) - Frequent layout recalls (check for conflicting access patterns) - Insufficient MDS threads (check <code>nfsd_threads</code>)</p> <p>Solution: <pre><code># Increase client attribute cache timeout\nmount -o remount,actimeo=3600 /mnt/pnfs\n\n# Add more MDS threads\necho 512 &gt; /proc/sys/sunrpc/nfsd_threads\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#problem-uneven-storage-utilization","title":"Problem: Uneven storage utilization","text":"<p>Symptoms: <pre><code># One storage node at 90%, others at 40%\ndf -h /storage/*\n</code></pre></p> <p>Diagnosis: <pre><code># Check file layout distribution\n# (Requires pNFS-aware tooling or manual inspection)\n</code></pre></p> <p>Solution: - Re-stripe files: Use pNFS restripe tools if available - Balance new files: Adjust MDS layout selection algorithm - Add/remove nodes: Trigger cluster rebalancing</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#advanced-topics","title":"Advanced Topics","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#1-hierarchical-storage-management-hsm-with-pnfs","title":"1. Hierarchical Storage Management (HSM) with pNFS","text":"<p>Implement tiered storage by combining: - Hot tier: NVMe-backed pNFS for active data - Warm tier: SATA SSD pNFS for recent data - Cold tier: HDD-based object storage (S3) for archives</p> <p>Layout policy: <pre><code>def select_storage_tier(file_metadata):\n    if file_metadata.access_count &gt; 100:\n        return TIER_NVME\n    elif file_metadata.age_days &lt; 30:\n        return TIER_SSD\n    else:\n        return TIER_HDD\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#2-erasure-coding-for-space-efficiency","title":"2. Erasure Coding for Space Efficiency","text":"<p>Instead of replication (2x-3x overhead), use erasure coding: - Reed-Solomon (8+3): 1.375x overhead for 3-drive fault tolerance - RAID 6 equivalent: Stripe across pNFS with parity - Rebuild time: ~2 hours for 10TB per failed drive</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#3-multi-site-pnfs-replication","title":"3. Multi-Site pNFS Replication","text":"<p>For disaster recovery: <pre><code>Site A (Primary):          Site B (DR):\n  10 storage nodes    \u2192      10 storage nodes\n  Active MDS cluster  \u2192      Standby MDS cluster\n\nAsync replication (rsync/DRBD async or Lustre HSM)\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#conclusion","title":"Conclusion","text":"<p>This pNFS v4.2 architecture provides:</p> <p>\u2705 High throughput: 80+ GB/s aggregate via parallel I/O \u2705 Low latency: &lt;5ms p99 with InfiniBand/RoCE \u2705 High availability: No single points of failure \u2705 Horizontal scalability: Add nodes without downtime \u2705 Operational simplicity: Standard NFS client compatibility</p> <p>Trade-offs: - Complexity: More moving parts than traditional NAS - Data durability: Requires additional replication/erasure coding - Cost: High-speed network and NVMe SSDs increase CapEx</p> <p>Ideal for: - AI/ML training clusters (GPU \u2192 storage throughput) - HPC workloads (parallel file access patterns) - Video rendering farms (large file streaming) - High-frequency trading (low-latency shared storage)</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#references","title":"References","text":"<ul> <li>RFC 8881 - NFSv4.1 Protocol</li> <li>RFC 7862 - NFSv4.2 Protocol</li> <li>Linux pNFS Documentation</li> <li>Lustre pNFS Guide</li> <li>Red Hat: Configuring pNFS</li> </ul> <p>Tags: #pNFS #distributed-storage #NVMe #high-availability #load-balancing #metadata #clustering #InfiniBand #RoCE #parallel-io #file-systems #linux #performance-tuning #scalability</p> <p>Category: Storage, Architecture</p> <p>Have questions or running a similar setup? Open a discussion or reach out.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/","title":"Security Contexts and Pod Security Standards","text":"<p>Harden workloads with Linux security mechanisms and enforce cluster-wide security policies</p> <p>Security Contexts define privilege and access control settings for Pods and containers, leveraging Linux kernel security features like capabilities, SELinux, AppArmor, and seccomp. Pod Security Standards (PSS) provide a framework for enforcing security best practices across your cluster. For the CKA exam, you'll need to configure SecurityContext fields, apply Pod Security admission policies, troubleshoot permission issues, and understand how privileged workloads differ from restricted ones. This guide covers everything from basic runAsUser configuration to advanced seccomp profiles and cluster-wide policy enforcement.</p> <p>CKA Exam Relevance: Cluster Architecture, Installation &amp; Configuration (25% of exam weight)</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<ul> <li>SecurityContext Architecture: Pod vs Container security settings</li> <li>User and Group Management: runAsUser, runAsGroup, fsGroup</li> <li>Linux Capabilities: Fine-grained privilege control beyond root/non-root</li> <li>Privileged Containers: When and why to use (or avoid) them</li> <li>Read-Only Filesystems: Immutable container root filesystems</li> <li>SELinux, AppArmor, Seccomp: Linux kernel security modules</li> <li>Pod Security Standards: Privileged, Baseline, Restricted policies</li> <li>Pod Security Admission: Enforcing PSS at namespace level</li> <li>Troubleshooting: Permission denied errors and policy violations</li> </ul>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#securitycontext-architecture","title":"\ud83c\udfaf SecurityContext Architecture","text":"<p>SecurityContext can be defined at two levels:</p> <ol> <li>PodSecurityContext (Pod level): Applies to all containers in the Pod</li> <li>SecurityContext (Container level): Applies to specific container, overrides Pod settings</li> </ol> <pre><code>graph TD\n    subgraph \"Pod: my-app\"\n        PSC[PodSecurityContext&lt;br/&gt;runAsUser: 1000&lt;br/&gt;fsGroup: 2000]\n\n        subgraph \"Container: app\"\n            CSC1[SecurityContext&lt;br/&gt;runAsUser: 1001&lt;br/&gt;OVERRIDES Pod]\n            Process1[Process runs as&lt;br/&gt;UID 1001]\n        end\n\n        subgraph \"Container: sidecar\"\n            CSC2[No SecurityContext&lt;br/&gt;INHERITS from Pod]\n            Process2[Process runs as&lt;br/&gt;UID 1000]\n        end\n\n        PSC --&gt; CSC1\n        PSC --&gt; CSC2\n        CSC1 --&gt; Process1\n        CSC2 --&gt; Process2\n    end\n\n    style PSC fill:#e1f5ff\n    style CSC1 fill:#fff4e1\n    style CSC2 fill:#e8f5e8\n    style Process1 fill:#f3e5f5\n    style Process2 fill:#f3e5f5</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-demo\nspec:\n  securityContext:              # Pod-level security context\n    runAsUser: 1000             # All containers run as UID 1000\n    runAsGroup: 3000            # Primary GID 3000\n    fsGroup: 2000               # Mounted volumes owned by GID 2000\n  containers:\n  - name: app\n    image: busybox:1.36\n    command: [\"sh\", \"-c\", \"sleep 3600\"]\n    securityContext:            # Container-level security context\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n</code></pre> <pre><code># Create and test\nkubectl apply -f security-demo.yaml\n\n# Verify user/group IDs\nkubectl exec security-demo -- id\n# Output: uid=1000 gid=3000 groups=2000\n\n# Verify read-only filesystem\nkubectl exec security-demo -- touch /test.txt\n# Output: touch: /test.txt: Read-only file system\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#user-and-group-management","title":"\ud83d\udc64 User and Group Management","text":"","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#runasuser-and-runasgroup","title":"runAsUser and runAsGroup","text":"<p>Control the UID and GID for container processes:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: user-demo\nspec:\n  securityContext:\n    runAsUser: 1000       # UID for all containers\n    runAsGroup: 3000      # Primary GID\n    fsGroup: 2000         # Supplementary group for volumes\n  containers:\n  - name: app\n    image: nginx:1.27\n    volumeMounts:\n    - name: data\n      mountPath: /data\n  volumes:\n  - name: data\n    emptyDir: {}\n</code></pre> <pre><code># Verify process ownership\nkubectl exec user-demo -- ps aux\n# nginx processes run as UID 1000\n\n# Verify file ownership in mounted volume\nkubectl exec user-demo -- ls -ld /data\n# Output: drwxrwsr-x 2 1000 2000 ... /data\n# Directory owned by UID 1000, GID 2000 (fsGroup)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#runasnonroot","title":"runAsNonRoot","text":"<p>Enforce non-root execution (kubelet validates at runtime):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nonroot-demo\nspec:\n  securityContext:\n    runAsNonRoot: true    # Require non-root user\n    runAsUser: 1000       # Explicit UID (best practice)\n  containers:\n  - name: app\n    image: nginx:1.27\n</code></pre> <p>If image defaults to root (UID 0): <pre><code>kubectl apply -f nonroot-demo.yaml\n# Pod created but fails to start\n\nkubectl describe pod nonroot-demo\n# Error: container has runAsNonRoot and image will run as root\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#fsgroup-behavior","title":"fsGroup Behavior","text":"<p>fsGroup sets the owning GID for mounted volumes:</p> <pre><code>graph LR\n    subgraph \"Before fsGroup\"\n        V1[Volume files&lt;br/&gt;GID: 0 root]\n    end\n\n    subgraph \"Apply fsGroup: 2000\"\n        Process[Container process]\n    end\n\n    subgraph \"After fsGroup\"\n        V2[Volume files&lt;br/&gt;GID: 2000]\n    end\n\n    V1 --&gt;|Mount with fsGroup| Process\n    Process --&gt;|Ownership changed| V2\n\n    style V1 fill:#ffe5e5\n    style Process fill:#e1f5ff\n    style V2 fill:#e8f5e8</code></pre> <p>fsGroupChangePolicy (Kubernetes 1.20+): - <code>Always</code>: Change ownership on every mount (default, slower) - <code>OnRootMismatch</code>: Only change if root GID doesn't match fsGroup (faster)</p> <pre><code>securityContext:\n  fsGroup: 2000\n  fsGroupChangePolicy: \"OnRootMismatch\"  # Performance optimization\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#linux-capabilities","title":"\ud83d\udd11 Linux Capabilities","text":"<p>Capabilities provide fine-grained control over privileged operations, avoiding full root access.</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#common-capabilities","title":"Common Capabilities","text":"Capability Description Use Case CAP_NET_BIND_SERVICE Bind to ports &lt; 1024 Web servers on port 80/443 CAP_SYS_TIME Modify system clock NTP services CAP_NET_ADMIN Network configuration VPN, routing, iptables CAP_SYS_ADMIN Many privileged ops Avoid (too broad) CAP_CHOWN Change file ownership File management tools CAP_DAC_OVERRIDE Bypass file permissions Backup tools CAP_SETUID/SETGID Change UID/GID su, sudo alternatives","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#adding-and-dropping-capabilities","title":"Adding and Dropping Capabilities","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: caps-demo\nspec:\n  containers:\n  - name: app\n    image: nginx:1.27\n    securityContext:\n      capabilities:\n        add: [\"NET_BIND_SERVICE\"]   # Allow binding to port 80\n        drop: [\"ALL\"]                # Drop all other capabilities\n</code></pre> <pre><code># Verify capabilities\nkubectl exec caps-demo -- cat /proc/1/status | grep Cap\n# CapEff: Shows effective capabilities bitmask\n\n# Test binding to privileged port\nkubectl exec caps-demo -- nc -l -p 80\n# Success with NET_BIND_SERVICE\n\nkubectl exec caps-demo -- nc -l -p 22\n# Would fail (no NET_ADMIN for SSH-like operations)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#best-practice-drop-all-add-specific","title":"Best Practice: Drop All, Add Specific","text":"<pre><code>securityContext:\n  capabilities:\n    drop: [\"ALL\"]                           # Start with nothing\n    add: [\"NET_BIND_SERVICE\", \"CHOWN\"]      # Add only what's needed\n</code></pre> <p>Default Capabilities (without explicit drop): - CAP_CHOWN, CAP_DAC_OVERRIDE, CAP_FOWNER, CAP_FSETID - CAP_KILL, CAP_SETGID, CAP_SETUID, CAP_SETPCAP, CAP_SETFCAP - CAP_NET_BIND_SERVICE, CAP_NET_RAW, CAP_SYS_CHROOT - CAP_MKNOD, CAP_AUDIT_WRITE</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#privileged-containers","title":"\ud83d\udd13 Privileged Containers","text":"<p>Privileged mode grants nearly all host capabilities (dangerous):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\nspec:\n  containers:\n  - name: privileged-container\n    image: busybox:1.36\n    securityContext:\n      privileged: true    # \u26a0\ufe0f DANGEROUS: Full host access\n</code></pre> <p>What privileged: true does: - Grants ALL Linux capabilities - Access to ALL host devices (<code>/dev/*</code>) - Bypasses AppArmor, SELinux, seccomp restrictions - Can modify kernel settings via <code>/proc</code>, <code>/sys</code></p> <p>When to use (very rare): - Container runtime engines (Docker-in-Docker) - System-level monitoring (node-problem-detector) - Device drivers or hardware access - Debugging (temporarily, never in production)</p> <p>CKA Exam: Avoid privileged containers unless explicitly required.</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#read-only-root-filesystem","title":"\ud83d\udcc1 Read-Only Root Filesystem","text":"<p>Immutable containers prevent runtime modifications:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: readonly-demo\nspec:\n  containers:\n  - name: app\n    image: nginx:1.27\n    securityContext:\n      readOnlyRootFilesystem: true    # Root FS immutable\n    volumeMounts:\n    - name: cache\n      mountPath: /var/cache/nginx     # Writable tmpfs for nginx cache\n    - name: run\n      mountPath: /var/run             # Writable tmpfs for nginx PID\n  volumes:\n  - name: cache\n    emptyDir: {}\n  - name: run\n    emptyDir: {}\n</code></pre> <pre><code># Test immutability\nkubectl exec readonly-demo -- touch /tmp/test\n# Error: Read-only file system\n\n# Mounted volumes still writable\nkubectl exec readonly-demo -- touch /var/cache/nginx/test\n# Success\n</code></pre> <p>Benefits: - Prevents runtime tampering - Detects misconfigurations (apps writing to root FS) - Immutable infrastructure pattern</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#allowprivilegeescalation","title":"\ud83d\udd12 allowPrivilegeEscalation","text":"<p>Prevents setuid binaries from escalating privileges:</p> <pre><code>securityContext:\n  allowPrivilegeEscalation: false   # Block privilege escalation\n  runAsNonRoot: true                # Must be non-root to enforce\n</code></pre> <p>What it controls: - Sets <code>no_new_privs</code> flag on Linux processes - Prevents gaining privileges via setuid/setgid binaries (e.g., <code>sudo</code>, <code>su</code>)</p> <p>Automatic behavior: - Defaults to <code>false</code> if <code>runAsNonRoot: true</code> - Defaults to <code>true</code> if <code>privileged: true</code></p> <pre><code># Test privilege escalation blocking\nkubectl exec priv-esc-demo -- sudo id\n# Error: sudo: effective uid is not 0\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#linux-security-modules","title":"\ud83d\udee1\ufe0f Linux Security Modules","text":"","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#selinux","title":"SELinux","text":"<p>SELinux (Security-Enhanced Linux) uses labels for mandatory access control:</p> <pre><code>securityContext:\n  seLinuxOptions:\n    level: \"s0:c123,c456\"     # Multi-Level Security (MLS) level\n    role: \"system_r\"          # SELinux role\n    type: \"container_t\"       # SELinux type (process domain)\n    user: \"system_u\"          # SELinux user\n</code></pre> <p>Common SELinux types for containers: - <code>container_t</code>: Standard container domain - <code>container_init_t</code>: Init process domain - <code>container_kvm_t</code>: KVM/virtualization domain</p> <pre><code># Verify SELinux context\nkubectl exec selinux-demo -- cat /proc/self/attr/current\n# Output: system_u:system_r:container_t:s0:c123,c456\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#apparmor","title":"AppArmor","text":"<p>AppArmor profiles restrict program capabilities:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: apparmor-demo\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/app: localhost/k8s-apparmor-example\nspec:\n  containers:\n  - name: app\n    image: busybox:1.36\n</code></pre> <p>Profile types: - <code>runtime/default</code>: Default Docker/containerd profile - <code>localhost/&lt;profile&gt;</code>: Custom profile loaded on node - <code>unconfined</code>: No AppArmor restrictions</p> <p>Check AppArmor status: <pre><code># On node\naa-status | grep k8s\n\n# In container\ncat /proc/self/attr/current\n# Output: k8s-apparmor-example (enforce)\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#seccomp","title":"Seccomp","text":"<p>Seccomp (Secure Computing Mode) filters system calls:</p> <pre><code>securityContext:\n  seccompProfile:\n    type: RuntimeDefault        # Use runtime's default profile\n    # OR\n    type: Localhost\n    localhostProfile: profiles/audit.json   # Custom profile on node\n</code></pre> <p>Profile types: - <code>RuntimeDefault</code>: Container runtime's default (recommended) - <code>Localhost</code>: Custom profile at <code>/var/lib/kubelet/seccomp/&lt;profile&gt;</code> - <code>Unconfined</code>: No seccomp filtering (not recommended)</p> <p>Kubernetes 1.27+: <code>RuntimeDefault</code> is the default seccomp profile.</p> <pre><code># Verify seccomp profile\nkubectl exec seccomp-demo -- grep Seccomp /proc/1/status\n# Seccomp: 2  (2 = filtering enabled)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#pod-security-standards-pss","title":"\ud83d\udcca Pod Security Standards (PSS)","text":"<p>Kubernetes defines three security policies with increasing restrictions:</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#1-privileged-unrestricted","title":"1. Privileged (Unrestricted)","text":"<p>No restrictions\u2014allows all workloads:</p> <pre><code># Everything allowed, including:\nsecurityContext:\n  privileged: true\n  hostNetwork: true\n  hostPID: true\n  hostIPC: true\n</code></pre> <p>Use cases: System daemons, CNI plugins, kube-proxy</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#2-baseline-minimally-restrictive","title":"2. Baseline (Minimally Restrictive)","text":"<p>Prevents known privilege escalations:</p> <pre><code># \u2705 Allowed\nsecurityContext:\n  runAsUser: 1000\n  capabilities:\n    add: [\"NET_BIND_SERVICE\"]   # Subset of safe capabilities\n\n# \u274c Prohibited\nsecurityContext:\n  privileged: true              # No privileged containers\n  hostNetwork: true             # No host networking\n  hostPID: true                 # No host PID namespace\n  hostIPC: true                 # No host IPC namespace\n  hostPath: true                # No hostPath volumes\n</code></pre> <p>Baseline restrictions: - No privileged containers - No host namespaces (network, PID, IPC) - No hostPath volumes - Limited capabilities (subset allowed) - SELinux type restrictions</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#3-restricted-hardened","title":"3. Restricted (Hardened)","text":"<p>Best practice security controls (most restrictive):</p> <pre><code># \u2705 Required configuration\nsecurityContext:\n  runAsNonRoot: true                    # Must be non-root\n  allowPrivilegeEscalation: false       # No privilege escalation\n  capabilities:\n    drop: [\"ALL\"]                       # Drop all capabilities\n  seccompProfile:\n    type: RuntimeDefault                # Seccomp required\n</code></pre> <p>Restricted requirements: - Must run as non-root (<code>runAsNonRoot: true</code>) - Drop ALL capabilities - No privilege escalation - Read-only root filesystem (recommended) - Seccomp profile required (<code>RuntimeDefault</code> or custom) - Limited volume types (no hostPath, no <code>*</code>)</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#pss-comparison-table","title":"PSS Comparison Table","text":"Requirement Privileged Baseline Restricted Privileged containers \u2705 Allowed \u274c Forbidden \u274c Forbidden Host namespaces \u2705 Allowed \u274c Forbidden \u274c Forbidden hostPath volumes \u2705 Allowed \u274c Forbidden \u274c Forbidden runAsNonRoot Optional Optional \u2705 Required Capabilities All allowed Subset allowed \u274c Must drop ALL allowPrivilegeEscalation Allowed Allowed \u274c Must be false Seccomp Optional Optional \u2705 RuntimeDefault required Read-only root FS Optional Optional Recommended","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#pod-security-admission","title":"\ud83c\udfdb\ufe0f Pod Security Admission","text":"<p>Pod Security Admission enforces PSS policies at the namespace level (Kubernetes 1.23+).</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#enforcement-modes","title":"Enforcement Modes","text":"<p>Three modes control policy enforcement:</p> Mode Behavior Use Case enforce Reject non-compliant Pods Production enforcement audit Allow but log violations Monitoring compliance warn Allow but show warnings Developer feedback","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#applying-pss-to-namespaces","title":"Applying PSS to Namespaces","text":"<pre><code># Enforce baseline, warn/audit for restricted\nkubectl label namespace my-namespace \\\n  pod-security.kubernetes.io/enforce=baseline \\\n  pod-security.kubernetes.io/enforce-version=latest \\\n  pod-security.kubernetes.io/warn=restricted \\\n  pod-security.kubernetes.io/warn-version=latest \\\n  pod-security.kubernetes.io/audit=restricted \\\n  pod-security.kubernetes.io/audit-version=latest\n\n# View applied labels\nkubectl get namespace my-namespace --show-labels\n\n# Check namespace PSS config\nkubectl describe namespace my-namespace\n</code></pre> <pre><code>graph TD\n    Pod[Pod Creation Request]\n\n    Pod --&gt; Enforce{Enforce Mode&lt;br/&gt;baseline}\n\n    Enforce --&gt;|Compliant| Warn{Warn Mode&lt;br/&gt;restricted}\n    Enforce --&gt;|Violation| Reject[\u274c Pod Rejected]\n\n    Warn --&gt;|Compliant| Audit{Audit Mode&lt;br/&gt;restricted}\n    Warn --&gt;|Violation| WarnMsg[\u26a0\ufe0f Warning Shown&lt;br/&gt;Pod Allowed]\n\n    Audit --&gt;|Compliant| Allow[\u2705 Pod Created]\n    Audit --&gt;|Violation| AuditLog[\ud83d\udcdd Audit Log Entry&lt;br/&gt;Pod Allowed]\n\n    WarnMsg --&gt; Allow\n    AuditLog --&gt; Allow\n\n    style Reject fill:#ffe5e5\n    style Allow fill:#e8f5e8\n    style WarnMsg fill:#fff4e1\n    style AuditLog fill:#e1f5ff</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#cluster-wide-defaults","title":"Cluster-Wide Defaults","text":"<p>Apply PSS to all namespaces (except kube-system, kube-public, kube-node-lease):</p> <pre><code># Label all namespaces with privileged\nkubectl label --overwrite namespace --all \\\n  pod-security.kubernetes.io/enforce=privileged\n\n# Apply baseline to specific namespace\nkubectl label --overwrite namespace production \\\n  pod-security.kubernetes.io/enforce=baseline\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#exemptions","title":"Exemptions","text":"<p>Exclude specific namespaces, users, or RuntimeClasses from PSS via AdmissionConfiguration:</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: PodSecurity\n  configuration:\n    apiVersion: pod-security.admission.config.k8s.io/v1\n    kind: PodSecurityConfiguration\n    defaults:\n      enforce: \"baseline\"\n      enforce-version: \"latest\"\n      audit: \"restricted\"\n      audit-version: \"latest\"\n      warn: \"restricted\"\n      warn-version: \"latest\"\n    exemptions:\n      usernames: [\"system:serviceaccount:kube-system:daemon-set-controller\"]\n      runtimeClasses: [\"privileged-class\"]\n      namespaces: [\"kube-system\", \"monitoring\"]\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#troubleshooting-security-context-issues","title":"\ud83d\udee0\ufe0f Troubleshooting Security Context Issues","text":"","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#issue-1-runasnonroot-violation","title":"Issue 1: runAsNonRoot Violation","text":"<p>Error: <pre><code>Error: container has runAsNonRoot and image will run as root (pod: \"app-pod\", container: \"app\")\n</code></pre></p> <p>Diagnosis: <pre><code># Check image default user\ndocker inspect nginx:1.27 | grep -i user\n# Many images default to root (UID 0)\n\n# Verify Pod SecurityContext\nkubectl get pod app-pod -o yaml | grep -A 5 securityContext\n</code></pre></p> <p>Solution: <pre><code>securityContext:\n  runAsNonRoot: true\n  runAsUser: 101         # Explicit non-root UID\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#issue-2-read-only-filesystem-failures","title":"Issue 2: Read-Only Filesystem Failures","text":"<p>Error: <pre><code>Error: failed to create directory: read-only file system\n</code></pre></p> <p>Diagnosis: <pre><code># Application writing to root filesystem\nkubectl logs failing-pod\n# Check for write attempts to /, /tmp, /var\n</code></pre></p> <p>Solution: Mount writable volumes for required paths <pre><code>securityContext:\n  readOnlyRootFilesystem: true\nvolumeMounts:\n- name: tmp\n  mountPath: /tmp              # Writable tmpfs\n- name: cache\n  mountPath: /var/cache        # Writable cache\nvolumes:\n- name: tmp\n  emptyDir: {}\n- name: cache\n  emptyDir: {}\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#issue-3-pss-policy-violations","title":"Issue 3: PSS Policy Violations","text":"<p>Error: <pre><code>Error: pods \"privileged-pod\" is forbidden: violates PodSecurity \"baseline:latest\":\nprivileged (container \"app\" must not set securityContext.privileged=true)\n</code></pre></p> <p>Diagnosis: <pre><code># Check namespace PSS labels\nkubectl get namespace my-namespace -o yaml | grep pod-security\n\n# Identify violation\nkubectl get pod privileged-pod -o yaml | grep -A 10 securityContext\n</code></pre></p> <p>Solution: Adjust SecurityContext to comply with policy <pre><code># Remove or change to false\nsecurityContext:\n  privileged: false             # Or remove entirely\n  allowPrivilegeEscalation: false\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#issue-4-capability-permission-denied","title":"Issue 4: Capability Permission Denied","text":"<p>Error: <pre><code>Error: Operation not permitted (setting system time)\n</code></pre></p> <p>Diagnosis: <pre><code># Check current capabilities\nkubectl exec time-pod -- cat /proc/1/status | grep Cap\n\n# Verify SecurityContext\nkubectl get pod time-pod -o yaml | grep -A 5 capabilities\n</code></pre></p> <p>Solution: Add required capability <pre><code>securityContext:\n  capabilities:\n    add: [\"SYS_TIME\"]         # Add specific capability\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#cka-exam-practice-exercises","title":"\ud83d\udcdd CKA Exam Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#exercise-1-configure-non-root-user-with-fsgroup","title":"Exercise 1: Configure Non-Root User with fsGroup","text":"<p>Scenario: Create a Pod named <code>secure-app</code> in namespace <code>production</code> that: - Runs nginx:1.27 as UID 1000, GID 3000 - Mounts a volume at <code>/data</code> owned by GID 2000 - Uses read-only root filesystem</p> Solution <pre><code># Create namespace\nkubectl create namespace production\n\n# Create Pod\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-app\n  namespace: production\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n  containers:\n  - name: nginx\n    image: nginx:1.27\n    securityContext:\n      readOnlyRootFilesystem: true\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    - name: cache\n      mountPath: /var/cache/nginx\n    - name: run\n      mountPath: /var/run\n  volumes:\n  - name: data\n    emptyDir: {}\n  - name: cache\n    emptyDir: {}\n  - name: run\n    emptyDir: {}\nEOF\n\n# Verify user/group\nkubectl exec -n production secure-app -- id\n# Output: uid=1000 gid=3000 groups=2000\n\n# Verify volume ownership\nkubectl exec -n production secure-app -- ls -ld /data\n# Output: drwxrwsr-x ... 1000 2000 ... /data\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#exercise-2-drop-all-capabilities-add-net_bind_service","title":"Exercise 2: Drop All Capabilities, Add NET_BIND_SERVICE","text":"<p>Scenario: Create a Pod <code>web-server</code> that runs nginx on port 80 with minimal capabilities.</p> Solution <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web-server\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.27\n    ports:\n    - containerPort: 80\n    securityContext:\n      runAsNonRoot: true\n      runAsUser: 101                # nginx user\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n        add: [\"NET_BIND_SERVICE\"]   # Allow port 80\n      readOnlyRootFilesystem: true\n    volumeMounts:\n    - name: cache\n      mountPath: /var/cache/nginx\n    - name: run\n      mountPath: /var/run\n  volumes:\n  - name: cache\n    emptyDir: {}\n  - name: run\n    emptyDir: {}\nEOF\n\n# Verify capabilities\nkubectl exec web-server -- cat /proc/1/status | grep Cap\n# Verify only NET_BIND_SERVICE is set\n\n# Test port 80 binding\nkubectl port-forward web-server 8080:80\ncurl localhost:8080  # Should work\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#exercise-3-apply-baseline-pss-to-namespace","title":"Exercise 3: Apply Baseline PSS to Namespace","text":"<p>Scenario: Create namespace <code>dev-team</code> with: - Enforce: baseline - Warn and audit: restricted</p> Solution <pre><code># Create namespace\nkubectl create namespace dev-team\n\n# Apply PSS labels\nkubectl label namespace dev-team \\\n  pod-security.kubernetes.io/enforce=baseline \\\n  pod-security.kubernetes.io/enforce-version=latest \\\n  pod-security.kubernetes.io/warn=restricted \\\n  pod-security.kubernetes.io/warn-version=latest \\\n  pod-security.kubernetes.io/audit=restricted \\\n  pod-security.kubernetes.io/audit-version=latest\n\n# Verify labels\nkubectl get namespace dev-team -o yaml | grep pod-security\n\n# Test with compliant Pod\nkubectl run test-baseline -n dev-team --image=nginx:1.27\n# Success\n\n# Test with non-compliant Pod (should be rejected)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-privileged\n  namespace: dev-team\nspec:\n  containers:\n  - name: test\n    image: nginx:1.27\n    securityContext:\n      privileged: true\nEOF\n# Error: violates PodSecurity \"baseline:latest\": privileged\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#exercise-4-troubleshoot-runasnonroot-failure","title":"Exercise 4: Troubleshoot runAsNonRoot Failure","text":"<p>Scenario: Pod <code>failing-app</code> won't start with error: \"container has runAsNonRoot and image will run as root\". Fix it.</p> Solution <pre><code># View Pod specification\nkubectl get pod failing-app -o yaml | grep -A 10 securityContext\n# Output shows: runAsNonRoot: true (but no runAsUser specified)\n\n# Check image default user\ndocker inspect &lt;image&gt; | grep -i user\n# Most images default to UID 0 (root)\n\n# Fix: Add explicit runAsUser\nkubectl delete pod failing-app\n\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-app\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000         # Explicit non-root UID\n  containers:\n  - name: app\n    image: &lt;original-image&gt;\nEOF\n\n# Verify Pod starts successfully\nkubectl get pod failing-app\n# STATUS: Running\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#exercise-5-create-restricted-compliant-pod","title":"Exercise 5: Create Restricted-Compliant Pod","text":"<p>Scenario: Create a Pod that complies with the <code>restricted</code> PSS policy.</p> Solution <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: restricted-pod\nspec:\n  securityContext:\n    runAsNonRoot: true              # Required\n    runAsUser: 1000\n    fsGroup: 2000\n    seccompProfile:\n      type: RuntimeDefault          # Required\n  containers:\n  - name: app\n    image: nginx:1.27\n    securityContext:\n      allowPrivilegeEscalation: false   # Required\n      capabilities:\n        drop: [\"ALL\"]                   # Required\n        add: [\"NET_BIND_SERVICE\"]       # Only NET_BIND_SERVICE allowed\n      readOnlyRootFilesystem: true      # Recommended\n    volumeMounts:\n    - name: cache\n      mountPath: /var/cache/nginx\n    - name: run\n      mountPath: /var/run\n  volumes:\n  - name: cache\n    emptyDir: {}\n  - name: run\n    emptyDir: {}\nEOF\n\n# Verify compliance\nkubectl get pod restricted-pod\n# Should succeed in restricted namespace\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#cka-exam-tips","title":"\ud83c\udfaf CKA Exam Tips","text":"","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#fast-securitycontext-configuration","title":"Fast SecurityContext Configuration","text":"<pre><code># Dry-run to generate YAML quickly\nkubectl run secure-pod --image=nginx:1.27 --dry-run=client -o yaml &gt; pod.yaml\n\n# Add SecurityContext to generated YAML\n# Edit pod.yaml and add securityContext sections\n\n# Verify SecurityContext syntax\nkubectl apply -f pod.yaml --dry-run=server\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#common-securitycontext-patterns","title":"Common SecurityContext Patterns","text":"<pre><code># Non-root with capabilities (most common)\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop: [\"ALL\"]\n    add: [\"NET_BIND_SERVICE\"]\n\n# Read-only filesystem (hardened)\nsecurityContext:\n  readOnlyRootFilesystem: true\n  # Add writable volume mounts as needed\n\n# Restricted policy compliant (most secure)\nsecurityContext:\n  runAsNonRoot: true\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop: [\"ALL\"]\n  seccompProfile:\n    type: RuntimeDefault\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#pss-label-commands","title":"PSS Label Commands","text":"<pre><code># Apply baseline enforcement\nkubectl label --overwrite namespace &lt;ns&gt; \\\n  pod-security.kubernetes.io/enforce=baseline\n\n# Apply all three modes\nkubectl label --overwrite namespace &lt;ns&gt; \\\n  pod-security.kubernetes.io/enforce=baseline \\\n  pod-security.kubernetes.io/warn=restricted \\\n  pod-security.kubernetes.io/audit=restricted\n\n# Check namespace labels\nkubectl get namespace &lt;ns&gt; --show-labels | grep pod-security\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#quick-reference","title":"\ud83d\udcda Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#securitycontext-fields","title":"SecurityContext Fields","text":"<pre><code># Pod-level (applies to all containers)\nspec:\n  securityContext:\n    runAsUser: &lt;UID&gt;\n    runAsGroup: &lt;GID&gt;\n    runAsNonRoot: &lt;true|false&gt;\n    fsGroup: &lt;GID&gt;\n    fsGroupChangePolicy: &lt;Always|OnRootMismatch&gt;\n    seccompProfile:\n      type: &lt;RuntimeDefault|Localhost|Unconfined&gt;\n    seLinuxOptions:\n      level: &lt;MLS-level&gt;\n      role: &lt;SELinux-role&gt;\n      type: &lt;SELinux-type&gt;\n      user: &lt;SELinux-user&gt;\n    supplementalGroups: [&lt;GID-list&gt;]\n    sysctls: [...]\n\n# Container-level (overrides Pod-level)\nspec:\n  containers:\n  - name: app\n    securityContext:\n      runAsUser: &lt;UID&gt;\n      runAsGroup: &lt;GID&gt;\n      runAsNonRoot: &lt;true|false&gt;\n      privileged: &lt;true|false&gt;\n      allowPrivilegeEscalation: &lt;true|false&gt;\n      readOnlyRootFilesystem: &lt;true|false&gt;\n      capabilities:\n        add: [&lt;CAP-list&gt;]\n        drop: [&lt;CAP-list&gt;]\n      seccompProfile:\n        type: &lt;RuntimeDefault|Localhost|Unconfined&gt;\n      seLinuxOptions: {...}\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#pss-policy-requirements","title":"PSS Policy Requirements","text":"<pre><code># Privileged: No restrictions\n# Everything allowed\n\n# Baseline: Minimal restrictions\n# \u274c privileged: true\n# \u274c hostNetwork, hostPID, hostIPC, hostPath\n# \u2705 Limited capabilities allowed\n\n# Restricted: Best practice\n# \u2705 runAsNonRoot: true\n# \u2705 allowPrivilegeEscalation: false\n# \u2705 capabilities drop: [\"ALL\"]\n# \u2705 seccompProfile: type: RuntimeDefault\n</code></pre>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Previous: Post 15 - RBAC: Role-Based Access Control</li> <li>Next: Post 17 - Custom Resources and Operators (CRDs)</li> <li>Reference: Kubernetes Security Context Documentation</li> <li>Reference: Pod Security Standards</li> <li>Series: Kubernetes CKA Mastery</li> </ul>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"blog/2025/11/11/security-contexts-pod-security-standards/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>SecurityContext controls privilege and access at Pod/Container level</li> <li>runAsNonRoot enforces non-root execution (validated at runtime)</li> <li>Capabilities provide fine-grained privileges beyond root/non-root</li> <li>readOnlyRootFilesystem creates immutable containers</li> <li>privileged: true is dangerous\u2014avoid unless absolutely necessary</li> <li>Pod Security Standards define three policies: Privileged, Baseline, Restricted</li> <li>Pod Security Admission enforces PSS at namespace level</li> <li>Restricted policy is best practice for production workloads</li> <li>CKA exam tip: Master SecurityContext fields and PSS label commands</li> </ul> <p>Next Steps: Extend Kubernetes functionality with Custom Resource Definitions and Operators.</p>","tags":["kubernetes","k8s","cka-prep","security-contexts","security"]},{"location":"journal/","title":"Journal","text":"<p>Time-based log entries, learning notes, and progress updates.</p>"},{"location":"journal/#coming-soon","title":"Coming Soon","text":"<p>This section will contain:</p> <ul> <li>Learning Logs: Daily/weekly learning summaries</li> <li>Project Progress: Implementation updates</li> <li>Experiments: Technical experiments and findings</li> <li>Quick Notes: Short observations and discoveries</li> </ul> <p>Check back soon or watch the repository for updates.</p>"},{"location":"kb/","title":"Knowledge Base","text":"<p>Evergreen reference material, technical documentation, and curated resources.</p>"},{"location":"kb/#coming-soon","title":"Coming Soon","text":"<p>This section will contain:</p> <ul> <li>System Design Patterns: Reusable architectural patterns</li> <li>Protocol References: Detailed protocol documentation</li> <li>Tool Guides: Configuration and usage guides</li> <li>Troubleshooting Playbooks: Common issues and solutions</li> <li>Performance Baselines: Benchmark data and analysis</li> </ul> <p>Check back soon or watch the repository for updates.</p>"},{"location":"kubernetes/","title":"Kubernetes CKA Mastery","text":"<p>Complete hands-on guide to Kubernetes administration and CKA certification</p>"},{"location":"kubernetes/#about-the-cka-exam","title":"\ud83c\udfaf About the CKA Exam","text":"<p>The Certified Kubernetes Administrator (CKA) certification demonstrates proficiency in Kubernetes cluster administration, troubleshooting, and operations.</p>"},{"location":"kubernetes/#exam-details","title":"Exam Details","text":"<ul> <li>Duration: 2 hours</li> <li>Format: ~17 performance-based tasks (100% hands-on terminal work)</li> <li>Pass Score: 66%</li> <li>Cost: $445 (includes one free retake)</li> <li>Environment: Remote proctored, browser-based terminal</li> </ul>"},{"location":"kubernetes/#exam-domains-weights","title":"Exam Domains &amp; Weights","text":"<pre><code>pie title CKA Exam Domain Distribution\n    \"Troubleshooting\" : 30\n    \"Cluster Architecture\" : 25\n    \"Services &amp; Networking\" : 20\n    \"Workloads &amp; Scheduling\" : 15\n    \"Storage\" : 10</code></pre> Domain Weight Focus Areas Troubleshooting 30% Cluster/node issues, application debugging, monitoring Cluster Architecture 25% Installation, upgrades, RBAC, security, CRDs Services &amp; Networking 20% Services, Ingress, Gateway API, Network Policies Workloads &amp; Scheduling 15% Deployments, scheduling, pod configuration Storage 10% PV/PVC, ConfigMaps, Secrets, StorageClasses"},{"location":"kubernetes/#learning-path","title":"\ud83d\udcda Learning Path","text":"<p>This series covers 22 comprehensive posts organized into 7 phases, following the optimal learning sequence for CKA exam success.</p> <pre><code>graph TD\n    A[Phase 1: Foundations] --&gt; B[Phase 2: Workloads]\n    B --&gt; C[Phase 3: Networking]\n    C --&gt; D[Phase 4: Storage]\n    D --&gt; E[Phase 5: Security]\n    E --&gt; F[Phase 6: Advanced Config]\n    F --&gt; G[Phase 7: Troubleshooting]\n\n    A --&gt; A1[Architecture]\n    A --&gt; A2[Lab Setup]\n    A --&gt; A3[kubectl Basics]\n    A --&gt; A4[YAML &amp; Objects]\n    A --&gt; A5[Namespaces]\n\n    B --&gt; B1[Pods]\n    B --&gt; B2[Deployments]\n    B --&gt; B3[Scheduling]\n\n    C --&gt; C1[Services]\n    C --&gt; C2[Ingress/Gateway]\n    C --&gt; C3[Network Policies]\n    C --&gt; C4[DNS]\n\n    D --&gt; D1[PV/PVC]\n    D --&gt; D2[ConfigMaps/Secrets]\n\n    E --&gt; E1[RBAC]\n    E --&gt; E2[Security Contexts]\n    E --&gt; E3[CRDs/Operators]\n\n    F --&gt; F1[Helm]\n    F --&gt; F2[Kustomize]\n\n    G --&gt; G1[Cluster Troubleshooting]\n    G --&gt; G2[App Troubleshooting]\n    G --&gt; G3[Monitoring]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e8\n    style D fill:#f3e5f5\n    style E fill:#ffe5e5\n    style F fill:#fff9e5\n    style G fill:#ffe5f0</code></pre>"},{"location":"kubernetes/#phase-1-foundations-5-posts","title":"\ud83c\udfd7\ufe0f Phase 1: Foundations (5 posts)","text":"<p>Build your foundational knowledge of Kubernetes architecture and essential tools.</p>"},{"location":"kubernetes/#1-kubernetes-architecture-fundamentals","title":"1. Kubernetes Architecture Fundamentals","text":"<p>Control plane components, worker nodes, etcd, API server, scheduler, controller manager Tags: <code>kubernetes</code> <code>architecture</code> <code>fundamentals</code> <code>cka-prep</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#2-setting-up-your-kubernetes-lab-environment","title":"2. Setting Up Your Kubernetes Lab Environment","text":"<p>kubeadm, Minikube, kind, kubectl installation, kubeconfig management Tags: <code>kubernetes</code> <code>installation</code> <code>lab-setup</code> <code>kubeadm</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#3-kubectl-essentials-your-kubernetes-swiss-army-knife","title":"3. kubectl Essentials: Your Kubernetes Swiss Army Knife","text":"<p>Master kubectl commands, aliases, output formats, context switching, imperative vs declarative Tags: <code>kubernetes</code> <code>kubectl</code> <code>cli</code> <code>basics</code> Domain: All (foundational skill)</p>"},{"location":"kubernetes/#4-understanding-kubernetes-objects-and-yaml-manifests","title":"4. Understanding Kubernetes Objects and YAML Manifests","text":"<p>API objects, YAML syntax, metadata, spec, status, labels, annotations, selectors Tags: <code>kubernetes</code> <code>yaml</code> <code>objects</code> <code>manifests</code> Domain: All (foundational skill)</p>"},{"location":"kubernetes/#5-namespaces-and-resource-quotas","title":"5. Namespaces and Resource Quotas","text":"<p>Namespace isolation, resource quotas, limit ranges, default namespace management Tags: <code>kubernetes</code> <code>namespaces</code> <code>resource-management</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#phase-2-workloads-scheduling-3-posts","title":"\u2699\ufe0f Phase 2: Workloads &amp; Scheduling (3 posts)","text":"<p>Master pod management, deployments, and advanced scheduling techniques.</p>"},{"location":"kubernetes/#6-pods-the-atomic-unit-of-kubernetes","title":"6. Pods: The Atomic Unit of Kubernetes","text":"<p>Pod lifecycle, init containers, sidecar patterns, multi-container communication Tags: <code>kubernetes</code> <code>pods</code> <code>workloads</code> <code>containers</code> Domain: Workloads &amp; Scheduling (15%)</p>"},{"location":"kubernetes/#7-deployments-replicasets-and-rolling-updates","title":"7. Deployments, ReplicaSets, and Rolling Updates","text":"<p>Deployments, ReplicaSets, DaemonSets, StatefulSets, rollouts, rollback strategies Tags: <code>kubernetes</code> <code>deployments</code> <code>replicasets</code> <code>workloads</code> Domain: Workloads &amp; Scheduling (15%)</p>"},{"location":"kubernetes/#8-advanced-scheduling-taints-tolerations-and-affinity","title":"8. Advanced Scheduling: Taints, Tolerations, and Affinity","text":"<p>Node selectors, taints/tolerations, node/pod affinity, anti-affinity, priority classes Tags: <code>kubernetes</code> <code>scheduling</code> <code>advanced</code> <code>affinity</code> Domain: Workloads &amp; Scheduling (15%)</p>"},{"location":"kubernetes/#phase-3-services-networking-4-posts","title":"\ud83c\udf10 Phase 3: Services &amp; Networking (4 posts)","text":"<p>Deep dive into Kubernetes networking, service discovery, and traffic management.</p>"},{"location":"kubernetes/#9-kubernetes-services-exposing-your-applications","title":"9. Kubernetes Services: Exposing Your Applications","text":"<p>ClusterIP, NodePort, LoadBalancer, ExternalName, service discovery, endpoints Tags: <code>kubernetes</code> <code>services</code> <code>networking</code> <code>service-discovery</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#10-ingress-and-gateway-api-modern-traffic-management","title":"10. Ingress and Gateway API: Modern Traffic Management","text":"<p>Ingress controllers, Ingress rules, Gateway API (GatewayClass, Gateway, HTTPRoute) Tags: <code>kubernetes</code> <code>ingress</code> <code>gateway-api</code> <code>traffic-management</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#11-network-policies-securing-pod-communication","title":"11. Network Policies: Securing Pod Communication","text":"<p>NetworkPolicy resources, ingress/egress rules, pod/namespace selectors, isolation Tags: <code>kubernetes</code> <code>network-policies</code> <code>security</code> <code>networking</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#12-coredns-and-service-discovery-deep-dive","title":"12. CoreDNS and Service Discovery Deep Dive","text":"<p>CoreDNS configuration, DNS for Services and Pods, troubleshooting DNS issues Tags: <code>kubernetes</code> <code>dns</code> <code>coredns</code> <code>service-discovery</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#phase-4-storage-2-posts","title":"\ud83d\udcbe Phase 4: Storage (2 posts)","text":"<p>Understand persistent storage and configuration management in Kubernetes.</p>"},{"location":"kubernetes/#13-persistent-volumes-and-claims-stateful-storage","title":"13. Persistent Volumes and Claims: Stateful Storage","text":"<p>PV, PVC, StorageClass, access modes, reclaim policies, dynamic provisioning Tags: <code>kubernetes</code> <code>storage</code> <code>persistent-volumes</code> <code>stateful</code> Domain: Storage (10%)</p>"},{"location":"kubernetes/#14-configmaps-secrets-and-volume-mounts","title":"14. ConfigMaps, Secrets, and Volume Mounts","text":"<p>ConfigMaps, Secrets, volume mounts, environment variables, projected volumes Tags: <code>kubernetes</code> <code>configmaps</code> <code>secrets</code> <code>configuration</code> Domain: Storage (10%)</p>"},{"location":"kubernetes/#phase-5-security-configuration-3-posts","title":"\ud83d\udd12 Phase 5: Security &amp; Configuration (3 posts)","text":"<p>Secure your cluster with RBAC, security contexts, and extensibility.</p>"},{"location":"kubernetes/#15-rbac-role-based-access-control","title":"15. RBAC: Role-Based Access Control","text":"<p>Roles, ClusterRoles, RoleBindings, ClusterRoleBindings, ServiceAccounts Tags: <code>kubernetes</code> <code>rbac</code> <code>security</code> <code>access-control</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#16-security-contexts-and-pod-security-standards","title":"16. Security Contexts and Pod Security Standards","text":"<p>SecurityContext, Pod Security Admission, privileged containers, capabilities, PSS Tags: <code>kubernetes</code> <code>security</code> <code>pod-security</code> <code>hardening</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#17-custom-resources-and-operators-crds","title":"17. Custom Resources and Operators (CRDs)","text":"<p>CustomResourceDefinitions, custom controllers, Operators, CRD inspection Tags: <code>kubernetes</code> <code>crds</code> <code>operators</code> <code>extensibility</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#phase-6-advanced-configuration-2-posts","title":"\ud83d\udd27 Phase 6: Advanced Configuration (2 posts)","text":"<p>Master Helm and Kustomize for production-grade configuration management.</p>"},{"location":"kubernetes/#18-helm-kubernetes-package-manager","title":"18. Helm: Kubernetes Package Manager","text":"<p>Helm charts, templating, values files, releases, hooks, chart repositories Tags: <code>kubernetes</code> <code>helm</code> <code>package-management</code> <code>charts</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#19-kustomize-template-free-configuration-management","title":"19. Kustomize: Template-Free Configuration Management","text":"<p>Kustomize bases, overlays, patches, transformers, generators, GitOps patterns Tags: <code>kubernetes</code> <code>kustomize</code> <code>configuration</code> <code>gitops</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#phase-7-troubleshooting-monitoring-3-posts","title":"\ud83d\udd0d Phase 7: Troubleshooting &amp; Monitoring (3 posts)","text":"<p>Become an expert at diagnosing and resolving Kubernetes issues.</p>"},{"location":"kubernetes/#20-troubleshooting-clusters-nodes-and-components","title":"20. Troubleshooting Clusters, Nodes, and Components","text":"<p>Node issues, control plane debugging, certificate problems, etcd health checks Tags: <code>kubernetes</code> <code>troubleshooting</code> <code>debugging</code> <code>cluster-health</code> Domain: Troubleshooting (30%)</p>"},{"location":"kubernetes/#21-application-troubleshooting-and-log-analysis","title":"21. Application Troubleshooting and Log Analysis","text":"<p>Pod debugging, container logs, exec commands, ephemeral containers, event analysis Tags: <code>kubernetes</code> <code>troubleshooting</code> <code>logs</code> <code>debugging</code> Domain: Troubleshooting (30%)</p>"},{"location":"kubernetes/#22-monitoring-metrics-and-resource-management","title":"22. Monitoring, Metrics, and Resource Management","text":"<p>Metrics Server, resource requests/limits, HPA, VPA, monitoring stack integration Tags: <code>kubernetes</code> <code>monitoring</code> <code>metrics</code> <code>autoscaling</code> Domain: Troubleshooting (30%)</p>"},{"location":"kubernetes/#how-to-use-this-series","title":"\ud83d\udcd6 How to Use This Series","text":""},{"location":"kubernetes/#recommended-study-approach","title":"Recommended Study Approach","text":"<ol> <li>Follow the Order: Posts are sequenced for optimal learning progression</li> <li>Hands-On Practice: Set up a lab environment (Post 2) and practice every command</li> <li>Take Notes: Create your own command cheat sheets as you progress</li> <li>Review Diagrams: Study the architecture diagrams to understand component relationships</li> <li>Do the Exercises: Complete practice tasks at the end of each post</li> <li>Cross-Reference: Use links between posts to review related concepts</li> </ol>"},{"location":"kubernetes/#study-timeline","title":"Study Timeline","text":"<ul> <li>Intensive: 4-6 weeks (1 post per day)</li> <li>Standard: 8-12 weeks (2-3 posts per week)</li> <li>Relaxed: 3-4 months (1-2 posts per week)</li> </ul>"},{"location":"kubernetes/#exam-preparation-tips","title":"Exam Preparation Tips","text":"<p>\u2705 Do: - Practice in a terminal environment (exam is 100% command-line) - Use <code>kubectl</code> imperative commands for speed - Master <code>kubectl explain</code> and <code>-h</code> flags for in-exam reference - Time yourself on practice exercises - Focus heavily on Troubleshooting (30% weight)</p> <p>\u274c Don't: - Memorize YAML templates (use <code>kubectl</code> generators instead) - Ignore troubleshooting topics (highest exam weight) - Skip hands-on practice (reading alone is insufficient) - Forget about time management (2 hours goes fast)</p>"},{"location":"kubernetes/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<p>Before starting this series, you should have:</p> <ul> <li>Basic understanding of Linux command line</li> <li>Familiarity with containerization concepts (Docker)</li> <li>Access to a Linux/macOS machine or Windows with WSL2</li> <li>Willingness to practice hands-on (not just read)</li> </ul>"},{"location":"kubernetes/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>Official CKA Exam Page</li> <li>Kubernetes Official Documentation</li> <li>kubectl Command Reference</li> <li>CKA Curriculum (Official)</li> </ul>"},{"location":"kubernetes/#ready-to-start","title":"\ud83d\ude80 Ready to Start?","text":"<p>Begin your journey with Post 1: Kubernetes Architecture Fundamentals and work through the series systematically.</p> <p>Good luck with your CKA certification! \ud83c\udf93</p> <p>Last Updated: 2025-11-11 Series Status: Complete (All 22 posts published \u2705\ud83c\udf89) Total Posts: 22</p>"},{"location":"principles/","title":"Principles","text":"<p>Engineering principles, design philosophies, and decision-making frameworks.</p>"},{"location":"principles/#coming-soon","title":"Coming Soon","text":"<p>This section will explore:</p> <ul> <li>Architecture Principles: Foundational design guidelines</li> <li>Performance Principles: Optimization philosophies</li> <li>Reliability Principles: Building resilient systems</li> <li>Scalability Principles: Growing systems effectively</li> <li>Simplicity Principles: Managing complexity</li> </ul> <p>Check back soon or watch the repository for updates.</p>"},{"location":"blog/archive/2025/","title":"November 2025","text":""},{"location":"blog/category/kubernetes/","title":"Kubernetes","text":""},{"location":"blog/category/troubleshooting/","title":"Troubleshooting","text":""},{"location":"blog/category/configuration/","title":"Configuration","text":""},{"location":"blog/category/architecture/","title":"Architecture","text":""},{"location":"blog/category/cli/","title":"CLI","text":""},{"location":"blog/category/networking/","title":"Networking","text":""},{"location":"blog/category/workloads/","title":"Workloads","text":""},{"location":"blog/category/infrastructure/","title":"Infrastructure","text":""},{"location":"blog/category/security/","title":"Security","text":""},{"location":"blog/category/monitoring/","title":"Monitoring","text":""},{"location":"blog/category/storage/","title":"Storage","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/archive/2025/page/2/","title":"November 2025","text":""},{"location":"blog/archive/2025/page/3/","title":"November 2025","text":""},{"location":"blog/category/kubernetes/page/2/","title":"Kubernetes","text":""},{"location":"blog/category/kubernetes/page/3/","title":"Kubernetes","text":""}]}