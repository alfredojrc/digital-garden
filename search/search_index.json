{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Digital Garden","text":"<p>A living knowledge base for distributed systems, storage architecture, and infrastructure engineering</p>"},{"location":"#latest-posts","title":"\ud83d\udcdd Latest Posts","text":"<p>\u27a1\ufe0f View All Posts</p>"},{"location":"#kubectl-essentials-your-kubernetes-swiss-army-knife","title":"kubectl Essentials: Your Kubernetes Swiss Army Knife","text":"<p>Kubernetes \u00b7 13 min read</p> <p>Master kubectl commands, output formats, and productivity patterns essential for CKA exam success. Learn imperative vs declarative approaches, JSONPath queries, context management, and time-saving aliases for Kubernetes cluster management.</p> <p>Topics: kubernetes, k8s, cka-prep, kubectl</p> <p>Read more \u2192</p>"},{"location":"#setting-up-your-kubernetes-lab-environment","title":"Setting Up Your Kubernetes Lab Environment","text":"<p>Kubernetes \u00b7 Infrastructure \u00b7 12 min read</p> <p>Complete guide to setting up Kubernetes lab environments for CKA preparation. Covers kubeadm cluster setup, Minikube for local development, kind for testing, kubectl installation, and kubeconfig management with practical examples.</p> <p>Topics: kubernetes, k8s, cka-prep, kubeadm, kubectl, minikube</p> <p>Read more \u2192</p>"},{"location":"#kubernetes-architecture-fundamentals","title":"Kubernetes Architecture Fundamentals","text":"<p>Kubernetes \u00b7 Architecture \u00b7 15 min read</p> <p>Deep dive into Kubernetes cluster architecture, control plane components, and the distributed systems design that powers container orchestration at scale. Essential foundations for CKA certification with comprehensive diagrams, kubectl commands, and hands-on practice exercises.</p> <p>Topics: kubernetes, k8s, cka-prep, architecture, control-plane, kubectl</p> <p>Read more \u2192</p>"},{"location":"#high-performance-pnfs-v42-distributed-storage-architecture","title":"High-Performance pNFS v4.2 Distributed Storage Architecture","text":"<p>Storage \u00b7 Architecture \u00b7 12 min read</p> <p>A deep dive into building a clustered, high-availability parallel NFS storage system with load-balanced metadata servers, NVMe-backed storage nodes, and low-latency interconnects. Features production-grade architecture with InfiniBand/RoCE fabrics, active-active MDS clustering, and parallel data paths achieving 28 GB/s aggregate throughput.</p> <p>Topics: pNFS v4.2, distributed storage, NVMe, InfiniBand, RoCE, high availability, load balancing, metadata clustering</p> <p>Read more \u2192</p>"},{"location":"#browse-by-theme","title":"\ud83d\uddc2\ufe0f Browse by Theme","text":"<ul> <li> <p> Kubernetes CKA Mastery</p> <p>Complete CKA certification prep with 22 comprehensive posts</p> <p>Explore Kubernetes \u2192</p> </li> <li> <p> AI &amp; Automation</p> <p>Agents, MCP servers, tool orchestration, LLM workflows</p> <p>Explore AI \u2192</p> </li> <li> <p> Cloud Infrastructure</p> <p>GCP, Azure, AWS, multi-cloud architectures</p> <p>Explore Cloud \u2192</p> </li> <li> <p> Infrastructure as Code</p> <p>Terraform, Ansible, GitOps, automation</p> <p>Explore IaC \u2192</p> </li> <li> <p> On-Premise Systems</p> <p>Bare metal, datacenter, networking, storage</p> <p>Explore On-Prem \u2192</p> </li> <li> <p> Cybersecurity</p> <p>Security architecture, hardening, compliance</p> <p>Explore Security \u2192</p> </li> <li> <p> Storage &amp; Networking</p> <p>File systems, protocols, high-performance I/O</p> <p>Explore Storage \u2192</p> </li> </ul>"},{"location":"#additional-resources","title":"\ud83e\udded Additional Resources","text":"<ul> <li>Knowledge Base: Curated reference material and evergreen documentation</li> <li>Principles: Engineering principles and design patterns</li> <li>Journal: Progress logs and learning notes</li> <li>Tags: Browse all content by tag</li> </ul> <p>This garden grows continuously \u00b7 Follow on GitHub \u00b7 RSS Feed</p>"},{"location":"blog/","title":"Blog","text":"<p>Technical deep-dives into distributed systems, storage architecture, and infrastructure engineering.</p>"},{"location":"blog/#navigate","title":"Navigate","text":"<ul> <li>Browse by Tags for topic-based exploration</li> <li>View the Archive for chronological browsing</li> <li>Filter by Categories below</li> </ul> <p>All posts include estimated read times and are optimized for both technical depth and practical application.</p>"},{"location":"blog/tags/","title":"Tag Index","text":"<p>Browse all posts by tag. Tags provide fine-grained topic classification across multiple dimensions.</p>"},{"location":"blog/tags/#tag:infiniband","title":"InfiniBand","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:nvme","title":"NVMe","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:roce","title":"RoCE","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:affinity","title":"affinity","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:architecture","title":"architecture","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:cka-prep","title":"cka-prep","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> <li>            Kubernetes Architecture Fundamentals          </li> <li>            Kubernetes Namespaces and Resource Quotas          </li> <li>            Pods: The Atomic Unit of Kubernetes          </li> <li>            Setting Up Your Kubernetes Lab Environment          </li> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:clustering","title":"clustering","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:command-line","title":"command-line","text":"<ul> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:containers","title":"containers","text":"<ul> <li>            Pods: The Atomic Unit of Kubernetes          </li> </ul>"},{"location":"blog/tags/#tag:control-plane","title":"control-plane","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:deployments","title":"deployments","text":"<ul> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> </ul>"},{"location":"blog/tags/#tag:distributed-storage","title":"distributed-storage","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:high-availability","title":"high-availability","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:k8s","title":"k8s","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> <li>            Kubernetes Architecture Fundamentals          </li> <li>            Kubernetes Namespaces and Resource Quotas          </li> <li>            Pods: The Atomic Unit of Kubernetes          </li> <li>            Setting Up Your Kubernetes Lab Environment          </li> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:kubeadm","title":"kubeadm","text":"<ul> <li>            Setting Up Your Kubernetes Lab Environment          </li> </ul>"},{"location":"blog/tags/#tag:kubectl","title":"kubectl","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> <li>            Setting Up Your Kubernetes Lab Environment          </li> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:kubernetes","title":"kubernetes","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> <li>            Kubernetes Architecture Fundamentals          </li> <li>            Kubernetes Namespaces and Resource Quotas          </li> <li>            Pods: The Atomic Unit of Kubernetes          </li> <li>            Setting Up Your Kubernetes Lab Environment          </li> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> <li>            kubectl Essentials: Your Kubernetes Swiss Army Knife          </li> </ul>"},{"location":"blog/tags/#tag:labels","title":"labels","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag:load-balancing","title":"load-balancing","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:manifests","title":"manifests","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag:metadata","title":"metadata","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:minikube","title":"minikube","text":"<ul> <li>            Setting Up Your Kubernetes Lab Environment          </li> </ul>"},{"location":"blog/tags/#tag:namespaces","title":"namespaces","text":"<ul> <li>            Kubernetes Namespaces and Resource Quotas          </li> </ul>"},{"location":"blog/tags/#tag:node-selector","title":"node-selector","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:objects","title":"objects","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag:pnfs","title":"pNFS","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:parallel-io","title":"parallel-io","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:pods","title":"pods","text":"<ul> <li>            Pods: The Atomic Unit of Kubernetes          </li> </ul>"},{"location":"blog/tags/#tag:quotas","title":"quotas","text":"<ul> <li>            Kubernetes Namespaces and Resource Quotas          </li> </ul>"},{"location":"blog/tags/#tag:replicasets","title":"replicasets","text":"<ul> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> </ul>"},{"location":"blog/tags/#tag:resource-management","title":"resource-management","text":"<ul> <li>            Kubernetes Namespaces and Resource Quotas          </li> </ul>"},{"location":"blog/tags/#tag:rolling-updates","title":"rolling-updates","text":"<ul> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> </ul>"},{"location":"blog/tags/#tag:scheduling","title":"scheduling","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:selectors","title":"selectors","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag:taints","title":"taints","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:tolerations","title":"tolerations","text":"<ul> <li>            Advanced Scheduling: Taints, Tolerations, and Affinity          </li> </ul>"},{"location":"blog/tags/#tag:workloads","title":"workloads","text":"<ul> <li>            Deployments, ReplicaSets, and Rolling Updates          </li> <li>            Pods: The Atomic Unit of Kubernetes          </li> </ul>"},{"location":"blog/tags/#tag:yaml","title":"yaml","text":"<ul> <li>            Understanding Kubernetes Objects and YAML Manifests          </li> </ul>"},{"location":"blog/tags/#tag-descriptions","title":"Tag Descriptions","text":"<ul> <li>pNFS: Parallel NFS protocol and implementations</li> <li>distributed-storage: Multi-node storage architectures</li> <li>NVMe: Non-Volatile Memory Express technologies</li> <li>high-availability: HA clustering and failover systems</li> <li>load-balancing: Traffic distribution and request routing</li> <li>metadata: Metadata server design and optimization</li> <li>clustering: Cluster management and coordination</li> <li>InfiniBand: InfiniBand networking and RDMA</li> <li>RoCE: RDMA over Converged Ethernet</li> <li>parallel-io: Parallel I/O patterns and optimization</li> <li>file-systems: File system design and internals</li> <li>linux: Linux kernel and system programming</li> <li>performance-tuning: System optimization techniques</li> <li>scalability: Scaling strategies and patterns</li> </ul>"},{"location":"blog/2025/11/11/kubectl-essentials/","title":"kubectl Essentials: Your Kubernetes Swiss Army Knife","text":"<p>Master kubectl for CKA exam success. Learn imperative commands for speed, output formats for precision, and productivity patterns that save critical exam minutes.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#overview","title":"Overview","text":"<p>kubectl is the command-line interface to Kubernetes and your primary tool during the CKA exam. While the exam allows access to official documentation, proficiency with kubectl commands determines whether you finish in time.</p> <p>CKA Exam Domain: All domains (kubectl is used for every task)</p> <p>Key Insight: CKA exam success correlates directly with kubectl speed. Candidates who master imperative commands and output formats consistently score higher and finish with time to spare.</p> <p>What You'll Learn: - Essential kubectl commands by category - Imperative vs declarative approaches for exam efficiency - Output formats and JSONPath for data extraction - Context and namespace management patterns - kubectl explain for in-exam documentation - Time-saving aliases and autocomplete workflows</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#command-categories-overview","title":"Command Categories Overview","text":"<p>kubectl commands follow functional patterns that map to exam task types.</p> <pre><code>graph TB\n    subgraph \"Resource Lifecycle\"\n        CREATE[Create/Generate&lt;br/&gt;run, create, expose]\n        READ[Inspect&lt;br/&gt;get, describe, logs]\n        UPDATE[Modify&lt;br/&gt;edit, patch, scale, set]\n        DELETE[Remove&lt;br/&gt;delete]\n    end\n\n    subgraph \"Debugging &amp; Troubleshooting\"\n        EXEC[Execute&lt;br/&gt;exec, cp]\n        DEBUG[Debug&lt;br/&gt;debug, logs, port-forward]\n        METRICS[Metrics&lt;br/&gt;top]\n    end\n\n    subgraph \"Configuration &amp; Context\"\n        CONFIG[Configure&lt;br/&gt;config, explain]\n        APPLY[Apply&lt;br/&gt;apply, replace]\n        ROLLOUT[Rollouts&lt;br/&gt;rollout, scale]\n    end\n\n    CREATE --&gt; UPDATE\n    UPDATE --&gt; ROLLOUT\n    READ --&gt; DEBUG\n    DEBUG --&gt; EXEC\n    CONFIG --&gt; APPLY\n\n    style CREATE fill:#e1f5ff\n    style READ fill:#e8f5e8\n    style UPDATE fill:#fff4e1\n    style DELETE fill:#ffe5e5\n    style DEBUG fill:#f5e1ff</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#essential-commands-by-category","title":"Essential Commands by Category","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#resource-creation-commands","title":"Resource Creation Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-run-pods","title":"kubectl run (Pods)","text":"<p>Purpose: Create pods imperatively - fastest method for simple pods</p> <pre><code># Basic pod creation\nkubectl run nginx --image=nginx\n\n# Pod with port specification\nkubectl run nginx --image=nginx --port=80\n\n# Pod with labels\nkubectl run nginx --image=nginx --labels=\"app=web,tier=frontend\"\n\n# Pod with environment variables\nkubectl run nginx --image=nginx --env=\"DB_HOST=mysql\" --env=\"DB_PORT=3306\"\n\n# Generate YAML without creating (CRITICAL for exam)\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n</code></pre> <p>Exam Pattern: Use <code>--dry-run=client -o yaml</code> to generate templates, then edit as needed.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-create-workloads","title":"kubectl create (Workloads)","text":"<p>Purpose: Generate deployments, jobs, services with imperative commands</p> <pre><code># Deployment creation\nkubectl create deployment webapp --image=nginx --replicas=3\n\n# Job creation\nkubectl create job hello --image=busybox:1.28 -- echo \"Hello World\"\n\n# CronJob creation\nkubectl create cronjob hello \\\n  --image=busybox:1.28 \\\n  --schedule=\"*/5 * * * *\" \\\n  -- /bin/sh -c \"date; echo Hello from CronJob\"\n\n# Service creation\nkubectl create service clusterip my-service --tcp=8080:80\nkubectl create service nodeport my-service --tcp=8080:80 --node-port=30080\n\n# ConfigMap from literals\nkubectl create configmap app-config \\\n  --from-literal=env=production \\\n  --from-literal=debug=false\n\n# Secret creation\nkubectl create secret generic db-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=secretpass\n\n# All with YAML generation\nkubectl create deployment webapp --image=nginx --dry-run=client -o yaml &gt; deploy.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-expose-services","title":"kubectl expose (Services)","text":"<p>Purpose: Expose existing resources as services</p> <pre><code># Expose pod\nkubectl expose pod nginx --port=80 --type=NodePort\n\n# Expose deployment\nkubectl expose deployment webapp --port=80 --target-port=8080 --type=LoadBalancer\n\n# Expose with specific name\nkubectl expose deployment webapp --port=80 --name=web-service --type=ClusterIP\n\n# Generate service YAML\nkubectl expose deployment webapp --port=80 --dry-run=client -o yaml &gt; service.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#resource-inspection-commands","title":"Resource Inspection Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-get-list-resources","title":"kubectl get (List Resources)","text":"<p>Purpose: Query cluster state - most frequently used command</p> <pre><code># Basic resource listing\nkubectl get pods                        # Current namespace\nkubectl get pods -A                     # All namespaces\nkubectl get pods -n kube-system        # Specific namespace\nkubectl get all                         # All resources in namespace\n\n# Wide output (additional columns)\nkubectl get pods -o wide                # Shows IP, Node, etc.\nkubectl get nodes -o wide              # Shows internal IP, OS, etc.\n\n# Show labels\nkubectl get pods --show-labels\n\n# Filter by labels\nkubectl get pods -l app=nginx\nkubectl get pods -l 'env in (dev,staging)'\nkubectl get pods -l app=nginx,tier!=frontend\n\n# Multiple resource types\nkubectl get pods,services,deployments\nkubectl get deploy,rs,pods\n</code></pre> <p>Output Formats (covered in detail later): - <code>-o wide</code> - Additional columns - <code>-o yaml</code> - Full YAML representation - <code>-o json</code> - Full JSON representation - <code>-o name</code> - Resource names only - <code>-o custom-columns</code> - User-defined columns - <code>-o jsonpath</code> - JSONPath expressions</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-describe-detailed-information","title":"kubectl describe (Detailed Information)","text":"<p>Purpose: Get detailed resource information with events</p> <pre><code># Describe resources\nkubectl describe pod nginx\nkubectl describe node worker-1\nkubectl describe deployment webapp\nkubectl describe service my-service\n\n# Describe from file\nkubectl describe -f deployment.yaml\n\n# Common use case: debugging\nkubectl describe pod failing-pod  # Check Events section for issues\n</code></pre> <p>Key Information in describe Output: - Events: Recent state changes, errors, scheduling decisions - Status: Current resource state - Spec: Resource configuration - Conditions: Health checks and readiness</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#resource-modification-commands","title":"Resource Modification Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-edit-interactive-editing","title":"kubectl edit (Interactive Editing)","text":"<p>Purpose: Edit resources in your default editor</p> <pre><code># Edit pod\nkubectl edit pod nginx\n\n# Edit deployment\nkubectl edit deployment webapp\n\n# Edit with specific editor\nKUBE_EDITOR=\"vim\" kubectl edit service my-service\n</code></pre> <p>Exam Tip: <code>kubectl edit</code> opens full resource YAML. Use with caution - easy to accidentally modify important fields. Prefer <code>kubectl patch</code> or <code>kubectl set</code> for targeted changes.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-patch-partial-updates","title":"kubectl patch (Partial Updates)","text":"<p>Purpose: Update specific fields without full resource replacement</p> <pre><code># Update image\nkubectl patch pod nginx -p '{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"image\":\"nginx:1.21\"}]}}'\n\n# Scale using patch\nkubectl patch deployment webapp -p '{\"spec\":{\"replicas\":5}}'\n\n# Strategic merge patch (default)\nkubectl patch deployment webapp --type=strategic -p '{\"spec\":{\"replicas\":3}}'\n\n# JSON patch\nkubectl patch deployment webapp --type=json -p='[{\"op\": \"replace\", \"path\": \"/spec/replicas\", \"value\":5}]'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-scale-replica-management","title":"kubectl scale (Replica Management)","text":"<p>Purpose: Change replica count for deployments, replica sets</p> <pre><code># Scale deployment\nkubectl scale deployment webapp --replicas=5\n\n# Scale replica set\nkubectl scale rs my-replicaset --replicas=3\n\n# Scale from file\nkubectl scale --replicas=3 -f deployment.yaml\n\n# Conditional scaling\nkubectl scale deployment webapp --current-replicas=3 --replicas=5\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-set-update-resource-fields","title":"kubectl set (Update Resource Fields)","text":"<p>Purpose: Update specific resource fields with simple syntax</p> <pre><code># Update image\nkubectl set image deployment/webapp nginx=nginx:1.21\nkubectl set image deployment/webapp nginx=nginx:1.21 --record\n\n# Update resources\nkubectl set resources deployment webapp \\\n  --limits=cpu=200m,memory=512Mi \\\n  --requests=cpu=100m,memory=256Mi\n\n# Update service account\nkubectl set serviceaccount deployment webapp my-service-account\n\n# Update selector\nkubectl set selector service my-service app=nginx,tier=frontend\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#deployment-management-commands","title":"Deployment Management Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-rollout-deployment-updates","title":"kubectl rollout (Deployment Updates)","text":"<p>Purpose: Manage deployment rollouts and history</p> <pre><code># Check rollout status\nkubectl rollout status deployment/webapp\nkubectl rollout status deployment/webapp --watch\n\n# View rollout history\nkubectl rollout history deployment/webapp\nkubectl rollout history deployment/webapp --revision=3\n\n# Undo rollout (rollback)\nkubectl rollout undo deployment/webapp                    # Rollback to previous\nkubectl rollout undo deployment/webapp --to-revision=2    # Rollback to specific\n\n# Restart deployment (rolling restart)\nkubectl rollout restart deployment/webapp\n\n# Pause/Resume rollout\nkubectl rollout pause deployment/webapp\nkubectl rollout resume deployment/webapp\n</code></pre> <p>Exam Scenario Flow: <pre><code>sequenceDiagram\n    participant User\n    participant Deployment\n    participant ReplicaSet\n    participant Pods\n\n    User-&gt;&gt;Deployment: kubectl set image deploy/webapp nginx=nginx:1.21\n    Deployment-&gt;&gt;ReplicaSet: Create new ReplicaSet (nginx:1.21)\n    ReplicaSet-&gt;&gt;Pods: Create new pods gradually\n\n    Note over Deployment,Pods: Rolling update in progress\n\n    User-&gt;&gt;Deployment: kubectl rollout status deploy/webapp\n    Deployment--&gt;&gt;User: Waiting for rollout to finish...\n\n    alt Update fails\n        User-&gt;&gt;Deployment: kubectl rollout undo deploy/webapp\n        Deployment-&gt;&gt;ReplicaSet: Scale up old ReplicaSet\n        ReplicaSet-&gt;&gt;Pods: Create pods with old image\n    else Update succeeds\n        Deployment-&gt;&gt;ReplicaSet: Scale down old ReplicaSet\n        ReplicaSet-&gt;&gt;Pods: Terminate old pods\n    end\n\n    User-&gt;&gt;Deployment: kubectl rollout history deploy/webapp\n    Deployment--&gt;&gt;User: Show revision history</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#debugging-commands","title":"Debugging Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-logs-container-logs","title":"kubectl logs (Container Logs)","text":"<p>Purpose: View container stdout/stderr logs</p> <pre><code># Basic logs\nkubectl logs pod-name\nkubectl logs pod-name -c container-name        # Multi-container pods\n\n# Follow logs (stream)\nkubectl logs -f pod-name\n\n# Previous container instance (after crash)\nkubectl logs pod-name --previous\nkubectl logs pod-name -c container-name --previous\n\n# Logs from deployment\nkubectl logs deployment/webapp\nkubectl logs deployment/webapp -c nginx\n\n# Logs with labels\nkubectl logs -l app=nginx                      # All pods with label\nkubectl logs -f -l app=nginx --all-containers  # Stream all containers\n\n# Tail last N lines\nkubectl logs pod-name --tail=50\nkubectl logs pod-name --since=1h              # Last hour\nkubectl logs pod-name --since-time=2024-01-01T00:00:00Z\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-exec-execute-commands","title":"kubectl exec (Execute Commands)","text":"<p>Purpose: Run commands inside containers</p> <pre><code># Single command\nkubectl exec pod-name -- ls /app\nkubectl exec pod-name -- env\nkubectl exec pod-name -- cat /etc/resolv.conf\n\n# Interactive shell\nkubectl exec -it pod-name -- /bin/bash\nkubectl exec -it pod-name -- /bin/sh\n\n# Multi-container pod\nkubectl exec -it pod-name -c container-name -- /bin/bash\n\n# Deployment exec (first pod)\nkubectl exec deployment/webapp -- env\nkubectl exec -it deployment/webapp -- /bin/bash\n</code></pre> <p>Common Exam Patterns: <pre><code># Check pod networking\nkubectl exec -it pod-name -- ping google.com\nkubectl exec -it pod-name -- nslookup kubernetes.default\n\n# Verify volume mounts\nkubectl exec pod-name -- ls /mnt/data\n\n# Test service connectivity\nkubectl exec -it pod-name -- curl http://service-name:8080\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-port-forward-local-access","title":"kubectl port-forward (Local Access)","text":"<p>Purpose: Forward local port to pod/service for debugging</p> <pre><code># Forward to pod\nkubectl port-forward pod/nginx 8080:80        # Local:8080 -&gt; Pod:80\n\n# Forward to service\nkubectl port-forward service/webapp 8080:80\n\n# Forward to deployment\nkubectl port-forward deployment/webapp 8080:80\n\n# Listen on all interfaces (use with caution)\nkubectl port-forward --address 0.0.0.0 pod/nginx 8080:80\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-debug-ephemeral-containers","title":"kubectl debug (Ephemeral Containers)","text":"<p>Purpose: Debug running pods with ephemeral debugging containers</p> <pre><code># Debug pod with new container\nkubectl debug pod-name -it --image=busybox:1.28\n\n# Debug node\nkubectl debug node/worker-1 -it --image=ubuntu\n\n# Copy pod and debug\nkubectl debug pod-name --copy-to=pod-name-debug --container=debugger --image=busybox\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#imperative-vs-declarative-exam-strategy","title":"Imperative vs Declarative: Exam Strategy","text":"<p>The CKA exam requires balancing speed with maintainability. Understanding when to use each approach is critical.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#imperative-commands-recommended-for-cka","title":"Imperative Commands (Recommended for CKA)","text":"<p>Definition: Direct CLI commands that immediately execute operations</p> <p>Advantages: - Fast execution (30-60 seconds vs 2-3 minutes) - No file management overhead - Perfect for exam time constraints - Easy to remember patterns</p> <p>Disadvantages: - Not reproducible - No version control - Limited complexity handling</p> <p>Exam Use Cases: <pre><code># Simple pod creation\nkubectl run nginx --image=nginx\n\n# Quick deployment\nkubectl create deployment webapp --image=nginx --replicas=3\n\n# Expose service\nkubectl expose deployment webapp --port=80 --type=NodePort\n\n# Scale resources\nkubectl scale deployment webapp --replicas=5\n\n# Update image\nkubectl set image deployment/webapp nginx=nginx:1.21\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#declarative-configuration-limited-exam-use","title":"Declarative Configuration (Limited Exam Use)","text":"<p>Definition: YAML manifests describing desired state, applied with <code>kubectl apply</code></p> <p>Advantages: - Reproducible deployments - Version controllable - Handles complex configurations - Production best practice</p> <p>Disadvantages: - Slower for simple tasks - File management overhead - Verbose for basic operations</p> <p>Exam Use Cases: <pre><code># Complex multi-container pods\nkubectl apply -f complex-pod.yaml\n\n# Resources with specific configurations\nkubectl apply -f deployment-with-resources.yaml\n\n# Multiple related resources\nkubectl apply -f ./manifests/\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#hybrid-approach-optimal-for-cka","title":"Hybrid Approach (Optimal for CKA)","text":"<p>Strategy: Use imperative commands to generate YAML templates, edit as needed, then apply.</p> <pre><code># 1. Generate base YAML\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# 2. Edit for specific requirements\nvim pod.yaml\n# Add: resource limits, volumes, labels, etc.\n\n# 3. Apply declaratively\nkubectl apply -f pod.yaml\n\n# 4. Verify\nkubectl get pods\nkubectl describe pod nginx\n</code></pre> <p>Time Comparison:</p> Task Purely Imperative Hybrid Approach Pure Declarative Simple pod 10 seconds 30 seconds 2 minutes Deployment with 3 replicas 15 seconds 45 seconds 3 minutes Pod with volumes + resources Impossible 90 seconds 4 minutes Multi-container pod Impossible 2 minutes 5 minutes","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#decision-tree-imperative-vs-declarative","title":"Decision Tree: Imperative vs Declarative","text":"<pre><code>flowchart TD\n    Start([New Resource Needed]) --&gt; Simple{Simple&lt;br/&gt;Configuration?}\n\n    Simple --&gt;|Yes| SingleContainer{Single&lt;br/&gt;Container?}\n    Simple --&gt;|No| Complex[Use Hybrid Approach]\n\n    SingleContainer --&gt;|Yes| Imperative[Use Pure Imperative&lt;br/&gt;kubectl run/create]\n    SingleContainer --&gt;|No| Complex\n\n    Complex --&gt; Generate[kubectl create --dry-run]\n    Generate --&gt; Edit[Edit YAML file]\n    Edit --&gt; Apply[kubectl apply -f]\n    Apply --&gt; Verify[kubectl get/describe]\n\n    Imperative --&gt; QuickVerify[Quick kubectl get]\n\n    style Imperative fill:#99ff99\n    style Complex fill:#fff4e1\n    style Generate fill:#e1f5ff</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#output-formats-extracting-information","title":"Output Formats: Extracting Information","text":"<p>kubectl supports multiple output formats for data extraction. Mastering these saves critical exam time.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#standard-output-formats","title":"Standard Output Formats","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#default-human-readable","title":"Default (Human-Readable)","text":"<pre><code>kubectl get pods\n# NAME        READY   STATUS    RESTARTS   AGE\n# nginx       1/1     Running   0          5m\n# webapp-1    2/2     Running   1          10m\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#wide-output","title":"Wide Output","text":"<pre><code>kubectl get pods -o wide\n# NAME     READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE\n# nginx    1/1     Running   0          5m    10.244.0.5   worker-1   &lt;none&gt;\n</code></pre> <p>Use Case: Get pod IPs, node placement, readiness gates in one view</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#yaml-output","title":"YAML Output","text":"<pre><code>kubectl get pod nginx -o yaml\n</code></pre> <p>Use Cases: - Create templates from existing resources - Understand full resource structure - Debug configuration issues - Generate manifests for reproduction</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#json-output","title":"JSON Output","text":"<pre><code>kubectl get pod nginx -o json\n</code></pre> <p>Use Cases: - Programmatic parsing - Integration with scripts - API response inspection</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#name-output","title":"Name Output","text":"<pre><code>kubectl get pods -o name\n# pod/nginx\n# pod/webapp-1\n# pod/webapp-2\n</code></pre> <p>Use Case: Pipe to other commands, scripting</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#advanced-output-formats","title":"Advanced Output Formats","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#custom-columns","title":"Custom Columns","text":"<p>Purpose: Define exactly which fields to display in table format</p> <p>Basic Syntax: <pre><code>kubectl get pods -o custom-columns=&lt;COLUMN_NAME&gt;:&lt;JSON_PATH&gt;\n</code></pre></p> <p>Examples: <pre><code># Pod name and image\nkubectl get pods -o custom-columns=NAME:.metadata.name,IMAGE:.spec.containers[0].image\n\n# Pod name, namespace, node\nkubectl get pods -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,NODE:.spec.nodeName\n\n# Multiple container images\nkubectl get pods -o custom-columns=NAME:.metadata.name,IMAGES:.spec.containers[*].image\n\n# Filter with JSONPath\nkubectl get pods -A -o custom-columns='POD:.metadata.name,IMAGE:.spec.containers[?(@.image!=\"registry.k8s.io/pause:3.9\")].image'\n</code></pre></p> <p>Common Patterns: <pre><code># Service types and IPs\nkubectl get svc -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,CLUSTER-IP:.spec.clusterIP,EXTERNAL-IP:.status.loadBalancer.ingress[0].ip\n\n# Node resource capacity\nkubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory\n\n# PV claim status\nkubectl get pv -o custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage,STATUS:.status.phase,CLAIM:.spec.claimRef.name\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#jsonpath-queries","title":"JSONPath Queries","text":"<p>Purpose: Extract specific data using JSONPath expressions</p> <p>Basic Syntax: <pre><code>kubectl get &lt;resource&gt; -o jsonpath='{&lt;JSONPath_expression&gt;}'\n</code></pre></p> <p>Essential Patterns:</p> <pre><code># All pod names\nkubectl get pods -o jsonpath='{.items[*].metadata.name}'\n\n# First pod name\nkubectl get pods -o jsonpath='{.items[0].metadata.name}'\n\n# Pod IPs with newlines\nkubectl get pods -o jsonpath='{range .items[*]}{.status.podIP}{\"\\n\"}{end}'\n\n# Node names and CPU capacity\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\n\"}{end}'\n\n# Filter by condition\nkubectl get pods -o jsonpath='{.items[?(@.metadata.labels.app==\"nginx\")].metadata.name}'\n\n# All container images (unique)\nkubectl get pods -A -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\\n' | sort -u\n\n# Service cluster IPs\nkubectl get svc -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.clusterIP}{\"\\n\"}{end}'\n</code></pre> <p>Complex Examples: <pre><code># Pods with high restart counts\nkubectl get pods -o jsonpath='{range .items[?(@.status.containerStatuses[0].restartCount&gt;5)]}{.metadata.name}{\"\\t\"}{.status.containerStatuses[0].restartCount}{\"\\n\"}{end}'\n\n# Users from kubeconfig\nkubectl config view -o jsonpath='{.users[*].name}'\n\n# Context cluster mapping\nkubectl config view -o jsonpath='{range .contexts[*]}{.name}{\"\\t\"}{.context.cluster}{\"\\n\"}{end}'\n</code></pre></p> <p>JSONPath Tips: - Use <code>{range}...{end}</code> for iteration - Use <code>{\\n}</code> for newlines, <code>{\\t}</code> for tabs - Filter with <code>[?(@.field==\"value\")]</code> - Access array elements with <code>[index]</code> or <code>[*]</code> for all - No regex support in JSONPath</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#output-format-comparison","title":"Output Format Comparison","text":"<pre><code>graph LR\n    Task[Information Need] --&gt; Decision{Data Type?}\n\n    Decision --&gt;|Quick Overview| Wide[Use -o wide]\n    Decision --&gt;|Specific Fields| Choice{How Many Fields?}\n    Decision --&gt;|Full Resource| Format{Human or Machine?}\n    Decision --&gt;|Resource Names| Name[Use -o name]\n\n    Choice --&gt;|1-2 fields| JSONPath[Use -o jsonpath]\n    Choice --&gt;|3+ fields| CustomCol[Use -o custom-columns]\n\n    Format --&gt;|Human| YAML[Use -o yaml]\n    Format --&gt;|Machine| JSON[Use -o json]\n\n    style Wide fill:#99ff99\n    style JSONPath fill:#e1f5ff\n    style CustomCol fill:#fff4e1\n    style YAML fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#context-and-namespace-management","title":"Context and Namespace Management","text":"<p>Managing multiple clusters and namespaces efficiently is a core CKA exam skill.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#understanding-contexts","title":"Understanding Contexts","text":"<p>Context Definition: A context groups cluster + user + namespace into a named configuration.</p> <p>Context Components: - Cluster: API server endpoint and CA certificate - User: Authentication credentials (client cert, token, etc.) - Namespace: Default namespace for commands (optional)</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#context-management-commands","title":"Context Management Commands","text":"<pre><code># List all contexts\nkubectl config get-contexts\n# CURRENT   NAME           CLUSTER        AUTHINFO       NAMESPACE\n# *         prod-context   prod-cluster   prod-admin     production\n#           dev-context    dev-cluster    dev-user       development\n\n# Show current context\nkubectl config current-context\n# prod-context\n\n# Switch context\nkubectl config use-context dev-context\n\n# Create new context\nkubectl config set-context staging \\\n  --cluster=prod-cluster \\\n  --user=staging-user \\\n  --namespace=staging\n\n# Modify existing context\nkubectl config set-context dev-context --namespace=testing\n\n# Set namespace for current context\nkubectl config set-context --current --namespace=kube-system\n\n# Delete context\nkubectl config delete-context old-context\n\n# Rename context\nkubectl config rename-context old-name new-name\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#namespace-operations","title":"Namespace Operations","text":"<pre><code># List namespaces\nkubectl get namespaces\nkubectl get ns  # Short form\n\n# Create namespace\nkubectl create namespace development\nkubectl create ns production\n\n# Describe namespace\nkubectl describe namespace development\n\n# Delete namespace (deletes all resources!)\nkubectl delete namespace development\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#multi-cluster-workflow","title":"Multi-Cluster Workflow","text":"<pre><code>sequenceDiagram\n    participant Exam as Exam Question\n    participant User\n    participant kubectl\n    participant Cluster1 as Cluster 1\n    participant Cluster2 as Cluster 2\n\n    Exam-&gt;&gt;User: \"Deploy to cluster 'k8s-prod', namespace 'production'\"\n\n    User-&gt;&gt;kubectl: kubectl config get-contexts\n    kubectl--&gt;&gt;User: List available contexts\n\n    User-&gt;&gt;kubectl: kubectl config use-context k8s-prod\n    kubectl--&gt;&gt;Cluster1: Switch connection\n\n    User-&gt;&gt;kubectl: kubectl config set-context --current --namespace=production\n    kubectl--&gt;&gt;kubectl: Set default namespace\n\n    User-&gt;&gt;kubectl: kubectl run app --image=nginx\n    kubectl-&gt;&gt;Cluster1: Create pod in production namespace\n\n    Note over Exam,User: Next question uses different cluster\n\n    Exam-&gt;&gt;User: \"Check pods in cluster 'k8s-dev', namespace 'default'\"\n\n    User-&gt;&gt;kubectl: kubectl config use-context k8s-dev\n    kubectl--&gt;&gt;Cluster2: Switch connection\n\n    User-&gt;&gt;kubectl: kubectl get pods\n    kubectl-&gt;&gt;Cluster2: Query default namespace\n    Cluster2--&gt;&gt;User: Pod list</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#namespace-best-practices","title":"Namespace Best Practices","text":"<p>Always Specify Namespace Explicitly (Exam Safety): <pre><code># Risky (uses context default)\nkubectl get pods\n\n# Safe (explicit namespace)\nkubectl get pods -n production\nkubectl get pods --namespace=production\n\n# All namespaces\nkubectl get pods -A\nkubectl get pods --all-namespaces\n</code></pre></p> <p>Context Switching Aliases (Pre-configured in Exam): <pre><code># Context switcher\nalias kx='f() { [ \"$1\" ] &amp;&amp; kubectl config use-context $1 || kubectl config current-context ; } ; f'\n\n# Usage:\nkx                  # Show current context\nkx prod-context     # Switch to prod-context\n\n# Namespace switcher\nalias kn='f() { [ \"$1\" ] &amp;&amp; kubectl config set-context --current --namespace $1 || kubectl config view --minify | grep namespace | cut -d\" \" -f6 ; } ; f'\n\n# Usage:\nkn                  # Show current namespace\nkn production       # Switch to production namespace\n</code></pre></p> <p>Exam Verification Checklist: <pre><code># Before EVERY task, verify:\nkubectl config current-context          # Am I in the right cluster?\nkubectl config view --minify | grep namespace  # What's my default namespace?\n\n# Or use the exam-provided aliases:\nkx  # Show context\nkn  # Show namespace\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#kubectl-explain-in-exam-documentation","title":"kubectl explain: In-Exam Documentation","text":"<p>kubectl explain provides API documentation directly in your terminal. This tool is available during the exam and is faster than searching web docs.</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#basic-usage","title":"Basic Usage","text":"<pre><code># Top-level resource\nkubectl explain pod\nkubectl explain deployment\nkubectl explain service\n\n# Nested field\nkubectl explain pod.spec\nkubectl explain deployment.spec.template\nkubectl explain service.spec\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exploring-resource-structure","title":"Exploring Resource Structure","text":"<pre><code># View pod spec fields\nkubectl explain pod.spec\n# FIELDS:\n#   containers    &lt;[]Container&gt; -required-\n#   volumes       &lt;[]Volume&gt;\n#   restartPolicy &lt;string&gt;\n#   nodeName      &lt;string&gt;\n\n# Drill into containers\nkubectl explain pod.spec.containers\n# FIELDS:\n#   name          &lt;string&gt; -required-\n#   image         &lt;string&gt; -required-\n#   command       &lt;[]string&gt;\n#   args          &lt;[]string&gt;\n#   env           &lt;[]EnvVar&gt;\n#   ports         &lt;[]ContainerPort&gt;\n#   volumeMounts  &lt;[]VolumeMount&gt;\n\n# Check container ports structure\nkubectl explain pod.spec.containers.ports\n# FIELDS:\n#   containerPort &lt;integer&gt; -required-\n#   hostPort      &lt;integer&gt;\n#   name          &lt;string&gt;\n#   protocol      &lt;string&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#common-exam-use-cases","title":"Common Exam Use Cases","text":"<p>Scenario 1: What fields are available for liveness probes? <pre><code>kubectl explain pod.spec.containers.livenessProbe\n# Shows: exec, httpGet, tcpSocket, initialDelaySeconds, periodSeconds, etc.\n\nkubectl explain pod.spec.containers.livenessProbe.httpGet\n# Shows: path, port, host, scheme, httpHeaders\n</code></pre></p> <p>Scenario 2: How to configure deployment strategy? <pre><code>kubectl explain deployment.spec.strategy\n# Shows: type (RollingUpdate, Recreate), rollingUpdate\n\nkubectl explain deployment.spec.strategy.rollingUpdate\n# Shows: maxSurge, maxUnavailable\n</code></pre></p> <p>Scenario 3: What volume types are available? <pre><code>kubectl explain pod.spec.volumes\n# Shows MANY types: configMap, secret, emptyDir, persistentVolumeClaim, hostPath, nfs, etc.\n\nkubectl explain pod.spec.volumes.persistentVolumeClaim\n# Shows: claimName, readOnly\n</code></pre></p> <p>Scenario 4: Resource limits and requests? <pre><code>kubectl explain pod.spec.containers.resources\n# Shows: limits, requests\n\nkubectl explain pod.spec.containers.resources.limits\n# Shows: can specify cpu, memory, ephemeral-storage\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#field-type-notation","title":"Field Type Notation","text":"<p>Understanding Output: - <code>&lt;string&gt;</code> - String field - <code>&lt;integer&gt;</code> - Integer field - <code>&lt;boolean&gt;</code> - Boolean (true/false) - <code>&lt;[]Type&gt;</code> - Array of Type - <code>&lt;Object&gt;</code> - Nested object - <code>&lt;map[string]string&gt;</code> - Key-value map - <code>-required-</code> - Required field marker</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#integration-with-workflow","title":"Integration with Workflow","text":"<p>Exam Pattern: <pre><code># 1. Generate base template\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# 2. Check available fields\nkubectl explain pod.spec.containers.resources\n\n# 3. Edit with correct field names\nvim pod.yaml\n# Add resources based on explain output\n\n# 4. Verify and apply\nkubectl apply -f pod.yaml\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#productivity-patterns","title":"Productivity Patterns","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#time-saving-aliases-pre-configured-in-exam","title":"Time-Saving Aliases (Pre-configured in Exam)","text":"<p>Core Alias (Already Active): <pre><code>alias k=kubectl\ncomplete -o default -F __start_kubectl k\n</code></pre></p> <p>Recommended Personal Additions: <pre><code># Dry-run YAML generation\nexport dry='--dry-run=client -o yaml'\nexport now='--force --grace-period=0'\n\n# Usage examples:\nk run nginx --image=nginx $dry &gt; pod.yaml\nk create deploy webapp --image=nginx $dry &gt; deploy.yaml\nk delete pod nginx $now  # Immediate deletion\n</code></pre></p> <p>Resource Shortcuts: <pre><code>alias kgp='kubectl get pods'\nalias kgd='kubectl get deployments'\nalias kgs='kubectl get services'\nalias kga='kubectl get all'\nalias kgpa='kubectl get pods -A'\n\nalias kdesc='kubectl describe'\nalias kl='kubectl logs'\nalias klf='kubectl logs -f'\nalias kex='kubectl exec -it'\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#autocomplete-usage-pre-configured","title":"Autocomplete Usage (Pre-configured)","text":"<p>Tab Completion Examples: <pre><code># Resource type completion\nk get po&lt;TAB&gt;            # Completes to: k get pods\nk get dep&lt;TAB&gt;           # Completes to: k get deployments\n\n# Resource name completion\nk get pods ng&lt;TAB&gt;       # Completes to existing pod name\nk describe node wor&lt;TAB&gt; # Completes to node name\n\n# Namespace completion\nk get pods -n kube-&lt;TAB&gt; # Completes to: k get pods -n kube-system\n\n# Flag completion\nk get pods --out&lt;TAB&gt;    # Completes to: k get pods --output\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#common-command-patterns","title":"Common Command Patterns","text":"<p>Deployment Lifecycle: <pre><code># Create\nkubectl create deployment webapp --image=nginx --replicas=3\n\n# Expose\nkubectl expose deployment webapp --port=80 --type=NodePort\n\n# Update\nkubectl set image deployment/webapp nginx=nginx:1.21\n\n# Scale\nkubectl scale deployment webapp --replicas=5\n\n# Check status\nkubectl rollout status deployment/webapp\n\n# Rollback if needed\nkubectl rollout undo deployment/webapp\n</code></pre></p> <p>Debugging Workflow: <pre><code># 1. Identify issue\nkubectl get pods                # Find problematic pod\n\n# 2. Get details\nkubectl describe pod pod-name   # Check Events section\n\n# 3. Check logs\nkubectl logs pod-name           # Current logs\nkubectl logs pod-name --previous # After crash\n\n# 4. Interactive debug\nkubectl exec -it pod-name -- /bin/sh\n\n# 5. Network test\nkubectl exec -it pod-name -- ping service-name\nkubectl exec -it pod-name -- curl http://service:8080\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#cka-exam-strategies","title":"CKA Exam Strategies","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#essential-commands-use-daily","title":"Essential Commands (Use Daily)","text":"<p>Top 10 Most-Used in Exam: 1. <code>kubectl run</code> - Create pods quickly 2. <code>kubectl create</code> - Generate resources 3. <code>kubectl get</code> - List and inspect 4. <code>kubectl describe</code> - Debug issues 5. <code>kubectl logs</code> - View application logs 6. <code>kubectl apply</code> - Deploy configurations 7. <code>kubectl delete</code> - Remove resources 8. <code>kubectl exec</code> - Container debugging 9. <code>kubectl edit</code> - Quick modifications 10. <code>kubectl explain</code> - Field reference</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#time-management","title":"Time Management","text":"<p>Command Time Budget: - Simple pod creation: 10-30 seconds - Deployment with service: 45-90 seconds - Complex multi-container pod: 2-3 minutes - Debugging scenario: 3-5 minutes</p> <p>Speed Optimization: <pre><code># SLOW (2-3 minutes)\nvim pod.yaml  # Write from scratch\nkubectl apply -f pod.yaml\n\n# FAST (30 seconds)\nkubectl run nginx --image=nginx $dry &gt; pod.yaml\nvim pod.yaml  # Edit generated template\nkubectl apply -f pod.yaml\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#common-exam-scenarios","title":"Common Exam Scenarios","text":"<p>Scenario 1: Create and Expose <pre><code># Create deployment\nkubectl create deployment webapp --image=nginx --replicas=3\n\n# Expose service\nkubectl expose deployment webapp --port=80 --type=NodePort\n\n# Verify\nkubectl get deploy,svc,pods\n</code></pre></p> <p>Scenario 2: Fix Failing Pod <pre><code># Identify\nkubectl get pods\nkubectl describe pod failing-pod\nkubectl logs failing-pod\n\n# Fix approach\nkubectl get pod failing-pod -o yaml &gt; fix.yaml\nvim fix.yaml  # Fix the issue\nkubectl delete pod failing-pod\nkubectl apply -f fix.yaml\n</code></pre></p> <p>Scenario 3: Scale and Update <pre><code># Scale\nkubectl scale deployment webapp --replicas=5\n\n# Update image\nkubectl set image deployment/webapp nginx=nginx:1.21\n\n# Monitor rollout\nkubectl rollout status deployment/webapp\n\n# Rollback if needed\nkubectl rollout undo deployment/webapp\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-1-command-mastery-15-minutes","title":"Exercise 1: Command Mastery (15 minutes)","text":"<p>Objective: Build muscle memory for essential commands</p> <p>Tasks: 1. Create pod 'web' with nginx:1.21 image and port 80 2. Create deployment 'api' with 3 replicas using httpd image 3. Expose 'api' deployment as NodePort on port 8080 4. Get all pod IPs using JSONPath 5. List pod names only 6. Scale 'api' to 5 replicas 7. Update 'api' image to httpd:2.4.57 8. View rollout history 9. Delete all resources</p> <p>Solution: <pre><code># 1\nkubectl run web --image=nginx:1.21 --port=80\n\n# 2\nkubectl create deployment api --image=httpd --replicas=3\n\n# 3\nkubectl expose deployment api --port=8080 --type=NodePort\n\n# 4\nkubectl get pods -o jsonpath='{range .items[*]}{.status.podIP}{\"\\n\"}{end}'\n\n# 5\nkubectl get pods -o name\n\n# 6\nkubectl scale deployment api --replicas=5\n\n# 7\nkubectl set image deployment/api httpd=httpd:2.4.57\n\n# 8\nkubectl rollout history deployment/api\n\n# 9\nkubectl delete deployment api\nkubectl delete pod web\nkubectl delete service api\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-2-output-formats-20-minutes","title":"Exercise 2: Output Formats (20 minutes)","text":"<p>Objective: Master data extraction with output formats</p> <p>Tasks: 1. List all pod names in kube-system namespace 2. Get pod names and node placement with custom columns 3. Extract all container images in cluster 4. Get services with type and cluster IP 5. Find pods with label app=nginx</p> <p>Solution: <pre><code># 1\nkubectl get pods -n kube-system -o name\n\n# 2\nkubectl get pods -A -o custom-columns=POD:.metadata.name,NODE:.spec.nodeName\n\n# 3\nkubectl get pods -A -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\\n' | sort -u\n\n# 4\nkubectl get svc -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,CLUSTER-IP:.spec.clusterIP\n\n# 5\nkubectl get pods -l app=nginx\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-3-context-management-15-minutes","title":"Exercise 3: Context Management (15 minutes)","text":"<p>Objective: Practice multi-cluster context switching</p> <p>Tasks: 1. List all contexts 2. Show current context 3. Create context 'dev-ctx' for development namespace 4. Switch to 'dev-ctx' 5. Set default namespace to 'kube-system' for current context 6. Verify namespace setting</p> <p>Solution: <pre><code># 1\nkubectl config get-contexts\n\n# 2\nkubectl config current-context\n\n# 3\nkubectl config set-context dev-ctx --namespace=development\n\n# 4\nkubectl config use-context dev-ctx\n\n# 5\nkubectl config set-context --current --namespace=kube-system\n\n# 6\nkubectl config view --minify | grep namespace\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-4-imperative-vs-declarative-25-minutes","title":"Exercise 4: Imperative vs Declarative (25 minutes)","text":"<p>Objective: Practice hybrid workflow</p> <p>Tasks: 1. Generate pod YAML for nginx with resource limits (don't create) 2. Add resource requests: cpu=100m, memory=128Mi 3. Add resource limits: cpu=200m, memory=256Mi 4. Add liveness probe: HTTP GET on port 80, path / 5. Apply and verify 6. Generate deployment YAML with 3 replicas 7. Modify to add nodeSelector: disk=ssd 8. Apply and verify pods scheduled on correct nodes</p> <p>Solution: <pre><code># 1\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# 2-4 (edit pod.yaml)\nvim pod.yaml\n# Add under containers:\n#   resources:\n#     requests:\n#       cpu: 100m\n#       memory: 128Mi\n#     limits:\n#       cpu: 200m\n#       memory: 256Mi\n#   livenessProbe:\n#     httpGet:\n#       path: /\n#       port: 80\n\n# 5\nkubectl apply -f pod.yaml\nkubectl describe pod nginx\n\n# 6\nkubectl create deployment webapp --image=nginx --replicas=3 --dry-run=client -o yaml &gt; deploy.yaml\n\n# 7\nvim deploy.yaml\n# Add under spec.template.spec:\n#   nodeSelector:\n#     disk: ssd\n\n# 8\nkubectl apply -f deploy.yaml\nkubectl get pods -o wide\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#exercise-5-debugging-simulation-30-minutes","title":"Exercise 5: Debugging Simulation (30 minutes)","text":"<p>Objective: Practice troubleshooting workflow</p> <p>Tasks: 1. Create pod with wrong image name 2. Identify why pod is failing 3. Get YAML and fix 4. Create deployment, then break it by scaling to 100 replicas 5. Observe pending pods 6. Identify resource constraints 7. Fix by scaling down</p> <p>Solution: <pre><code># 1\nkubectl run broken --image=nginxxx  # Wrong image\n\n# 2\nkubectl get pods\nkubectl describe pod broken  # Check Events: ImagePullBackOff\n\n# 3\nkubectl get pod broken -o yaml &gt; fixed.yaml\nvim fixed.yaml  # Change image to nginx\nkubectl delete pod broken\nkubectl apply -f fixed.yaml\n\n# 4\nkubectl create deployment overload --image=nginx\nkubectl scale deployment overload --replicas=100\n\n# 5\nkubectl get pods | grep Pending\n\n# 6\nkubectl describe pod &lt;pending-pod-name&gt;\n# Events show: Insufficient cpu/memory\n\n# 7\nkubectl scale deployment overload --replicas=3\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#quick-reference","title":"Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#command-syntax-patterns","title":"Command Syntax Patterns","text":"<pre><code># Resource Management\nkubectl &lt;verb&gt; &lt;resource&gt; &lt;name&gt; [options]\nkubectl get pods nginx -o yaml\nkubectl describe deployment webapp\n\n# Imperative Creation\nkubectl run &lt;name&gt; --image=&lt;image&gt; [options]\nkubectl create &lt;resource&gt; &lt;name&gt; [options]\nkubectl expose &lt;resource&gt; &lt;name&gt; [options]\n\n# Declarative Operations\nkubectl apply -f &lt;file&gt;\nkubectl delete -f &lt;file&gt;\n\n# Resource Modification\nkubectl edit &lt;resource&gt; &lt;name&gt;\nkubectl patch &lt;resource&gt; &lt;name&gt; -p '&lt;patch&gt;'\nkubectl scale &lt;resource&gt; &lt;name&gt; --replicas=&lt;n&gt;\nkubectl set image &lt;resource&gt;/&lt;name&gt; &lt;container&gt;=&lt;image&gt;\n\n# Debugging\nkubectl logs &lt;pod&gt; [-c &lt;container&gt;] [options]\nkubectl exec &lt;pod&gt; [-c &lt;container&gt;] -- &lt;command&gt;\nkubectl port-forward &lt;resource&gt; &lt;local&gt;:&lt;remote&gt;\n\n# Context &amp; Config\nkubectl config &lt;subcommand&gt;\nkubectl config use-context &lt;context&gt;\nkubectl config set-context --current --namespace=&lt;ns&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#output-format-quick-reference","title":"Output Format Quick Reference","text":"<pre><code># Standard formats\n-o wide               # Additional columns\n-o yaml               # Full YAML\n-o json               # Full JSON\n-o name               # Resource names only\n\n# Advanced formats\n-o custom-columns=&lt;spec&gt;\n-o jsonpath='{&lt;path&gt;}'\n-o jsonpath-file=&lt;file&gt;\n\n# Examples\nkubectl get pods -o wide\nkubectl get pods -o jsonpath='{.items[*].metadata.name}'\nkubectl get pods -o custom-columns=NAME:.metadata.name,IP:.status.podIP\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#common-flags","title":"Common Flags","text":"<pre><code>-n, --namespace         # Specify namespace\n-A, --all-namespaces    # All namespaces\n-l, --selector          # Label selector\n-f, --filename          # File path\n--dry-run=client        # Don't create, just print\n-o, --output            # Output format\n-w, --watch             # Watch for changes\n--sort-by               # Sort output\n--field-selector        # Field selector\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Imperative commands save exam minutes - Use <code>kubectl run</code>, <code>create</code>, <code>expose</code> for speed</p> <p>\u2705 --dry-run=client -o yaml is critical - Generate templates, never write YAML from scratch</p> <p>\u2705 kubectl explain is your friend - Faster than searching docs during exam</p> <p>\u2705 Output formats extract data precisely - Master JSONPath and custom-columns</p> <p>\u2705 Context awareness prevents mistakes - Always verify cluster and namespace</p> <p>\u2705 Autocomplete is pre-configured - Use tab completion aggressively</p> <p>\u2705 Aliases save time only if practiced - Use <code>k</code>, <code>$dry</code>, <code>$now</code> extensively before exam</p> <p>\u2705 Hybrid approach balances speed and complexity - Imperative generation + declarative application</p> <p>\u2705 Verification is non-negotiable - Always check resources created correctly</p> <p>\u2705 Practice makes permanent - Build muscle memory through repetition</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubectl-essentials/#next-steps","title":"Next Steps","text":"<p>After mastering kubectl, continue with:</p> <p>Post 4: Pod Lifecycle and Management - Deep dive into the fundamental Kubernetes workload unit</p> <p>Related Posts: - Kubernetes Architecture Fundamentals - Understanding cluster components - Setting Up Your Kubernetes Lab - Build your practice environment - Kubernetes CKA Mastery - Complete Learning Path - Full exam preparation series</p> <p>External Resources: - kubectl Official Documentation - kubectl Cheat Sheet - kubectl Commands Reference - JSONPath Support in kubectl - CKA Exam Curriculum</p>","tags":["kubernetes","k8s","cka-prep","kubectl","command-line"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/","title":"Kubernetes Architecture Fundamentals","text":"<p>Deep dive into Kubernetes cluster architecture, control plane components, and the distributed systems design that powers container orchestration at scale.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#overview","title":"Overview","text":"<p>Kubernetes is a distributed system designed to manage containerized applications across a cluster of machines. Understanding its architecture is foundational for the CKA exam and real-world cluster administration.</p> <p>CKA Exam Domain: Cluster Architecture, Installation &amp; Configuration (25%)</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#cluster-architecture","title":"Cluster Architecture","text":"<p>A Kubernetes cluster consists of two types of nodes:</p> <ol> <li>Control Plane Nodes: Run the core components that manage the cluster</li> <li>Worker Nodes: Run application workloads (pods)</li> </ol> <pre><code>graph TB\n    subgraph \"Control Plane Nodes\"\n        API[API Server&lt;br/&gt;:6443]\n        SCHED[Scheduler]\n        CM[Controller&lt;br/&gt;Manager]\n        CCM[Cloud Controller&lt;br/&gt;Manager]\n        ETCD[(etcd&lt;br/&gt;:2379-2380)]\n    end\n\n    subgraph \"Worker Node 1\"\n        KUB1[kubelet]\n        KPXY1[kube-proxy]\n        POD1[Pods]\n        CRI1[Container&lt;br/&gt;Runtime]\n    end\n\n    subgraph \"Worker Node 2\"\n        KUB2[kubelet]\n        KPXY2[kube-proxy]\n        POD2[Pods]\n        CRI2[Container&lt;br/&gt;Runtime]\n    end\n\n    subgraph \"Worker Node 3\"\n        KUB3[kubelet]\n        KPXY3[kube-proxy]\n        POD3[Pods]\n        CRI3[Container&lt;br/&gt;Runtime]\n    end\n\n    API --&gt;|watches| ETCD\n    API --&gt; SCHED\n    API --&gt; CM\n    API --&gt; CCM\n\n    KUB1 --&gt;|API calls| API\n    KUB2 --&gt;|API calls| API\n    KUB3 --&gt;|API calls| API\n\n    KUB1 --&gt; CRI1 --&gt; POD1\n    KUB2 --&gt; CRI2 --&gt; POD2\n    KUB3 --&gt; CRI3 --&gt; POD3\n\n    KPXY1 -.-&gt;|iptables/IPVS| POD1\n    KPXY2 -.-&gt;|iptables/IPVS| POD2\n    KPXY3 -.-&gt;|iptables/IPVS| POD3\n\n    style API fill:#e1f5ff\n    style ETCD fill:#ffe5e5\n    style SCHED fill:#fff4e1\n    style CM fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#control-plane-components","title":"Control Plane Components","text":"<p>The control plane makes global decisions about the cluster and detects/responds to cluster events.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#1-api-server-kube-apiserver","title":"1. API Server (kube-apiserver)","text":"<p>Purpose: Front-end for the Kubernetes control plane. All communication goes through the API server.</p> <p>Key Responsibilities: - Exposes the Kubernetes API (REST interface) - Validates and configures API objects (pods, services, replication controllers) - Serves as the only component that directly interacts with etcd - Handles authentication, authorization, and admission control</p> <p>Default Port: <code>6443</code> (HTTPS)</p> <pre><code># Check API server status\nkubectl get --raw='/healthz?verbose'\n\n# View API server configuration\nkubectl -n kube-system get pod kube-apiserver-&lt;node-name&gt; -o yaml\n\n# Check API server logs\nkubectl -n kube-system logs kube-apiserver-&lt;node-name&gt;\n</code></pre> <p>API Request Flow:</p> <pre><code>sequenceDiagram\n    participant U as User/kubectl\n    participant API as API Server\n    participant AUTH as Authentication\n    participant AUTHZ as Authorization\n    participant ADM as Admission&lt;br/&gt;Controllers\n    participant ETCD as etcd\n\n    U-&gt;&gt;+API: HTTP Request&lt;br/&gt;(create Pod)\n\n    API-&gt;&gt;+AUTH: Authenticate\n    Note right of AUTH: Verify user identity&lt;br/&gt;(certs, tokens, SA)\n    AUTH--&gt;&gt;-API: User ID\n\n    API-&gt;&gt;+AUTHZ: Authorize\n    Note right of AUTHZ: Check RBAC rules&lt;br/&gt;(can user create pod?)\n    AUTHZ--&gt;&gt;-API: Authorized\n\n    API-&gt;&gt;+ADM: Admission Control\n    Note right of ADM: Validate &amp; Mutate&lt;br/&gt;(defaults, quotas, PSP)\n    ADM--&gt;&gt;-API: Admitted\n\n    API-&gt;&gt;+ETCD: Write to etcd\n    ETCD--&gt;&gt;-API: Persisted\n\n    API--&gt;&gt;-U: 201 Created\n\n    Note over API,ETCD: Object now exists in desired state</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#2-etcd","title":"2. etcd","text":"<p>Purpose: Consistent, highly-available key-value store used as Kubernetes' backing store for all cluster data.</p> <p>Key Characteristics: - Distributed consensus using Raft algorithm - Stores all cluster state and configuration - Only the API server writes to etcd - Supports watch operations for real-time updates</p> <p>Default Ports: - <code>2379</code> - Client requests - <code>2380</code> - Server-to-server communication</p> <pre><code># Check etcd cluster health\nETCDCTL_API=3 etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  endpoint health\n\n# List etcd members\nETCDCTL_API=3 etcdctl member list\n\n# Get all keys (see what's stored)\nETCDCTL_API=3 etcdctl get / --prefix --keys-only\n\n# Backup etcd\nETCDCTL_API=3 etcdctl snapshot save snapshot.db\n</code></pre> <p>High Availability: For production clusters, run etcd with at least 3 nodes (odd number for quorum).</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#3-scheduler-kube-scheduler","title":"3. Scheduler (kube-scheduler)","text":"<p>Purpose: Watches for newly created pods with no assigned node and selects a node for them to run on.</p> <p>Scheduling Algorithm: 1. Filtering: Find nodes that satisfy pod requirements (feasible nodes)    - Resource requests (CPU, memory)    - Node selectors    - Taints and tolerations    - Affinity/anti-affinity rules</p> <ol> <li>Scoring: Rank feasible nodes</li> <li>Spread pods across nodes for availability</li> <li>Prefer nodes with available resources</li> <li> <p>Consider pod priorities</p> </li> <li> <p>Binding: Assign pod to highest-scoring node</p> </li> </ol> <pre><code>graph TD\n    START([New Pod Created]) --&gt; FILTER[Filtering Phase]\n\n    FILTER --&gt; CHECK1{Resource&lt;br/&gt;Requirements?}\n    CHECK1 --&gt;|Fail| UNSCHEDULABLE[Pod Unschedulable]\n    CHECK1 --&gt;|Pass| CHECK2{Node&lt;br/&gt;Selectors?}\n\n    CHECK2 --&gt;|Fail| UNSCHEDULABLE\n    CHECK2 --&gt;|Pass| CHECK3{Taints/&lt;br/&gt;Tolerations?}\n\n    CHECK3 --&gt;|Fail| UNSCHEDULABLE\n    CHECK3 --&gt;|Pass| CHECK4{Affinity&lt;br/&gt;Rules?}\n\n    CHECK4 --&gt;|Fail| UNSCHEDULABLE\n    CHECK4 --&gt;|Pass| FEASIBLE[Feasible Nodes]\n\n    FEASIBLE --&gt; SCORE[Scoring Phase]\n    SCORE --&gt; RANK[Rank Nodes&lt;br/&gt;by Score]\n    RANK --&gt; BIND[Bind Pod to&lt;br/&gt;Top Node]\n    BIND --&gt; SUCCESS([Pod Scheduled])\n\n    style START fill:#e1f5ff\n    style SUCCESS fill:#e8f5e8\n    style UNSCHEDULABLE fill:#ffe5e5</code></pre> <pre><code># View scheduler configuration\nkubectl -n kube-system get pod kube-scheduler-&lt;node-name&gt; -o yaml\n\n# Check scheduler logs\nkubectl -n kube-system logs kube-scheduler-&lt;node-name&gt;\n\n# View events (scheduling decisions)\nkubectl get events --sort-by='.lastTimestamp'\n\n# See why a pod is not scheduled\nkubectl describe pod &lt;pod-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#4-controller-manager-kube-controller-manager","title":"4. Controller Manager (kube-controller-manager)","text":"<p>Purpose: Runs controller processes that regulate the state of the cluster.</p> <p>Key Controllers: - Node Controller: Monitors node health, marks nodes as unreachable - Replication Controller: Maintains correct number of pod replicas - Endpoints Controller: Populates Endpoints objects (joins Services &amp; Pods) - Service Account &amp; Token Controllers: Create default accounts and API access tokens</p> <p>Control Loop Pattern:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Watch: Controller starts\n    Watch --&gt; Compare: Detect change\n    Compare --&gt; DesiredState: Check desired state\n    DesiredState --&gt; CurrentState: Check current state\n    CurrentState --&gt; Match: States match?\n\n    Match --&gt; Watch: Yes, no action\n    Match --&gt; Reconcile: No, take action\n\n    Reconcile --&gt; CreateResources: Create missing resources\n    Reconcile --&gt; UpdateResources: Update existing resources\n    Reconcile --&gt; DeleteResources: Delete extra resources\n\n    CreateResources --&gt; Watch\n    UpdateResources --&gt; Watch\n    DeleteResources --&gt; Watch</code></pre> <pre><code># View controller manager logs\nkubectl -n kube-system logs kube-controller-manager-&lt;node-name&gt;\n\n# Check which controllers are enabled\nkubectl -n kube-system get pod kube-controller-manager-&lt;node-name&gt; -o yaml | grep enable\n\n# Watch controller actions in events\nkubectl get events --watch\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#5-cloud-controller-manager-cloud-controller-manager","title":"5. Cloud Controller Manager (cloud-controller-manager)","text":"<p>Purpose: Embeds cloud-specific control logic. Allows cloud providers to integrate with Kubernetes.</p> <p>Key Controllers: - Node Controller: Check cloud provider to determine if a deleted node has been removed - Route Controller: Set up routes in the cloud infrastructure - Service Controller: Create, update, delete cloud load balancers</p> <p>Note: Only relevant when running Kubernetes on cloud platforms (AWS, GCP, Azure).</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#worker-node-components","title":"Worker Node Components","text":"<p>Worker nodes run application workloads and maintain running pods.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#1-kubelet","title":"1. kubelet","text":"<p>Purpose: Agent that runs on each node, ensures containers are running in pods.</p> <p>Key Responsibilities: - Registers node with API server - Watches for pod assignments to its node - Ensures containers described in PodSpec are running and healthy - Reports node and pod status back to API server - Performs container health checks (liveness, readiness probes)</p> <p>Default Port: <code>10250</code></p> <pre><code># Check kubelet status (on node directly)\nsystemctl status kubelet\n\n# View kubelet configuration\ncat /var/lib/kubelet/config.yaml\n\n# Check kubelet logs\njournalctl -u kubelet -f\n\n# View node status from control plane\nkubectl get nodes\nkubectl describe node &lt;node-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#2-kube-proxy","title":"2. kube-proxy","text":"<p>Purpose: Network proxy that maintains network rules on nodes, enabling communication to pods.</p> <p>Key Responsibilities: - Implements Service abstraction - Maintains iptables/IPVS rules for service IPs - Forwards traffic to correct backend pods - Performs load balancing across pod replicas</p> <p>Modes: - iptables (default): Uses Linux iptables for packet filtering - IPVS: Uses Linux IPVS for better performance at scale - userspace: Legacy mode (rarely used)</p> <pre><code># Check kube-proxy mode\nkubectl -n kube-system logs kube-proxy-&lt;pod-name&gt; | grep \"proxy mode\"\n\n# View kube-proxy configuration\nkubectl -n kube-system get cm kube-proxy -o yaml\n\n# Check iptables rules (on node)\niptables-save | grep -i kube\n\n# View IPVS rules (if using IPVS mode)\nipvsadm -Ln\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#3-container-runtime","title":"3. Container Runtime","text":"<p>Purpose: Software responsible for running containers.</p> <p>Supported Runtimes (via Container Runtime Interface - CRI): - containerd: Lightweight, industry-standard (default for most distributions) - CRI-O: Lightweight alternative specifically for Kubernetes - Docker Engine: Via cri-dockerd shim (removed as default in Kubernetes 1.24)</p> <pre><code># Check container runtime\nkubectl get nodes -o wide\n\n# On node: check containerd\nsystemctl status containerd\ncrictl ps\n\n# List images\ncrictl images\n\n# Pull image\ncrictl pull nginx:latest\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#component-communication","title":"Component Communication","text":"<p>All components communicate through the API server. No direct component-to-component communication exists.</p> <pre><code>graph LR\n    subgraph \"Control Plane\"\n        API[API Server]\n        SCHED[Scheduler]\n        CM[Controller&lt;br/&gt;Manager]\n        ETCD[(etcd)]\n    end\n\n    subgraph \"Worker Nodes\"\n        KUB1[kubelet]\n        KUB2[kubelet]\n        KUB3[kubelet]\n    end\n\n    CLIENT[kubectl/Users]\n\n    CLIENT --&gt;|REST API| API\n    SCHED --&gt;|watch/update| API\n    CM --&gt;|watch/update| API\n    API &lt;--&gt;|read/write| ETCD\n    KUB1 --&gt;|watch/update| API\n    KUB2 --&gt;|watch/update| API\n    KUB3 --&gt;|watch/update| API\n\n    style API fill:#e1f5ff\n    style ETCD fill:#ffe5e5</code></pre> <p>Communication Patterns:</p> <ol> <li>kubectl \u2192 API Server: Users interact with cluster via kubectl</li> <li>API Server \u2194 etcd: All state stored in etcd</li> <li>Scheduler \u2192 API Server: Watches for unscheduled pods, updates bindings</li> <li>Controller Manager \u2192 API Server: Watches resources, reconciles state</li> <li>kubelet \u2192 API Server: Reports node/pod status, watches for assigned pods</li> </ol>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#kubernetes-object-model","title":"Kubernetes Object Model","text":"<p>Kubernetes manages objects that represent the desired state of your cluster.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#object-anatomy","title":"Object Anatomy","text":"<pre><code>apiVersion: v1              # API version\nkind: Pod                   # Object type\nmetadata:                   # Object metadata\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx\n    tier: frontend\n  annotations:\n    description: \"Example pod\"\nspec:                       # Desired state\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\nstatus:                     # Current state (managed by system)\n  phase: Running\n  conditions: [...]\n</code></pre> <p>Key Fields: - apiVersion: API group and version (<code>v1</code>, <code>apps/v1</code>, <code>networking.k8s.io/v1</code>) - kind: Object type (Pod, Deployment, Service) - metadata: Identifying information (name, namespace, labels) - spec: Desired state defined by user - status: Current state observed by system (read-only)</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#common-exam-tasks","title":"Common Exam Tasks","text":"","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#scenario-1-check-cluster-component-health","title":"Scenario 1: Check Cluster Component Health","text":"<pre><code># Check all control plane components\nkubectl get componentstatuses\n\n# Check system pods\nkubectl -n kube-system get pods\n\n# Verify API server\nkubectl get --raw='/healthz?verbose'\n\n# Check etcd health\nkubectl -n kube-system exec etcd-&lt;node&gt; -- etcdctl endpoint health\n\n# View node status\nkubectl get nodes\nkubectl describe node &lt;node-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#scenario-2-troubleshoot-kubelet-issues","title":"Scenario 2: Troubleshoot kubelet Issues","text":"<pre><code># On worker node:\nsystemctl status kubelet\njournalctl -u kubelet -f\n\n# Check kubelet config\ncat /var/lib/kubelet/config.yaml\n\n# Restart kubelet\nsystemctl restart kubelet\n\n# From control plane:\nkubectl describe node &lt;node-name&gt;\nkubectl get events --field-selector involvedObject.kind=Node\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#scenario-3-backup-and-restore-etcd","title":"Scenario 3: Backup and Restore etcd","text":"<pre><code># Backup\nETCDCTL_API=3 etcdctl snapshot save /backup/snapshot.db \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key\n\n# Verify backup\nETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db\n\n# Restore (advanced - exam may require)\nETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\\n  --data-dir=/var/lib/etcd-restore\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#practice-exercises","title":"Practice Exercises","text":"<ol> <li>Inspect Cluster Architecture</li> <li>List all control plane pods</li> <li>Check which nodes are running control plane components</li> <li> <p>Identify the API server endpoint and port</p> </li> <li> <p>Component Analysis</p> </li> <li>View logs from each control plane component</li> <li>Check resource usage of control plane pods</li> <li> <p>Identify which container runtime each node is using</p> </li> <li> <p>Troubleshooting Simulation</p> </li> <li>Simulate kubelet failure (stop service) and observe effects</li> <li>Check events to see scheduling decisions</li> <li>Examine etcd data to see how objects are stored</li> </ol>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Kubernetes is a distributed system with control plane and worker nodes</p> <p>\u2705 API server is the central hub - all communication flows through it</p> <p>\u2705 etcd stores all cluster state - critical for backups and disaster recovery</p> <p>\u2705 Scheduler assigns pods to nodes using filtering and scoring</p> <p>\u2705 Controllers maintain desired state through continuous reconciliation loops</p> <p>\u2705 kubelet is the node agent ensuring containers run as specified</p> <p>\u2705 kube-proxy handles networking enabling Service abstraction</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#quick-reference-commands","title":"Quick Reference Commands","text":"<pre><code># Cluster info\nkubectl cluster-info\nkubectl version\nkubectl api-resources\nkubectl api-versions\n\n# Component health\nkubectl get componentstatuses\nkubectl -n kube-system get pods\nkubectl get nodes\n\n# Component logs\nkubectl -n kube-system logs &lt;component-pod&gt;\njournalctl -u kubelet (on node)\njournalctl -u containerd (on node)\n\n# etcd operations\nETCDCTL_API=3 etcdctl endpoint health\nETCDCTL_API=3 etcdctl snapshot save &lt;file&gt;\nETCDCTL_API=3 etcdctl member list\n\n# Node inspection\nkubectl describe node &lt;node-name&gt;\nkubectl get nodes -o wide\nkubectl top nodes (requires metrics-server)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#next-steps","title":"Next Steps","text":"<p>Continue to Post 2: Setting Up Your Kubernetes Lab Environment to build your hands-on learning environment for practicing CKA exam scenarios.</p> <p>Related Posts: - Kubernetes CKA Mastery - Complete Learning Path - Post 3: kubectl Essentials (coming soon) - Post 15: RBAC and Security (coming soon)</p> <p>External Resources: - Kubernetes Components (Official Docs) - Kubernetes Architecture Diagram (CNCF) - CKA Exam Curriculum</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/","title":"Deployments, ReplicaSets, and Rolling Updates","text":"<p>Master Kubernetes workload controllers for production-grade application management. Learn deployment strategies, rolling updates, rollback procedures, and the complete controller hierarchy essential for CKA exam success.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#introduction","title":"Introduction","text":"<p>Deployments are the cornerstone of stateless application management in Kubernetes. While you can create individual pods, deployments provide self-healing, scaling, rolling updates, and declarative version management - capabilities essential for production environments.</p> <p>CKA Exam Domain: Workloads &amp; Scheduling (15% of exam)</p> <p>Why Deployments Matter for CKA: - Exam Frequency: Deployment tasks appear in nearly every CKA exam - Time Efficiency: Imperative deployment commands save critical exam minutes - Practical Relevance: 90% of production Kubernetes workloads use deployments - Foundation Knowledge: Understanding deployments builds intuition for other controllers</p> <p>Real-World Context: According to the 2024 CNCF survey, deployments manage over 80% of stateless workloads in production clusters. In the CKA exam, you'll create deployments, scale them, update images, rollback failed updates, and troubleshoot deployment issues - often under time pressure.</p> <p>What You'll Learn: - Deployment fundamentals and the controller hierarchy - ReplicaSet mechanics and pod lifecycle management - Rolling update strategies with surge and unavailability controls - Rollout management, history tracking, and rollback procedures - Other workload controllers: DaemonSets, StatefulSets, Jobs, CronJobs - CKA-specific commands, shortcuts, and time-saving patterns - Hands-on exercises replicating exam scenarios</p> <p>Prerequisites: Basic understanding of pods, YAML syntax, and kubectl commands. Review the \"Kubernetes Objects &amp; YAML\" post if needed.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#deployment-fundamentals","title":"Deployment Fundamentals","text":"","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#what-are-deployments","title":"What Are Deployments?","text":"<p>A Deployment is a Kubernetes controller that manages stateless applications through declarative configuration. Instead of manually creating and managing pods, you declare the desired state (image, replicas, update strategy), and the deployment controller continuously works to maintain that state.</p> <p>Core Benefits: - Self-Healing: Automatically replaces failed pods - Scaling: Horizontal scaling with a single command - Rolling Updates: Zero-downtime application updates - Rollback: Quick reversion to previous versions - Version History: Track and manage deployment revisions - Declarative Management: Infrastructure-as-code compatibility</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#the-controller-hierarchy","title":"The Controller Hierarchy","text":"<p>Deployments don't manage pods directly. They use a three-tier hierarchy:</p> <pre><code>graph TB\n    subgraph \"User Layer\"\n        USER[kubectl apply deployment.yaml]\n    end\n\n    subgraph \"Controller Layer\"\n        DEPLOY[Deployment Controller&lt;br/&gt;Manages: ReplicaSets]\n        RS[ReplicaSet Controller&lt;br/&gt;Manages: Pods]\n    end\n\n    subgraph \"Workload Layer\"\n        POD1[Pod 1&lt;br/&gt;app:nginx v1.21]\n        POD2[Pod 2&lt;br/&gt;app:nginx v1.21]\n        POD3[Pod 3&lt;br/&gt;app:nginx v1.21]\n    end\n\n    USER --&gt; DEPLOY\n    DEPLOY --&gt;|Creates/Updates| RS\n    RS --&gt;|Creates| POD1\n    RS --&gt;|Creates| POD2\n    RS --&gt;|Creates| POD3\n\n    style DEPLOY fill:#e1f5ff\n    style RS fill:#e8f5e8\n    style POD1 fill:#fff4e1\n    style POD2 fill:#fff4e1\n    style POD3 fill:#fff4e1</code></pre> <p>Hierarchy Responsibilities:</p> <ol> <li>Deployment: Manages application versions and update strategies</li> <li>Creates new ReplicaSets for updates</li> <li>Scales old ReplicaSets down during rollouts</li> <li>Maintains revision history</li> <li> <p>Handles rollback operations</p> </li> <li> <p>ReplicaSet: Ensures pod replica count matches desired state</p> </li> <li>Creates/deletes pods to match replica count</li> <li>Monitors pod health</li> <li> <p>Adopts orphaned pods matching its selector</p> </li> <li> <p>Pods: Run the actual application containers</p> </li> <li>Execute application workloads</li> <li>Report health status</li> <li>Managed entirely by their parent ReplicaSet</li> </ol>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#desired-state-reconciliation","title":"Desired State Reconciliation","text":"<p>Kubernetes operates on a reconciliation loop - the controller constantly compares desired state (your specification) with actual state (cluster reality) and takes corrective action.</p> <p>Reconciliation Example: <pre><code>Desired State: 3 nginx:1.21 pods\nActual State:  2 nginx:1.21 pods (1 pod crashed)\nAction:        Create 1 new pod \u2192 Desired = Actual\n</code></pre></p> <p>This self-healing behavior happens automatically without human intervention.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#basic-deployment-yaml-structure","title":"Basic Deployment YAML Structure","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3                    # Desired pod count\n  selector:\n    matchLabels:\n      app: nginx                 # Must match template labels\n  template:                      # Pod template\n    metadata:\n      labels:\n        app: nginx               # Pod labels\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\n</code></pre> <p>Critical YAML Elements:</p> <ul> <li><code>apiVersion: apps/v1</code>: Deployment API group (v1 is stable)</li> <li><code>kind: Deployment</code>: Resource type</li> <li><code>replicas</code>: Number of pod copies to maintain</li> <li><code>selector.matchLabels</code>: How deployment finds its pods</li> <li><code>template</code>: Pod specification (same as standalone pod YAML)</li> <li>Label Matching Rule: <code>selector.matchLabels</code> must match <code>template.metadata.labels</code></li> </ul> <p>CKA Tip: The label matching rule is strictly enforced. Mismatched labels cause <code>error: selector does not match template labels</code> - a common exam mistake.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#replicasets-deep-dive","title":"ReplicaSets Deep Dive","text":"","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#what-replicasets-do","title":"What ReplicaSets Do","text":"<p>A ReplicaSet is a controller that maintains a stable set of replica pods at any given time. It's primarily used by deployments but can be created independently (though this is discouraged).</p> <p>ReplicaSet Controller Responsibilities: 1. Count Maintenance: Ensure replica count matches <code>spec.replicas</code> 2. Pod Creation: Create pods from the template when count is low 3. Pod Deletion: Remove excess pods when count is high 4. Health Monitoring: Watch for pod failures and replace them 5. Label Selection: Find and manage pods matching the selector 6. Ownership: Set controller references on managed pods</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#pod-template-specifications","title":"Pod Template Specifications","text":"<p>The ReplicaSet's pod template defines the blueprint for all created pods:</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: nginx-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: nginx\n        tier: frontend\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 80\n</code></pre> <p>Template Components: - metadata.labels: Must match selector (required) - spec.containers: Container specifications - spec.volumes: Shared storage (if needed) - spec.nodeSelector: Node placement constraints - spec.affinity: Advanced scheduling rules</p> <p>CKA Insight: ReplicaSet templates are immutable - changes don't affect existing pods. You must delete old pods to get new configuration, which is why deployments handle updates instead.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#label-selectors-and-matching","title":"Label Selectors and Matching","text":"<p>Label selectors determine which pods a ReplicaSet manages:</p> <pre><code># Equality-based selector\nselector:\n  matchLabels:\n    app: nginx\n    environment: production\n\n# Set-based selector (more flexible)\nselector:\n  matchExpressions:\n  - key: app\n    operator: In\n    values: [nginx, apache]\n  - key: tier\n    operator: NotIn\n    values: [cache]\n  - key: environment\n    operator: Exists\n</code></pre> <p>Selector Operators: - <code>In</code>: Label value in specified set - <code>NotIn</code>: Label value not in specified set - <code>Exists</code>: Label key exists (value irrelevant) - <code>DoesNotExist</code>: Label key absent</p> <p>Selection Logic: <pre><code># ReplicaSet finds pods using label query\nkubectl get pods -l app=nginx,environment=production\n\n# Must return exactly 'replicas' count of pods\n# If less: Create more pods\n# If more: Delete excess pods\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#scale-operations","title":"Scale Operations","text":"<p>Scaling changes replica count imperatively or declaratively:</p> <pre><code># Imperative scaling (fast for exam)\nkubectl scale deployment nginx-deployment --replicas=5\n\n# Declarative scaling\nkubectl edit deployment nginx-deployment  # Change spec.replicas\nkubectl apply -f deployment.yaml          # Updated replicas in file\n\n# Autoscaling (HorizontalPodAutoscaler)\nkubectl autoscale deployment nginx-deployment --min=3 --max=10 --cpu-percent=80\n</code></pre> <p>Scaling Behavior: - Scale Up: ReplicaSet creates (replicas - current) new pods - Scale Down: ReplicaSet deletes (current - replicas) pods (newest first) - Immediate: Scaling is instant (no gradual rollout)</p> <p>CKA Time-Saver: Use <code>kubectl scale</code> for quick replica changes. Use <code>kubectl autoscale</code> only when explicitly required.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#orphaned-pods-and-adoption","title":"Orphaned Pods and Adoption","text":"<p>ReplicaSets adopt pods that match their selector, even if not originally created by that ReplicaSet:</p> <pre><code># Create standalone pod with matching labels\nkubectl run orphan --image=nginx --labels=app=nginx\n\n# Create ReplicaSet with same selector\nkubectl apply -f replicaset.yaml  # selector: app=nginx, replicas: 3\n\n# ReplicaSet adopts orphan pod\n# Only creates 2 new pods (3 - 1 existing = 2)\n</code></pre> <p>Adoption Mechanics: 1. ReplicaSet queries API for pods matching selector 2. Counts matching pods (including orphans) 3. Creates/deletes pods to match desired count 4. Sets <code>ownerReferences</code> on adopted pods</p> <p>CKA Warning: Be careful with label overlap. Pods can be unexpectedly adopted or deleted if selectors overlap.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#deployment-strategies","title":"Deployment Strategies","text":"<p>Kubernetes supports multiple deployment strategies for different use cases.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#rolling-updates-default-strategy","title":"Rolling Updates (Default Strategy)","text":"<p>Rolling updates gradually replace old pods with new ones, maintaining availability:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 10\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2              # Max pods above desired count\n      maxUnavailable: 1        # Max pods below desired count\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n</code></pre> <p>Rolling Update Parameters:</p> <ul> <li><code>maxSurge</code>: Maximum pods above <code>replicas</code> during update</li> <li>Absolute number: <code>maxSurge: 2</code> \u2192 10 + 2 = 12 max pods</li> <li>Percentage: <code>maxSurge: 25%</code> \u2192 10 + 2.5 = 12 max pods (rounds up)</li> <li> <p>Default: <code>25%</code></p> </li> <li> <p><code>maxUnavailable</code>: Maximum pods below <code>replicas</code> during update</p> </li> <li>Absolute number: <code>maxUnavailable: 1</code> \u2192 10 - 1 = 9 min available</li> <li>Percentage: <code>maxUnavailable: 10%</code> \u2192 10 - 1 = 9 min available</li> <li>Default: <code>25%</code></li> </ul> <p>Update Process Visualization:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Deployment\n    participant NewRS as New ReplicaSet\n    participant OldRS as Old ReplicaSet\n    participant Pods\n\n    User-&gt;&gt;Deployment: Update image: nginx:1.22\n    Deployment-&gt;&gt;NewRS: Create ReplicaSet (nginx:1.22, replicas=0)\n    Deployment-&gt;&gt;NewRS: Scale up by maxSurge (0 \u2192 2)\n    NewRS-&gt;&gt;Pods: Create 2 new pods\n\n    Note over Pods: Wait for new pods Ready\n\n    Deployment-&gt;&gt;OldRS: Scale down by maxUnavailable (10 \u2192 9)\n    OldRS-&gt;&gt;Pods: Delete 1 old pod\n\n    Deployment-&gt;&gt;NewRS: Scale up (2 \u2192 4)\n    NewRS-&gt;&gt;Pods: Create 2 new pods\n\n    Note over Pods: Repeat until complete\n\n    Deployment-&gt;&gt;OldRS: Scale down (1 \u2192 0)\n    Deployment-&gt;&gt;NewRS: Scale up (9 \u2192 10)\n\n    Note over Deployment: Update complete</code></pre> <p>Rolling Update Calculation: <pre><code>Given: replicas=10, maxSurge=2, maxUnavailable=1\n\nMax pods during update: 10 + 2 = 12\nMin available pods:     10 - 1 = 9\n\nUpdate proceeds in waves:\nWave 1: Create 2 new (total: 12), delete 1 old (available: 11)\nWave 2: Create 2 new (total: 13), delete 1 old (available: 11)\n...continue until all old pods replaced\n</code></pre></p> <p>CKA Strategy: Default rolling update works well for most exam scenarios. Only modify if explicitly required.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#recreate-strategy","title":"Recreate Strategy","text":"<p>Recreate terminates all old pods before creating new ones - causes downtime:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 10\n  strategy:\n    type: Recreate         # All old pods deleted first\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.22\n</code></pre> <p>Recreate Process: 1. Scale old ReplicaSet to 0 (delete all pods) 2. Wait for all old pods to terminate 3. Create new ReplicaSet with full replica count 4. Wait for all new pods to become ready</p> <p>Use Cases: - Stateful applications requiring clean shutdown - Database migrations needing downtime - Incompatible versions that can't coexist - Resource constraints preventing extra pods during update</p> <p>Downtime: Complete unavailability from step 1 to step 4 completion.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#bluegreen-deployments","title":"Blue/Green Deployments","text":"<p>Blue/Green maintains two complete environments and switches traffic atomically:</p> <pre><code># Blue deployment (current production)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-blue\n  labels:\n    version: blue\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: nginx\n      version: blue\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: blue\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n\n---\n# Green deployment (new version)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-green\n  labels:\n    version: green\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: nginx\n      version: green\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: green\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.22\n\n---\n# Service (traffic routing)\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx\n    version: blue      # Switch to 'green' for cutover\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <p>Blue/Green Process: 1. Deploy green environment alongside blue 2. Test green environment thoroughly 3. Switch service selector from <code>version: blue</code> to <code>version: green</code> 4. Monitor for issues 5. Keep blue environment running for quick rollback 6. Delete blue environment after validation period</p> <p>Advantages: - Instant Rollback: Change service selector back to blue - Zero Downtime: Both environments ready before cutover - Full Testing: Test production-like environment before cutover</p> <p>Disadvantages: - Resource Cost: 2x resources during transition - Database Complexity: Schema migrations require careful coordination</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#canary-deployments","title":"Canary Deployments","text":"<p>Canary deployments route small percentage of traffic to new version:</p> <pre><code># Stable deployment (90% traffic)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-stable\nspec:\n  replicas: 9\n  selector:\n    matchLabels:\n      app: nginx\n      track: stable\n  template:\n    metadata:\n      labels:\n        app: nginx\n        track: stable\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n\n---\n# Canary deployment (10% traffic)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-canary\nspec:\n  replicas: 1           # 1 of 10 total = 10% traffic\n  selector:\n    matchLabels:\n      app: nginx\n      track: canary\n  template:\n    metadata:\n      labels:\n        app: nginx\n        track: canary\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.22\n\n---\n# Service (routes to both)\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx          # Matches both stable and canary\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <p>Canary Process: 1. Deploy canary with small replica count (10%) 2. Monitor error rates, latency, user feedback 3. Gradually increase canary replicas (10% \u2192 25% \u2192 50%) 4. Decrease stable replicas correspondingly 5. Once validated, promote canary to 100% 6. Delete old stable deployment</p> <p>Traffic Distribution: <pre><code>Total pods: 10\nStable: 9 pods (90% traffic)\nCanary: 1 pod  (10% traffic)\n\n\u2192 Gradually shift:\nStable: 5 pods (50%), Canary: 5 pods (50%)\n\u2192 Finally:\nStable: 0 pods (0%),  Canary: 10 pods (100%)\n</code></pre></p> <p>CKA Note: Canary deployments rarely appear in CKA exam. Understand the concept but prioritize rolling updates and rollbacks.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#strategy-comparison","title":"Strategy Comparison","text":"Strategy Downtime Resource Cost Rollback Speed Complexity Use Case Rolling Update None Low (slight surge) Medium (gradual) Low Default, stateless apps Recreate Yes (full) None Medium (redeploy) Low Stateful apps, migrations Blue/Green None High (2x) Instant Medium Critical apps, easy rollback Canary None Medium Medium High Risk mitigation, gradual validation <p>CKA Exam Strategy: - 90% of exam tasks: Use default rolling update - Explicit downtime acceptable: Use recreate - \"Zero downtime\" emphasized: Verify rolling update configuration - \"Gradual rollout\" mentioned: Consider canary approach (but likely rolling update)</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#rollout-management","title":"Rollout Management","text":"<p>Kubernetes tracks deployment history and provides rollout control commands.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#kubectl-rollout-commands","title":"kubectl rollout Commands","text":"<p>Check Rollout Status: <pre><code># Watch rollout progress\nkubectl rollout status deployment/nginx-deployment\n\n# Output examples:\n# \"Waiting for deployment 'nginx-deployment' rollout to finish: 2 of 3 updated replicas...\"\n# \"deployment 'nginx-deployment' successfully rolled out\"\n</code></pre></p> <p>View Rollout History: <pre><code># List all revisions\nkubectl rollout history deployment/nginx-deployment\n\n# Output:\n# REVISION  CHANGE-CAUSE\n# 1         &lt;none&gt;\n# 2         kubectl set image deployment/nginx-deployment nginx=nginx:1.22\n# 3         kubectl set image deployment/nginx-deployment nginx=nginx:1.23\n\n# View specific revision details\nkubectl rollout history deployment/nginx-deployment --revision=2\n</code></pre></p> <p>Pause Rollout: <pre><code># Pause ongoing rollout\nkubectl rollout pause deployment/nginx-deployment\n\n# Make multiple changes while paused\nkubectl set image deployment/nginx-deployment nginx=nginx:1.24\nkubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=500m\n\n# Resume rollout (batches all changes into one revision)\nkubectl rollout resume deployment/nginx-deployment\n</code></pre></p> <p>Restart Rollout: <pre><code># Restart all pods (useful for config updates)\nkubectl rollout restart deployment/nginx-deployment\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#revision-history-tracking","title":"Revision History Tracking","text":"<p>Kubernetes stores old ReplicaSets for rollback purposes:</p> <pre><code># Default revision history limit\nkubectl get deployment nginx-deployment -o yaml | grep revisionHistoryLimit\n# spec.revisionHistoryLimit: 10 (default)\n\n# View all ReplicaSets (including old revisions)\nkubectl get replicasets\n\n# Output:\n# NAME                         DESIRED   CURRENT   READY   AGE\n# nginx-deployment-7d9c8f8d5   3         3         3       5m    # Current revision\n# nginx-deployment-6b8c7d6c4   0         0         0       10m   # Old revision\n# nginx-deployment-5a7b6c5b3   0         0         0       15m   # Old revision\n</code></pre> <p>Old ReplicaSets: - Scaled to 0 replicas (no pods running) - Retain pod template for rollback - Count limited by <code>revisionHistoryLimit</code> - Oldest deleted when limit exceeded</p> <p>Change Revision Limit: <pre><code>spec:\n  revisionHistoryLimit: 5  # Keep only 5 old ReplicaSets\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#rollback-procedures","title":"Rollback Procedures","text":"<p>Rollback to Previous Revision: <pre><code># Undo last rollout (rollback to revision N-1)\nkubectl rollout undo deployment/nginx-deployment\n\n# Process:\n# 1. Find previous ReplicaSet\n# 2. Scale previous ReplicaSet up\n# 3. Scale current ReplicaSet down\n# 4. Creates new revision number\n</code></pre></p> <p>Rollback to Specific Revision: <pre><code># Rollback to revision 2\nkubectl rollout undo deployment/nginx-deployment --to-revision=2\n\n# Verify rollback\nkubectl rollout status deployment/nginx-deployment\nkubectl describe deployment nginx-deployment | grep Image\n</code></pre></p> <p>Rollback Visualization:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Deployment\n    participant RevN as Revision N (current)\n    participant RevN1 as Revision N-1 (previous)\n\n    User-&gt;&gt;Deployment: kubectl rollout undo\n    Deployment-&gt;&gt;Deployment: Find previous revision (N-1)\n    Deployment-&gt;&gt;RevN1: Scale up (0 \u2192 3)\n\n    Note over RevN1: Create pods from old template\n\n    Deployment-&gt;&gt;RevN: Scale down (3 \u2192 2)\n    Deployment-&gt;&gt;RevN: Scale down (2 \u2192 1)\n    Deployment-&gt;&gt;RevN: Scale down (1 \u2192 0)\n\n    Note over Deployment: Rollback complete&lt;br/&gt;Creates new revision N+1\n\n    Deployment--&gt;&gt;User: Successfully rolled back</code></pre> <p>CKA Rollback Pattern: 1. Verify issue: <code>kubectl rollout status</code> shows failure 2. Check history: <code>kubectl rollout history</code> to see revisions 3. Rollback: <code>kubectl rollout undo</code> (or <code>--to-revision=N</code>) 4. Verify fix: <code>kubectl rollout status</code> confirms success 5. Validate: <code>kubectl get pods</code> shows healthy pods</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#pauseresume-rollouts","title":"Pause/Resume Rollouts","text":"<p>Pausing allows batching multiple changes into single revision:</p> <pre><code># Start with deployment\nkubectl create deployment nginx --image=nginx:1.21 --replicas=3\n\n# Pause rollout\nkubectl rollout pause deployment/nginx\n\n# Make multiple changes (no rollout triggered)\nkubectl set image deployment/nginx nginx=nginx:1.22\nkubectl set resources deployment/nginx -c=nginx --requests=cpu=100m,memory=64Mi\nkubectl set resources deployment/nginx -c=nginx --limits=cpu=200m,memory=128Mi\n\n# Resume rollout (all changes applied together)\nkubectl rollout resume deployment/nginx\n</code></pre> <p>Pause Benefits: - Atomic Changes: Multiple updates in single revision - Testing: Verify manifest changes before rollout - Coordination: Synchronize with other system changes</p> <p>CKA Usage: Rarely used in exam but valuable for complex updates.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#change-cause-annotations","title":"Change-Cause Annotations","text":"<p>Track why deployments were updated using annotations:</p> <pre><code># Annotate during update\nkubectl set image deployment/nginx nginx=nginx:1.22 \\\n  --record  # Deprecated but useful for history\n\n# Better approach: annotate explicitly\nkubectl annotate deployment/nginx kubernetes.io/change-cause=\"Update to nginx 1.22 for security patch\"\n\n# View in history\nkubectl rollout history deployment/nginx\n\n# Output:\n# REVISION  CHANGE-CAUSE\n# 1         &lt;none&gt;\n# 2         Update to nginx 1.22 for security patch\n</code></pre> <p>CKA Tip: <code>--record</code> flag is deprecated. Use explicit annotations for change tracking in production scenarios.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#other-workload-controllers","title":"Other Workload Controllers","text":"<p>Beyond deployments, Kubernetes provides specialized controllers for different workload patterns.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#daemonsets-one-pod-per-node","title":"DaemonSets (One Pod Per Node)","text":"<p>DaemonSets ensure exactly one pod runs on each node (or subset of nodes):</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: prom/node-exporter:latest\n        ports:\n        - containerPort: 9100\n      hostNetwork: true        # Access node metrics\n      hostPID: true            # Access node processes\n</code></pre> <p>DaemonSet Behavior: - Node Addition: Automatically create pod on new nodes - Node Removal: Delete pod when node deleted - Node Selector: Use <code>nodeSelector</code> or <code>affinity</code> to target node subset</p> <p>Common Use Cases: - Monitoring Agents: Prometheus node-exporter, Datadog agent - Log Collectors: Fluentd, Logstash - Network Plugins: CNI agents, kube-proxy - Storage Plugins: CSI drivers - Security Agents: Falco, Sysdig</p> <p>DaemonSet vs Deployment: <pre><code># DaemonSet\nkubectl get daemonset node-exporter\n# DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE\n# 5         5         5       5            5\n# (Matches node count)\n\n# Deployment\nkubectl get deployment nginx\n# READY   UP-TO-DATE   AVAILABLE\n# 3/3     3            3\n# (Explicitly set replicas)\n</code></pre></p> <p>CKA Commands: <pre><code># Create DaemonSet\nkubectl create -f daemonset.yaml\n\n# Update DaemonSet image\nkubectl set image daemonset/node-exporter node-exporter=prom/node-exporter:v1.3.0\n\n# Check DaemonSet rollout\nkubectl rollout status daemonset/node-exporter\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#statefulsets-ordered-stable-workloads","title":"StatefulSets (Ordered, Stable Workloads)","text":"<p>StatefulSets manage stateful applications requiring stable network identities and persistent storage:</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: mysql       # Headless service name\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        ports:\n        - containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n  volumeClaimTemplates:    # Creates PVC per pod\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre> <p>StatefulSet Guarantees:</p> <ol> <li>Stable Network Identity: Predictable pod names</li> <li><code>mysql-0</code>, <code>mysql-1</code>, <code>mysql-2</code> (not random hashes)</li> <li> <p>DNS: <code>mysql-0.mysql.default.svc.cluster.local</code></p> </li> <li> <p>Ordered Deployment: Sequential pod creation</p> </li> <li> <p>Creates <code>mysql-0</code>, waits for Ready, then <code>mysql-1</code>, etc.</p> </li> <li> <p>Ordered Termination: Reverse sequential deletion</p> </li> <li> <p>Deletes <code>mysql-2</code>, then <code>mysql-1</code>, then <code>mysql-0</code></p> </li> <li> <p>Stable Storage: PersistentVolumeClaims bound to pods</p> </li> <li><code>data-mysql-0</code> always attached to <code>mysql-0</code></li> <li>PVCs persist even if pod deleted</li> </ol> <p>StatefulSet vs Deployment:</p> <pre><code>graph TB\n    subgraph \"Deployment (Stateless)\"\n        D[Deployment]\n        DP1[nginx-7d9c8-abc12&lt;br/&gt;Random Name]\n        DP2[nginx-7d9c8-def34&lt;br/&gt;Random Name]\n        DP3[nginx-7d9c8-ghi56&lt;br/&gt;Random Name]\n\n        D --&gt; DP1\n        D --&gt; DP2\n        D --&gt; DP3\n    end\n\n    subgraph \"StatefulSet (Stateful)\"\n        S[StatefulSet]\n        SP1[mysql-0&lt;br/&gt;Stable Name&lt;br/&gt;PVC: data-mysql-0]\n        SP2[mysql-1&lt;br/&gt;Stable Name&lt;br/&gt;PVC: data-mysql-1]\n        SP3[mysql-2&lt;br/&gt;Stable Name&lt;br/&gt;PVC: data-mysql-2]\n\n        S --&gt; SP1\n        SP1 --&gt; SP2\n        SP2 --&gt; SP3\n    end\n\n    style DP1 fill:#fff4e1\n    style DP2 fill:#fff4e1\n    style DP3 fill:#fff4e1\n    style SP1 fill:#e1f5ff\n    style SP2 fill:#e8f5e8\n    style SP3 fill:#f5e1ff</code></pre> <p>Common Use Cases: - Databases: MySQL, PostgreSQL, MongoDB - Distributed Systems: Kafka, Zookeeper, Elasticsearch - Stateful Applications: Requires persistent identity or storage</p> <p>CKA Commands: <pre><code># Create StatefulSet\nkubectl apply -f statefulset.yaml\n\n# Scale StatefulSet\nkubectl scale statefulset mysql --replicas=5\n\n# Delete StatefulSet (keeps PVCs)\nkubectl delete statefulset mysql\n\n# Delete StatefulSet and PVCs\nkubectl delete statefulset mysql\nkubectl delete pvc -l app=mysql\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#jobs-run-to-completion","title":"Jobs (Run-to-Completion)","text":"<p>Jobs run pods to completion (successful termination) rather than continuously:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: database-backup\nspec:\n  template:\n    spec:\n      containers:\n      - name: backup\n        image: mysql:8.0\n        command:\n        - /bin/bash\n        - -c\n        - mysqldump -h mysql -u root -p$MYSQL_ROOT_PASSWORD mydb &gt; /backup/dump.sql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: password\n      restartPolicy: OnFailure  # Required for Jobs\n  backoffLimit: 4              # Retry failed jobs up to 4 times\n  completions: 1               # Run 1 pod successfully\n  parallelism: 1               # Run 1 pod at a time\n</code></pre> <p>Job Parameters:</p> <ul> <li><code>completions</code>: Number of successful pod completions required</li> <li> <p><code>completions: 3</code> \u2192 Run until 3 pods succeed</p> </li> <li> <p><code>parallelism</code>: Number of pods running simultaneously</p> </li> <li> <p><code>parallelism: 2</code> \u2192 Run 2 pods at once</p> </li> <li> <p><code>backoffLimit</code>: Number of retries before marking job failed</p> </li> <li> <p>Default: 6 retries</p> </li> <li> <p><code>activeDeadlineSeconds</code>: Maximum job runtime</p> </li> <li>Job fails if exceeds deadline</li> </ul> <p>Job Patterns:</p> <pre><code># Single Job (run once)\ncompletions: 1\nparallelism: 1\n\n# Parallel Job (run N times in parallel)\ncompletions: 10\nparallelism: 5      # 5 pods at a time until 10 complete\n\n# Work Queue Job (process queue until empty)\ncompletions: null   # No fixed count\nparallelism: 5      # 5 workers processing queue\n</code></pre> <p>CKA Commands: <pre><code># Create job\nkubectl create job backup --image=mysql:8.0 -- mysqldump ...\n\n# View job status\nkubectl get jobs\n\n# View job pods\nkubectl get pods --selector=job-name=backup\n\n# View job logs\nkubectl logs job/backup\n\n# Delete completed jobs\nkubectl delete job backup\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#cronjobs-scheduled-jobs","title":"CronJobs (Scheduled Jobs)","text":"<p>CronJobs create Jobs on a schedule:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: database-backup\nspec:\n  schedule: \"0 2 * * *\"        # 2am daily (cron syntax)\n  jobTemplate:                 # Job spec (same as Job)\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: mysql:8.0\n            command:\n            - /bin/bash\n            - -c\n            - mysqldump -h mysql -u root -p$MYSQL_ROOT_PASSWORD mydb &gt; /backup/dump-$(date +%Y%m%d).sql\n          restartPolicy: OnFailure\n  successfulJobsHistoryLimit: 3  # Keep last 3 successful jobs\n  failedJobsHistoryLimit: 1      # Keep last 1 failed job\n  concurrencyPolicy: Forbid      # Don't run if previous job still running\n</code></pre> <p>Cron Schedule Syntax: <pre><code># \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n# \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n# \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31)\n# \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n# \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of week (0 - 6) (Sunday to Saturday)\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# * * * * *\n\nExamples:\n\"0 2 * * *\"       # 2am daily\n\"*/5 * * * *\"     # Every 5 minutes\n\"0 */2 * * *\"     # Every 2 hours\n\"0 9 * * 1-5\"     # 9am weekdays\n\"0 0 1 * *\"       # 1st of month\n</code></pre></p> <p>Concurrency Policies: - <code>Allow</code> (default): Allow concurrent jobs - <code>Forbid</code>: Skip new job if previous still running - <code>Replace</code>: Cancel previous job, start new one</p> <p>CKA Commands: <pre><code># Create CronJob\nkubectl create cronjob backup --image=mysql:8.0 --schedule=\"0 2 * * *\" -- mysqldump ...\n\n# View CronJobs\nkubectl get cronjobs\n\n# Manually trigger CronJob (create job immediately)\nkubectl create job backup-manual --from=cronjob/backup\n\n# Suspend CronJob\nkubectl patch cronjob backup -p '{\"spec\":{\"suspend\":true}}'\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#controller-comparison","title":"Controller Comparison","text":"Controller Replicas Pod Names Restart Policy Storage Use Case Deployment Fixed count Random hash Always Ephemeral Stateless apps (web, API) ReplicaSet Fixed count Random hash Always Ephemeral Managed by Deployment DaemonSet 1 per node Random hash Always Ephemeral Node agents (monitoring, logging) StatefulSet Fixed count Ordered (mysql-0) Always Persistent Databases, distributed systems Job Completions Random hash OnFailure/Never Ephemeral Batch processing, migrations CronJob Scheduled Random hash OnFailure/Never Ephemeral Scheduled tasks (backups, reports) <p>CKA Controller Selection: - Web application: Deployment - Database cluster: StatefulSet - Log aggregator on each node: DaemonSet - Database migration: Job - Daily backup: CronJob</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#cka-exam-skills","title":"CKA Exam Skills","text":"","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#imperative-deployment-creation","title":"Imperative Deployment Creation","text":"<p>Create Deployment Quickly: <pre><code># Basic deployment\nkubectl create deployment nginx --image=nginx:1.21\n\n# Deployment with replicas\nkubectl create deployment nginx --image=nginx:1.21 --replicas=3\n\n# Generate YAML without creating\nkubectl create deployment nginx --image=nginx:1.21 --dry-run=client -o yaml &gt; deployment.yaml\n\n# Create and expose\nkubectl create deployment nginx --image=nginx:1.21 --replicas=3\nkubectl expose deployment nginx --port=80 --target-port=80\n</code></pre></p> <p>CKA Time-Saver: Use <code>kubectl create deployment</code> for basic deployments. Only write YAML for complex configurations.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#scaling-commands","title":"Scaling Commands","text":"<pre><code># Scale deployment\nkubectl scale deployment nginx --replicas=5\n\n# Scale multiple deployments\nkubectl scale deployment nginx frontend backend --replicas=3\n\n# Conditional scaling\nkubectl scale deployment nginx --current-replicas=3 --replicas=5\n\n# Autoscale (HPA)\nkubectl autoscale deployment nginx --min=3 --max=10 --cpu-percent=80\n</code></pre> <p>Verify Scaling: <pre><code>kubectl get deployment nginx\nkubectl get replicaset\nkubectl get pods -l app=nginx\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#image-updates","title":"Image Updates","text":"<pre><code># Update deployment image\nkubectl set image deployment/nginx nginx=nginx:1.22\n\n# Update multiple containers\nkubectl set image deployment/nginx nginx=nginx:1.22 sidecar=sidecar:2.0\n\n# Verify update\nkubectl rollout status deployment/nginx\nkubectl describe deployment nginx | grep Image\n</code></pre> <p>CKA Pattern: <pre><code># Update image and watch rollout\nkubectl set image deployment/nginx nginx=nginx:1.22 &amp;&amp; \\\nkubectl rollout status deployment/nginx\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#rollback-procedures_1","title":"Rollback Procedures","text":"<pre><code># View rollout history\nkubectl rollout history deployment/nginx\n\n# Rollback to previous version\nkubectl rollout undo deployment/nginx\n\n# Rollback to specific revision\nkubectl rollout undo deployment/nginx --to-revision=2\n\n# Verify rollback\nkubectl rollout status deployment/nginx\nkubectl get pods\n</code></pre> <p>CKA Rollback Scenario: <pre><code># Problem: Deployment stuck in rollout\nkubectl rollout status deployment/nginx\n# \"Waiting for rollout to finish: 1 of 3 updated replicas...\"\n\n# Investigate\nkubectl get pods\n# nginx-deployment-7d9c8f8d5-xyz12   0/1     ImagePullBackOff\n\n# Rollback immediately\nkubectl rollout undo deployment/nginx\n\n# Verify fix\nkubectl get pods\n# All pods Running\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#troubleshooting-deployments","title":"Troubleshooting Deployments","text":"<p>Common Issues and Solutions:</p> <ol> <li> <p>ImagePullBackOff: <pre><code># Symptom\nkubectl get pods\n# nginx-7d9c8-abc12   0/1   ImagePullBackOff\n\n# Diagnose\nkubectl describe pod nginx-7d9c8-abc12\n# Events: Failed to pull image \"nginx:typo\"\n\n# Fix\nkubectl set image deployment/nginx nginx=nginx:1.22\n</code></pre></p> </li> <li> <p>CrashLoopBackOff: <pre><code># Symptom\nkubectl get pods\n# nginx-7d9c8-abc12   0/1   CrashLoopBackOff\n\n# Diagnose\nkubectl logs nginx-7d9c8-abc12\n# Error: Configuration file not found\n\n# Fix (edit config)\nkubectl edit deployment nginx\n</code></pre></p> </li> <li> <p>Insufficient Resources: <pre><code># Symptom\nkubectl get pods\n# nginx-7d9c8-abc12   0/1   Pending\n\n# Diagnose\nkubectl describe pod nginx-7d9c8-abc12\n# Events: 0/3 nodes are available: insufficient cpu\n\n# Fix (reduce resources or scale cluster)\nkubectl set resources deployment/nginx -c=nginx --requests=cpu=100m\n</code></pre></p> </li> <li> <p>Label Selector Mismatch: <pre><code># Symptom\nkubectl get deployment nginx\n# READY: 0/3\n\nkubectl get replicaset\n# DESIRED: 3, CURRENT: 0\n\n# Diagnose\nkubectl get deployment nginx -o yaml | grep -A5 selector\n# selector.matchLabels doesn't match template.metadata.labels\n\n# Fix\nkubectl edit deployment nginx  # Align selector and labels\n</code></pre></p> </li> </ol> <p>CKA Troubleshooting Workflow: 1. Check pod status: <code>kubectl get pods</code> 2. Describe pod: <code>kubectl describe pod &lt;name&gt;</code> 3. Check logs: <code>kubectl logs &lt;name&gt;</code> 4. Check events: <code>kubectl get events --sort-by=.metadata.creationTimestamp</code> 5. Fix and verify: Update deployment, watch rollout</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exam-time-savers","title":"Exam Time-Savers","text":"<p>Aliases (provided in exam environment): <pre><code>alias k=kubectl\nalias kgp='kubectl get pods'\nalias kgd='kubectl get deployment'\nalias kd='kubectl describe'\n</code></pre></p> <p>kubectl Shortcuts: <pre><code># Short resource names\nkubectl get deploy       # Instead of 'deployments'\nkubectl get rs           # Instead of 'replicasets'\nkubectl get po           # Instead of 'pods'\n\n# Wide output (more columns)\nkubectl get pods -o wide\n\n# Watch mode (auto-refresh)\nkubectl get pods -w\n\n# All namespaces\nkubectl get pods -A\n</code></pre></p> <p>YAML Generation: <pre><code># Generate deployment YAML\nkubectl create deployment nginx --image=nginx:1.21 --dry-run=client -o yaml\n\n# Generate service YAML\nkubectl expose deployment nginx --port=80 --dry-run=client -o yaml\n\n# Combine and apply\nkubectl create deployment nginx --image=nginx:1.21 --dry-run=client -o yaml | kubectl apply -f -\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-1-create-and-scale-deployment","title":"Exercise 1: Create and Scale Deployment","text":"<p>Scenario: Deploy nginx web application with 3 replicas.</p> <pre><code># Create deployment\nkubectl create deployment web --image=nginx:1.21 --replicas=3\n\n# Verify deployment\nkubectl get deployment web\nkubectl get pods -l app=web\n\n# Scale to 5 replicas\nkubectl scale deployment web --replicas=5\n\n# Verify scaling\nkubectl get pods -l app=web -w  # Watch pods being created\n</code></pre> <p>Expected Output: <pre><code>NAME   READY   UP-TO-DATE   AVAILABLE   AGE\nweb    5/5     5            5           2m\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-2-rolling-update-and-rollback","title":"Exercise 2: Rolling Update and Rollback","text":"<p>Scenario: Update nginx to version 1.22, then rollback due to issues.</p> <pre><code># Initial deployment\nkubectl create deployment nginx --image=nginx:1.21 --replicas=3\n\n# Update to 1.22\nkubectl set image deployment/nginx nginx=nginx:1.22\n\n# Watch rollout\nkubectl rollout status deployment/nginx\n\n# Check history\nkubectl rollout history deployment/nginx\n\n# Simulate issue - rollback\nkubectl rollout undo deployment/nginx\n\n# Verify rollback\nkubectl describe deployment nginx | grep Image:\n# Should show nginx:1.21\n</code></pre> <p>Expected History: <pre><code>REVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment/nginx nginx=nginx:1.22\n3         kubectl rollout undo deployment/nginx\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-3-deployment-with-resource-limits","title":"Exercise 3: Deployment with Resource Limits","text":"<p>Scenario: Create deployment with resource requests and limits.</p> <pre><code># Generate base YAML\nkubectl create deployment app --image=nginx:1.21 --replicas=2 --dry-run=client -o yaml &gt; app.yaml\n\n# Edit app.yaml to add resources\n</code></pre> <pre><code># app.yaml (add under containers)\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n</code></pre> <pre><code># Apply deployment\nkubectl apply -f app.yaml\n\n# Verify resources\nkubectl describe deployment app | grep -A5 Limits\n</code></pre> <p>Expected Output: <pre><code>    Limits:\n      cpu:     500m\n      memory:  128Mi\n    Requests:\n      cpu:        250m\n      memory:     64Mi\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-4-daemonset-creation","title":"Exercise 4: DaemonSet Creation","text":"<p>Scenario: Deploy monitoring agent to all nodes.</p> <pre><code># monitoring-agent.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-monitor\nspec:\n  selector:\n    matchLabels:\n      app: node-monitor\n  template:\n    metadata:\n      labels:\n        app: node-monitor\n    spec:\n      containers:\n      - name: monitor\n        image: prom/node-exporter:latest\n        ports:\n        - containerPort: 9100\n</code></pre> <pre><code># Create DaemonSet\nkubectl apply -f monitoring-agent.yaml\n\n# Verify one pod per node\nkubectl get daemonset node-monitor\nkubectl get pods -l app=node-monitor -o wide\n\n# Count should match node count\nkubectl get nodes --no-headers | wc -l\nkubectl get pods -l app=node-monitor --no-headers | wc -l\n</code></pre>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#exercise-5-statefulset-with-headless-service","title":"Exercise 5: StatefulSet with Headless Service","text":"<p>Scenario: Deploy a 3-replica MySQL StatefulSet.</p> <pre><code># mysql-statefulset.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  clusterIP: None           # Headless service\n  selector:\n    app: mysql\n  ports:\n  - port: 3306\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: mysql\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"password\"  # Use secrets in production!\n        ports:\n        - containerPort: 3306\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre> <pre><code># Create StatefulSet\nkubectl apply -f mysql-statefulset.yaml\n\n# Watch ordered pod creation\nkubectl get pods -l app=mysql -w\n\n# Verify stable names\nkubectl get pods -l app=mysql\n# mysql-0, mysql-1, mysql-2\n\n# Verify PVCs\nkubectl get pvc\n# data-mysql-0, data-mysql-1, data-mysql-2\n\n# Test stable network identity\nkubectl run -it --rm debug --image=busybox --restart=Never -- nslookup mysql-0.mysql\n</code></pre> <p>Expected Pod Order: <pre><code>mysql-0   1/1   Running   # Created first\nmysql-1   1/1   Running   # Created second (after mysql-0 Ready)\nmysql-2   1/1   Running   # Created third (after mysql-1 Ready)\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/kubernetes-deployments-replicasets/#summary","title":"Summary","text":"<p>Deployments form the foundation of stateless workload management in Kubernetes. Key takeaways for CKA success:</p> <p>Essential Concepts: - Deployment Hierarchy: Deployment \u2192 ReplicaSet \u2192 Pods - Rolling Updates: Default strategy for zero-downtime updates - Rollback Capability: Revision history enables quick recovery - Controller Specialization: DaemonSet (per-node), StatefulSet (stateful), Job (batch)</p> <p>CKA Exam Priorities: 1. Imperative Commands: <code>kubectl create deployment</code>, <code>kubectl scale</code>, <code>kubectl set image</code> 2. Rollout Management: <code>kubectl rollout status</code>, <code>kubectl rollout undo</code> 3. Troubleshooting: Diagnose ImagePullBackOff, CrashLoopBackOff, Pending pods 4. YAML Generation: Use <code>--dry-run=client -o yaml</code> for complex resources</p> <p>Time-Saving Strategies: - Use imperative commands for simple deployments - Generate YAML templates for complex configurations - Master <code>kubectl rollout</code> commands for update scenarios - Understand label selectors to avoid common pitfalls</p> <p>Next Steps: - Practice deployment creation and scaling until automatic - Build muscle memory for rollout and rollback commands - Understand when to use each controller type - Review Services and Ingress for complete application deployment</p> <p>Related Topics: - Services: Expose deployments with stable networking - ConfigMaps &amp; Secrets: Inject configuration into deployments - Persistent Volumes: StatefulSet storage requirements - Resource Quotas: Control deployment resource consumption</p> <p>With deployments mastered, you control application lifecycle management - a core competency for CKA certification and production Kubernetes operations.</p>","tags":["kubernetes","k8s","cka-prep","deployments","replicasets","workloads","rolling-updates"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/","title":"Setting Up Your Kubernetes Lab Environment","text":"<p>Master the art of building Kubernetes clusters for CKA exam preparation. Learn kubeadm for production-grade setups, kind for rapid testing, and essential kubectl configuration for efficient cluster management.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#overview","title":"Overview","text":"<p>A proper lab environment is critical for CKA exam success. The exam uses kubeadm-based clusters, making hands-on practice with real cluster setup essential. This guide covers four primary methods for building Kubernetes environments:</p> <ol> <li>kubeadm - Production-grade multi-node clusters (exam environment)</li> <li>kind - Fast container-based clusters for rapid iteration</li> <li>Minikube - Single-node local development with rich addons</li> <li>kubectl - Essential CLI tool configuration and management</li> </ol> <p>CKA Exam Domain: Cluster Architecture, Installation &amp; Configuration (25%)</p> <p>Key Finding: While the CKA exam uses kubeadm clusters, kind offers the fastest iteration for practice exercises. A combination of both provides optimal preparation.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#lab-environment-decision-tree","title":"Lab Environment Decision Tree","text":"<p>Choosing the right lab setup depends on your immediate needs and learning phase.</p> <pre><code>flowchart TD\n    Start([Need Kubernetes Lab?]) --&gt; Purpose{What's the purpose?}\n\n    Purpose --&gt;|CKA Exam Prep| Exam{Practice&lt;br/&gt;or&lt;br/&gt;Full Simulation?}\n    Purpose --&gt;|Local Development| Dev{Single or&lt;br/&gt;Multi-Node?}\n    Purpose --&gt;|CI/CD Testing| CICD[Use kind&lt;br/&gt;Fast iteration]\n\n    Exam --&gt;|Quick Practice| kind_exam[Use kind&lt;br/&gt;30-second clusters]\n    Exam --&gt;|Full Simulation| kubeadm_exam[Use kubeadm&lt;br/&gt;Production-like setup]\n\n    Dev --&gt;|Single Node&lt;br/&gt;+ Addons| Minikube[Use Minikube&lt;br/&gt;Rich ecosystem]\n    Dev --&gt;|Multi-Node&lt;br/&gt;Testing| kind_dev[Use kind&lt;br/&gt;Config-based]\n    Dev --&gt;|Production Sim| kubeadm_dev[Use kubeadm&lt;br/&gt;Real cluster]\n\n    CICD --&gt; fast{Speed&lt;br/&gt;Important?}\n    fast --&gt;|Yes| kind_fast[Use kind&lt;br/&gt;Container-based]\n    fast --&gt;|No| minikube_ci[Use Minikube&lt;br/&gt;More features]\n\n    style kubeadm_exam fill:#ff9999\n    style kubeadm_dev fill:#ff9999\n    style Minikube fill:#99ccff\n    style kind_exam fill:#99ff99\n    style kind_dev fill:#99ff99\n    style kind_fast fill:#99ff99\n    style CICD fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeadm-production-grade-cluster-setup","title":"kubeadm: Production-Grade Cluster Setup","text":"<p>kubeadm is the official Kubernetes tool for bootstrapping production-grade clusters. This is the tool used in the CKA exam environment.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#prerequisites","title":"Prerequisites","text":"<p>System Requirements (Per Node): - Ubuntu 20.04/22.04/24.04 or equivalent Debian-based system - 2+ CPUs - 2GB+ RAM - Network connectivity between all nodes - Unique hostname, MAC address, and product_uuid per node - Swap disabled (critical requirement)</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#complete-installation-process","title":"Complete Installation Process","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-1-disable-swap-critical","title":"Step 1: Disable Swap (CRITICAL)","text":"<p>Kubernetes scheduler relies on accurate resource allocation. Swap can cause unpredictable behavior.</p> <pre><code># Temporary disable\nsudo swapoff -a\n\n# Permanent disable - comment out swap in /etc/fstab\nsudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\n\n# Verify (swap line should show 0)\nfree -h\n</code></pre> <p>Exam Alert</p> <p>The CKA exam expects swap to be disabled. Forgetting this step causes kubeadm init to fail.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-2-install-container-runtime-containerd","title":"Step 2: Install Container Runtime (containerd)","text":"<pre><code># Install containerd\nsudo apt-get update\nsudo apt-get install -y containerd\n\n# Configure containerd\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\n\n# Restart and enable containerd\nsudo systemctl restart containerd\nsudo systemctl enable containerd\n\n# Verify containerd is running\nsystemctl status containerd\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-3-install-kubeadm-kubelet-kubectl","title":"Step 3: Install kubeadm, kubelet, kubectl","text":"<pre><code># Update package index and install prerequisites\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\n\n# Add Kubernetes GPG key (NEW REPOSITORY)\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | \\\n  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# Add Kubernetes repository\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | \\\n  sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n# Install Kubernetes components\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\n\n# Hold packages at current version (prevent auto-upgrade)\nsudo apt-mark hold kubelet kubeadm kubectl\n\n# Enable kubelet\nsudo systemctl enable --now kubelet\n</code></pre> <p>Repository Change</p> <p>The old repository <code>https://apt.kubernetes.io</code> is deprecated. Always use <code>pkgs.k8s.io</code> for new installations.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-4-initialize-control-plane-node","title":"Step 4: Initialize Control Plane Node","text":"<pre><code># Basic initialization with pod network CIDR\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n\n# For HA setup with load balancer\nsudo kubeadm init \\\n  --control-plane-endpoint \"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\" \\\n  --upload-certs \\\n  --pod-network-cidr=10.244.0.0/16\n</code></pre> <p>Expected Output: <pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a Pod network to the cluster.\n\nYou can now join any number of machines by running the following on each node:\n\n  kubeadm join 192.168.0.200:6443 \\\n    --token 9vr73a.a8uxyaju799qwdjv \\\n    --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d\n</code></pre></p> <p>Save the join command - you'll need it for worker nodes.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-5-configure-kubectl-access","title":"Step 5: Configure kubectl Access","text":"<pre><code># Configure kubectl for regular user\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n# Verify cluster access\nkubectl cluster-info\nkubectl get nodes\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-6-install-cni-network-plugin-critical","title":"Step 6: Install CNI Network Plugin (CRITICAL)","text":"<p>Without a CNI plugin, nodes remain in \"NotReady\" state and pods cannot communicate.</p> <p>Option A: Calico (Recommended for CKA)</p> <pre><code># Install Calico operator\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\n\n# Apply custom resources\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml\n\n# Verify installation (wait for all pods to be Running)\nwatch kubectl get pods -n calico-system\n</code></pre> <p>Option B: Flannel</p> <pre><code>kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n</code></pre> <p>Option C: Weave Net</p> <pre><code>kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml\n</code></pre> <p>After CNI installation, verify nodes are Ready:</p> <pre><code>kubectl get nodes\n# NAME            STATUS   ROLES           AGE   VERSION\n# control-plane   Ready    control-plane   5m    v1.34.0\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#step-7-join-worker-nodes","title":"Step 7: Join Worker Nodes","text":"<p>On each worker node (after completing Steps 1-3):</p> <pre><code># Use the join command from Step 4 output\nsudo kubeadm join 192.168.0.200:6443 \\\n  --token 9vr73a.a8uxyaju799qwdjv \\\n  --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d\n</code></pre> <p>If token expired (tokens are valid for 24 hours):</p> <pre><code># Generate new join command on control plane\nkubeadm token create --print-join-command\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeadm-cluster-architecture","title":"kubeadm Cluster Architecture","text":"<pre><code>graph TB\n    subgraph \"Control Plane Node\"\n        APIServer[API Server&lt;br/&gt;:6443]\n        Scheduler[Scheduler]\n        Controller[Controller Manager]\n        etcd[(etcd&lt;br/&gt;:2379-2380)]\n        KubeletCP[Kubelet&lt;br/&gt;:10250]\n        ProxyCP[kube-proxy]\n    end\n\n    subgraph \"Worker Node 1\"\n        KubeletW1[Kubelet&lt;br/&gt;:10250]\n        ProxyW1[kube-proxy]\n        Pod1[Pod]\n        Pod2[Pod]\n        CRI1[containerd]\n    end\n\n    subgraph \"Worker Node 2\"\n        KubeletW2[Kubelet&lt;br/&gt;:10250]\n        ProxyW2[kube-proxy]\n        Pod3[Pod]\n        Pod4[Pod]\n        CRI2[containerd]\n    end\n\n    subgraph \"CNI Network Layer\"\n        CNI[Calico/Flannel&lt;br/&gt;10.244.0.0/16]\n    end\n\n    APIServer --&gt; Scheduler\n    APIServer --&gt; Controller\n    APIServer --&gt; etcd\n    APIServer -.-&gt;|watches| KubeletCP\n    APIServer -.-&gt;|watches| KubeletW1\n    APIServer -.-&gt;|watches| KubeletW2\n\n    KubeletW1 --&gt; CRI1\n    KubeletW2 --&gt; CRI2\n    CRI1 --&gt; Pod1\n    CRI1 --&gt; Pod2\n    CRI2 --&gt; Pod3\n    CRI2 --&gt; Pod4\n\n    CNI --&gt; Pod1\n    CNI --&gt; Pod2\n    CNI --&gt; Pod3\n    CNI --&gt; Pod4\n\n    style APIServer fill:#e1f5ff\n    style etcd fill:#ffe5e5\n    style CNI fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#common-kubeadm-issues-and-solutions","title":"Common kubeadm Issues and Solutions","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#issue-1-swap-is-enabled-production-deployments-should-disable-swap","title":"Issue 1: \"swap is enabled; production deployments should disable swap\"","text":"<p>Cause: Swap not properly disabled</p> <p>Solution: <pre><code>sudo swapoff -a\nsudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\nsudo reboot\nfree -h  # Verify swap shows 0\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#issue-2-container-runtime-network-not-ready-cni-config-uninitialized","title":"Issue 2: \"container runtime network not ready: cni config uninitialized\"","text":"<p>Cause: No CNI plugin installed</p> <p>Solution: <pre><code># Check CNI config directory\nls /etc/cni/net.d/\n\n# If empty, install CNI plugin (Calico/Flannel)\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml\n\n# Verify CNI pods running\nkubectl get pods -n calico-system\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#issue-3-port-6443-already-in-use","title":"Issue 3: Port 6443 already in use","text":"<p>Cause: Previous kubeadm installation not cleaned up</p> <p>Solution: <pre><code># Reset kubeadm completely\nsudo kubeadm reset\n\n# Clean up directories\nsudo rm -rf /etc/cni/net.d\nsudo rm -rf $HOME/.kube/config\n\n# Try initialization again\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#issue-4-worker-node-join-fails","title":"Issue 4: Worker node join fails","text":"<p>Causes: Token expired, network connectivity, firewall rules</p> <p>Solutions: <pre><code># Generate new join command\nkubeadm token create --print-join-command\n\n# Check connectivity from worker to control plane\nping &lt;control-plane-ip&gt;\nnc -zv &lt;control-plane-ip&gt; 6443\n\n# Required firewall ports:\n# Control Plane: 6443, 2379-2380, 10250-10252\n# Worker Nodes: 10250, 30000-32767\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeadm-configuration-file-approach","title":"kubeadm Configuration File Approach","text":"<p>For repeatable setups, use configuration files:</p> <pre><code># kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta4\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.0.200\n  bindPort: 6443\n---\napiVersion: kubeadm.k8s.io/v1beta4\nkind: ClusterConfiguration\nnetworking:\n  podSubnet: 10.244.0.0/16\n  serviceSubnet: 10.96.0.0/16\nkubernetesVersion: v1.34.0\n---\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\n</code></pre> <pre><code># Initialize with config file\nsudo kubeadm init --config=kubeadm-config.yaml\n\n# Generate default config template\nkubeadm config print init-defaults &gt; kubeadm-config.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kind-kubernetes-in-docker","title":"kind: Kubernetes in Docker","text":"<p>kind runs Kubernetes clusters using Docker containers as \"nodes\". Excellent for rapid iteration and CKA practice.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#installation","title":"Installation","text":"<p>Linux/macOS: <pre><code># Using Homebrew (macOS)\nbrew install kind\n\n# Direct binary download (Linux)\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/latest/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Verify installation\nkind version\n</code></pre></p> <p>Prerequisites: - Docker installed and running - kubectl installed - 4GB+ RAM allocated to Docker - 2+ CPUs for multi-node clusters</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#basic-usage","title":"Basic Usage","text":"<pre><code># Create default single-node cluster\nkind create cluster\n\n# Create cluster with custom name\nkind create cluster --name cka-practice\n\n# List clusters\nkind get clusters\n\n# Delete cluster\nkind delete cluster --name cka-practice\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#multi-node-cluster-configuration","title":"Multi-Node Cluster Configuration","text":"<p>2-Node Cluster (1 Control Plane + 1 Worker):</p> <pre><code># kind-2node.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n  - role: worker\n</code></pre> <pre><code>kind create cluster --config kind-2node.yaml --name cka-lab\n</code></pre> <p>3-Node Cluster with Port Mapping:</p> <pre><code># kind-3node.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n    extraPortMappings:\n    - containerPort: 30080\n      hostPort: 8080\n      protocol: TCP\n  - role: worker\n  - role: worker\n</code></pre> <pre><code>kind create cluster --config kind-3node.yaml --name multinode\n</code></pre> <p>High Availability Control Plane (3 Control Plane + 3 Worker):</p> <pre><code># kind-ha.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n  - role: control-plane\n  - role: control-plane\n  - role: worker\n  - role: worker\n  - role: worker\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kind-cluster-lifecycle","title":"kind Cluster Lifecycle","text":"<pre><code>sequenceDiagram\n    participant User\n    participant kind\n    participant Docker\n    participant kubectl\n\n    User-&gt;&gt;kind: kind create cluster --config kind-3node.yaml\n    kind-&gt;&gt;Docker: Pull kindest/node:v1.34.0 image\n    Docker--&gt;&gt;kind: Image ready\n\n    kind-&gt;&gt;Docker: Create control-plane container\n    kind-&gt;&gt;Docker: Create worker-1 container\n    kind-&gt;&gt;Docker: Create worker-2 container\n    Docker--&gt;&gt;kind: 3 containers running\n\n    kind-&gt;&gt;kind: Bootstrap Kubernetes in containers\n    kind-&gt;&gt;kind: Configure CNI (kindnetd)\n    kind-&gt;&gt;kind: Wait for cluster ready\n\n    kind-&gt;&gt;kubectl: Update kubeconfig\n    kind--&gt;&gt;User: Cluster ready (30-60 seconds)\n\n    User-&gt;&gt;kubectl: kubectl get nodes\n    kubectl--&gt;&gt;User: 3 nodes Ready\n\n    Note over User,kubectl: Practice CKA scenarios\n\n    User-&gt;&gt;kind: kind delete cluster\n    kind-&gt;&gt;Docker: Remove all containers\n    Docker--&gt;&gt;kind: Cleanup complete\n    kind--&gt;&gt;User: Cluster deleted</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#load-docker-images-into-kind","title":"Load Docker Images into kind","text":"<p>For testing custom applications without pushing to registry:</p> <pre><code># Build image locally\ndocker build -t my-app:1.0 .\n\n# Load into kind cluster\nkind load docker-image my-app:1.0 --name cka-lab\n\n# Verify image available in cluster\ndocker exec -it cka-lab-control-plane crictl images | grep my-app\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#when-to-use-kind","title":"When to Use kind","text":"<p>Best For: - CKA rapid practice: Create/destroy clusters in seconds - Multi-node testing: Easy configuration for complex topologies - CI/CD pipelines: Fast, reproducible test environments - Integration testing: Test applications in real cluster - Quick experiments: Try configurations without VM overhead</p> <p>Not Ideal For: - Learning basics: Minikube has better addon ecosystem - Production simulation: kubeadm provides real multi-machine setup - Resource-constrained systems: Requires Docker overhead</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#minikube-local-development-environment","title":"Minikube: Local Development Environment","text":"<p>Minikube creates single-node Kubernetes clusters on your local machine. Ideal for learning with rich addon support.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#installation_1","title":"Installation","text":"<p>macOS: <pre><code>brew install minikube\nminikube version\n</code></pre></p> <p>Linux: <pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\nminikube version\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#driver-options","title":"Driver Options","text":"<p>Docker Driver (Recommended): <pre><code># Start with Docker driver\nminikube start --driver=docker\n\n# Set as default\nminikube config set driver docker\n</code></pre></p> <p>QEMU Driver (ARM/M1 Macs): <pre><code>minikube start --driver=qemu\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#resource-configuration","title":"Resource Configuration","text":"<pre><code># Specify CPUs, memory, disk\nminikube start \\\n  --driver=docker \\\n  --cpus=4 \\\n  --memory=8192 \\\n  --disk-size=40g \\\n  --kubernetes-version=v1.34.0\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#common-minikube-commands","title":"Common Minikube Commands","text":"<pre><code># Start cluster\nminikube start\n\n# Stop cluster (preserves state)\nminikube stop\n\n# Delete cluster\nminikube delete\n\n# Check status\nminikube status\n\n# SSH into node\nminikube ssh\n\n# Open dashboard\nminikube dashboard\n\n# List addons\nminikube addons list\n\n# Enable addons\nminikube addons enable ingress\nminikube addons enable metrics-server\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#minikube-addons-for-cka-practice","title":"Minikube Addons for CKA Practice","text":"<pre><code># Metrics Server (for kubectl top)\nminikube addons enable metrics-server\n\n# Ingress Controller\nminikube addons enable ingress\n\n# Storage Provisioner (dynamic PVs)\nminikube addons enable storage-provisioner\n\n# Dashboard\nminikube addons enable dashboard\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubectl-installation-and-configuration","title":"kubectl Installation and Configuration","text":"<p>kubectl is the Kubernetes command-line tool. Mastery is essential for CKA exam success.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#installation_2","title":"Installation","text":"<p>Linux: <pre><code># Download latest release\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n\n# Install\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\n# Verify\nkubectl version --client\n</code></pre></p> <p>macOS: <pre><code># Using Homebrew\nbrew install kubectl\n\n# Verify\nkubectl version --client\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#version-compatibility","title":"Version Compatibility","text":"<p>kubectl version must be within \u00b11 minor version of cluster version.</p> <pre><code># Check versions\nkubectl version --short\n\n# Example compatible versions:\n# Cluster: v1.34.0\n# kubectl: v1.33.x, v1.34.x, v1.35.x \u2705\n# kubectl: v1.32.x, v1.36.x \u274c\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#shell-completion-setup-critical-for-exam-speed","title":"Shell Completion Setup (CRITICAL for Exam Speed)","text":"<p>Bash (Linux): <pre><code># Install bash-completion package\nsudo apt-get install bash-completion\n\n# Add completion to current shell\nsource &lt;(kubectl completion bash)\n\n# Add to .bashrc for persistence\necho 'source &lt;(kubectl completion bash)' &gt;&gt; ~/.bashrc\n\n# Alias completion (essential for exam)\necho 'alias k=kubectl' &gt;&gt; ~/.bashrc\necho 'complete -o default -F __start_kubectl k' &gt;&gt; ~/.bashrc\n\n# Reload\nsource ~/.bashrc\n</code></pre></p> <p>Zsh: <pre><code># Add completion to .zshrc\necho 'source &lt;(kubectl completion zsh)' &gt;&gt; ~/.zshrc\n\n# Alias completion\necho 'alias k=kubectl' &gt;&gt; ~/.zshrc\necho 'complete -o default -F __start_kubectl k' &gt;&gt; ~/.zshrc\n\n# Reload\nsource ~/.zshrc\n</code></pre></p> <p>Exam Tip</p> <p>Practice using <code>k</code> instead of <code>kubectl</code> extensively. Tab completion saves critical minutes during the exam.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeconfig-management","title":"kubeconfig Management","text":"<p>kubeconfig files contain cluster connection details. Managing multiple clusters efficiently is essential for the exam.</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeconfig-file-structure","title":"kubeconfig File Structure","text":"<pre><code>apiVersion: v1\nkind: Config\ncurrent-context: my-cluster\n\nclusters:\n- cluster:\n    certificate-authority-data: &lt;base64-encoded-ca&gt;\n    server: https://192.168.1.100:6443\n  name: my-cluster\n\nusers:\n- name: my-user\n  user:\n    client-certificate-data: &lt;base64-encoded-cert&gt;\n    client-key-data: &lt;base64-encoded-key&gt;\n\ncontexts:\n- context:\n    cluster: my-cluster\n    user: my-user\n    namespace: default\n  name: my-cluster-context\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#context-management-flow","title":"Context Management Flow","text":"<pre><code>graph TB\n    subgraph \"kubeconfig File\"\n        Clusters[Clusters&lt;br/&gt;- prod-cluster&lt;br/&gt;- dev-cluster&lt;br/&gt;- cka-cluster]\n        Users[Users&lt;br/&gt;- admin-user&lt;br/&gt;- dev-user&lt;br/&gt;- readonly-user]\n        Contexts[Contexts&lt;br/&gt;- prod-admin&lt;br/&gt;- dev-user&lt;br/&gt;- cka-practice]\n        CurrentContext[Current Context:&lt;br/&gt;cka-practice]\n    end\n\n    subgraph \"Context: cka-practice\"\n        CKACluster[Cluster: cka-cluster]\n        CKAUser[User: admin-user]\n        CKANamespace[Namespace: default]\n    end\n\n    subgraph \"Actual Clusters\"\n        ProdCluster[Production&lt;br/&gt;10.0.1.100:6443]\n        DevCluster[Development&lt;br/&gt;10.0.2.100:6443]\n        CKAClusterActual[CKA Lab&lt;br/&gt;10.0.3.100:6443]\n    end\n\n    Contexts --&gt; CurrentContext\n    CurrentContext --&gt; CKACluster\n    CurrentContext --&gt; CKAUser\n    CurrentContext --&gt; CKANamespace\n\n    CKACluster -.-&gt;|kubectl commands| CKAClusterActual\n\n    style CurrentContext fill:#ffff99\n    style CKAClusterActual fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#essential-context-commands","title":"Essential Context Commands","text":"<pre><code># List all contexts\nkubectl config get-contexts\n\n# Show current context\nkubectl config current-context\n\n# Switch context\nkubectl config use-context cka-practice\n\n# Set default namespace for current context\nkubectl config set-context --current --namespace=kube-system\n\n# Create new context\nkubectl config set-context dev-user \\\n  --cluster=dev-cluster \\\n  --namespace=development \\\n  --user=dev-user\n\n# Rename context\nkubectl config rename-context old-name new-name\n\n# Delete context\nkubectl config delete-context old-context\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#multiple-cluster-management","title":"Multiple Cluster Management","text":"<p>Strategy 1: Merged Configuration (Recommended for Exam):</p> <pre><code># Merge multiple kubeconfig files\nexport KUBECONFIG=~/.kube/config:~/.kube/dev-config:~/.kube/prod-config\nkubectl config view --flatten &gt; ~/.kube/merged-config\n\n# Set as default\nexport KUBECONFIG=~/.kube/merged-config\n\n# Add to .bashrc for persistence\necho 'export KUBECONFIG=~/.kube/merged-config' &gt;&gt; ~/.bashrc\n</code></pre> <p>Strategy 2: Context-Specific Commands:</p> <pre><code># Use specific context for single command\nkubectl --context=prod-cluster get nodes\n\n# Use specific kubeconfig for single command\nkubectl --kubeconfig=~/.kube/prod-config get pods\n\n# Override namespace for single command\nkubectl -n kube-system get pods\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#security-best-practices","title":"Security Best Practices","text":"<pre><code># Protect kubeconfig files\nchmod 600 ~/.kube/config\nchmod 0400 ~/.kube/prod-config  # Read-only for production\n\n# Never commit kubeconfig to version control\necho '.kube/' &gt;&gt; ~/.gitignore\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#common-exam-tasks","title":"Common Exam Tasks","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#scenario-1-rapid-cluster-setup-for-practice","title":"Scenario 1: Rapid Cluster Setup for Practice","text":"<p>Objective: Create disposable cluster for practice scenario</p> <pre><code># Create kind cluster\ncat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n- role: worker\n- role: worker\nEOF\n\n# Verify cluster ready\nkubectl get nodes\n\n# Practice exam task\nkubectl run nginx --image=nginx\nkubectl expose pod nginx --port=80 --type=NodePort\n\n# Clean up when done\nkind delete cluster\n</code></pre> <p>Time: 30-60 seconds for cluster creation</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#scenario-2-multi-cluster-context-switching","title":"Scenario 2: Multi-Cluster Context Switching","text":"<p>Objective: Practice switching between exam cluster contexts</p> <pre><code># Assume exam provides contexts: cluster1-context, cluster2-context\n\n# List available contexts\nkubectl config get-contexts\n\n# Switch to cluster1\nkubectl config use-context cluster1-context\n\n# Verify current context\nkubectl config current-context\n\n# Deploy to cluster1\nkubectl run web --image=nginx\n\n# Switch to cluster2\nkubectl config use-context cluster2-context\n\n# Deploy to cluster2\nkubectl run database --image=postgres\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#scenario-3-troubleshoot-cni-issues","title":"Scenario 3: Troubleshoot CNI Issues","text":"<p>Objective: Fix cluster networking</p> <pre><code># Check node status\nkubectl get nodes\n# NAME     STATUS     ROLES           AGE   VERSION\n# node-1   NotReady   control-plane   2m    v1.34.0\n\n# Check system pods\nkubectl -n kube-system get pods\n\n# Verify CNI config exists\nkubectl exec -n kube-system &lt;any-pod&gt; -- ls /etc/cni/net.d/\n\n# If empty, install CNI\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml\n\n# Verify nodes transition to Ready\nkubectl get nodes --watch\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#cni-network-communication","title":"CNI Network Communication","text":"<p>Understanding how CNI plugins enable pod-to-pod communication is critical for troubleshooting.</p> <pre><code>sequenceDiagram\n    participant Pod1 as Pod 1&lt;br/&gt;10.244.1.5&lt;br/&gt;Node1\n    participant CNI1 as CNI Plugin&lt;br/&gt;Node1\n    participant CNI2 as CNI Plugin&lt;br/&gt;Node2\n    participant Pod2 as Pod 2&lt;br/&gt;10.244.2.8&lt;br/&gt;Node2\n\n    Note over Pod1,Pod2: Pod-to-Pod Communication Across Nodes\n\n    Pod1-&gt;&gt;CNI1: Send packet to 10.244.2.8\n    CNI1-&gt;&gt;CNI1: Check routing table&lt;br/&gt;Pod CIDR 10.244.0.0/16\n    CNI1-&gt;&gt;CNI2: Forward via overlay network&lt;br/&gt;(VXLAN/BGP)\n    CNI2-&gt;&gt;CNI2: Lookup destination pod&lt;br/&gt;10.244.2.8 on Node2\n    CNI2-&gt;&gt;Pod2: Deliver packet\n    Pod2-&gt;&gt;CNI2: Send response\n    CNI2-&gt;&gt;CNI1: Return via overlay\n    CNI1-&gt;&gt;Pod1: Deliver response\n\n    Note over CNI1,CNI2: CNI ensures IP reachability&lt;br/&gt;without NodePort/Service</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-1-complete-kubeadm-cluster-setup-60-minutes","title":"Exercise 1: Complete kubeadm Cluster Setup (60 minutes)","text":"<p>Objective: Build production-like 3-node cluster</p> <p>Tasks: 1. Prepare 3 VMs (1 control plane, 2 workers) 2. Disable swap on all nodes 3. Install container runtime 4. Install kubeadm, kubelet, kubectl 5. Initialize control plane with Calico pod CIDR 6. Install Calico CNI 7. Join worker nodes 8. Verify all nodes Ready 9. Deploy test workload</p> <p>Success Criteria: - All 3 nodes show Ready status - CNI pods running in calico-system namespace - Test pod can communicate across nodes</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-2-kind-multi-node-lab-20-minutes","title":"Exercise 2: kind Multi-Node Lab (20 minutes)","text":"<p>Objective: Create multi-node cluster for rapid testing</p> <p>Tasks: 1. Create kind config for 3-node cluster 2. Map NodePort 30000 to localhost:8080 3. Create cluster 4. Deploy nginx with NodePort 30000 5. Access from host browser 6. Test pod scheduling across workers 7. Delete and recreate cluster</p> <p>Success Criteria: - Cluster creation completes in &lt;60 seconds - Nginx accessible on localhost:8080 - Can repeat cycle 5 times in 10 minutes</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-3-kubeconfig-context-mastery-30-minutes","title":"Exercise 3: kubeconfig Context Mastery (30 minutes)","text":"<p>Objective: Manage multiple clusters efficiently</p> <p>Tasks: 1. Create 3 different clusters (kubeadm, kind, minikube) 2. Export kubeconfig from each 3. Merge into single kubeconfig 4. Rename contexts meaningfully 5. Set default namespace per context 6. Practice rapid context switching 7. Deploy workload to specific context without switching</p> <p>Success Criteria: - Switch contexts in &lt;5 seconds - Deploy to specific cluster without errors - Verify deployment in correct cluster</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-4-shell-completion-and-aliases-15-minutes","title":"Exercise 4: Shell Completion and Aliases (15 minutes)","text":"<p>Objective: Optimize kubectl workflow for exam speed</p> <p>Tasks: 1. Install bash-completion 2. Configure kubectl completion 3. Test tab completion 4. Create aliases: k, kg, kd, kl 5. Configure alias completion 6. Time yourself: deploy nginx with and without aliases</p> <p>Success Criteria: - Tab completion works for resources - Aliases reduce command length by 50%+ - Muscle memory for common patterns</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#exercise-5-troubleshooting-simulation-45-minutes","title":"Exercise 5: Troubleshooting Simulation (45 minutes)","text":"<p>Objective: Diagnose and fix common cluster issues</p> <p>Tasks: 1. Initialize cluster WITHOUT CNI 2. Observe NotReady nodes 3. Check kubelet logs for errors 4. Identify CNI-related messages 5. Install CNI plugin 6. Verify nodes transition to Ready 7. Test pod communication 8. Break and fix: stop kubelet, observe effects 9. Break and fix: remove CNI config, restore</p> <p>Success Criteria: - Can identify CNI issues from logs - Successfully install and verify CNI - Understand node status transitions - Troubleshooting workflow muscle memory</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#lab-environment-comparison","title":"Lab Environment Comparison","text":"<pre><code>graph TD\n    subgraph \"Setup Method Comparison\"\n        Feature[Feature]\n        kubeadm[kubeadm]\n        minikube[Minikube]\n        kind[kind]\n    end\n\n    Feature --&gt; |Startup Time| Time\n    Time --&gt; kubeadm_time[5-10 min&lt;br/&gt;Multi-machine]\n    Time --&gt; minikube_time[2-5 min&lt;br/&gt;VM boot]\n    Time --&gt; kind_time[30-60 sec&lt;br/&gt;Containers]\n\n    Feature --&gt; |Resource Usage| Resources\n    Resources --&gt; kubeadm_res[High&lt;br/&gt;Real VMs]\n    Resources --&gt; minikube_res[Medium&lt;br/&gt;Single VM]\n    Resources --&gt; kind_res[Low&lt;br/&gt;Docker only]\n\n    Feature --&gt; |Multi-Node| MultiNode\n    MultiNode --&gt; kubeadm_mn[Native&lt;br/&gt;Production-like]\n    MultiNode --&gt; minikube_mn[Experimental&lt;br/&gt;Limited]\n    MultiNode --&gt; kind_mn[Native&lt;br/&gt;Config-based]\n\n    Feature --&gt; |CKA Relevance| CKA\n    CKA --&gt; kubeadm_cka[Essential&lt;br/&gt;Exam environment]\n    CKA --&gt; minikube_cka[Supplemental&lt;br/&gt;Learning]\n    CKA --&gt; kind_cka[High&lt;br/&gt;Practice speed]\n\n    style kind_time fill:#99ff99\n    style kind_res fill:#99ff99\n    style kubeadm_mn fill:#ff9999\n    style kubeadm_cka fill:#ff9999</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#quick-reference-commands","title":"Quick Reference Commands","text":"","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubeadm-cluster-lifecycle","title":"kubeadm Cluster Lifecycle","text":"<pre><code># Initialize control plane\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n\n# Configure kubectl\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n# Install CNI (Calico)\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml\n\n# Generate join command\nkubeadm token create --print-join-command\n\n# Reset cluster\nsudo kubeadm reset\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kind-quick-start","title":"kind Quick Start","text":"<pre><code># Create cluster\nkind create cluster --name cka-lab\n\n# Create multi-node cluster\ncat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n- role: worker\n- role: worker\nEOF\n\n# Delete cluster\nkind delete cluster --name cka-lab\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#minikube-quick-start","title":"Minikube Quick Start","text":"<pre><code># Start with resource spec\nminikube start --driver=docker --cpus=4 --memory=8192\n\n# Enable addons\nminikube addons enable metrics-server\nminikube addons enable ingress\n\n# Access service\nminikube service &lt;service-name&gt;\n\n# Clean up\nminikube delete\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#kubectl-context-management","title":"kubectl Context Management","text":"<pre><code># View contexts\nkubectl config get-contexts\n\n# Switch context\nkubectl config use-context &lt;context-name&gt;\n\n# Set namespace\nkubectl config set-context --current --namespace=&lt;namespace&gt;\n\n# Context-specific command\nkubectl --context=&lt;context&gt; get pods\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<pre><code># Check cluster health\nkubectl get nodes -o wide\nkubectl -n kube-system get pods\n\n# View kubelet logs (on node)\nsudo journalctl -u kubelet -f\n\n# Check CNI config\nls -l /etc/cni/net.d/\n\n# View events\nkubectl get events --sort-by='.lastTimestamp'\n\n# Check etcd health\nsudo ETCDCTL_API=3 etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  endpoint health\n</code></pre>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 kubeadm is essential for CKA - The exam environment uses kubeadm clusters</p> <p>\u2705 kind enables rapid iteration - 30-second cluster creation for practice</p> <p>\u2705 kubectl proficiency is critical - Shell completion and aliases save exam minutes</p> <p>\u2705 kubeconfig mastery matters - Context switching is a core exam skill</p> <p>\u2705 CNI is non-negotiable - Clusters are non-functional without CNI plugins</p> <p>\u2705 Swap must be disabled - Kubernetes does not support swap memory</p> <p>\u2705 Version compatibility awareness - kubectl and cluster versions must align</p> <p>\u2705 Hands-on practice wins - Deploy clusters repeatedly until muscle memory forms</p> <p>\u2705 Troubleshooting is 30% of CKA - Practice breaking and fixing clusters</p> <p>\u2705 Speed through preparation - Optimize workflow before exam day</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/setting-up-kubernetes-lab/#next-steps","title":"Next Steps","text":"<p>After mastering lab setup, continue with:</p> <p>Post 3: kubectl Essentials and Resource Management - Master the command-line tool for all Kubernetes operations</p> <p>Related Posts: - Kubernetes Architecture Fundamentals - Understanding cluster components - Kubernetes CKA Mastery - Complete Learning Path - Full exam preparation series</p> <p>External Resources: - kubeadm Official Documentation - kind Quick Start Guide - Minikube Documentation - kubectl Cheat Sheet - CKA Exam Curriculum</p>","tags":["kubernetes","k8s","cka-prep","kubeadm","kubectl","minikube"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/","title":"Understanding Kubernetes Objects and YAML Manifests","text":"<p>Master the foundation of Kubernetes declarative configuration. Learn object anatomy, YAML syntax, labels, selectors, and annotations for CKA exam success and production deployments.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#overview","title":"Overview","text":"<p>Kubernetes objects are persistent entities in the Kubernetes system that represent the desired state of your cluster. Understanding object structure and YAML manifests is fundamental to the CKA exam and real-world Kubernetes administration.</p> <p>CKA Exam Domain: All domains (objects are used everywhere)</p> <p>Key Insight: Every Kubernetes resource you create, modify, or delete is an object with a consistent structure. Mastering this structure enables you to work with any Kubernetes resource confidently.</p> <p>What You'll Learn: - Kubernetes API object model and resource types - YAML syntax fundamentals and best practices - Object anatomy: metadata, spec, status structure - Labels and selectors for resource organization - Annotations for non-identifying metadata - Field validation and troubleshooting strategies</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#kubernetes-object-model","title":"Kubernetes Object Model","text":"<p>Kubernetes uses a declarative model where you describe the desired state, and the control plane works continuously to maintain that state.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#what-is-a-kubernetes-object","title":"What is a Kubernetes Object?","text":"<p>Definition: A Kubernetes object is a persistent entity that represents: - What applications are running (and on which nodes) - How many replicas should exist - Which resources are available to applications - Policies around behavior (restart, upgrades, fault-tolerance)</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#object-categories","title":"Object Categories","text":"<pre><code>graph TB\n    subgraph \"Workload Resources\"\n        POD[Pod]\n        DEPLOY[Deployment]\n        RS[ReplicaSet]\n        SS[StatefulSet]\n        DS[DaemonSet]\n        JOB[Job]\n        CRON[CronJob]\n    end\n\n    subgraph \"Service &amp; Networking\"\n        SVC[Service]\n        ING[Ingress]\n        NP[NetworkPolicy]\n        EP[Endpoints]\n    end\n\n    subgraph \"Configuration &amp; Storage\"\n        CM[ConfigMap]\n        SECRET[Secret]\n        PV[PersistentVolume]\n        PVC[PersistentVolumeClaim]\n        SC[StorageClass]\n    end\n\n    subgraph \"Cluster Resources\"\n        NS[Namespace]\n        NODE[Node]\n        SA[ServiceAccount]\n        ROLE[Role/ClusterRole]\n    end\n\n    DEPLOY --&gt; RS\n    RS --&gt; POD\n    SS --&gt; POD\n    DS --&gt; POD\n    SVC --&gt; POD\n    ING --&gt; SVC\n\n    style POD fill:#e1f5ff\n    style DEPLOY fill:#e8f5e8\n    style SVC fill:#fff4e1\n    style CM fill:#f5e1ff</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#object-lifecycle","title":"Object Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Desired: User creates manifest\n\n    Desired --&gt; Submitted: kubectl apply\n    Submitted --&gt; Validated: API server validates\n\n    Validated --&gt; Rejected: Validation fails\n    Rejected --&gt; [*]\n\n    Validated --&gt; Persisted: Write to etcd\n    Persisted --&gt; Scheduled: Controller processes\n    Scheduled --&gt; Running: Kubelet executes\n\n    Running --&gt; Updated: User modifies\n    Updated --&gt; Validated\n\n    Running --&gt; Deleted: User deletes\n    Deleted --&gt; [*]\n\n    Running --&gt; Running: System reconciles</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#object-anatomy-the-four-essential-fields","title":"Object Anatomy: The Four Essential Fields","text":"<p>Every Kubernetes object manifest contains four top-level fields:</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#complete-object-structure","title":"Complete Object Structure","text":"<pre><code>apiVersion: apps/v1              # 1. API version\nkind: Deployment                 # 2. Object type\nmetadata:                        # 3. Identifying metadata\n  name: nginx-deployment\n  namespace: production\n  labels:\n    app: nginx\n    tier: frontend\n  annotations:\n    description: \"Production web server\"\nspec:                            # 4. Desired state\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\nstatus:                          # System-managed current state\n  availableReplicas: 3\n  readyReplicas: 3\n  observedGeneration: 1\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#1-apiversion","title":"1. apiVersion","text":"<p>Purpose: Specifies the API group and version for the object type.</p> <p>Format: <code>&lt;group&gt;/&lt;version&gt;</code> or just <code>&lt;version&gt;</code> for core API</p> <p>Common API Versions: <pre><code># Core API (no group)\nv1                              # Pod, Service, ConfigMap, Secret, Namespace\n\n# Apps API\napps/v1                         # Deployment, StatefulSet, DaemonSet, ReplicaSet\n\n# Batch API\nbatch/v1                        # Job, CronJob\n\n# Networking API\nnetworking.k8s.io/v1           # Ingress, NetworkPolicy\n\n# RBAC API\nrbac.authorization.k8s.io/v1   # Role, ClusterRole, RoleBinding\n\n# Storage API\nstorage.k8s.io/v1              # StorageClass, VolumeAttachment\n</code></pre></p> <p>Finding API Versions: <pre><code># List all API resources and versions\nkubectl api-resources\n\n# Check specific resource\nkubectl explain deployment | head -5\n# KIND:       Deployment\n# VERSION:    apps/v1\n\n# List all API versions\nkubectl api-versions\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#2-kind","title":"2. kind","text":"<p>Purpose: Identifies the type of object being created.</p> <p>Common Object Kinds: - Workloads: Pod, Deployment, StatefulSet, DaemonSet, Job, CronJob - Services: Service, Ingress, Endpoints - Configuration: ConfigMap, Secret - Storage: PersistentVolume, PersistentVolumeClaim, StorageClass - Cluster: Namespace, Node, ServiceAccount - Access: Role, ClusterRole, RoleBinding, ClusterRoleBinding</p> <p>Case Sensitivity: Kind names are case-sensitive (must be exact capitalization).</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#3-metadata","title":"3. metadata","text":"<p>Purpose: Data that uniquely identifies the object.</p> <p>Required Fields: - <code>name</code> - Unique within namespace and object type - <code>namespace</code> - (optional for cluster-scoped resources)</p> <p>Common Optional Fields: - <code>labels</code> - Key-value pairs for organization and selection - <code>annotations</code> - Non-identifying metadata - <code>uid</code> - System-generated unique identifier - <code>resourceVersion</code> - Internal version for optimistic concurrency - <code>creationTimestamp</code> - When object was created</p> <p>Metadata Example: <pre><code>metadata:\n  name: webapp                           # Required: object name\n  namespace: production                  # Namespace (required for namespaced resources)\n  labels:                                # Labels for selection\n    app: webapp\n    tier: frontend\n    version: \"2.0\"\n    environment: production\n  annotations:                           # Annotations for metadata\n    description: \"Main production web application\"\n    owner: \"platform-team@company.com\"\n    version: \"2.0.1\"\n    deployment-date: \"2024-01-15\"\n  uid: 12345678-1234-1234-1234-123456789012    # System-generated\n  resourceVersion: \"1234567\"             # System-managed version\n  creationTimestamp: \"2024-01-15T10:30:00Z\"  # System-generated\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#4-spec","title":"4. spec","text":"<p>Purpose: Describes the desired state of the object.</p> <p>Characteristics: - Varies by object <code>kind</code> - User-defined and user-managed - Controller reads spec to understand desired state - Declarative: describes \"what\", not \"how\"</p> <p>Pod Spec Example: <pre><code>spec:\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 200m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 80\n      initialDelaySeconds: 30\n      periodSeconds: 10\n  restartPolicy: Always\n  nodeSelector:\n    disk: ssd\n</code></pre></p> <p>Deployment Spec Example: <pre><code>spec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:                    # Pod template\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#5-status-system-managed","title":"5. status (System-Managed)","text":"<p>Purpose: Describes the current observed state of the object.</p> <p>Characteristics: - Read-only for users - Managed by Kubernetes controllers - Updated continuously by the control plane - Represents actual state vs desired state (spec)</p> <p>Pod Status Example: <pre><code>status:\n  phase: Running               # Pod lifecycle phase\n  conditions:                  # Detailed status conditions\n  - type: Initialized\n    status: \"True\"\n    lastTransitionTime: \"2024-01-15T10:30:05Z\"\n  - type: Ready\n    status: \"True\"\n    lastTransitionTime: \"2024-01-15T10:30:10Z\"\n  - type: ContainersReady\n    status: \"True\"\n    lastTransitionTime: \"2024-01-15T10:30:10Z\"\n  - type: PodScheduled\n    status: \"True\"\n    lastTransitionTime: \"2024-01-15T10:30:00Z\"\n  containerStatuses:\n  - name: nginx\n    ready: true\n    restartCount: 0\n    state:\n      running:\n        startedAt: \"2024-01-15T10:30:08Z\"\n  hostIP: 192.168.1.100\n  podIP: 10.244.1.50\n  startTime: \"2024-01-15T10:30:00Z\"\n</code></pre></p> <p>Deployment Status Example: <pre><code>status:\n  availableReplicas: 3\n  readyReplicas: 3\n  replicas: 3\n  updatedReplicas: 3\n  observedGeneration: 5\n  conditions:\n  - type: Available\n    status: \"True\"\n    reason: MinimumReplicasAvailable\n  - type: Progressing\n    status: \"True\"\n    reason: NewReplicaSetAvailable\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#yaml-syntax-fundamentals","title":"YAML Syntax Fundamentals","text":"<p>YAML (YAML Ain't Markup Language) is the standard format for Kubernetes manifests.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#yaml-basics","title":"YAML Basics","text":"<p>Data Types: <pre><code># Strings\nname: nginx-deployment\ndescription: \"Multi-word string in quotes\"\nmultiline: |\n  This is a multi-line string\n  that preserves newlines\n\n# Numbers\nreplicas: 3\nport: 80\ncpu: 0.5\n\n# Booleans\nenabled: true\ndebug: false\n\n# Lists (arrays)\nargs:\n- \"arg1\"\n- \"arg2\"\n- \"arg3\"\n\n# Or inline\nargs: [\"arg1\", \"arg2\", \"arg3\"]\n\n# Maps (key-value pairs)\nlabels:\n  app: nginx\n  tier: frontend\n\n# Nested structures\nmetadata:\n  labels:\n    app: nginx\n  annotations:\n    description: \"Production app\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#indentation-rules","title":"Indentation Rules","text":"<p>Critical: YAML uses spaces only (no tabs) for indentation.</p> <pre><code># CORRECT - 2 spaces per level\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    tier: frontend\n\n# CORRECT - 4 spaces also works (be consistent)\nmetadata:\n    name: nginx\n    labels:\n        app: nginx\n\n# WRONG - mixing tabs and spaces\nmetadata:\n    name: nginx        # Tab used (will fail)\n  labels:\n    app: nginx       # Spaces used\n</code></pre> <p>Best Practice: Use 2-space indentation consistently.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-yaml-patterns","title":"Common YAML Patterns","text":"<p>Lists of Objects: <pre><code>containers:\n- name: nginx                   # First container\n  image: nginx:1.21\n  ports:\n  - containerPort: 80\n- name: sidecar                 # Second container\n  image: busybox:latest\n  command: [\"sh\", \"-c\", \"sleep 3600\"]\n</code></pre></p> <p>Multi-line Strings: <pre><code># Literal block (preserves newlines)\nscript: |\n  #!/bin/bash\n  echo \"Line 1\"\n  echo \"Line 2\"\n\n# Folded block (newlines become spaces)\ndescription: &gt;\n  This is a very long description\n  that spans multiple lines but\n  will be folded into a single line.\n\n# Result: \"This is a very long description that spans multiple lines...\"\n</code></pre></p> <p>Environment Variables: <pre><code>env:\n- name: DATABASE_HOST\n  value: \"mysql.default.svc.cluster.local\"\n- name: DATABASE_PORT\n  value: \"3306\"\n- name: DATABASE_PASSWORD\n  valueFrom:\n    secretKeyRef:\n      name: db-secret\n      key: password\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#yaml-validation","title":"YAML Validation","text":"<p>Common Errors:</p> <pre><code># ERROR: Missing colon\nmetadata\n  name: nginx         # Should be \"metadata:\"\n\n# ERROR: Wrong indentation\nmetadata:\n  name: nginx\n labels:              # Misaligned (should be 2 spaces)\n   app: nginx\n\n# ERROR: Missing quotes for special characters\nannotation: \"true\"   # Boolean - needs quotes to be string\nannotation: true     # Boolean value\n\n# ERROR: List item indentation\ncontainers:\n  - name: nginx      # Correct\n- name: sidecar      # Wrong (should align with first item)\n</code></pre> <p>Validation Tools: <pre><code># kubectl validates before applying\nkubectl apply --dry-run=client -f manifest.yaml\n\n# Check syntax only\nkubectl apply --dry-run=server -f manifest.yaml\n\n# Use yamllint for detailed validation\nyamllint manifest.yaml\n\n# Python YAML validation\npython -c 'import yaml, sys; yaml.safe_load(sys.stdin)' &lt; manifest.yaml\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#labels-organizing-and-selecting-resources","title":"Labels: Organizing and Selecting Resources","text":"<p>Labels are key-value pairs attached to objects for identification and grouping.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#label-syntax","title":"Label Syntax","text":"<p>Format: <code>key: value</code></p> <p>Key Syntax: - Optional prefix: <code>&lt;prefix&gt;/&lt;name&gt;</code> (prefix \u2264 253 chars, name \u2264 63 chars) - Name: alphanumeric, <code>-</code>, <code>_</code>, <code>.</code> (must start/end with alphanumeric)</p> <p>Value Syntax: - \u2264 63 characters - Alphanumeric, <code>-</code>, <code>_</code>, <code>.</code> (can be empty)</p> <p>Examples: <pre><code>labels:\n  # Simple labels\n  app: nginx\n  tier: frontend\n  environment: production\n\n  # Prefixed labels (for third-party tools)\n  example.com/team: platform\n  app.kubernetes.io/name: nginx\n  app.kubernetes.io/version: \"1.21\"\n\n  # Valid special characters\n  app.version: \"2.0.1\"\n  team_name: platform-team\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#recommended-labels","title":"Recommended Labels","text":"<p>Kubernetes recommends a standard set of labels for consistency:</p> <pre><code>metadata:\n  labels:\n    # Application identity\n    app.kubernetes.io/name: nginx                    # Application name\n    app.kubernetes.io/instance: nginx-prod           # Unique instance\n    app.kubernetes.io/version: \"1.21.0\"             # Application version\n    app.kubernetes.io/component: webserver           # Component role\n    app.kubernetes.io/part-of: ecommerce-platform   # Parent application\n\n    # Management metadata\n    app.kubernetes.io/managed-by: helm              # Management tool\n\n    # Custom organizational labels\n    team: platform\n    cost-center: engineering\n    environment: production\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#label-use-cases","title":"Label Use Cases","text":"<p>1. Resource Organization: <pre><code># Development pods\nmetadata:\n  labels:\n    environment: dev\n    team: backend\n\n# Production pods\nmetadata:\n  labels:\n    environment: production\n    team: backend\n</code></pre></p> <p>2. Service Selection: <pre><code># Service selects pods with matching labels\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx           # Selects all pods with app=nginx\n    tier: frontend\n  ports:\n  - port: 80\n</code></pre></p> <p>3. Deployment Management: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx        # Deployment manages pods with this label\n  template:\n    metadata:\n      labels:\n        app: nginx      # Pods get this label\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#label-operations","title":"Label Operations","text":"<p>View Labels: <pre><code># Show labels column\nkubectl get pods --show-labels\n\n# Show specific labels as columns\nkubectl get pods -L app,tier,environment\n\n# Filter output\nkubectl get pods -l app=nginx\nkubectl get pods -l 'environment in (dev,staging)'\nkubectl get pods -l app=nginx,tier!=backend\n</code></pre></p> <p>Add/Modify Labels: <pre><code># Add label\nkubectl label pod nginx-pod version=1.0\n\n# Modify label (requires --overwrite)\nkubectl label pod nginx-pod version=2.0 --overwrite\n\n# Add label to all pods\nkubectl label pods --all environment=production\n\n# Remove label\nkubectl label pod nginx-pod version-\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#selectors-matching-resources","title":"Selectors: Matching Resources","text":"<p>Selectors use labels to identify sets of objects.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#selector-types","title":"Selector Types","text":"<pre><code>graph TB\n    SELECTOR[Label Selectors] --&gt; EQUALITY[Equality-Based]\n    SELECTOR --&gt; SET[Set-Based]\n\n    EQUALITY --&gt; EQ[Equal: =, ==]\n    EQUALITY --&gt; NEQ[Not Equal: !=]\n\n    SET --&gt; IN[In: in (...)]\n    SET --&gt; NOTIN[Not In: notin (...)]\n    SET --&gt; EXISTS[Exists: key]\n    SET --&gt; NOTEXISTS[Not Exists: !key]\n\n    style EQUALITY fill:#e1f5ff\n    style SET fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#equality-based-selectors","title":"Equality-Based Selectors","text":"<p>Syntax: <code>key=value</code> or <code>key!=value</code></p> <p>Command-Line Examples: <pre><code># Single equality\nkubectl get pods -l app=nginx\n\n# Multiple conditions (AND logic)\nkubectl get pods -l app=nginx,tier=frontend\n\n# Not equal\nkubectl get pods -l app=nginx,environment!=production\n</code></pre></p> <p>Manifest Examples: <pre><code># Service selector (equality-based only)\nselector:\n  app: nginx\n  tier: frontend\n\n# Equivalent to:\n# app=nginx AND tier=frontend\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#set-based-selectors","title":"Set-Based Selectors","text":"<p>Syntax: More expressive matching with <code>in</code>, <code>notin</code>, <code>exists</code></p> <p>Command-Line Examples: <pre><code># In set\nkubectl get pods -l 'environment in (dev,staging)'\n\n# Not in set\nkubectl get pods -l 'tier notin (cache,db)'\n\n# Label exists\nkubectl get pods -l app\n\n# Label does not exist\nkubectl get pods -l '!app'\n\n# Complex combination\nkubectl get pods -l 'environment in (prod),tier notin (cache),app'\n</code></pre></p> <p>Manifest Examples: <pre><code># Deployment selector (supports both)\nselector:\n  matchLabels:                    # Equality-based\n    app: nginx\n  matchExpressions:               # Set-based\n  - key: tier\n    operator: In\n    values:\n    - frontend\n    - api\n  - key: environment\n    operator: NotIn\n    values:\n    - testing\n  - key: critical\n    operator: Exists\n  - key: deprecated\n    operator: DoesNotExist\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#selector-operators","title":"Selector Operators","text":"Operator Description Example <code>In</code> Value in set <code>tier in (frontend, api)</code> <code>NotIn</code> Value not in set <code>env notin (test, dev)</code> <code>Exists</code> Label exists <code>critical</code> (key exists) <code>DoesNotExist</code> Label doesn't exist <code>!deprecated</code> (key absent)","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#selector-matching-logic","title":"Selector Matching Logic","text":"<pre><code>flowchart TD\n    START([Resource with Labels]) --&gt; CHECK1{matchLabels&lt;br/&gt;satisfied?}\n\n    CHECK1 --&gt;|No| NOMATCH[No Match]\n    CHECK1 --&gt;|Yes| CHECK2{matchExpressions&lt;br/&gt;all true?}\n\n    CHECK2 --&gt;|No| NOMATCH\n    CHECK2 --&gt;|Yes| MATCH[Match Found]\n\n    MATCH --&gt; SELECTED[Resource Selected]\n    NOMATCH --&gt; IGNORED[Resource Ignored]\n\n    style MATCH fill:#e8f5e8\n    style NOMATCH fill:#ffe5e5</code></pre> <p>Example: <pre><code># Pod labels\nmetadata:\n  labels:\n    app: nginx\n    tier: frontend\n    environment: production\n    version: \"2.0\"\n\n# Selector\nselector:\n  matchLabels:\n    app: nginx                    # \u2705 Match\n  matchExpressions:\n  - key: tier\n    operator: In\n    values: [frontend, api]       # \u2705 Match (tier=frontend)\n  - key: environment\n    operator: NotIn\n    values: [dev, test]           # \u2705 Match (production not in list)\n  - key: version\n    operator: Exists              # \u2705 Match (version label exists)\n\n# Result: Pod matches selector\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#annotations-non-identifying-metadata","title":"Annotations: Non-Identifying Metadata","text":"<p>Annotations store arbitrary metadata that doesn't identify or select objects.</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#annotations-vs-labels","title":"Annotations vs Labels","text":"Aspect Labels Annotations Purpose Identify and select Store metadata Selectable Yes (with selectors) No Size Limit 63 chars (value) 256 KB (total) Structure Simple key-value Can store JSON, YAML Use Case Grouping, selection Documentation, config","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#annotation-syntax","title":"Annotation Syntax","text":"<p>Format: Same as labels but values can be larger and more complex</p> <pre><code>metadata:\n  annotations:\n    # Documentation\n    description: \"Production Nginx deployment with 3 replicas\"\n    owner: \"platform-team@company.com\"\n    documentation: \"https://wiki.company.com/nginx-deployment\"\n\n    # Build information\n    build-version: \"2.0.1\"\n    git-commit: \"a3f2b1c\"\n    ci-pipeline: \"https://jenkins.company.com/job/nginx/123\"\n\n    # Operational metadata\n    deployment-date: \"2024-01-15T10:30:00Z\"\n    last-updated-by: \"john.doe@company.com\"\n\n    # Tool-specific configuration\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\n    prometheus.io/path: \"/metrics\"\n\n    # JSON configuration\n    custom-config: '{\"timeout\": 30, \"retries\": 3}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-annotation-use-cases","title":"Common Annotation Use Cases","text":"<p>1. Tool Integration: <pre><code>annotations:\n  # Prometheus monitoring\n  prometheus.io/scrape: \"true\"\n  prometheus.io/port: \"8080\"\n\n  # Nginx Ingress\n  nginx.ingress.kubernetes.io/rewrite-target: \"/\"\n  nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n\n  # Cert-manager\n  cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n\n  # Istio service mesh\n  sidecar.istio.io/inject: \"true\"\n</code></pre></p> <p>2. Change Tracking: <pre><code>annotations:\n  kubernetes.io/change-cause: \"Update nginx to version 1.21\"\n  deployment.kubernetes.io/revision: \"5\"\n</code></pre></p> <p>3. Documentation: <pre><code>annotations:\n  description: |\n    Main production web server deployment.\n\n    Handles customer-facing traffic with:\n    - 3 replicas for high availability\n    - Rolling update strategy\n    - Health checks configured\n\n    Contact: platform-team@company.com\n\n  runbook: \"https://wiki.company.com/runbooks/nginx-troubleshooting\"\n  oncall: \"https://pagerduty.com/schedules/nginx-team\"\n</code></pre></p> <p>4. Configuration Storage: <pre><code>annotations:\n  # Complex JSON configuration\n  fluentd-config: |\n    {\n      \"outputs\": [\n        {\"type\": \"elasticsearch\", \"host\": \"es.logging.svc\"},\n        {\"type\": \"s3\", \"bucket\": \"logs-backup\"}\n      ]\n    }\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#annotation-operations","title":"Annotation Operations","text":"<pre><code># View annotations\nkubectl describe pod nginx-pod | grep -A 10 \"Annotations:\"\n\n# Add annotation\nkubectl annotate pod nginx-pod description=\"Production web server\"\n\n# Update annotation\nkubectl annotate pod nginx-pod description=\"Updated description\" --overwrite\n\n# Remove annotation\nkubectl annotate pod nginx-pod description-\n\n# Annotate all resources of type\nkubectl annotate deployments --all team=platform\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#field-validation-and-troubleshooting","title":"Field Validation and Troubleshooting","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#required-vs-optional-fields","title":"Required vs Optional Fields","text":"<p>Required Fields (most common): <pre><code># All objects\napiVersion: apps/v1              # Required\nkind: Deployment                 # Required\nmetadata:\n  name: nginx                    # Required\nspec:                            # Required\n\n# Pod spec\nspec:\n  containers:                    # Required\n  - name: nginx                  # Required\n    image: nginx:1.21            # Required\n\n# Service spec\nspec:\n  selector:                      # Required\n    app: nginx\n  ports:                         # Required\n  - port: 80\n</code></pre></p> <p>Determining Required Fields: <pre><code># Use kubectl explain\nkubectl explain pod.spec\n# FIELDS:\n#   containers    &lt;[]Container&gt; -required-\n#   volumes       &lt;[]Volume&gt;\n\nkubectl explain pod.spec.containers\n# FIELDS:\n#   name          &lt;string&gt; -required-\n#   image         &lt;string&gt; -required-\n#   command       &lt;[]string&gt;        # Optional (no -required-)\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-validation-errors","title":"Common Validation Errors","text":"<p>1. Missing Required Field: <pre><code># ERROR: Missing image\nspec:\n  containers:\n  - name: nginx\n    # image: nginx:1.21    # Missing required field\n\n# Error message:\n# error: error validating \"pod.yaml\": error validating data:\n# ValidationError(Pod.spec.containers[0]): missing required field \"image\"\n</code></pre></p> <p>2. Invalid Field Name: <pre><code># ERROR: Typo in field name\nmetadata:\n  name: nginx\n  lables:              # Should be \"labels\"\n    app: nginx\n\n# Error message:\n# error: error validating \"pod.yaml\": error validating data:\n# ValidationError(Pod.metadata): unknown field \"lables\"\n</code></pre></p> <p>3. Wrong Data Type: <pre><code># ERROR: String instead of integer\nspec:\n  replicas: \"3\"        # Should be: replicas: 3\n\n# Error message:\n# error: error validating data: ValidationError(Deployment.spec.replicas):\n# invalid type for io.k8s.api.apps.v1.DeploymentSpec.replicas: got \"string\", expected \"integer\"\n</code></pre></p> <p>4. Invalid Value: <pre><code># ERROR: Invalid restart policy\nspec:\n  restartPolicy: OnError    # Valid: Always, OnFailure, Never\n\n# Error message:\n# error: error validating data: ValidationError(Pod.spec.restartPolicy):\n# unsupported value: \"OnError\": supported values: \"Always\", \"OnFailure\", \"Never\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#validation-workflow","title":"Validation Workflow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant kubectl\n    participant API\n    participant Validation\n    participant etcd\n\n    User-&gt;&gt;kubectl: kubectl apply -f manifest.yaml\n    kubectl-&gt;&gt;kubectl: Parse YAML syntax\n\n    alt YAML syntax error\n        kubectl--&gt;&gt;User: Syntax error (invalid YAML)\n    end\n\n    kubectl-&gt;&gt;API: Send parsed object\n    API-&gt;&gt;Validation: Validate object\n\n    Validation-&gt;&gt;Validation: Check required fields\n    Validation-&gt;&gt;Validation: Validate field types\n    Validation-&gt;&gt;Validation: Validate field values\n    Validation-&gt;&gt;Validation: Run admission controllers\n\n    alt Validation fails\n        Validation--&gt;&gt;API: Validation error\n        API--&gt;&gt;User: Error message with details\n    else Validation succeeds\n        Validation-&gt;&gt;etcd: Persist object\n        etcd--&gt;&gt;User: Created/Updated confirmation\n    end</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<pre><code># Validate YAML without creating\nkubectl apply --dry-run=client -f manifest.yaml\n\n# Server-side validation (includes admission controllers)\nkubectl apply --dry-run=server -f manifest.yaml\n\n# Explain field structure\nkubectl explain deployment.spec.template.spec.containers\n\n# Get field path for specific property\nkubectl explain deployment --recursive | grep -A 5 \"replicas\"\n\n# Validate all manifests in directory\nkubectl apply --dry-run=client -f ./manifests/\n\n# Check API resource availability\nkubectl api-resources | grep -i deployment\n\n# View object in YAML (for comparison)\nkubectl get deployment nginx -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#complete-object-examples","title":"Complete Object Examples","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#example-1-multi-container-pod-with-full-metadata","title":"Example 1: Multi-Container Pod with Full Metadata","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-container-pod\n  namespace: production\n  labels:\n    app: webapp\n    tier: frontend\n    environment: production\n    version: \"2.0\"\n  annotations:\n    description: \"Production web application with logging sidecar\"\n    owner: \"platform-team@company.com\"\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\nspec:\n  containers:\n  - name: webapp\n    image: nginx:1.21\n    ports:\n    - name: http\n      containerPort: 80\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 200m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 80\n      initialDelaySeconds: 30\n      periodSeconds: 10\n    env:\n    - name: ENVIRONMENT\n      value: \"production\"\n  - name: log-forwarder\n    image: fluent/fluent-bit:1.9\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n  volumes:\n  - name: logs\n    emptyDir: {}\n  restartPolicy: Always\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#example-2-deployment-with-complete-selectors","title":"Example 2: Deployment with Complete Selectors","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp-deployment\n  namespace: production\n  labels:\n    app.kubernetes.io/name: webapp\n    app.kubernetes.io/version: \"2.0.1\"\n    app.kubernetes.io/component: frontend\n    app.kubernetes.io/part-of: ecommerce-platform\n    app.kubernetes.io/managed-by: kubectl\n  annotations:\n    deployment.kubernetes.io/revision: \"3\"\n    kubernetes.io/change-cause: \"Update to version 2.0.1\"\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: webapp\n      tier: frontend\n    matchExpressions:\n    - key: environment\n      operator: In\n      values:\n      - production\n      - staging\n    - key: deprecated\n      operator: DoesNotExist\n  template:\n    metadata:\n      labels:\n        app: webapp\n        tier: frontend\n        environment: production\n        version: \"2.0.1\"\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"8080\"\n    spec:\n      containers:\n      - name: webapp\n        image: webapp:2.0.1\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 200m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#example-3-service-with-label-selector","title":"Example 3: Service with Label Selector","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: webapp-service\n  namespace: production\n  labels:\n    app: webapp\n    tier: frontend\n  annotations:\n    service.kubernetes.io/topology-aware-hints: \"auto\"\nspec:\n  type: ClusterIP\n  selector:\n    app: webapp           # Selects pods with app=webapp\n    tier: frontend        # AND tier=frontend\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n  sessionAffinity: ClientIP\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-exam-tasks","title":"Common Exam Tasks","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-1-create-object-with-labels","title":"Scenario 1: Create Object with Labels","text":"<p>Task: Create a pod named <code>web</code> with nginx image, labels <code>app=web</code> and <code>tier=frontend</code></p> <pre><code># Imperative with labels\nkubectl run web --image=nginx --labels=\"app=web,tier=frontend\"\n\n# Or generate and modify\nkubectl run web --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n# Edit pod.yaml to add labels\nkubectl apply -f pod.yaml\n\n# Verify\nkubectl get pod web --show-labels\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-2-select-resources-by-labels","title":"Scenario 2: Select Resources by Labels","text":"<p>Task: Get all pods with label <code>environment=production</code> and <code>tier!=backend</code></p> <pre><code># Single label\nkubectl get pods -l environment=production\n\n# Multiple labels (AND)\nkubectl get pods -l environment=production,tier!=backend\n\n# Set-based\nkubectl get pods -l 'environment in (production,staging),tier!=backend'\n\n# Show labels\nkubectl get pods -l environment=production --show-labels\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-3-addmodify-annotations","title":"Scenario 3: Add/Modify Annotations","text":"<p>Task: Add annotation <code>description=\"Production web server\"</code> to deployment <code>webapp</code></p> <pre><code># Add annotation\nkubectl annotate deployment webapp description=\"Production web server\"\n\n# Modify existing (requires --overwrite)\nkubectl annotate deployment webapp description=\"Updated description\" --overwrite\n\n# Verify\nkubectl describe deployment webapp | grep -A 5 \"Annotations:\"\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-4-validate-manifest-before-apply","title":"Scenario 4: Validate Manifest Before Apply","text":"<p>Task: Check if <code>deployment.yaml</code> is valid without creating it</p> <pre><code># Client-side validation (YAML syntax + basic structure)\nkubectl apply --dry-run=client -f deployment.yaml\n\n# Server-side validation (includes admission controllers)\nkubectl apply --dry-run=server -f deployment.yaml\n\n# Explain specific fields\nkubectl explain deployment.spec.template.spec.containers.resources\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#scenario-5-extract-object-yaml","title":"Scenario 5: Extract Object YAML","text":"<p>Task: Get running pod's YAML to create template</p> <pre><code># Get full YAML (includes status)\nkubectl get pod nginx -o yaml\n\n# Get YAML without system fields (for template)\nkubectl get pod nginx -o yaml | kubectl neat\n\n# Or manually clean\nkubectl get pod nginx -o yaml &gt; template.yaml\n# Remove status, uid, resourceVersion, creationTimestamp, etc.\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-1-object-creation-10-minutes","title":"Exercise 1: Object Creation (10 minutes)","text":"<p>Objective: Create objects with proper metadata structure</p> <p>Tasks: 1. Create pod <code>frontend-pod</code> with nginx:1.21 image 2. Add labels: <code>app=frontend</code>, <code>tier=web</code>, <code>env=dev</code> 3. Add annotation: <code>description=\"Development frontend pod\"</code> 4. Verify labels and annotations</p> <p>Solution: <pre><code># Generate template\nkubectl run frontend-pod --image=nginx:1.21 --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit pod.yaml\nvim pod.yaml\n# Add to metadata:\n#   labels:\n#     app: frontend\n#     tier: web\n#     env: dev\n#   annotations:\n#     description: \"Development frontend pod\"\n\n# Apply\nkubectl apply -f pod.yaml\n\n# Verify\nkubectl get pod frontend-pod --show-labels\nkubectl describe pod frontend-pod | grep -A 5 \"Annotations:\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-2-label-selection-15-minutes","title":"Exercise 2: Label Selection (15 minutes)","text":"<p>Objective: Practice label selectors</p> <p>Tasks: 1. Create 5 pods with various labels 2. Select pods with <code>environment=production</code> 3. Select pods with <code>tier</code> in (frontend, api) 4. Select pods with <code>environment=production</code> AND <code>tier!=backend</code> 5. Count pods matching each selector</p> <p>Solution: <pre><code># Create pods with different labels\nkubectl run pod1 --image=nginx --labels=\"app=web,environment=production,tier=frontend\"\nkubectl run pod2 --image=nginx --labels=\"app=api,environment=production,tier=api\"\nkubectl run pod3 --image=nginx --labels=\"app=db,environment=production,tier=backend\"\nkubectl run pod4 --image=nginx --labels=\"app=web,environment=staging,tier=frontend\"\nkubectl run pod5 --image=nginx --labels=\"app=cache,environment=dev,tier=cache\"\n\n# Select by environment\nkubectl get pods -l environment=production\n# Expected: pod1, pod2, pod3\n\n# Select by tier (set-based)\nkubectl get pods -l 'tier in (frontend,api)'\n# Expected: pod1, pod2, pod4\n\n# Complex selector\nkubectl get pods -l environment=production,tier!=backend\n# Expected: pod1, pod2\n\n# Count matches\nkubectl get pods -l environment=production --no-headers | wc -l\n# Expected: 3\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-3-deployment-with-selectors-20-minutes","title":"Exercise 3: Deployment with Selectors (20 minutes)","text":"<p>Objective: Create deployment with proper label selectors</p> <p>Tasks: 1. Create deployment <code>webapp</code> with 3 replicas 2. Use nginx:1.21 image 3. Add deployment labels: <code>app=webapp</code>, <code>version=v1</code> 4. Pod template labels: <code>app=webapp</code>, <code>tier=frontend</code>, <code>version=v1</code> 5. Configure selector to match pod labels 6. Verify pods are created with correct labels</p> <p>Solution: <pre><code># Generate template\nkubectl create deployment webapp --image=nginx:1.21 --replicas=3 --dry-run=client -o yaml &gt; deployment.yaml\n\n# Edit deployment.yaml\nvim deployment.yaml\n</code></pre></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp\n  labels:\n    app: webapp\n    version: v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: webapp\n      tier: frontend\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: webapp\n        tier: frontend\n        version: v1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n</code></pre> <pre><code># Apply\nkubectl apply -f deployment.yaml\n\n# Verify deployment\nkubectl get deployment webapp\n\n# Verify pods have correct labels\nkubectl get pods --show-labels -l app=webapp\n\n# Verify selector works\nkubectl get pods -l app=webapp,tier=frontend,version=v1\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-4-annotations-and-documentation-15-minutes","title":"Exercise 4: Annotations and Documentation (15 minutes)","text":"<p>Objective: Use annotations for metadata</p> <p>Tasks: 1. Create deployment with comprehensive annotations 2. Add build info, owner, documentation links 3. Add tool-specific annotations (e.g., Prometheus) 4. Extract and view annotations</p> <p>Solution: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: documented-app\n  annotations:\n    description: \"Production application with comprehensive documentation\"\n    owner: \"platform-team@company.com\"\n    documentation: \"https://wiki.company.com/apps/documented-app\"\n    runbook: \"https://wiki.company.com/runbooks/documented-app\"\n    build-version: \"2.0.1\"\n    git-commit: \"abc123def456\"\n    ci-pipeline: \"https://jenkins.company.com/job/app/123\"\n    deployment-date: \"2024-01-15T10:30:00Z\"\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: documented-app\n  template:\n    metadata:\n      labels:\n        app: documented-app\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9090\"\n    spec:\n      containers:\n      - name: app\n        image: nginx:1.21\n</code></pre></p> <pre><code># Apply\nkubectl apply -f documented-app.yaml\n\n# View annotations\nkubectl describe deployment documented-app | grep -A 15 \"Annotations:\"\n\n# Get specific annotation\nkubectl get deployment documented-app -o jsonpath='{.metadata.annotations.owner}'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#exercise-5-validation-and-troubleshooting-20-minutes","title":"Exercise 5: Validation and Troubleshooting (20 minutes)","text":"<p>Objective: Practice validation and error fixing</p> <p>Tasks: 1. Create manifest with intentional errors 2. Use validation to identify errors 3. Fix each error 4. Successfully create object</p> <p>Broken Manifest: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: broken-deploy\n  lables:                          # ERROR: Typo\n    app: broken\nspec:\n  replicas: \"3\"                    # ERROR: String instead of int\n  selector:\n    matchLabels:\n      app: broken\n  template:\n    metadata:\n      labels:\n        app: broken\n    spec:\n      containers:\n      - name: nginx\n        # image: nginx:1.21         # ERROR: Missing required field\n        ports:\n        - containerPort: 80\n        restartPolicy: OnError      # ERROR: Invalid value (wrong location too)\n</code></pre></p> <p>Solution: <pre><code># Try to validate\nkubectl apply --dry-run=client -f broken.yaml\n# See errors for each issue\n\n# Fixed version:\n</code></pre></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fixed-deploy\n  labels:                          # Fixed: labels (not lables)\n    app: fixed\nspec:\n  replicas: 3                      # Fixed: integer (not string)\n  selector:\n    matchLabels:\n      app: fixed\n  template:\n    metadata:\n      labels:\n        app: fixed\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21          # Fixed: added required field\n        ports:\n        - containerPort: 80\n      restartPolicy: Always        # Fixed: moved to pod spec, valid value\n</code></pre> <pre><code># Validate fixed version\nkubectl apply --dry-run=client -f fixed.yaml\n# Should succeed\n\n# Apply\nkubectl apply -f fixed.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#quick-reference","title":"Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#object-structure-template","title":"Object Structure Template","text":"<pre><code>apiVersion: &lt;group&gt;/&lt;version&gt;\nkind: &lt;ObjectKind&gt;\nmetadata:\n  name: &lt;object-name&gt;\n  namespace: &lt;namespace&gt;\n  labels:\n    &lt;key&gt;: &lt;value&gt;\n  annotations:\n    &lt;key&gt;: &lt;value&gt;\nspec:\n  # Object-specific desired state\nstatus:\n  # System-managed current state (read-only)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#common-apiversion-values","title":"Common apiVersion Values","text":"<pre><code>v1                              # Core: Pod, Service, ConfigMap\napps/v1                         # Deployment, StatefulSet, DaemonSet\nbatch/v1                        # Job, CronJob\nnetworking.k8s.io/v1           # Ingress, NetworkPolicy\nrbac.authorization.k8s.io/v1   # Role, ClusterRole, RoleBinding\nstorage.k8s.io/v1              # StorageClass\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#label-selector-syntax","title":"Label Selector Syntax","text":"<pre><code># Equality-based (command-line)\nkubectl get pods -l app=nginx                    # Single label\nkubectl get pods -l app=nginx,tier=frontend      # Multiple (AND)\nkubectl get pods -l app=nginx,tier!=backend      # Not equal\n\n# Set-based (command-line)\nkubectl get pods -l 'environment in (prod,staging)'\nkubectl get pods -l 'tier notin (cache,db)'\nkubectl get pods -l app                          # Label exists\nkubectl get pods -l '!app'                       # Label doesn't exist\n\n# Manifest (YAML)\nselector:\n  matchLabels:                   # Equality-based\n    app: nginx\n  matchExpressions:              # Set-based\n  - key: tier\n    operator: In                 # In, NotIn, Exists, DoesNotExist\n    values: [frontend, api]\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#essential-commands","title":"Essential Commands","text":"<pre><code># Object creation\nkubectl apply -f manifest.yaml\nkubectl create -f manifest.yaml\n\n# Label operations\nkubectl label pod nginx app=web                  # Add\nkubectl label pod nginx app=web --overwrite      # Modify\nkubectl label pod nginx app-                     # Remove\nkubectl get pods --show-labels                   # View\nkubectl get pods -L app,tier                     # Show specific labels\nkubectl get pods -l app=nginx                    # Filter by labels\n\n# Annotation operations\nkubectl annotate pod nginx description=\"Web server\"    # Add\nkubectl annotate pod nginx description- # Remove\nkubectl describe pod nginx                       # View\n\n# Validation\nkubectl apply --dry-run=client -f manifest.yaml  # Client-side\nkubectl apply --dry-run=server -f manifest.yaml  # Server-side\nkubectl explain deployment.spec                  # Field documentation\n\n# Extraction\nkubectl get pod nginx -o yaml                    # Full YAML\nkubectl get pod nginx -o json                    # Full JSON\n</code></pre>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Every Kubernetes resource is an object with consistent structure: apiVersion, kind, metadata, spec</p> <p>\u2705 YAML is the standard format - master 2-space indentation and common patterns</p> <p>\u2705 Four essential fields define objects - apiVersion, kind, metadata (name/namespace/labels), spec</p> <p>\u2705 Labels enable selection - use for grouping, Services, Deployments, and queries</p> <p>\u2705 Selectors match labels - equality-based (simple) and set-based (complex matching)</p> <p>\u2705 Annotations store metadata - documentation, configuration, tool integration (no selection)</p> <p>\u2705 Use kubectl explain extensively - fastest way to discover field structure during exam</p> <p>\u2705 Validation catches errors early - always use --dry-run before applying</p> <p>\u2705 Recommended labels maintain consistency - app.kubernetes.io/* prefix for standard labels</p> <p>\u2705 Master label/annotation operations - critical for exam speed and production work</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-objects-yaml-manifests/#next-steps","title":"Next Steps","text":"<p>Continue your CKA journey with:</p> <p>Post 5: Namespaces and Resource Quotas - Learn cluster resource organization and multi-tenancy</p> <p>Related Posts: - Kubernetes Architecture Fundamentals - Understanding cluster components - kubectl Essentials - Command-line mastery - Kubernetes CKA Mastery - Complete Learning Path - Full exam preparation series</p> <p>External Resources: - Kubernetes Objects (Official Docs) - Labels and Selectors - Annotations - Recommended Labels - Understanding Kubernetes Objects</p>","tags":["kubernetes","k8s","cka-prep","yaml","objects","manifests","labels","selectors"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/","title":"Pods: The Atomic Unit of Kubernetes","text":"<p>Master Kubernetes Pods - the fundamental building block of container orchestration. Learn multi-container patterns, lifecycle management, health probes, and essential troubleshooting techniques for the CKA exam.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#introduction","title":"Introduction","text":"<p>In the vast ecosystem of Kubernetes, pods stand as the fundamental building block - the smallest deployable unit you can create and manage. Understanding pods is not just important for the Certified Kubernetes Administrator (CKA) exam; it's absolutely essential for anyone working with Kubernetes in production environments.</p> <p>Think of a pod as a logical host for your containers. Just as traditional applications might run multiple processes on a single server that share resources and communicate via localhost, pods enable containers to share networking, storage, and a common context. This design pattern enables powerful architectural patterns while maintaining the benefits of container isolation.</p> <p>In this comprehensive guide, you'll master:</p> <ul> <li>Pod fundamentals: What pods are, why they're atomic, and how they work</li> <li>Lifecycle management: Understanding pod phases and state transitions</li> <li>Multi-container patterns: Sidecar, init containers, adapter, and ambassador patterns</li> <li>Configuration mastery: Resource management, health probes, and environment configuration</li> <li>Troubleshooting skills: Debugging common pod issues with kubectl</li> <li>CKA exam techniques: Fast pod creation, debugging strategies, and time-saving tips</li> </ul> <p>CKA Exam Relevance: Pods are covered extensively in the \"Workloads &amp; Scheduling\" domain (15% of the exam). You'll need to demonstrate proficiency in creating, configuring, and troubleshooting pods under time pressure. This guide provides the knowledge and hands-on skills you need to excel.</p> <p>Whether you're preparing for the CKA exam or working to deepen your Kubernetes expertise, mastering pods is your foundation for success. Let's dive in.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#pod-fundamentals","title":"Pod Fundamentals","text":"","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#what-is-a-pod","title":"What is a Pod?","text":"<p>A pod is the smallest deployable unit in Kubernetes. It represents a single instance of a running process in your cluster. While pods can contain one or more containers, they're designed to run a single instance of an application.</p> <pre><code>graph TB\n    subgraph \"Pod: web-app\"\n        subgraph \"Shared Network Namespace\"\n            C1[App Container&lt;br/&gt;nginx:1.21&lt;br/&gt;localhost:80]\n            C2[Logging Sidecar&lt;br/&gt;fluentd&lt;br/&gt;localhost:24224]\n        end\n\n        subgraph \"Shared Storage\"\n            V1[Volume: logs&lt;br/&gt;/var/log/nginx]\n            V2[Volume: config&lt;br/&gt;/etc/nginx]\n        end\n\n        NET[Pod IP: 10.244.1.5&lt;br/&gt;Shared Network]\n    end\n\n    C1 -.-&gt;|reads/writes| V1\n    C1 -.-&gt;|reads| V2\n    C2 -.-&gt;|reads| V1\n    C1 -.-&gt;|localhost| C2\n\n    style C1 fill:#e1f5ff\n    style C2 fill:#fff4e1\n    style V1 fill:#e8f5e8\n    style V2 fill:#e8f5e8\n    style NET fill:#ffe5e5</code></pre> <p>Key Characteristics:</p> <ol> <li>Atomic Unit: Pods are created, scheduled, and managed as a single entity</li> <li>Shared Network: All containers share the same network namespace and IP address</li> <li>Shared Storage: Containers can share volumes mounted to the pod</li> <li>Co-located: Containers in a pod are always scheduled on the same node</li> <li>Co-managed: Containers in a pod have the same lifecycle</li> </ol>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#why-are-pods-atomic","title":"Why Are Pods Atomic?","text":"<p>Kubernetes treats pods as the smallest unit of deployment because:</p> <ul> <li>Scheduling: The scheduler places entire pods on nodes, not individual containers</li> <li>Scaling: Replicas create complete pod copies, not individual containers</li> <li>Lifecycle: All containers in a pod start and stop together</li> <li>Resource allocation: Resources are allocated at the pod level</li> </ul> <p>This design enables tight coupling between containers that need to work together while maintaining loose coupling between pods.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#pod-lifecycle-phases","title":"Pod Lifecycle Phases","text":"<p>Pods move through distinct phases during their lifetime:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Pending: Pod created\n    Pending --&gt; Running: Containers starting\n    Running --&gt; Succeeded: All containers exit 0\n    Running --&gt; Failed: Container exits non-zero\n    Pending --&gt; Failed: Cannot schedule\n    Running --&gt; Unknown: Node communication lost\n    Unknown --&gt; Running: Communication restored\n    Unknown --&gt; Failed: Timeout exceeded\n    Succeeded --&gt; [*]\n    Failed --&gt; [*]\n\n    note right of Pending\n        Waiting for scheduling\n        Pulling images\n        Starting containers\n    end note\n\n    note right of Running\n        At least one container\n        is executing\n    end note\n\n    note right of Succeeded\n        All containers\n        terminated successfully\n    end note\n\n    note right of Failed\n        At least one container\n        failed (non-zero exit)\n    end note</code></pre> Phase Description Common Reasons Pending Pod accepted but not running Scheduling delays, image pulling, insufficient resources Running Pod bound to node, containers created Normal operation Succeeded All containers terminated successfully Batch jobs, init containers completed Failed All containers terminated, at least one failed Application errors, configuration issues Unknown Pod state cannot be determined Node communication failure","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#single-vs-multi-container-pods","title":"Single vs Multi-Container Pods","text":"<p>Single-Container Pods (most common):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: web\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n</code></pre> <p>Multi-Container Pods (for tightly coupled components):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-with-logging\nspec:\n  containers:\n  # Main application container\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n\n  # Sidecar container for log processing\n  - name: log-processor\n    image: fluentd:latest\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n      readOnly: true\n\n  volumes:\n  - name: logs\n    emptyDir: {}\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#shared-networking","title":"Shared Networking","text":"<p>All containers in a pod share the same network namespace:</p> <ul> <li>Same IP address: All containers share the pod's IP</li> <li>localhost communication: Containers can reach each other on <code>localhost</code></li> <li>Port space: Containers must use different ports (no conflicts)</li> <li>Network policies: Applied at the pod level, not container level</li> </ul> <pre><code># Container 1 listens on port 80\nnginx -g 'daemon off;'\n\n# Container 2 can access it via localhost\ncurl localhost:80\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#shared-storage","title":"Shared Storage","text":"<p>Pods can define volumes that containers share:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: shared-storage-example\nspec:\n  containers:\n  - name: writer\n    image: busybox\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - while true; do\n        echo \"$(date): Writing data\" &gt;&gt; /data/log.txt;\n        sleep 10;\n      done\n    volumeMounts:\n    - name: shared-data\n      mountPath: /data\n\n  - name: reader\n    image: busybox\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - tail -f /data/log.txt\n    volumeMounts:\n    - name: shared-data\n      mountPath: /data\n      readOnly: true\n\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n</code></pre> <p>Common Volume Types:</p> <ul> <li><code>emptyDir</code>: Temporary storage, deleted with pod</li> <li><code>hostPath</code>: Mount from host node (use sparingly)</li> <li><code>configMap</code>: Configuration files</li> <li><code>secret</code>: Sensitive data</li> <li><code>persistentVolumeClaim</code>: Persistent storage</li> </ul>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#multi-container-patterns","title":"Multi-Container Patterns","text":"<p>Multi-container pods enable powerful design patterns. Here are the four most common patterns you'll encounter in production and the CKA exam.</p> <pre><code>graph TB\n    subgraph \"Multi-Container Patterns\"\n        subgraph \"Sidecar Pattern\"\n            S1[Main App]\n            S2[Sidecar&lt;br/&gt;Enhancement]\n            S1 &lt;-.-&gt; S2\n        end\n\n        subgraph \"Init Container Pattern\"\n            I1[Init Container&lt;br/&gt;Runs First]\n            I2[Main App&lt;br/&gt;Starts After]\n            I1 --&gt; I2\n        end\n\n        subgraph \"Adapter Pattern\"\n            A1[Main App&lt;br/&gt;Custom Format]\n            A2[Adapter&lt;br/&gt;Standardizes]\n            A3[Monitoring&lt;br/&gt;System]\n            A1 --&gt; A2 --&gt; A3\n        end\n\n        subgraph \"Ambassador Pattern\"\n            AM1[Main App&lt;br/&gt;Simple Client]\n            AM2[Ambassador&lt;br/&gt;Proxy/Router]\n            AM3[External&lt;br/&gt;Services]\n            AM1 --&gt; AM2 --&gt; AM3\n        end\n    end\n\n    style S1 fill:#e1f5ff\n    style I1 fill:#fff4e1\n    style A2 fill:#e8f5e8\n    style AM2 fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#1-sidecar-pattern","title":"1. Sidecar Pattern","text":"<p>Purpose: Extend or enhance the main container's functionality without modifying it.</p> <p>Common Use Cases: - Log collection and shipping - Configuration synchronization - Monitoring and metrics collection - Security proxies (mTLS, auth)</p> <p>Example: Log Collection Sidecar</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app-with-logging\nspec:\n  containers:\n  # Main application\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n\n  # Sidecar: Log collector\n  - name: fluentd\n    image: fluent/fluentd:latest\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd/fluentd.conf\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/nginx\n      readOnly: true\n    - name: fluentd-config\n      mountPath: /etc/fluentd\n\n  volumes:\n  - name: logs\n    emptyDir: {}\n  - name: fluentd-config\n    configMap:\n      name: fluentd-config\n</code></pre> <p>Why It Works: - Main app (nginx) is unmodified and focused on serving requests - Sidecar (fluentd) handles log shipping independently - Both containers share the logs volume - Sidecar can be updated without touching the main app</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#2-init-container-pattern","title":"2. Init Container Pattern","text":"<p>Purpose: Run initialization tasks before the main container starts.</p> <p>Characteristics: - Init containers run sequentially (one at a time) - Main containers don't start until all init containers succeed - Init containers can use different images and tools - Perfect for setup tasks, migrations, waiting for dependencies</p> <p>Example: Database Migration Init Container</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app-with-init\nspec:\n  # Init containers run first\n  initContainers:\n  # Wait for database to be ready\n  - name: wait-for-db\n    image: busybox:1.35\n    command:\n    - sh\n    - -c\n    - |\n      until nc -z postgres-service 5432; do\n        echo \"Waiting for database...\"\n        sleep 2\n      done\n      echo \"Database is ready!\"\n\n  # Run database migrations\n  - name: run-migrations\n    image: myapp:migrations\n    command: [\"/app/migrate.sh\"]\n    env:\n    - name: DATABASE_URL\n      valueFrom:\n        secretKeyRef:\n          name: db-secret\n          key: url\n\n  # Download configuration\n  - name: fetch-config\n    image: curlimages/curl:latest\n    command:\n    - sh\n    - -c\n    - curl -o /config/app.json https://config-server/api/config\n    volumeMounts:\n    - name: config\n      mountPath: /config\n\n  # Main container starts only after all init containers succeed\n  containers:\n  - name: web-app\n    image: myapp:latest\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: config\n      mountPath: /etc/app\n\n  volumes:\n  - name: config\n    emptyDir: {}\n</code></pre> <p>CKA Tip: Init containers are heavily tested. Know how to troubleshoot them with:</p> <pre><code># Check init container status\nkubectl describe pod web-app-with-init\n\n# View init container logs\nkubectl logs web-app-with-init -c wait-for-db\nkubectl logs web-app-with-init -c run-migrations\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#3-adapter-pattern","title":"3. Adapter Pattern","text":"<p>Purpose: Transform the main container's output to a standard format.</p> <p>Common Use Cases: - Standardizing log formats for centralized logging - Converting metrics to Prometheus format - API response transformation - Protocol translation</p> <p>Example: Log Format Adapter</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-with-adapter\nspec:\n  containers:\n  # Main app produces custom log format\n  - name: legacy-app\n    image: legacy-app:1.0\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/app\n\n  # Adapter converts logs to JSON format\n  - name: log-adapter\n    image: log-transformer:latest\n    command:\n    - sh\n    - -c\n    - |\n      tail -f /var/log/app/app.log | while read line; do\n        # Transform: \"2025-01-01 ERROR message\"\n        # To: {\"timestamp\":\"2025-01-01\",\"level\":\"ERROR\",\"message\":\"message\"}\n        echo \"$line\" | /app/transform-to-json.sh\n      done\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log/app\n      readOnly: true\n\n  volumes:\n  - name: logs\n    emptyDir: {}\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#4-ambassador-pattern","title":"4. Ambassador Pattern","text":"<p>Purpose: Proxy network connections, hiding complexity from the main container.</p> <p>Common Use Cases: - Service mesh proxies (Istio, Linkerd) - Database connection pooling - Rate limiting and circuit breaking - Load balancing to multiple backends</p> <p>Example: Database Proxy Ambassador</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-with-ambassador\nspec:\n  containers:\n  # Main app connects to localhost:5432\n  - name: web-app\n    image: myapp:latest\n    env:\n    - name: DATABASE_HOST\n      value: \"localhost\"  # Connects to ambassador\n    - name: DATABASE_PORT\n      value: \"5432\"\n\n  # Ambassador handles connection pooling and routing\n  - name: db-proxy\n    image: pgbouncer:latest\n    ports:\n    - containerPort: 5432\n    env:\n    - name: DB_HOST\n      value: \"postgres-primary.database.svc.cluster.local\"\n    - name: DB_PORT\n      value: \"5432\"\n    - name: POOL_MODE\n      value: \"transaction\"\n    - name: MAX_CLIENT_CONN\n      value: \"100\"\n    volumeMounts:\n    - name: proxy-config\n      mountPath: /etc/pgbouncer\n\n  volumes:\n  - name: proxy-config\n    configMap:\n      name: pgbouncer-config\n</code></pre> <p>Benefits: - Main app uses simple <code>localhost:5432</code> connection - Ambassador handles connection pooling, failover, load balancing - Can swap ambassador without changing main app - Easier testing (mock ambassador in dev/test)</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#real-world-multi-container-scenarios","title":"Real-World Multi-Container Scenarios","text":"<p>Scenario 1: Microservice with Service Mesh</p> <pre><code># Istio automatically injects envoy sidecar\napiVersion: v1\nkind: Pod\nmetadata:\n  name: order-service\n  annotations:\n    sidecar.istio.io/inject: \"true\"\nspec:\n  containers:\n  - name: order-service\n    image: order-service:v2.1\n    ports:\n    - containerPort: 8080\n\n  # Istio injects this automatically:\n  # - name: istio-proxy\n  #   image: istio/proxyv2:1.18.0\n  #   # Handles mTLS, traffic routing, telemetry\n</code></pre> <p>Scenario 2: Application with Config Sync</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-with-config-sync\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    volumeMounts:\n    - name: config\n      mountPath: /etc/app\n\n  # Sidecar syncs config from Git every 60s\n  - name: config-sync\n    image: git-sync:latest\n    env:\n    - name: GIT_SYNC_REPO\n      value: \"https://github.com/myorg/config\"\n    - name: GIT_SYNC_BRANCH\n      value: \"main\"\n    - name: GIT_SYNC_WAIT\n      value: \"60\"\n    volumeMounts:\n    - name: config\n      mountPath: /config\n\n  volumes:\n  - name: config\n    emptyDir: {}\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#pod-configuration","title":"Pod Configuration","text":"<p>Effective pod configuration is critical for running reliable, secure, and efficient applications in Kubernetes. Let's explore essential configuration patterns.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#resource-requests-and-limits","title":"Resource Requests and Limits","text":"<p>Resource management determines pod scheduling and runtime behavior.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: resource-demo\nspec:\n  containers:\n  - name: app\n    image: nginx:1.21\n    resources:\n      # Requests: Minimum guaranteed resources\n      requests:\n        memory: \"64Mi\"   # 64 mebibytes\n        cpu: \"250m\"      # 250 millicores (0.25 CPU)\n\n      # Limits: Maximum allowed resources\n      limits:\n        memory: \"128Mi\"  # Pod killed (OOMKilled) if exceeded\n        cpu: \"500m\"      # Throttled if exceeded\n</code></pre> <p>How It Works:</p> <pre><code>graph TB\n    subgraph \"Resource Management\"\n        REQ[Resource Request&lt;br/&gt;CPU: 250m&lt;br/&gt;Memory: 64Mi]\n        LIM[Resource Limit&lt;br/&gt;CPU: 500m&lt;br/&gt;Memory: 128Mi]\n\n        REQ --&gt;|Guarantees| SCHED[Scheduler&lt;br/&gt;Finds Node]\n        LIM --&gt;|Enforces| RUNTIME[Container Runtime&lt;br/&gt;cgroups]\n\n        SCHED --&gt;|Sufficient&lt;br/&gt;resources?| NODE[Node Placement]\n        RUNTIME --&gt;|CPU| THROTTLE[CPU Throttling]\n        RUNTIME --&gt;|Memory| OOM[OOMKilled if&lt;br/&gt;limit exceeded]\n    end\n\n    style REQ fill:#e8f5e8\n    style LIM fill:#ffe5e5\n    style SCHED fill:#e1f5ff\n    style OOM fill:#ffcccc</code></pre> <p>Best Practices:</p> <ol> <li>Always set requests: Required for proper scheduling</li> <li>Set limits carefully: Prevents resource starvation</li> <li>Memory limits = OOMKilled: Exceeding memory limit kills the pod</li> <li>CPU limits = Throttling: Exceeding CPU limit slows the container</li> <li>Requests \u2264 Limits: Requests should never exceed limits</li> </ol> <pre><code># Check resource usage\nkubectl top pod resource-demo\n\n# Common OOMKilled scenario\nkubectl describe pod resource-demo\n# Last State: Terminated\n#   Reason: OOMKilled\n#   Exit Code: 137\n</code></pre> <p>Quality of Service (QoS) Classes:</p> QoS Class Condition Behavior Guaranteed Requests = Limits for all resources Last to be evicted Burstable Requests &lt; Limits or only requests set Evicted before Guaranteed BestEffort No requests or limits First to be evicted","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#environment-variables-and-secrets","title":"Environment Variables and Secrets","text":"<p>Environment Variables:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: env-demo\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    env:\n    # Static value\n    - name: ENVIRONMENT\n      value: \"production\"\n\n    # From ConfigMap\n    - name: APP_CONFIG\n      valueFrom:\n        configMapKeyRef:\n          name: app-config\n          key: config.json\n\n    # From Secret\n    - name: DATABASE_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: db-secret\n          key: password\n\n    # Pod metadata\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n\n    # Resource limits\n    - name: MEMORY_LIMIT\n      valueFrom:\n        resourceFieldRef:\n          containerName: app\n          resource: limits.memory\n</code></pre> <p>Load All Keys from ConfigMap/Secret:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: env-from-demo\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    envFrom:\n    # All keys from ConfigMap as environment variables\n    - configMapRef:\n        name: app-config\n\n    # All keys from Secret\n    - secretRef:\n        name: app-secrets\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#health-probes","title":"Health Probes","text":"<p>Health probes determine container health and availability.</p> <pre><code>graph TB\n    START[Container Starts] --&gt; STARTUP{Startup Probe&lt;br/&gt;Configured?}\n\n    STARTUP --&gt;|Yes| SP[Startup Probe&lt;br/&gt;Runs First]\n    STARTUP --&gt;|No| LP[Liveness Probe&lt;br/&gt;Begins]\n\n    SP --&gt;|Success| LP\n    SP --&gt;|Failure| WAIT[Wait failureThreshold&lt;br/&gt;\u00d7 periodSeconds]\n    WAIT --&gt;|Still Failing| RESTART1[Restart Container]\n    RESTART1 --&gt; START\n\n    LP --&gt;|Running| LIVE{Liveness&lt;br/&gt;Check}\n    LIVE --&gt;|Healthy| RP{Readiness&lt;br/&gt;Check}\n    LIVE --&gt;|Unhealthy| RESTART2[Restart Container]\n    RESTART2 --&gt; START\n\n    RP --&gt;|Ready| TRAFFIC[Receive Traffic&lt;br/&gt;Added to Service]\n    RP --&gt;|Not Ready| NOTRAFFIC[No Traffic&lt;br/&gt;Removed from Service]\n    NOTRAFFIC --&gt;|Recovers| RP\n    TRAFFIC --&gt; LP\n\n    style START fill:#e8f5e8\n    style TRAFFIC fill:#e1f5ff\n    style RESTART1 fill:#ffcccc\n    style RESTART2 fill:#ffcccc\n    style NOTRAFFIC fill:#fff4e1</code></pre> <p>Complete Health Probe Example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: health-demo\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    ports:\n    - containerPort: 8080\n\n    # Startup Probe: For slow-starting containers\n    startupProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 0\n      periodSeconds: 10        # Check every 10s\n      failureThreshold: 30     # Allow 300s (5min) to start\n      successThreshold: 1\n      timeoutSeconds: 5\n\n    # Liveness Probe: Restart if unhealthy\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: X-Custom-Header\n          value: Liveness\n      initialDelaySeconds: 15\n      periodSeconds: 10\n      failureThreshold: 3      # Restart after 3 failures\n      successThreshold: 1\n      timeoutSeconds: 5\n\n    # Readiness Probe: Remove from Service if not ready\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n      initialDelaySeconds: 5\n      periodSeconds: 5\n      failureThreshold: 3\n      successThreshold: 1\n      timeoutSeconds: 3\n</code></pre> <p>Probe Types:</p> <ol> <li> <p>HTTP GET: <pre><code>httpGet:\n  path: /healthz\n  port: 8080\n  scheme: HTTP  # or HTTPS\n</code></pre></p> </li> <li> <p>TCP Socket: <pre><code>tcpSocket:\n  port: 8080\n</code></pre></p> </li> <li> <p>Exec Command: <pre><code>exec:\n  command:\n  - cat\n  - /tmp/healthy\n</code></pre></p> </li> </ol> <p>Common Patterns:</p> Scenario Startup Liveness Readiness Simple web app Not needed HTTP <code>/healthz</code> HTTP <code>/ready</code> Slow startup (&gt;60s) HTTP <code>/healthz</code> HTTP <code>/healthz</code> HTTP <code>/ready</code> Database TCP socket TCP socket Exec query Batch job Not needed Not needed Not needed <p>CKA Tip: Know the difference!</p> <ul> <li>Startup Probe: Protects slow-starting containers from being killed by liveness</li> <li>Liveness Probe: Restarts unhealthy containers</li> <li>Readiness Probe: Controls traffic routing, doesn't restart</li> </ul>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#restart-policies","title":"Restart Policies","text":"<p>Control what happens when containers exit.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: restart-demo\nspec:\n  restartPolicy: Always  # Always | OnFailure | Never\n  containers:\n  - name: app\n    image: myapp:latest\n</code></pre> Policy Behavior Use Case Always (default) Restart on any termination Long-running services OnFailure Restart only on non-zero exit Batch jobs, retryable tasks Never Never restart One-time tasks, debugging <p>Exit Code Meanings:</p> <ul> <li><code>0</code>: Success (clean exit)</li> <li><code>1</code>: General error</li> <li><code>137</code>: OOMKilled (128 + 9 SIGKILL)</li> <li><code>143</code>: Terminated gracefully (128 + 15 SIGTERM)</li> </ul>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#pod-affinity-and-anti-affinity-basics","title":"Pod Affinity and Anti-Affinity Basics","text":"<p>Control pod placement relative to other pods.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-server\nspec:\n  # Prefer to run on nodes where cache pods are running\n  affinity:\n    podAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchLabels:\n              app: cache\n          topologyKey: kubernetes.io/hostname\n\n    # Never run on nodes where other web-server pods are running\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchLabels:\n            app: web-server\n        topologyKey: kubernetes.io/hostname\n\n  containers:\n  - name: nginx\n    image: nginx:1.21\n</code></pre> <p>Common Use Cases:</p> <ul> <li>High Availability: Spread replicas across nodes (anti-affinity)</li> <li>Performance: Co-locate related services (affinity)</li> <li>Data Locality: Place compute near storage</li> </ul>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#cka-exam-skills","title":"CKA Exam Skills","text":"<p>Speed and accuracy are critical for the CKA exam. Master these imperative commands and troubleshooting techniques.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#imperative-pod-creation","title":"Imperative Pod Creation","text":"<p>Fastest Pod Creation:</p> <pre><code># Basic pod\nkubectl run nginx --image=nginx\n\n# Pod with port\nkubectl run nginx --image=nginx --port=80\n\n# Pod with labels\nkubectl run nginx --image=nginx --labels=\"app=web,tier=frontend\"\n\n# Pod with environment variable\nkubectl run nginx --image=nginx --env=\"ENV=prod\"\n\n# Pod with resource limits\nkubectl run nginx --image=nginx --requests='cpu=100m,memory=256Mi' --limits='cpu=200m,memory=512Mi'\n\n# Dry run to generate YAML (don't create)\nkubectl run nginx --image=nginx --dry-run=client -o yaml\n\n# Save to file for editing\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n</code></pre> <p>CKA Time-Saving Technique:</p> <pre><code># Generate base YAML quickly, then edit\nkubectl run mypod --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit with vi/vim\nvi pod.yaml\n\n# Apply the edited version\nkubectl apply -f pod.yaml\n</code></pre> <p>Multi-Container Pod (manual YAML):</p> <pre><code># Start with single container\nkubectl run webapp --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit to add second container\nvi pod.yaml\n# Add another container to the containers array\n\nkubectl apply -f pod.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<pre><code>graph TB\n    START[kubectl get pods] --&gt; STATUS{Pod Status?}\n\n    STATUS --&gt;|Pending| PEND[Check Events]\n    PEND --&gt; SCHED{Scheduling&lt;br/&gt;Issue?}\n    SCHED --&gt;|Insufficient resources| RESOURCES[Scale cluster&lt;br/&gt;or reduce requests]\n    SCHED --&gt;|Node selector| SELECTOR[Fix node selector&lt;br/&gt;or labels]\n\n    STATUS --&gt;|ImagePullBackOff| IMG[Image Issue]\n    IMG --&gt; IMGFIX{Fix:}\n    IMGFIX --&gt; IMGNAME[Correct image name]\n    IMGFIX --&gt; IMGREG[Fix registry auth]\n    IMGFIX --&gt; IMGTAG[Verify tag exists]\n\n    STATUS --&gt;|CrashLoopBackOff| CRASH[Container Crashing]\n    CRASH --&gt; LOGS[Check Logs]\n    LOGS --&gt; LOGSFIX{Common Issues:}\n    LOGSFIX --&gt; APPBUG[Application bug]\n    LOGSFIX --&gt; MISCONFIG[Missing config]\n    LOGSFIX --&gt; DEPS[Missing dependencies]\n\n    STATUS --&gt;|Running| READY{Ready?}\n    READY --&gt;|0/1| PROBE[Readiness Probe&lt;br/&gt;Failing]\n    PROBE --&gt; PROBEFIX[Check probe&lt;br/&gt;endpoint]\n\n    style START fill:#e1f5ff\n    style RESOURCES fill:#ffcccc\n    style IMGFIX fill:#fff4e1\n    style LOGSFIX fill:#e8f5e8</code></pre> <p>1. CrashLoopBackOff</p> <p>Container starts but crashes immediately.</p> <pre><code># Check pod status\nkubectl get pod mypod\n\n# View recent logs\nkubectl logs mypod\n\n# View logs from previous container (after crash)\nkubectl logs mypod --previous\n\n# Describe pod for events\nkubectl describe pod mypod\n\n# Common causes in events:\n# - Error: failed to start container\n# - Back-off restarting failed container\n</code></pre> <p>Quick Fixes:</p> <pre><code># Wrong command\nkubectl run mypod --image=busybox --command -- /bin/sh -c \"invalid-command\"\n\n# Fix: Use valid command\nkubectl run mypod --image=busybox --command -- /bin/sh -c \"sleep 3600\"\n\n# Missing environment variable\nkubectl set env pod/mypod DATABASE_URL=postgres://db:5432\n\n# Check container configuration\nkubectl get pod mypod -o yaml | grep -A 10 command\n</code></pre> <p>2. ImagePullBackOff</p> <p>Cannot pull container image.</p> <pre><code># Describe pod to see error\nkubectl describe pod mypod\n# Events:\n#   Failed to pull image \"ngin:1.21\": rpc error: code = NotFound\n\n# Common causes:\n# 1. Typo in image name (ngin vs nginx)\n# 2. Tag doesn't exist (nginx:999)\n# 3. Private registry without auth\n# 4. Network issues\n\n# Fix: Update image\nkubectl set image pod/mypod container-name=nginx:1.21\n\n# For private registries, create secret\nkubectl create secret docker-registry regcred \\\n  --docker-server=myregistry.com \\\n  --docker-username=myuser \\\n  --docker-password=mypass\n\n# Reference in pod\nkubectl patch pod mypod -p '{\"spec\":{\"imagePullSecrets\":[{\"name\":\"regcred\"}]}}'\n</code></pre> <p>3. Pending Status</p> <p>Pod cannot be scheduled.</p> <pre><code># Check events\nkubectl describe pod mypod\n# Events:\n#   0/3 nodes are available: insufficient memory\n\n# Check node resources\nkubectl describe nodes\n\n# Common causes:\n# 1. Insufficient CPU/memory\n# 2. Node selector doesn't match\n# 3. Taints/tolerations mismatch\n# 4. PVC cannot be bound\n\n# Fix: Reduce resource requests\nkubectl set resources pod mypod -c=container-name --requests=cpu=100m,memory=128Mi\n\n# Or add nodes to cluster\n</code></pre> <p>4. Pod Running but Not Ready</p> <pre><code># Check readiness probe\nkubectl get pod mypod\n# NAME    READY   STATUS    RESTARTS   AGE\n# mypod   0/1     Running   0          2m\n\n# Describe to see probe failures\nkubectl describe pod mypod\n# Readiness probe failed: HTTP probe failed with statuscode: 500\n\n# Fix readiness probe endpoint\n# Edit pod or deployment\nkubectl edit pod mypod\n\n# Or exec into container to debug\nkubectl exec -it mypod -- /bin/sh\ncurl localhost:8080/ready\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#essential-kubectl-commands","title":"Essential kubectl Commands","text":"<pre><code># View pods\nkubectl get pods\nkubectl get pods -o wide              # Show IP and node\nkubectl get pods -l app=nginx         # Filter by label\nkubectl get pods --all-namespaces     # All namespaces\n\n# Detailed information\nkubectl describe pod mypod\n\n# Logs\nkubectl logs mypod                    # Current logs\nkubectl logs mypod -f                 # Follow logs\nkubectl logs mypod --previous         # Previous container\nkubectl logs mypod -c container-name  # Specific container\n\n# Execute commands\nkubectl exec mypod -- ls /app\nkubectl exec -it mypod -- /bin/bash   # Interactive shell\n\n# Port forwarding (access pod from local machine)\nkubectl port-forward mypod 8080:80\n\n# Copy files\nkubectl cp mypod:/app/config.json ./config.json\nkubectl cp ./config.json mypod:/app/config.json\n\n# Delete pods\nkubectl delete pod mypod\nkubectl delete pod -l app=nginx       # Delete by label\nkubectl delete pod mypod --force --grace-period=0  # Force delete\n\n# Edit running pod\nkubectl edit pod mypod                # Opens in editor\n\n# Replace pod\nkubectl replace -f pod.yaml --force   # Delete and recreate\n\n# Show resource usage\nkubectl top pod mypod\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#fast-debugging-workflow","title":"Fast Debugging Workflow","text":"<p>CKA Exam Strategy:</p> <ol> <li> <p>Understand the problem (30 seconds)    <pre><code>kubectl get pods\n</code></pre></p> </li> <li> <p>Gather information (1 minute)    <pre><code>kubectl describe pod &lt;name&gt;\nkubectl logs &lt;name&gt;\n</code></pre></p> </li> <li> <p>Identify root cause (1 minute)</p> </li> <li>Check events in <code>describe</code> output</li> <li>Look for error messages in logs</li> <li> <p>Verify configuration (image, env vars, probes)</p> </li> <li> <p>Apply fix (2 minutes)    <pre><code># Quick fixes\nkubectl delete pod &lt;name&gt;  # Let controller recreate\nkubectl edit pod &lt;name&gt;    # Edit configuration\nkubectl set image pod/&lt;name&gt; container=new-image\n</code></pre></p> </li> <li> <p>Verify solution (30 seconds)    <pre><code>kubectl get pod &lt;name&gt;\nkubectl logs &lt;name&gt;\n</code></pre></p> </li> </ol> <p>CKA Time-Savers:</p> <pre><code># Alias kubectl to k\nalias k=kubectl\n\n# Set default namespace\nkubectl config set-context --current --namespace=mynamespace\n\n# Use -o wide for extra info\nk get pods -o wide\n\n# Combine commands\nk describe pod mypod | grep -A 5 Events\nk logs mypod | tail -20\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#practice-exercises","title":"Practice Exercises","text":"<p>Hands-on practice is essential for CKA success. Complete these exercises to reinforce your pod mastery.</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-1-create-multi-container-pod","title":"Exercise 1: Create Multi-Container Pod","text":"<p>Task: Create a pod with nginx and busybox containers sharing a volume.</p> <p>Solution:</p> <pre><code># Generate base YAML\nkubectl run multi --image=nginx --dry-run=client -o yaml &gt; multi.yaml\n\n# Edit to add second container\ncat &lt;&lt;EOF &gt; multi.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: multi\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    volumeMounts:\n    - name: shared\n      mountPath: /usr/share/nginx/html\n\n  - name: busybox\n    image: busybox:1.35\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - while true; do\n        echo \"Hello from busybox - $(date)\" &gt; /html/index.html;\n        sleep 10;\n      done\n    volumeMounts:\n    - name: shared\n      mountPath: /html\n\n  volumes:\n  - name: shared\n    emptyDir: {}\nEOF\n\n# Apply\nkubectl apply -f multi.yaml\n\n# Verify\nkubectl exec multi -c nginx -- cat /usr/share/nginx/html/index.html\nkubectl exec multi -c busybox -- cat /html/index.html\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-2-configure-health-probes","title":"Exercise 2: Configure Health Probes","text":"<p>Task: Add startup, liveness, and readiness probes to a pod.</p> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: probes\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n\n    startupProbe:\n      httpGet:\n        path: /\n        port: 80\n      periodSeconds: 5\n      failureThreshold: 12\n\n    livenessProbe:\n      httpGet:\n        path: /\n        port: 80\n      periodSeconds: 10\n      failureThreshold: 3\n\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      periodSeconds: 5\n      failureThreshold: 3\nEOF\n\n# Watch pod startup\nkubectl get pod probes -w\n\n# Verify probes in pod spec\nkubectl describe pod probes | grep -A 10 Liveness\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-3-troubleshoot-crashloopbackoff","title":"Exercise 3: Troubleshoot CrashLoopBackOff","text":"<p>Task: Fix a pod stuck in CrashLoopBackOff.</p> <p>Setup: <pre><code>kubectl run crash --image=busybox --command -- /bin/sh -c \"exit 1\"\n</code></pre></p> <p>Solution:</p> <pre><code># Check status\nkubectl get pod crash\n# STATUS: CrashLoopBackOff\n\n# Check logs\nkubectl logs crash\nkubectl logs crash --previous\n\n# Describe to see crash reason\nkubectl describe pod crash\n# Last State: Terminated\n#   Reason: Error\n#   Exit Code: 1\n\n# Fix: Use valid command\nkubectl delete pod crash\nkubectl run crash --image=busybox --command -- /bin/sh -c \"sleep 3600\"\n\n# Verify\nkubectl get pod crash\n# STATUS: Running\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-4-set-resource-requests-and-limits","title":"Exercise 4: Set Resource Requests and Limits","text":"<p>Task: Create a pod with CPU and memory constraints.</p> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: resources\nspec:\n  containers:\n  - name: stress\n    image: polinux/stress\n    command: [\"stress\"]\n    args: [\"--cpu\", \"1\", \"--vm\", \"1\", \"--vm-bytes\", \"128M\"]\n    resources:\n      requests:\n        cpu: \"250m\"\n        memory: \"128Mi\"\n      limits:\n        cpu: \"500m\"\n        memory: \"256Mi\"\nEOF\n\n# Monitor resource usage\nkubectl top pod resources\n\n# Check QoS class\nkubectl get pod resources -o jsonpath='{.status.qosClass}'\n# Output: Burstable\n\n# Test memory limit (cause OOMKilled)\nkubectl exec resources -- stress --vm 1 --vm-bytes 300M --timeout 10s\nkubectl describe pod resources | grep -A 5 \"Last State\"\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#exercise-5-use-init-containers","title":"Exercise 5: Use Init Containers","text":"<p>Task: Create a pod with an init container that downloads configuration.</p> <p>Solution:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: init-demo\nspec:\n  initContainers:\n  - name: download-config\n    image: busybox:1.35\n    command:\n    - sh\n    - -c\n    - |\n      echo \"Downloading configuration...\"\n      echo '{\"app\":\"demo\",\"version\":\"1.0\"}' &gt; /config/app.json\n      echo \"Configuration ready!\"\n    volumeMounts:\n    - name: config\n      mountPath: /config\n\n  containers:\n  - name: app\n    image: busybox:1.35\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n    - |\n      echo \"Starting application...\"\n      cat /etc/app/app.json\n      sleep 3600\n    volumeMounts:\n    - name: config\n      mountPath: /etc/app\n\n  volumes:\n  - name: config\n    emptyDir: {}\nEOF\n\n# Watch init container execute\nkubectl get pod init-demo -w\n\n# Check init container logs\nkubectl logs init-demo -c download-config\n\n# Check main container logs\nkubectl logs init-demo -c app\n</code></pre>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#time-saving-tips-for-exercises","title":"Time-Saving Tips for Exercises","text":"<ol> <li>Use imperative commands for base YAML generation</li> <li>Master kubectl edit for quick modifications</li> <li>Practice with --dry-run=client -o yaml to generate templates</li> <li>Use kubectl explain for field documentation:    <pre><code>kubectl explain pod.spec.containers.livenessProbe\n</code></pre></li> <li>Create reusable snippets for common patterns</li> <li>Use tab completion in bash/zsh:    <pre><code>source &lt;(kubectl completion bash)\n</code></pre></li> </ol>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-pods-atomic-unit/#conclusion","title":"Conclusion","text":"<p>Pods are the foundation of Kubernetes - master them and you've built a solid base for everything else. In this guide, you've learned:</p> <ul> <li>Pod fundamentals: The atomic unit of Kubernetes with shared networking and storage</li> <li>Lifecycle management: Understanding pod phases and state transitions</li> <li>Multi-container patterns: Sidecar, init, adapter, and ambassador patterns for real-world scenarios</li> <li>Configuration mastery: Resource management, health probes, environment variables, and affinity</li> <li>Troubleshooting skills: Debugging CrashLoopBackOff, ImagePullBackOff, and Pending pods</li> <li>CKA exam techniques: Fast imperative commands and efficient debugging workflows</li> </ul> <p>Next Steps for CKA Preparation:</p> <ol> <li>Practice daily: Create 5-10 pods with different configurations</li> <li>Break things: Intentionally create failing pods and fix them</li> <li>Time yourself: Practice pod creation and troubleshooting under time pressure</li> <li>Explore workload controllers: Deployments, StatefulSets, DaemonSets (build on pods)</li> <li>Master kubectl: Speed and accuracy with imperative commands</li> </ol> <p>Related CKA Topics:</p> <ul> <li>Deployments: Manage pod replicas and rolling updates</li> <li>Services: Expose pods via stable network endpoints</li> <li>ConfigMaps and Secrets: External configuration for pods</li> <li>Persistent Volumes: Durable storage for stateful pods</li> <li>Network Policies: Control pod-to-pod communication</li> <li>RBAC: Secure pod access to Kubernetes API</li> </ul> <p>Resources for Continued Learning:</p> <ul> <li>Kubernetes Documentation: Pods</li> <li>Kubernetes Documentation: Pod Lifecycle</li> <li>kubectl Cheat Sheet</li> </ul> <p>Final CKA Tip: The exam tests your ability to solve problems quickly under pressure. Focus on:</p> <ul> <li>Speed: Imperative commands over writing YAML from scratch</li> <li>Troubleshooting: Most questions involve fixing broken resources</li> <li>Documentation: Know how to navigate Kubernetes docs efficiently</li> <li>Practice: Hands-on experience beats reading theory</li> </ul> <p>Good luck with your CKA preparation! Remember: every complex Kubernetes application starts with understanding pods. Master this fundamental building block, and you're well on your way to Kubernetes expertise.</p> <p>About the CKA Exam Series:</p> <p>This post is part of a comprehensive Certified Kubernetes Administrator (CKA) exam preparation series. Each article focuses on a specific exam domain with hands-on exercises, real-world examples, and exam-specific tips.</p> <p>Related Posts:</p> <ul> <li>Kubernetes Architecture Fundamentals</li> <li>Kubernetes Objects &amp; YAML Mastery</li> <li>kubectl Essentials for CKA</li> <li>Namespaces &amp; Resource Quotas</li> </ul> <p>Stay tuned for upcoming posts on Deployments, Services, Persistent Storage, and Security!</p>","tags":["kubernetes","k8s","cka-prep","pods","containers","workloads"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/","title":"Advanced Scheduling: Taints, Tolerations, and Affinity","text":"<p>Master Kubernetes pod scheduling with taints, tolerations, node affinity, and pod affinity/anti-affinity. Essential knowledge for CKA exam success and production-grade workload placement.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#overview","title":"Overview","text":"<p>The Kubernetes scheduler is responsible for assigning pods to nodes, making intelligent decisions based on resource requirements, constraints, and policies. While the default scheduler works well for simple deployments, production environments require fine-grained control over pod placement.</p> <p>CKA Exam Domain: Workloads &amp; Scheduling (15% of exam)</p> <p>Key Insight: Advanced scheduling isn't just about passing the CKA exam\u2014it's essential for high availability, resource optimization, cost management, and regulatory compliance in production Kubernetes clusters.</p> <p>What You'll Learn: - How the Kubernetes scheduler makes placement decisions - Node selectors for simple node selection - Taints and tolerations for node restriction patterns - Node affinity for complex node selection rules - Pod affinity and anti-affinity for inter-pod placement - Priority classes and preemption mechanisms - CKA exam speed techniques for scheduling tasks</p> <p>Understanding these scheduling mechanisms enables you to: - Optimize costs by directing workloads to specific node types - Improve reliability through strategic pod distribution - Meet compliance requirements with dedicated node pools - Enhance performance by co-locating related services - Manage maintenance without disrupting critical workloads</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#how-the-scheduler-works","title":"How the Scheduler Works","text":"<p>The Kubernetes scheduler is a control plane component that watches for newly created pods with no assigned node and selects an optimal node for them to run on.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#scheduling-process","title":"Scheduling Process","text":"<pre><code>graph TB\n    START[New Pod Created] --&gt; FILTER[Filter Phase]\n    FILTER --&gt; |Remove unsuitable nodes| SCORE[Score Phase]\n    SCORE --&gt; |Rank remaining nodes| SELECT[Select Best Node]\n    SELECT --&gt; BIND[Bind Pod to Node]\n\n    FILTER --&gt; CHECK1{Sufficient Resources?}\n    FILTER --&gt; CHECK2{Node Selectors Match?}\n    FILTER --&gt; CHECK3{Taints Tolerated?}\n    FILTER --&gt; CHECK4{Affinity Rules Met?}\n\n    CHECK1 --&gt; |No| REJECT1[Remove Node]\n    CHECK2 --&gt; |No| REJECT2[Remove Node]\n    CHECK3 --&gt; |No| REJECT3[Remove Node]\n    CHECK4 --&gt; |No| REJECT4[Remove Node]\n\n    CHECK1 --&gt; |Yes| PASS1[Keep Node]\n    CHECK2 --&gt; |Yes| PASS2[Keep Node]\n    CHECK3 --&gt; |Yes| PASS3[Keep Node]\n    CHECK4 --&gt; |Yes| PASS4[Keep Node]\n\n    PASS1 --&gt; SCORE\n    PASS2 --&gt; SCORE\n    PASS3 --&gt; SCORE\n    PASS4 --&gt; SCORE</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#two-phase-algorithm","title":"Two-Phase Algorithm","text":"<p>Phase 1: Filtering (Feasibility) The scheduler eliminates nodes that don't meet pod requirements:</p> <ul> <li>Resource availability: Does the node have enough CPU, memory, and ephemeral storage?</li> <li>Node selector constraints: Does the node have required labels?</li> <li>Taints and tolerations: Can the pod tolerate node taints?</li> <li>Affinity/anti-affinity rules: Do inter-pod placement rules allow this node?</li> <li>Port availability: Are required host ports available?</li> <li>Volume constraints: Can required volumes be mounted?</li> </ul> <p>Phase 2: Scoring (Prioritization) For remaining nodes, the scheduler assigns scores based on:</p> <ul> <li>Resource balance: Prefer nodes with balanced resource utilization</li> <li>Image locality: Favor nodes with container images already pulled</li> <li>Spreading: Distribute pods across zones/nodes for high availability</li> <li>Affinity preferences: Honor preferred (not required) affinity rules</li> <li>Custom priorities: Apply user-defined priority functions</li> </ul> <p>The node with the highest score wins. Ties are broken randomly to ensure even distribution.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#default-behavior","title":"Default Behavior","text":"<p>Without any scheduling constraints, the scheduler: 1. Filters nodes with sufficient resources 2. Scores nodes based on resource balance and spreading 3. Selects the optimal node 4. Binds the pod to that node</p> <p>This works well for stateless applications but isn't sufficient for: - Specialized hardware requirements (GPUs, SSDs, specific CPU types) - Regulatory compliance (data locality, dedicated infrastructure) - Performance optimization (co-location, anti-affinity) - Cost optimization (spot instances, cheaper node pools)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#node-selectors-simple-label-based-selection","title":"Node Selectors: Simple Label-Based Selection","text":"<p>Node selectors provide the simplest way to constrain pods to nodes with specific labels.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#how-node-selectors-work","title":"How Node Selectors Work","text":"<p>Node selectors use key-value pairs to match node labels. A pod will only be scheduled on nodes that have all specified labels.</p> <p>Labeling a Node: <pre><code># Add label to node\nkubectl label nodes node01 disktype=ssd\n\n# Add multiple labels\nkubectl label nodes node02 disktype=hdd environment=production\n\n# Verify labels\nkubectl get nodes --show-labels\nkubectl describe node node01 | grep -A5 Labels\n</code></pre></p> <p>Using Node Selectors in Pods: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-ssd\nspec:\n  nodeSelector:\n    disktype: ssd  # Only schedule on nodes with this label\n  containers:\n  - name: nginx\n    image: nginx:1.24\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#multiple-node-selectors","title":"Multiple Node Selectors","text":"<p>When you specify multiple node selectors, all labels must match (AND logic):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: database\nspec:\n  nodeSelector:\n    disktype: ssd\n    environment: production\n    region: us-west\n  containers:\n  - name: postgres\n    image: postgres:15\n</code></pre> <p>This pod will only schedule on nodes that have all three labels.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#built-in-node-labels","title":"Built-In Node Labels","text":"<p>Kubernetes automatically applies several useful labels to nodes:</p> <pre><code># Architecture and OS\nkubernetes.io/arch: amd64\nkubernetes.io/os: linux\n\n# Cloud provider metadata (on cloud-managed clusters)\ntopology.kubernetes.io/region: us-west-2\ntopology.kubernetes.io/zone: us-west-2a\nnode.kubernetes.io/instance-type: m5.xlarge\n\n# Hostname\nkubernetes.io/hostname: node01\n</code></pre> <p>Example - Zone-Specific Scheduling: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-west\nspec:\n  nodeSelector:\n    topology.kubernetes.io/zone: us-west-2a\n  containers:\n  - name: app\n    image: myapp:v1\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#limitations-of-node-selectors","title":"Limitations of Node Selectors","text":"<p>While simple and fast, node selectors have significant limitations:</p> <ol> <li>No OR logic: Cannot express \"disktype=ssd OR disktype=nvme\"</li> <li>No NOT logic: Cannot express \"NOT environment=development\"</li> <li>No numeric comparisons: Cannot express \"cpu-count &gt; 8\"</li> <li>Hard requirements only: Cannot express preferences (e.g., \"prefer SSD but allow HDD\")</li> <li>Single level: Cannot express complex nested logic</li> </ol> <p>When to Use Node Selectors: - \u2705 Simple label matching requirements - \u2705 Quick temporary constraints - \u2705 CKA exam speed (fastest to type)</p> <p>When to Upgrade to Node Affinity: - \u274c Need OR/NOT logic - \u274c Need soft preferences - \u274c Need numeric comparisons - \u274c Complex multi-condition rules</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#taints-and-tolerations-node-restriction-pattern","title":"Taints and Tolerations: Node Restriction Pattern","text":"<p>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. Think of taints as a \"repellent\" on nodes, and tolerations as a pod's \"immunity\" to specific taints.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#conceptual-model","title":"Conceptual Model","text":"<p>Taints are applied to nodes to mark them as unsuitable for most pods: <pre><code># Syntax: key=value:effect\nkubectl taint nodes node01 gpu=true:NoSchedule\n</code></pre></p> <p>Tolerations are specified in pod specs to allow scheduling on tainted nodes: <pre><code>tolerations:\n- key: \"gpu\"\n  operator: \"Equal\"\n  value: \"true\"\n  effect: \"NoSchedule\"\n</code></pre></p> <p>Key Concept: Taints repel pods by default. Only pods with matching tolerations can be scheduled on tainted nodes. This is the opposite of affinity (which attracts pods to nodes).</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#taint-effects","title":"Taint Effects","text":"<p>Kubernetes supports three taint effects that control scheduling and eviction behavior:</p> <pre><code>graph TB\n    subgraph \"Taint Effects\"\n        NOSCHEDULE[NoSchedule]\n        PREFERNOSCHEDULE[PreferNoSchedule]\n        NOEXECUTE[NoExecute]\n    end\n\n    NOSCHEDULE --&gt; |Behavior| NS_BEHAVIOR[Hard constraint&lt;br/&gt;No new pods scheduled&lt;br/&gt;Existing pods stay]\n    PREFERNOSCHEDULE --&gt; |Behavior| PNS_BEHAVIOR[Soft constraint&lt;br/&gt;Avoid if possible&lt;br/&gt;Schedule if necessary]\n    NOEXECUTE --&gt; |Behavior| NE_BEHAVIOR[Hard constraint + eviction&lt;br/&gt;No new pods scheduled&lt;br/&gt;Existing pods evicted]\n\n    subgraph \"Use Cases\"\n        NS_USE[Dedicated nodes&lt;br/&gt;Special hardware&lt;br/&gt;Maintenance prep]\n        PNS_USE[Spot instances&lt;br/&gt;Lower priority nodes&lt;br/&gt;Cost optimization]\n        NE_USE[Node draining&lt;br/&gt;Emergency eviction&lt;br/&gt;Hardware failures]\n    end\n\n    NOSCHEDULE -.-&gt; NS_USE\n    PREFERNOSCHEDULE -.-&gt; PNS_USE\n    NOEXECUTE -.-&gt; NE_USE</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#noschedule-hard-constraint","title":"NoSchedule (Hard Constraint)","text":"<p>Prevents new pods from being scheduled on the node. Existing pods remain running.</p> <pre><code># Apply NoSchedule taint\nkubectl taint nodes node01 dedicated=ml:NoSchedule\n\n# Verify taint\nkubectl describe node node01 | grep Taints\n# Output: Taints: dedicated=ml:NoSchedule\n</code></pre> <p>Pod Toleration: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ml-training\nspec:\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Equal\"\n    value: \"ml\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: trainer\n    image: tensorflow:latest\n</code></pre></p> <p>Use Cases: - Dedicated node pools (GPU nodes, high-memory nodes) - Isolating production workloads from development - Reserving nodes for specific teams or projects</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#prefernoschedule-soft-constraint","title":"PreferNoSchedule (Soft Constraint)","text":"<p>Scheduler tries to avoid placing pods on the node but will schedule them if no other nodes are available.</p> <pre><code># Apply PreferNoSchedule taint\nkubectl taint nodes node02 spot-instance=true:PreferNoSchedule\n</code></pre> <p>Behavior: - Scheduler prefers untainted nodes - Falls back to tainted nodes under resource pressure - No eviction of existing pods</p> <p>Use Cases: - Spot/preemptible instances (prefer stable nodes but allow spot) - Cost optimization (prefer cheaper nodes) - Gradual migration (discourage but don't prevent scheduling)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#noexecute-hard-constraint-eviction","title":"NoExecute (Hard Constraint + Eviction)","text":"<p>Prevents scheduling and evicts existing pods that don't tolerate the taint.</p> <pre><code># Apply NoExecute taint\nkubectl taint nodes node03 maintenance=true:NoExecute\n</code></pre> <p>Immediate Effects: 1. No new pods scheduled on the node 2. Existing pods without tolerations are evicted immediately 3. Pods with tolerations remain (unless <code>tolerationSeconds</code> expires)</p> <p>Toleration with Grace Period: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  tolerations:\n  - key: \"maintenance\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 300  # Evict after 5 minutes\n  containers:\n  - name: nginx\n    image: nginx:1.24\n</code></pre></p> <p>Use Cases: - Node draining: Gracefully migrate workloads before maintenance - Failure handling: Automatically evict pods from degraded nodes - Time-bounded tolerations: Allow temporary execution (e.g., batch jobs)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#toleration-operators","title":"Toleration Operators","text":"<p>Kubernetes supports two toleration operators:</p> <p>Equal Operator (exact match): <pre><code>tolerations:\n- key: \"gpu\"\n  operator: \"Equal\"\n  value: \"nvidia-a100\"\n  effect: \"NoSchedule\"\n</code></pre> Matches taint: <code>gpu=nvidia-a100:NoSchedule</code></p> <p>Exists Operator (key-only match): <pre><code>tolerations:\n- key: \"gpu\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n</code></pre> Matches any taint with key <code>gpu</code> and effect <code>NoSchedule</code>, regardless of value.</p> <p>Wildcard Tolerations (tolerate all taints): <pre><code>tolerations:\n- operator: \"Exists\"  # Tolerates ALL taints (dangerous!)\n</code></pre></p> <p>Effect Wildcard (tolerate all effects for a key): <pre><code>tolerations:\n- key: \"gpu\"\n  operator: \"Exists\"  # Tolerates gpu=* with any effect\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#control-plane-taints","title":"Control Plane Taints","text":"<p>By default, Kubernetes taints control plane nodes to prevent workload pods from being scheduled there:</p> <pre><code># View control plane taints\nkubectl describe node controlplane | grep Taints\n\n# Typical output:\n# Taints: node-role.kubernetes.io/control-plane:NoSchedule\n</code></pre> <p>Scheduling on Control Plane (not recommended for production): <pre><code>tolerations:\n- key: \"node-role.kubernetes.io/control-plane\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n</code></pre></p> <p>CKA Exam Note: You may need to schedule pods on control plane nodes in exam clusters. Use the toleration above.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#taint-management-commands","title":"Taint Management Commands","text":"<pre><code># Add taint\nkubectl taint nodes node01 key=value:NoSchedule\n\n# Remove taint (note the trailing dash)\nkubectl taint nodes node01 key=value:NoSchedule-\n\n# Remove taint by key only\nkubectl taint nodes node01 key-\n\n# Update taint value\nkubectl taint nodes node01 key=newvalue:NoSchedule --overwrite\n\n# View all node taints\nkubectl get nodes -o json | jq '.items[].spec.taints'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#practical-taint-scenarios","title":"Practical Taint Scenarios","text":"<p>Scenario 1: Dedicated GPU Nodes <pre><code># Taint GPU nodes\nkubectl taint nodes gpu-node-01 hardware=gpu:NoSchedule\nkubectl label nodes gpu-node-01 accelerator=nvidia-a100\n\n# Deploy GPU workload\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-training\nspec:\n  nodeSelector:\n    accelerator: nvidia-a100\n  tolerations:\n  - key: \"hardware\"\n    operator: \"Equal\"\n    value: \"gpu\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: trainer\n    image: tensorflow/tensorflow:latest-gpu\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre></p> <p>Scenario 2: Node Maintenance Preparation <pre><code># Step 1: Prevent new pods (NoSchedule)\nkubectl taint nodes node02 maintenance=scheduled:NoSchedule\n\n# Step 2: After workloads migrated, evict remaining pods\nkubectl taint nodes node02 maintenance=scheduled:NoExecute --overwrite\n\n# Step 3: Perform maintenance...\n\n# Step 4: Remove taint and return to service\nkubectl taint nodes node02 maintenance-\n</code></pre></p> <p>Scenario 3: Spot Instance Node Pool <pre><code># Taint spot instances as less preferred\nkubectl taint nodes spot-node-01 spot-instance=true:PreferNoSchedule\n\n# Workload that can tolerate spot instances\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: batch-processor\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: batch\n  template:\n    metadata:\n      labels:\n        app: batch\n    spec:\n      tolerations:\n      - key: \"spot-instance\"\n        operator: \"Exists\"\n      containers:\n      - name: processor\n        image: batch-app:v1\nEOF\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#node-affinity-advanced-node-selection","title":"Node Affinity: Advanced Node Selection","text":"<p>Node affinity is a powerful evolution of node selectors that provides expressive label-based selection with support for operators, soft preferences, and complex logic.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#node-affinity-vs-node-selectors","title":"Node Affinity vs Node Selectors","text":"Feature nodeSelector Node Affinity Label matching \u2705 Exact match only \u2705 Multiple operators OR logic \u274c Not supported \u2705 <code>In</code> operator NOT logic \u274c Not supported \u2705 <code>NotIn</code>, <code>DoesNotExist</code> Numeric comparison \u274c Not supported \u2705 <code>Gt</code>, <code>Lt</code> operators Soft preferences \u274c Hard only \u2705 <code>preferred</code> rules Multiple rules \u274c AND only \u2705 AND + OR combinations","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#affinity-rule-types","title":"Affinity Rule Types","text":"<pre><code>graph TB\n    subgraph \"Node Affinity Types\"\n        REQUIRED[requiredDuringSchedulingIgnoredDuringExecution]\n        PREFERRED[preferredDuringSchedulingIgnoredDuringExecution]\n    end\n\n    REQUIRED --&gt; |Behavior| REQ_BEHAVIOR[Hard constraint&lt;br/&gt;Must match or pod pending&lt;br/&gt;Scheduling phase only]\n    PREFERRED --&gt; |Behavior| PREF_BEHAVIOR[Soft constraint&lt;br/&gt;Prefer but not required&lt;br/&gt;Weighted scoring]\n\n    subgraph \"Naming Convention\"\n        SCHEDULING[DuringScheduling&lt;br/&gt;Rules apply when scheduling]\n        EXECUTION[DuringExecution&lt;br/&gt;IgnoredDuringExecution = no eviction]\n    end\n\n    REQUIRED -.-&gt; SCHEDULING\n    REQUIRED -.-&gt; EXECUTION\n    PREFERRED -.-&gt; SCHEDULING\n    PREFERRED -.-&gt; EXECUTION</code></pre> <p>Naming Convention Explained: - <code>requiredDuringScheduling</code>: Pod must match rules to be scheduled - <code>preferredDuringScheduling</code>: Scheduler prefers matching nodes but allows others - <code>IgnoredDuringExecution</code>: Rules are not re-evaluated after scheduling (no eviction)</p> <p>Future (not yet implemented): - <code>requiredDuringExecution</code>: Would evict pods if node labels change (similar to NoExecute taint)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#required-node-affinity","title":"Required Node Affinity","text":"<p>Required affinity creates hard constraints that must be satisfied for scheduling.</p> <p>Basic Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: database\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - ssd\n            - nvme\n  containers:\n  - name: postgres\n    image: postgres:15\n</code></pre></p> <p>Logic: Schedule on nodes with <code>disktype=ssd</code> OR <code>disktype=nvme</code>.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#all-node-affinity-operators","title":"All Node Affinity Operators","text":"<p>In - Label value in list (OR logic): <pre><code>- key: environment\n  operator: In\n  values:\n  - production\n  - staging\n</code></pre></p> <p>NotIn - Label value not in list: <pre><code>- key: environment\n  operator: NotIn\n  values:\n  - development\n  - test\n</code></pre></p> <p>Exists - Label key exists (value doesn't matter): <pre><code>- key: ssd\n  operator: Exists\n</code></pre></p> <p>DoesNotExist - Label key doesn't exist: <pre><code>- key: spot-instance\n  operator: DoesNotExist\n</code></pre></p> <p>Gt - Numeric greater than: <pre><code>- key: cpu-count\n  operator: Gt\n  values:\n  - \"8\"  # Note: values are always strings\n</code></pre></p> <p>Lt - Numeric less than: <pre><code>- key: age-days\n  operator: Lt\n  values:\n  - \"30\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#multiple-node-selector-terms-or-logic","title":"Multiple Node Selector Terms (OR Logic)","text":"<p>Multiple <code>nodeSelectorTerms</code> are ORed together. Each term's <code>matchExpressions</code> are ANDed.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: flexible-app\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        # Term 1: High-performance nodes\n        - matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - nvme\n          - key: cpu-count\n            operator: Gt\n            values:\n            - \"16\"\n        # OR\n        # Term 2: GPU nodes (even if slower disk)\n        - matchExpressions:\n          - key: accelerator\n            operator: Exists\n  containers:\n  - name: app\n    image: myapp:v1\n</code></pre> <p>Logic: Schedule on nodes that match: - (disktype=nvme AND cpu-count&gt;16) OR (accelerator exists)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#preferred-node-affinity","title":"Preferred Node Affinity","text":"<p>Preferred affinity creates soft preferences with configurable weights.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  affinity:\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 80  # Higher weight = stronger preference\n        preference:\n          matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - ssd\n      - weight: 20\n        preference:\n          matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - us-west-2a\n  containers:\n  - name: nginx\n    image: nginx:1.24\n</code></pre> <p>Behavior: - Scheduler calculates scores for each node - Node score += weight if preference matches - Highest total score wins - Pod will still schedule even if no preferences match</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#combining-required-and-preferred","title":"Combining Required and Preferred","text":"<p>Best practice: Use required for must-have constraints, preferred for nice-to-have optimizations.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: critical-db\nspec:\n  affinity:\n    nodeAffinity:\n      # MUST be in production environment\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: environment\n            operator: In\n            values:\n            - production\n      # PREFER high-performance and specific zone\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        preference:\n          matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - nvme\n      - weight: 50\n        preference:\n          matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - us-west-2a\n  containers:\n  - name: postgres\n    image: postgres:15\n</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#node-affinity-evaluation-flow","title":"Node Affinity Evaluation Flow","text":"<pre><code>graph TB\n    START[Evaluate Node] --&gt; REQUIRED{Required Affinity&lt;br/&gt;Rules Exist?}\n    REQUIRED --&gt; |Yes| EVAL_REQ[Evaluate Required Rules]\n    REQUIRED --&gt; |No| PREFERRED{Preferred Affinity&lt;br/&gt;Rules Exist?}\n\n    EVAL_REQ --&gt; REQ_MATCH{All Required&lt;br/&gt;Rules Match?}\n    REQ_MATCH --&gt; |No| REJECT[Remove Node from Consideration]\n    REQ_MATCH --&gt; |Yes| PREFERRED\n\n    PREFERRED --&gt; |Yes| EVAL_PREF[Calculate Preference Score]\n    PREFERRED --&gt; |No| BASE_SCORE[Use Base Score]\n\n    EVAL_PREF --&gt; SCORE[Add to Node Score]\n    BASE_SCORE --&gt; SCORE\n    SCORE --&gt; NEXT[Continue to Next Node]</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#cka-exam-speed-techniques","title":"CKA Exam Speed Techniques","text":"<p>Fast YAML generation: <pre><code># Generate pod with dry-run, then edit\nkubectl run myapp --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit to add node affinity\nvim pod.yaml\n</code></pre></p> <p>Quick affinity template (memorize this structure): <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: KEY\n          operator: In\n          values:\n          - VALUE\n</code></pre></p> <p>Common exam pattern: <pre><code># 1. Label node\nkubectl label nodes node01 tier=frontend\n\n# 2. Create pod with affinity\nkubectl run web --image=nginx --dry-run=client -o yaml | \\\nkubectl set selector --local -f - 'app=web' -o yaml | \\\n# ... add affinity in vim ...\nkubectl apply -f -\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-affinity-and-anti-affinity","title":"Pod Affinity and Anti-Affinity","text":"<p>Pod affinity and anti-affinity allow you to constrain pod scheduling based on labels of other pods already running on nodes, rather than node labels.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#use-cases","title":"Use Cases","text":"<p>Pod Affinity (co-location): - Deploy web tier pods on same nodes as cache pods - Co-locate related microservices for low-latency communication - Place pods near data for performance</p> <p>Pod Anti-Affinity (spreading): - Distribute replicas across nodes for high availability - Prevent multiple critical services on same node - Spread pods across failure domains (zones, racks)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#topology-keys","title":"Topology Keys","text":"<p>Pod affinity rules use topology keys to define the scope of \"co-location\":</p> <pre><code>topologyKey: kubernetes.io/hostname  # Same node\ntopologyKey: topology.kubernetes.io/zone  # Same zone\ntopologyKey: topology.kubernetes.io/region  # Same region\ntopologyKey: kubernetes.io/os  # Same OS (all nodes)\n</code></pre> <p>Topology Key Behavior: - Scheduler groups nodes by the topology key value - Affinity/anti-affinity rules apply within each group - <code>kubernetes.io/hostname</code> = node-level granularity (most common)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-affinity-visualization","title":"Pod Affinity Visualization","text":"<pre><code>graph TB\n    subgraph \"Zone us-west-2a\"\n        subgraph \"Node 1\"\n            CACHE1[Cache Pod&lt;br/&gt;app=redis]\n            WEB1[Web Pod&lt;br/&gt;affinity to redis]\n        end\n        subgraph \"Node 2\"\n            CACHE2[Cache Pod&lt;br/&gt;app=redis]\n            WEB2[Web Pod&lt;br/&gt;affinity to redis]\n        end\n    end\n\n    subgraph \"Zone us-west-2b\"\n        subgraph \"Node 3\"\n            DB1[Database Pod&lt;br/&gt;anti-affinity]\n        end\n        subgraph \"Node 4\"\n            DB2[Database Pod&lt;br/&gt;anti-affinity]\n        end\n    end\n\n    WEB1 -.-&gt;|Pod Affinity| CACHE1\n    WEB2 -.-&gt;|Pod Affinity| CACHE2\n    DB1 -.-&gt;|Anti-Affinity| DB2</code></pre>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-affinity-examples","title":"Pod Affinity Examples","text":"<p>Required Pod Affinity (hard constraint): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-frontend\n  labels:\n    app: web\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - cache\n        topologyKey: kubernetes.io/hostname\n  containers:\n  - name: nginx\n    image: nginx:1.24\n</code></pre></p> <p>Logic: This web pod must be scheduled on a node that already has a pod with <code>app=cache</code>.</p> <p>Preferred Pod Affinity (soft constraint): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-server\nspec:\n  affinity:\n    podAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: component\n              operator: In\n              values:\n              - database\n          topologyKey: kubernetes.io/hostname\n  containers:\n  - name: api\n    image: api:v1\n</code></pre></p> <p>Logic: Prefer to schedule this API pod on nodes with database pods, but allow other nodes if necessary.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-anti-affinity-examples","title":"Pod Anti-Affinity Examples","text":"<p>High Availability Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: critical-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: critical\n  template:\n    metadata:\n      labels:\n        app: critical\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - critical\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: app\n        image: critical-app:v1\n</code></pre></p> <p>Logic: No two <code>app=critical</code> pods can run on the same node. Each replica must be on a different node.</p> <p>Zone-Level Anti-Affinity: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: database-replica\n  labels:\n    app: database\nspec:\n  affinity:\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: app\n              operator: In\n              values:\n              - database\n          topologyKey: topology.kubernetes.io/zone\n  containers:\n  - name: postgres\n    image: postgres:15\n</code></pre></p> <p>Logic: Prefer to spread database pods across different availability zones for disaster recovery.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#symmetric-vs-asymmetric-affinity","title":"Symmetric vs Asymmetric Affinity","text":"<p>Symmetric (mutual attraction/repulsion): <pre><code># Pod A has affinity to Pod B\n# Pod B has affinity to Pod A\n# Result: Co-located pairs\n</code></pre></p> <p>Asymmetric (one-way): <pre><code># Pod A (web) has affinity to Pod B (cache)\n# Pod B (cache) has NO affinity rules\n# Result: Web pods follow cache pods, but cache pods schedule independently\n</code></pre></p> <p>Best Practice: Use asymmetric affinity where one component (cache, database) should schedule freely, and dependent components (web, API) follow them.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#performance-considerations","title":"Performance Considerations","text":"<p>Pod affinity/anti-affinity has performance costs:</p> <p>Scheduling Latency: - Required rules: Moderate impact (must check all nodes) - Preferred rules: Higher impact (calculate scores for all nodes) - Large clusters (&gt;500 nodes): Significant delay</p> <p>Recommendations: - Use affinity sparingly (only where truly needed) - Prefer node affinity over pod affinity when possible - Use preferred over required when acceptable - Limit label selector complexity</p> <p>Exam Note: Pod affinity can cause pods to remain Pending if no nodes satisfy the constraint. Always verify with <code>kubectl describe pod</code> to check scheduling events.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#advanced-scheduling-concepts","title":"Advanced Scheduling Concepts","text":"","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#priority-classes-and-preemption","title":"Priority Classes and Preemption","text":"<p>Priority classes allow you to define the relative importance of pods. The scheduler can evict (preempt) lower-priority pods to make room for higher-priority ones.</p> <p>Creating a Priority Class: <pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000  # Higher = more important\nglobalDefault: false\ndescription: \"High-priority workloads\"\n</code></pre></p> <pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: low-priority\nvalue: 100\nglobalDefault: false\ndescription: \"Best-effort workloads\"\n</code></pre> <p>Using Priority Classes: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: critical-app\nspec:\n  priorityClassName: high-priority\n  containers:\n  - name: app\n    image: critical:v1\n</code></pre></p> <p>Preemption Flow: <pre><code>graph TB\n    START[High-Priority Pod Created] --&gt; FILTER[Filter Feasible Nodes]\n    FILTER --&gt; SPACE{Sufficient&lt;br/&gt;Resources?}\n    SPACE --&gt; |Yes| SCHEDULE[Schedule Normally]\n    SPACE --&gt; |No| PREEMPT{Can Preempt&lt;br/&gt;Lower-Priority Pods?}\n    PREEMPT --&gt; |Yes| EVICT[Evict Lower-Priority Pods]\n    EVICT --&gt; SCHEDULE2[Schedule High-Priority Pod]\n    PREEMPT --&gt; |No| PENDING[Pod Remains Pending]\n\n    EVICT --&gt; GRACE[Respect PodDisruptionBudget]\n    GRACE --&gt; TERM[Graceful Termination]</code></pre></p> <p>Built-In Priority Classes: <pre><code>kubectl get priorityclasses\n\n# Typical output:\n# NAME                      VALUE        GLOBAL-DEFAULT   AGE\n# system-cluster-critical   2000000000   false            1d\n# system-node-critical      2000001000   false            1d\n</code></pre></p> <p>Use Cases: - Critical infrastructure: Cluster DNS, monitoring, logging - Production vs development: Preempt dev pods for production - Batch processing: Low-priority batch jobs yield to interactive workloads</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#pod-topology-spread-constraints","title":"Pod Topology Spread Constraints","text":"<p>Topology spread constraints provide fine-grained control over pod distribution across failure domains.</p> <p>Basic Spread Constraint: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1  # Max difference between zones\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule  # Hard constraint\n        labelSelector:\n          matchLabels:\n            app: web\n      containers:\n      - name: nginx\n        image: nginx:1.24\n</code></pre></p> <p>Behavior: - With 3 zones and 6 replicas: Each zone gets 2 pods - <code>maxSkew: 1</code> means max difference of 1 pod between zones - If a zone is unavailable, pods remain Pending (<code>DoNotSchedule</code>)</p> <p>Soft Constraint (use <code>ScheduleAnyway</code>): <pre><code>topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: topology.kubernetes.io/zone\n  whenUnsatisfiable: ScheduleAnyway  # Soft: prefer but allow skew\n  labelSelector:\n    matchLabels:\n      app: web\n</code></pre></p> <p>Multi-Level Spreading (zones and nodes): <pre><code>topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: topology.kubernetes.io/zone\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchLabels:\n      app: web\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: ScheduleAnyway\n  labelSelector:\n    matchLabels:\n      app: web\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#resource-requests-and-scheduling","title":"Resource Requests and Scheduling","text":"<p>While not a scheduling constraint per se, resource requests heavily influence scheduling decisions.</p> <p>Resource-Aware Scheduling: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: resource-demo\nspec:\n  containers:\n  - name: app\n    image: myapp:v1\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n</code></pre></p> <p>Scheduler Behavior: - Filters out nodes with insufficient allocatable resources - Considers requests (not limits) for scheduling - Spreads pods to balance resource utilization</p> <p>QoS Classes and Scheduling: - Guaranteed: requests = limits (highest priority, never preempted) - Burstable: requests &lt; limits (medium priority) - BestEffort: no requests/limits (lowest priority, first to be evicted)</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#custom-schedulers","title":"Custom Schedulers","text":"<p>Kubernetes allows custom schedulers for specialized scheduling logic.</p> <p>Specifying a Custom Scheduler: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: custom-scheduled\nspec:\n  schedulerName: my-custom-scheduler  # Default: \"default-scheduler\"\n  containers:\n  - name: app\n    image: myapp:v1\n</code></pre></p> <p>Use Cases (rare in practice): - Machine learning job scheduling (gang scheduling) - Highly specialized hardware placement - Custom cost optimization algorithms</p> <p>CKA Exam: Custom schedulers are not required knowledge for the exam.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#cka-exam-skills-fast-scheduling-commands","title":"CKA Exam Skills: Fast Scheduling Commands","text":"","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#speed-techniques-for-taints-and-tolerations","title":"Speed Techniques for Taints and Tolerations","text":"<p>Fast Taint Application: <pre><code># Standard taint\nkubectl taint nodes node01 key=value:NoSchedule\n\n# Multiple taints at once\nkubectl taint nodes node01 gpu=true:NoSchedule dedicated=ml:NoSchedule\n\n# Remove taint (trailing dash)\nkubectl taint nodes node01 key:NoSchedule-\nkubectl taint nodes node01 key-  # Remove all effects for key\n\n# List taints on all nodes\nkubectl describe nodes | grep -i taint\n</code></pre></p> <p>Generating Tolerations: <pre><code># Start with pod template\nkubectl run mypod --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit to add toleration (use vim macros or snippets)\nvim pod.yaml\n</code></pre></p> <p>Toleration YAML snippet (memorize): <pre><code>tolerations:\n- key: \"KEY\"\n  operator: \"Equal\"\n  value: \"VALUE\"\n  effect: \"NoSchedule\"\n</code></pre></p> <p>Exists operator (faster typing): <pre><code>tolerations:\n- key: \"KEY\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#speed-techniques-for-node-affinity","title":"Speed Techniques for Node Affinity","text":"<p>Quick nodeSelector (use when possible): <pre><code># Generate pod with dry-run\nkubectl run web --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Add nodeSelector inline\ncat &lt;&lt;EOF &gt;&gt; pod.yaml\n  nodeSelector:\n    disktype: ssd\nEOF\n\nkubectl apply -f pod.yaml\n</code></pre></p> <p>Affinity template (for complex rules): <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: KEY\n          operator: In\n          values:\n          - VALUE1\n          - VALUE2\n</code></pre></p> <p>Vim snippet (create in <code>~/.vimrc</code>): <pre><code>iabbrev affin affinity:&lt;CR&gt;  nodeAffinity:&lt;CR&gt;    requiredDuringSchedulingIgnoredDuringExecution:&lt;CR&gt;      nodeSelectorTerms:&lt;CR&gt;      - matchExpressions:&lt;CR&gt;        - key: &lt;CR&gt;          operator: In&lt;CR&gt;          values:&lt;CR&gt;          -\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#troubleshooting-pending-pods","title":"Troubleshooting Pending Pods","text":"<p>Fast Diagnosis: <pre><code># Check pod status\nkubectl get pods\n\n# Describe pod for events\nkubectl describe pod &lt;pod-name&gt;\n\n# Look for scheduling failures\nkubectl describe pod &lt;pod-name&gt; | grep -A10 Events\n\n# Common issues:\n# - \"0/3 nodes are available: 3 node(s) didn't match node selector.\"\n# - \"0/3 nodes are available: 3 node(s) had taints that the pod didn't tolerate.\"\n# - \"0/3 nodes are available: 3 Insufficient cpu.\"\n</code></pre></p> <p>Check Node Resources: <pre><code># View allocatable resources\nkubectl describe nodes | grep -A5 \"Allocatable\"\n\n# View node labels\nkubectl get nodes --show-labels\n\n# View node taints\nkubectl describe nodes | grep -i taint\n</code></pre></p> <p>Debugging Workflow: <pre><code>graph TB\n    PENDING[Pod Status: Pending] --&gt; DESCRIBE[kubectl describe pod]\n    DESCRIBE --&gt; EVENTS{Check Events&lt;br/&gt;Section}\n\n    EVENTS --&gt; SELECTOR{Node selector&lt;br/&gt;mismatch?}\n    SELECTOR --&gt; |Yes| FIX1[Fix labels or selector]\n\n    EVENTS --&gt; TAINT{Taint not&lt;br/&gt;tolerated?}\n    TAINT --&gt; |Yes| FIX2[Add toleration]\n\n    EVENTS --&gt; RESOURCES{Insufficient&lt;br/&gt;resources?}\n    RESOURCES --&gt; |Yes| FIX3[Reduce requests or add nodes]\n\n    EVENTS --&gt; AFFINITY{Affinity not&lt;br/&gt;satisfied?}\n    AFFINITY --&gt; |Yes| FIX4[Fix affinity rules or labels]\n\n    FIX1 --&gt; VERIFY[Verify pod schedules]\n    FIX2 --&gt; VERIFY\n    FIX3 --&gt; VERIFY\n    FIX4 --&gt; VERIFY</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exam-time-savers","title":"Exam Time-Savers","text":"<p>1. Use <code>--dry-run=client -o yaml</code> aggressively: <pre><code>kubectl run mypod --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n# Edit pod.yaml to add scheduling constraints\nkubectl apply -f pod.yaml\n</code></pre></p> <p>2. Label nodes before creating pods: <pre><code># Label first\nkubectl label nodes node01 tier=frontend\n\n# Then create pod with nodeSelector\n</code></pre></p> <p>3. Use vim search/replace for bulk edits: <pre><code># In vim, replace all instances\n:%s/old-value/new-value/g\n</code></pre></p> <p>4. Keep common YAML snippets handy: Create <code>~/snippets/</code> directory with common patterns: - <code>nodeSelector.yaml</code> - <code>toleration.yaml</code> - <code>affinity.yaml</code> - <code>anti-affinity.yaml</code></p> <p>5. Practice <code>kubectl set</code> commands (where applicable): <pre><code># Set image\nkubectl set image deployment/web nginx=nginx:1.24\n\n# Unfortunately, no `kubectl set` for affinity/taints (use edit)\n</code></pre></p> <p>6. Use <code>kubectl edit</code> for quick fixes: <pre><code># Edit pod in-place (creates new pod for immutable fields)\nkubectl edit pod mypod\n\n# Edit deployment (updates in-place)\nkubectl edit deployment web\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-1-dedicated-node-pool","title":"Exercise 1: Dedicated Node Pool","text":"<p>Scenario: Create a dedicated node pool for database workloads.</p> <p>Tasks: 1. Taint node <code>node01</code> with <code>workload=database:NoSchedule</code> 2. Label node <code>node01</code> with <code>tier=database</code> and <code>disktype=ssd</code> 3. Create a pod named <code>postgres</code> with image <code>postgres:15</code> that:    - Tolerates the <code>workload=database</code> taint    - Uses nodeSelector for <code>tier=database</code>    - Requests 2Gi memory and 1 CPU</p> <p>Solution: <pre><code># 1. Taint and label node\nkubectl taint nodes node01 workload=database:NoSchedule\nkubectl label nodes node01 tier=database disktype=ssd\n\n# 2. Create pod\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\nspec:\n  nodeSelector:\n    tier: database\n  tolerations:\n  - key: \"workload\"\n    operator: \"Equal\"\n    value: \"database\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: postgres\n    image: postgres:15\n    resources:\n      requests:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n    env:\n    - name: POSTGRES_PASSWORD\n      value: example\nEOF\n\n# 3. Verify scheduling\nkubectl get pods -o wide\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-2-high-availability-web-deployment","title":"Exercise 2: High-Availability Web Deployment","text":"<p>Scenario: Deploy a web application with high availability across nodes and zones.</p> <p>Tasks: 1. Create a deployment <code>web-ha</code> with 6 replicas using <code>nginx:1.24</code> 2. Configure pod anti-affinity to prevent multiple replicas on the same node 3. Add preferred affinity to spread across zones (weight: 100)</p> <p>Solution: <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ha\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - web\n            topologyKey: kubernetes.io/hostname\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n              - key: topology.kubernetes.io/zone\n                operator: Exists\n      containers:\n      - name: nginx\n        image: nginx:1.24\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\nEOF\n\n# Verify spreading\nkubectl get pods -o wide -l app=web\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-3-cache-co-location","title":"Exercise 3: Cache Co-Location","text":"<p>Scenario: Deploy an application tier that must run on the same nodes as cache pods.</p> <p>Tasks: 1. Create a deployment <code>redis-cache</code> with 3 replicas using <code>redis:7</code> 2. Label the pods with <code>component=cache</code> 3. Create a deployment <code>web-app</code> with 6 replicas using <code>nginx:1.24</code> 4. Configure <code>web-app</code> with required pod affinity to <code>component=cache</code> pods</p> <p>Solution: <pre><code># 1. Deploy cache\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-cache\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: cache\n  template:\n    metadata:\n      labels:\n        component: cache\n    spec:\n      containers:\n      - name: redis\n        image: redis:7\nEOF\n\n# 2. Wait for cache pods to schedule\nkubectl wait --for=condition=ready pod -l component=cache --timeout=60s\n\n# 3. Deploy web-app with affinity\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - cache\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nginx\n        image: nginx:1.24\nEOF\n\n# 4. Verify co-location\nkubectl get pods -o wide -l app=web\nkubectl get pods -o wide -l component=cache\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-4-spot-instance-workloads","title":"Exercise 4: Spot Instance Workloads","text":"<p>Scenario: Configure a node pool for spot instances and deploy tolerant workloads.</p> <p>Tasks: 1. Taint node <code>node02</code> with <code>instance-type=spot:PreferNoSchedule</code> 2. Create a deployment <code>batch-processor</code> with 5 replicas using <code>busybox</code> that:    - Tolerates the spot instance taint    - Sleeps for 3600 seconds (simulating batch work)</p> <p>Solution: <pre><code># 1. Taint spot node\nkubectl taint nodes node02 instance-type=spot:PreferNoSchedule\n\n# 2. Deploy batch workload\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: batch-processor\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: batch\n  template:\n    metadata:\n      labels:\n        app: batch\n    spec:\n      tolerations:\n      - key: \"instance-type\"\n        operator: \"Equal\"\n        value: \"spot\"\n        effect: \"PreferNoSchedule\"\n      containers:\n      - name: processor\n        image: busybox\n        command: [\"sleep\", \"3600\"]\nEOF\n\n# 3. Observe scheduling\nkubectl get pods -o wide -l app=batch\n# Some pods may schedule on node02 (spot), others on regular nodes\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#exercise-5-node-maintenance-workflow","title":"Exercise 5: Node Maintenance Workflow","text":"<p>Scenario: Prepare a node for maintenance without disrupting workloads.</p> <p>Tasks: 1. Apply <code>NoSchedule</code> taint to <code>node03</code> with key <code>maintenance=pending</code> 2. Verify no new pods schedule on <code>node03</code> 3. After workloads migrate, change taint to <code>NoExecute</code> 4. Verify existing pods are evicted 5. After maintenance, remove taint</p> <p>Solution: <pre><code># 1. Apply NoSchedule taint\nkubectl taint nodes node03 maintenance=pending:NoSchedule\n\n# 2. Create test pod (should not schedule on node03)\nkubectl run test-pod --image=nginx\nkubectl get pod test-pod -o wide\n# Should show node != node03\n\n# 3. Check existing pods on node03\nkubectl get pods -A -o wide --field-selector spec.nodeName=node03\n\n# 4. Upgrade to NoExecute (evict existing pods)\nkubectl taint nodes node03 maintenance=pending:NoExecute --overwrite\n\n# 5. Verify pods evicted\nkubectl get pods -A -o wide --field-selector spec.nodeName=node03\n# Should show no pods (or only those with tolerations)\n\n# 6. Perform maintenance...\n# (simulated)\n\n# 7. Remove taint and return to service\nkubectl taint nodes node03 maintenance-\n\n# 8. Verify node accepts workloads\nkubectl get nodes\nkubectl describe node node03 | grep Taints\n# Should show: Taints: &lt;none&gt;\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/kubernetes-advanced-scheduling/#summary","title":"Summary","text":"<p>Advanced scheduling is a critical skill for both the CKA exam and production Kubernetes operations.</p> <p>Key Takeaways:</p> <ol> <li>Scheduler Basics:</li> <li>Filter phase removes unsuitable nodes</li> <li>Score phase ranks remaining nodes</li> <li> <p>Highest score wins (ties broken randomly)</p> </li> <li> <p>Node Selectors:</p> </li> <li>Simple label-based matching</li> <li>Fast to type (ideal for CKA exam speed)</li> <li> <p>Limited to exact equality matching</p> </li> <li> <p>Taints and Tolerations:</p> </li> <li>Taints repel pods from nodes</li> <li>Tolerations allow pods to tolerate taints</li> <li>Three effects: NoSchedule, PreferNoSchedule, NoExecute</li> <li>NoExecute evicts existing pods</li> <li> <p>Critical for dedicated nodes and maintenance</p> </li> <li> <p>Node Affinity:</p> </li> <li>Advanced label-based node selection</li> <li>Supports In, NotIn, Exists, DoesNotExist, Gt, Lt operators</li> <li>Required vs preferred (hard vs soft constraints)</li> <li> <p>Multiple selector terms for OR logic</p> </li> <li> <p>Pod Affinity/Anti-Affinity:</p> </li> <li>Schedule based on other pods' labels</li> <li>Affinity = co-location</li> <li>Anti-affinity = spreading</li> <li>Topology keys define scope (node, zone, region)</li> <li> <p>Performance costs in large clusters</p> </li> <li> <p>Advanced Concepts:</p> </li> <li>Priority classes enable preemption</li> <li>Topology spread constraints for even distribution</li> <li>Resource requests influence scheduling</li> <li> <p>Custom schedulers for specialized logic</p> </li> <li> <p>CKA Exam Skills:</p> </li> <li>Use <code>--dry-run=client -o yaml</code> for YAML generation</li> <li>Memorize affinity/toleration YAML structure</li> <li>Practice <code>kubectl describe</code> for troubleshooting</li> <li>Label nodes before creating pods</li> <li>Keep common snippets handy</li> </ol> <p>Production Best Practices: - Use node selectors for simple cases (fastest, clearest) - Use taints for dedicated node pools and maintenance - Combine required + preferred affinity for optimal placement - Always test anti-affinity with realistic replica counts - Monitor scheduling latency in large clusters - Document scheduling decisions for team knowledge sharing</p> <p>Next Steps: - Practice all five exercises until comfortable - Set up a multi-node cluster (kubeadm or kind) - Experiment with topology spread constraints - Review official Kubernetes scheduling docs - Time yourself on exam-style tasks</p> <p>Mastering these scheduling techniques will not only help you pass the CKA exam but also enable you to build resilient, optimized, and cost-effective Kubernetes deployments in production.</p>","tags":["kubernetes","k8s","cka-prep","scheduling","taints","tolerations","affinity","node-selector"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/","title":"Kubernetes Namespaces and Resource Quotas","text":"<p>Master namespace isolation and resource management for CKA exam success. Learn how to partition clusters, enforce resource limits, and prevent resource exhaustion in multi-tenant environments.</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#overview","title":"Overview","text":"<p>Namespaces provide virtual cluster partitioning within a physical Kubernetes cluster, enabling multi-tenancy, resource isolation, and access control. Resource quotas and limit ranges ensure fair resource distribution and prevent resource starvation.</p> <p>CKA Exam Domain: Workloads &amp; Scheduling (15%), Services &amp; Networking (20%)</p> <p>Key Insight: CKA exam scenarios frequently test namespace-aware operations and resource constraint troubleshooting. Understanding namespace scope and quota enforcement is critical for multi-tenant cluster management.</p> <p>What You'll Learn: - Namespace fundamentals and scope boundaries - Resource quota design and enforcement - Limit ranges for default resource constraints - Multi-tenant isolation strategies - Troubleshooting resource quota issues - CKA exam patterns and time-saving workflows</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-fundamentals","title":"Namespace Fundamentals","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#what-are-namespaces","title":"What Are Namespaces?","text":"<p>Definition: Namespaces are logical partitions within a Kubernetes cluster that provide scope for resource names and enable resource isolation.</p> <p>Core Concepts: - Resource names must be unique within a namespace, not across cluster - Most Kubernetes resources are namespace-scoped (pods, services, deployments) - Some resources are cluster-scoped (nodes, persistent volumes, namespaces) - Namespaces enable RBAC policies, network policies, and resource quotas</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-architecture","title":"Namespace Architecture","text":"<pre><code>graph TB\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"default Namespace\"\n            POD1[app-pod-1]\n            SVC1[app-service]\n            DEP1[app-deployment]\n        end\n\n        subgraph \"kube-system Namespace\"\n            POD2[coredns-xxx]\n            SVC2[kube-dns]\n            DEP2[coredns]\n        end\n\n        subgraph \"production Namespace\"\n            POD3[web-pod-1]\n            SVC3[web-service]\n            DEP3[web-deployment]\n            QUOTA1[ResourceQuota]\n            LIMIT1[LimitRange]\n        end\n\n        subgraph \"development Namespace\"\n            POD4[test-pod-1]\n            SVC4[test-service]\n            DEP4[test-deployment]\n            QUOTA2[ResourceQuota]\n            LIMIT2[LimitRange]\n        end\n\n        NODE1[Node: worker-1]\n        NODE2[Node: worker-2]\n        PV[(PersistentVolume)]\n    end\n\n    POD1 -.-&gt; NODE1\n    POD2 -.-&gt; NODE2\n    POD3 -.-&gt; NODE1\n    POD4 -.-&gt; NODE2\n\n    QUOTA1 -.-&gt;|enforces limits| POD3\n    QUOTA2 -.-&gt;|enforces limits| POD4\n\n    PV -.-&gt;|cluster-scoped| POD3\n    PV -.-&gt;|cluster-scoped| POD4\n\n    style default fill:#e1f5ff\n    style kube-system fill:#ffe5e5\n    style production fill:#e8f5e8\n    style development fill:#fff4e1\n    style NODE1 fill:#f5e1ff\n    style PV fill:#ffe5e5</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#default-namespaces","title":"Default Namespaces","text":"<p>Kubernetes creates several namespaces automatically:</p> Namespace Purpose Default Resources default Default namespace for resources without explicit namespace User workloads kube-system Kubernetes system components API server, scheduler, controller manager, DNS kube-public Publicly readable resources Cluster information kube-node-lease Node heartbeat objects for performance Node lease objects","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-operations","title":"Namespace Operations","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#creating-namespaces","title":"Creating Namespaces","text":"<p>Imperative Method (Fast for CKA): <pre><code># Create namespace\nkubectl create namespace production\nkubectl create ns development  # Short form\n\n# Create with labels\nkubectl create namespace staging --dry-run=client -o yaml | \\\n  kubectl label -f - --local environment=staging -o yaml | \\\n  kubectl apply -f -\n</code></pre></p> <p>Declarative Method: <pre><code># namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    environment: production\n    team: backend\n</code></pre></p> <pre><code>kubectl apply -f namespace.yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#listing-and-inspecting-namespaces","title":"Listing and Inspecting Namespaces","text":"<pre><code># List all namespaces\nkubectl get namespaces\nkubectl get ns\n\n# Show labels\nkubectl get ns --show-labels\n\n# Describe namespace (shows quota and limits)\nkubectl describe namespace production\n\n# Get namespace details in YAML\nkubectl get ns production -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#working-with-namespaced-resources","title":"Working with Namespaced Resources","text":"<pre><code># Create resources in specific namespace\nkubectl run nginx --image=nginx -n production\nkubectl create deployment webapp --image=nginx --replicas=3 -n development\n\n# Get resources from specific namespace\nkubectl get pods -n production\nkubectl get all -n development\n\n# Get resources from all namespaces\nkubectl get pods -A\nkubectl get pods --all-namespaces\n\n# Set default namespace for current context\nkubectl config set-context --current --namespace=production\n\n# Verify current namespace\nkubectl config view --minify | grep namespace\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#deleting-namespaces","title":"Deleting Namespaces","text":"<p>\u26a0\ufe0f WARNING: Deleting a namespace deletes ALL resources within it.</p> <pre><code># Delete namespace (deletes all resources)\nkubectl delete namespace development\n\n# Delete with confirmation\nkubectl delete ns development --force --grace-period=0\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resource-quotas","title":"Resource Quotas","text":"<p>Resource quotas provide constraints that limit aggregate resource consumption per namespace, preventing resource exhaustion and ensuring fair allocation.</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resourcequota-types","title":"ResourceQuota Types","text":"<p>Compute Resource Quotas: - <code>requests.cpu</code> - Sum of CPU requests - <code>requests.memory</code> - Sum of memory requests - <code>limits.cpu</code> - Sum of CPU limits - <code>limits.memory</code> - Sum of memory limits - <code>requests.storage</code> - Sum of storage requests</p> <p>Object Count Quotas: - <code>count/pods</code> - Maximum pod count - <code>count/services</code> - Maximum service count - <code>count/configmaps</code> - Maximum ConfigMap count - <code>count/secrets</code> - Maximum secret count - <code>count/persistentvolumeclaims</code> - Maximum PVC count - <code>count/deployments.apps</code> - Maximum deployment count - <code>count/replicasets.apps</code> - Maximum ReplicaSet count</p> <p>Storage Class Quotas: - <code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/requests.storage</code> - <code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/persistentvolumeclaims</code></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#creating-resource-quotas","title":"Creating Resource Quotas","text":"<p>Basic Compute Quota: <pre><code># quota-compute.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"10\"           # Max 10 CPU cores requested\n    requests.memory: 20Gi        # Max 20Gi memory requested\n    limits.cpu: \"20\"             # Max 20 CPU cores limit\n    limits.memory: 40Gi          # Max 40Gi memory limit\n</code></pre></p> <pre><code>kubectl apply -f quota-compute.yaml\n</code></pre> <p>Object Count Quota: <pre><code># quota-objects.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: object-quota\n  namespace: production\nspec:\n  hard:\n    count/pods: \"100\"\n    count/services: \"50\"\n    count/configmaps: \"20\"\n    count/secrets: \"30\"\n    count/persistentvolumeclaims: \"10\"\n    count/deployments.apps: \"30\"\n    count/replicasets.apps: \"50\"\n</code></pre></p> <p>Combined Quota (CKA Exam Pattern): <pre><code># quota-comprehensive.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: production-quota\n  namespace: production\nspec:\n  hard:\n    # Compute resources\n    requests.cpu: \"50\"\n    requests.memory: 100Gi\n    limits.cpu: \"100\"\n    limits.memory: 200Gi\n\n    # Storage\n    requests.storage: 500Gi\n    persistentvolumeclaims: \"20\"\n\n    # Object counts\n    count/pods: \"200\"\n    count/services: \"50\"\n    count/configmaps: \"50\"\n    count/secrets: \"50\"\n\n    # Workloads\n    count/deployments.apps: \"50\"\n    count/statefulsets.apps: \"10\"\n    count/jobs.batch: \"20\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#checking-quota-usage","title":"Checking Quota Usage","text":"<pre><code># View quota details\nkubectl get resourcequota -n production\nkubectl describe resourcequota production-quota -n production\n\n# Output shows:\n# Name:                   production-quota\n# Namespace:              production\n# Resource                Used    Hard\n# --------                ----    ----\n# requests.cpu            5       50\n# requests.memory         10Gi    100Gi\n# limits.cpu              10      100\n# limits.memory           20Gi    200Gi\n# count/pods              15      200\n\n# Get quota in YAML format\nkubectl get quota production-quota -n production -o yaml\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resourcequota-enforcement-flow","title":"ResourceQuota Enforcement Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant API as API Server\n    participant Admission as Admission Controller\n    participant Quota as ResourceQuota\n    participant Scheduler\n    participant Kubelet\n\n    User-&gt;&gt;API: kubectl apply -f pod.yaml\n    API-&gt;&gt;Admission: Validate request\n    Admission-&gt;&gt;Quota: Check quota limits\n\n    alt Quota Available\n        Quota--&gt;&gt;Admission: \u2705 Within limits\n        Admission--&gt;&gt;API: Approve\n        API-&gt;&gt;Scheduler: Schedule pod\n        Scheduler-&gt;&gt;Kubelet: Assign to node\n        Kubelet--&gt;&gt;User: Pod created\n        Quota-&gt;&gt;Quota: Update used resources\n    else Quota Exceeded\n        Quota--&gt;&gt;Admission: \u274c Quota exceeded\n        Admission--&gt;&gt;API: Reject\n        API--&gt;&gt;User: Error: exceeded quota\n        Note over User,Quota: Pod creation fails\n    end</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limit-ranges","title":"Limit Ranges","text":"<p>LimitRange objects set default resource requests/limits and enforce min/max constraints for containers and pods within a namespace.</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limitrange-components","title":"LimitRange Components","text":"<p>Container-Level Constraints: - <code>defaultRequest</code> - Default resource requests if not specified - <code>default</code> - Default resource limits if not specified - <code>min</code> - Minimum allowed resource values - <code>max</code> - Maximum allowed resource values - <code>maxLimitRequestRatio</code> - Max ratio of limit to request</p> <p>Pod-Level Constraints: - Total resource consumption across all containers</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#creating-limit-ranges","title":"Creating Limit Ranges","text":"<p>Container Defaults and Constraints: <pre><code># limitrange-container.yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: container-limits\n  namespace: production\nspec:\n  limits:\n  - type: Container\n    default:                      # Default limits\n      cpu: 500m\n      memory: 512Mi\n    defaultRequest:               # Default requests\n      cpu: 100m\n      memory: 128Mi\n    min:                          # Minimum allowed\n      cpu: 50m\n      memory: 64Mi\n    max:                          # Maximum allowed\n      cpu: 2\n      memory: 2Gi\n    maxLimitRequestRatio:         # Max limit/request ratio\n      cpu: 4\n      memory: 4\n</code></pre></p> <p>Pod-Level Constraints: <pre><code># limitrange-pod.yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: pod-limits\n  namespace: production\nspec:\n  limits:\n  - type: Pod\n    max:\n      cpu: \"4\"\n      memory: 8Gi\n    min:\n      cpu: 100m\n      memory: 128Mi\n</code></pre></p> <p>PersistentVolumeClaim Constraints: <pre><code># limitrange-pvc.yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: pvc-limits\n  namespace: production\nspec:\n  limits:\n  - type: PersistentVolumeClaim\n    max:\n      storage: 100Gi\n    min:\n      storage: 1Gi\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#checking-limit-ranges","title":"Checking Limit Ranges","text":"<pre><code># View limit ranges\nkubectl get limitrange -n production\nkubectl describe limitrange container-limits -n production\n\n# Output shows:\n# Name:       container-limits\n# Namespace:  production\n# Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio\n# ----        --------  ---   ---   ---------------  -------------  -----------------------\n# Container   cpu       50m   2     100m             500m           4\n# Container   memory    64Mi  2Gi   128Mi            512Mi          4\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limitrange-application-flow","title":"LimitRange Application Flow","text":"<pre><code>flowchart TD\n    Start([Pod Creation]) --&gt; HasLimits{Has resource&lt;br/&gt;limits/requests?}\n\n    HasLimits --&gt;|Yes| ValidateRange[Validate against LimitRange]\n    HasLimits --&gt;|No| ApplyDefaults[Apply LimitRange defaults]\n\n    ApplyDefaults --&gt; ValidateRange\n\n    ValidateRange --&gt; InRange{Within&lt;br/&gt;min/max?}\n\n    InRange --&gt;|Yes| RatioCheck{Limit/Request&lt;br/&gt;ratio OK?}\n    InRange --&gt;|No| Reject1[\u274c Reject: Out of range]\n\n    RatioCheck --&gt;|Yes| CheckQuota[Check ResourceQuota]\n    RatioCheck --&gt;|No| Reject2[\u274c Reject: Ratio exceeded]\n\n    CheckQuota --&gt; QuotaOK{Quota&lt;br/&gt;available?}\n\n    QuotaOK --&gt;|Yes| Accept[\u2705 Accept pod]\n    QuotaOK --&gt;|No| Reject3[\u274c Reject: Quota exceeded]\n\n    Accept --&gt; Schedule[Schedule pod]\n\n    style Accept fill:#99ff99\n    style Reject1 fill:#ff9999\n    style Reject2 fill:#ff9999\n    style Reject3 fill:#ff9999</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#multi-tenant-resource-management","title":"Multi-Tenant Resource Management","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#design-patterns","title":"Design Patterns","text":"<p>Pattern 1: Environment-Based Namespaces <pre><code># production namespace - Strict quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    environment: production\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: production-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"100\"\n    requests.memory: 200Gi\n    limits.cpu: \"200\"\n    limits.memory: 400Gi\n    count/pods: \"500\"\n</code></pre></p> <pre><code># development namespace - Relaxed quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: development\n  labels:\n    environment: development\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: development-quota\n  namespace: development\nspec:\n  hard:\n    requests.cpu: \"20\"\n    requests.memory: 40Gi\n    limits.cpu: \"40\"\n    limits.memory: 80Gi\n    count/pods: \"100\"\n</code></pre> <p>Pattern 2: Team-Based Namespaces <pre><code># backend-team namespace\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: backend-team\n  labels:\n    team: backend\n    cost-center: \"1234\"\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: backend-quota\n  namespace: backend-team\nspec:\n  hard:\n    requests.cpu: \"50\"\n    requests.memory: 100Gi\n    count/deployments.apps: \"30\"\n</code></pre></p> <p>Pattern 3: Application-Based Namespaces <pre><code># ecommerce-app namespace\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ecommerce-app\n  labels:\n    app: ecommerce\n    tier: frontend\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: ecommerce-quota\n  namespace: ecommerce-app\nspec:\n  hard:\n    requests.cpu: \"30\"\n    requests.memory: 60Gi\n    count/services: \"20\"\n    count/configmaps: \"30\"\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#quota-scope-selectors","title":"Quota Scope Selectors","text":"<p>Priority Class-Based Quotas: <pre><code># quota-high-priority.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: high-priority-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"50\"\n    requests.memory: 100Gi\n    count/pods: \"100\"\n  scopeSelector:\n    matchExpressions:\n    - operator: In\n      scopeName: PriorityClass\n      values:\n      - high-priority\n</code></pre></p> <p>BestEffort/NotBestEffort Quotas: <pre><code># quota-besteffort.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: besteffort-quota\n  namespace: production\nspec:\n  hard:\n    count/pods: \"10\"  # Limit BestEffort pods\n  scopes:\n  - BestEffort\n---\n# quota-guaranteed.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: guaranteed-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"80\"\n    requests.memory: 160Gi\n  scopes:\n  - NotBestEffort\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#troubleshooting-quota-issues","title":"Troubleshooting Quota Issues","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#common-error-scenarios","title":"Common Error Scenarios","text":"<p>Error 1: Insufficient Quota <pre><code># Error message:\n# Error from server (Forbidden): pods \"nginx\" is forbidden:\n# exceeded quota: production-quota, requested: requests.cpu=1,requests.memory=1Gi,\n# used: requests.cpu=49,requests.memory=99Gi,\n# limited: requests.cpu=50,requests.memory=100Gi\n</code></pre></p> <p>Diagnosis and Fix: <pre><code># 1. Check current quota usage\nkubectl describe quota production-quota -n production\n\n# 2. Identify resource hogs\nkubectl top pods -n production --sort-by=cpu\nkubectl top pods -n production --sort-by=memory\n\n# 3. Options:\n# Option A: Increase quota\nkubectl edit quota production-quota -n production\n\n# Option B: Scale down workloads\nkubectl scale deployment high-cpu-app --replicas=2 -n production\n\n# Option C: Delete unused resources\nkubectl get pods -n production --field-selector=status.phase=Succeeded -o name | \\\n  xargs kubectl delete -n production\n</code></pre></p> <p>Error 2: Missing Resource Requests <pre><code># Error message:\n# Error from server (Forbidden): pods \"nginx\" is forbidden:\n# failed quota: production-quota: must specify requests.cpu,requests.memory\n</code></pre></p> <p>Diagnosis and Fix: <pre><code># When ResourceQuota exists, ALL pods must specify requests/limits\n# Option A: Add requests/limits to pod\nkubectl run nginx --image=nginx -n production \\\n  --requests='cpu=100m,memory=128Mi' \\\n  --limits='cpu=200m,memory=256Mi'\n\n# Option B: Create LimitRange to provide defaults\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: default-limits\n  namespace: production\nspec:\n  limits:\n  - type: Container\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    default:\n      cpu: 200m\n      memory: 256Mi\nEOF\n</code></pre></p> <p>Error 3: Limit/Request Ratio Exceeded <pre><code># Error message:\n# Error from server (Forbidden): pods \"nginx\" is forbidden:\n# maximum cpu limit to request ratio per Container is 4, but provided ratio is 10.000000\n</code></pre></p> <p>Diagnosis and Fix: <pre><code># Check LimitRange constraints\nkubectl describe limitrange -n production\n\n# Fix: Adjust pod resources to meet ratio\n# If request=100m and maxLimitRequestRatio=4, then limit cannot exceed 400m\nkubectl run nginx --image=nginx -n production \\\n  --requests='cpu=100m' \\\n  --limits='cpu=400m'  # 4:1 ratio\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#debugging-workflow","title":"Debugging Workflow","text":"<pre><code>flowchart TD\n    Error([Pod Creation Failed]) --&gt; CheckMsg{Error Message}\n\n    CheckMsg --&gt;|exceeded quota| QuotaDiag[Check Quota Status]\n    CheckMsg --&gt;|must specify| MissingRes[Add Resources or LimitRange]\n    CheckMsg --&gt;|ratio exceeded| RatioDiag[Check LimitRange Ratio]\n    CheckMsg --&gt;|insufficient| InsufficientRes[Increase Quota or Scale Down]\n\n    QuotaDiag --&gt; DescQuota[kubectl describe quota]\n    DescQuota --&gt; AnalyzeUsage{Quota Full?}\n\n    AnalyzeUsage --&gt;|Yes| Options[Choose Fix]\n    AnalyzeUsage --&gt;|No| CheckLR[Check LimitRange]\n\n    Options --&gt; OptA[Increase Quota]\n    Options --&gt; OptB[Delete Resources]\n    Options --&gt; OptC[Scale Down]\n\n    MissingRes --&gt; AddReq[Add requests/limits]\n    MissingRes --&gt; CreateLR[Create LimitRange]\n\n    RatioDiag --&gt; AdjustRatio[Adjust limit:request ratio]\n\n    CheckLR --&gt; DescLR[kubectl describe limitrange]\n\n    style Error fill:#ff9999\n    style OptA fill:#e1f5ff\n    style OptB fill:#fff4e1\n    style OptC fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#cka-exam-scenarios","title":"CKA Exam Scenarios","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-1-create-namespace-with-quota","title":"Scenario 1: Create Namespace with Quota","text":"<p>Task: Create namespace 'webapp' with quota: max 10 pods, 5 CPU cores, 10Gi memory.</p> <p>Solution: <pre><code># Create namespace\nkubectl create namespace webapp\n\n# Create quota (imperative)\nkubectl create quota webapp-quota \\\n  --hard=count/pods=10,requests.cpu=5,requests.memory=10Gi \\\n  -n webapp\n\n# Verify\nkubectl describe quota webapp-quota -n webapp\n</code></pre></p> <p>Alternative (Declarative): <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: webapp\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: webapp-quota\n  namespace: webapp\nspec:\n  hard:\n    count/pods: \"10\"\n    requests.cpu: \"5\"\n    requests.memory: 10Gi\nEOF\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-2-create-pod-in-quota-constrained-namespace","title":"Scenario 2: Create Pod in Quota-Constrained Namespace","text":"<p>Task: Create nginx pod in namespace with quota requiring resource specifications.</p> <p>Solution: <pre><code># Generate pod YAML with resources\nkubectl run nginx --image=nginx -n webapp \\\n  --requests='cpu=100m,memory=128Mi' \\\n  --limits='cpu=200m,memory=256Mi' \\\n  --dry-run=client -o yaml &gt; pod.yaml\n\n# Apply\nkubectl apply -f pod.yaml\n\n# Verify quota usage\nkubectl describe quota -n webapp\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-3-troubleshoot-quota-exceeded-error","title":"Scenario 3: Troubleshoot Quota Exceeded Error","text":"<p>Task: Deployment fails to scale due to quota. Identify issue and fix.</p> <p>Solution: <pre><code># 1. Check error\nkubectl get events -n webapp --sort-by='.lastTimestamp'\n\n# 2. Check quota usage\nkubectl describe quota -n webapp\n# Shows: requests.cpu used=4.8/5, requests.memory used=9.5Gi/10Gi\n\n# 3. Identify resource usage\nkubectl top pods -n webapp --sort-by=cpu\nkubectl top pods -n webapp --sort-by=memory\n\n# 4. Fix - Option A: Delete completed pods\nkubectl delete pod --field-selector=status.phase=Succeeded -n webapp\n\n# 4. Fix - Option B: Increase quota\nkubectl edit quota webapp-quota -n webapp\n# Increase limits\n\n# 5. Verify\nkubectl describe quota -n webapp\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-4-set-default-resource-constraints","title":"Scenario 4: Set Default Resource Constraints","text":"<p>Task: Create LimitRange in 'development' namespace with defaults: cpu=100m/200m, memory=128Mi/256Mi.</p> <p>Solution: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: dev-limits\n  namespace: development\nspec:\n  limits:\n  - type: Container\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    default:\n      cpu: 200m\n      memory: 256Mi\n    min:\n      cpu: 50m\n      memory: 64Mi\n    max:\n      cpu: 1\n      memory: 1Gi\nEOF\n\n# Verify\nkubectl describe limitrange dev-limits -n development\n\n# Test - create pod without resources (should get defaults)\nkubectl run test --image=nginx -n development\nkubectl get pod test -n development -o yaml | grep -A 10 resources:\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#scenario-5-multi-namespace-resource-isolation","title":"Scenario 5: Multi-Namespace Resource Isolation","text":"<p>Task: Create 3 namespaces (prod, staging, dev) with appropriate quotas.</p> <p>Solution: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\n# Production - Strict quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: prod\n  labels:\n    environment: production\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: prod-quota\n  namespace: prod\nspec:\n  hard:\n    requests.cpu: \"100\"\n    requests.memory: 200Gi\n    limits.cpu: \"200\"\n    limits.memory: 400Gi\n    count/pods: \"500\"\n    count/services: \"100\"\n---\n# Staging - Moderate quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: staging\n  labels:\n    environment: staging\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: staging-quota\n  namespace: staging\nspec:\n  hard:\n    requests.cpu: \"50\"\n    requests.memory: 100Gi\n    limits.cpu: \"100\"\n    limits.memory: 200Gi\n    count/pods: \"250\"\n---\n# Development - Relaxed quotas\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n  labels:\n    environment: development\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: dev-quota\n  namespace: dev\nspec:\n  hard:\n    requests.cpu: \"20\"\n    requests.memory: 40Gi\n    limits.cpu: \"40\"\n    limits.memory: 80Gi\n    count/pods: \"100\"\nEOF\n\n# Verify all quotas\nkubectl get quota -A\nkubectl describe quota -n prod\nkubectl describe quota -n staging\nkubectl describe quota -n dev\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#best-practices","title":"Best Practices","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-design","title":"Namespace Design","text":"<p>\u2705 DO: - Use namespaces to separate environments (prod, staging, dev) - Use namespaces for team isolation (backend-team, frontend-team) - Use namespaces for application isolation (app-a, app-b) - Apply labels to namespaces for organization - Document namespace ownership and purpose</p> <p>\u274c DON'T: - Use namespaces for version separation (use labels instead) - Create excessive granularity (1 namespace per microservice) - Rely solely on namespaces for security (combine with RBAC, NetworkPolicy) - Use special characters in namespace names</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resource-quota-design","title":"Resource Quota Design","text":"<p>\u2705 DO: - Set both requests and limits quotas - Include object count quotas to prevent resource proliferation - Monitor quota usage regularly - Set quotas based on measured usage patterns - Use quota scope selectors for fine-grained control</p> <p>\u274c DON'T: - Set quotas too tight (causes frequent failures) - Set quotas too loose (defeats the purpose) - Forget to account for system pods in kube-system - Ignore quota usage metrics</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limitrange-design","title":"LimitRange Design","text":"<p>\u2705 DO: - Always create LimitRange when using ResourceQuota - Set reasonable defaults for requests and limits - Enforce min/max constraints to prevent extremes - Set maxLimitRequestRatio to prevent wasteful overcommit - Document LimitRange rationale</p> <p>\u274c DON'T: - Set defaults too high (wastes resources) - Set defaults too low (causes performance issues) - Set maxLimitRequestRatio too strict (prevents legitimate use cases) - Forget to test LimitRange impact on existing workloads</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#monitoring-and-metrics","title":"Monitoring and Metrics","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#quota-usage-monitoring","title":"Quota Usage Monitoring","text":"<pre><code># Check all quotas in cluster\nkubectl get quota -A\n\n# Monitor specific namespace quota\nkubectl describe quota -n production\n\n# Watch quota usage\nkubectl get quota -n production -w\n\n# Get quota usage metrics\nkubectl get quota -n production -o json | \\\n  jq '.items[].status.used'\n\n# Custom columns for quota overview\nkubectl get quota -A -o custom-columns=\\\nNAMESPACE:.metadata.namespace,\\\nNAME:.metadata.name,\\\nCPU_USED:.status.used.\\'requests\\.cpu\\',\\\nCPU_HARD:.status.hard.\\'requests\\.cpu\\',\\\nMEM_USED:.status.used.\\'requests\\.memory\\',\\\nMEM_HARD:.status.hard.\\'requests\\.memory\\'\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#quota-utilization-metrics","title":"Quota Utilization Metrics","text":"<pre><code>graph LR\n    subgraph \"Namespace: production\"\n        Used[Used Resources&lt;br/&gt;CPU: 45/50&lt;br/&gt;Memory: 90Gi/100Gi&lt;br/&gt;Pods: 180/200]\n        Util[Utilization&lt;br/&gt;CPU: 90%&lt;br/&gt;Memory: 90%&lt;br/&gt;Pods: 90%]\n        Alert[\u26a0\ufe0f Alert Threshold&lt;br/&gt;\u2265 85%]\n    end\n\n    Used --&gt; Util\n    Util --&gt; Alert\n\n    Alert -.-&gt;|Trigger| Action1[Increase Quota]\n    Alert -.-&gt;|Trigger| Action2[Scale Down Workloads]\n    Alert -.-&gt;|Trigger| Action3[Optimize Resources]\n\n    style Used fill:#e1f5ff\n    style Util fill:#fff4e1\n    style Alert fill:#ff9999\n    style Action1 fill:#99ff99</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#practice-exercises","title":"Practice Exercises","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#exercise-1-basic-namespace-and-quota-10-minutes","title":"Exercise 1: Basic Namespace and Quota (10 minutes)","text":"<p>Tasks: 1. Create namespace 'test-ns' 2. Create ResourceQuota limiting to 5 pods, 2 CPU, 4Gi memory 3. Create 3 nginx pods with appropriate resources 4. Verify quota usage 5. Try creating 3 more pods (should fail) 6. Delete namespace</p> <p>Solution: <pre><code># 1. Create namespace\nkubectl create ns test-ns\n\n# 2. Create quota\nkubectl create quota test-quota \\\n  --hard=count/pods=5,requests.cpu=2,requests.memory=4Gi \\\n  -n test-ns\n\n# 3. Create pods\nfor i in {1..3}; do\n  kubectl run nginx-$i --image=nginx \\\n    --requests='cpu=200m,memory=512Mi' \\\n    --limits='cpu=400m,memory=1Gi' \\\n    -n test-ns\ndone\n\n# 4. Check quota\nkubectl describe quota test-quota -n test-ns\n\n# 5. Try more pods (will fail after 2 more)\nkubectl run nginx-4 --image=nginx \\\n  --requests='cpu=200m,memory=512Mi' -n test-ns\n\n# 6. Cleanup\nkubectl delete ns test-ns\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#exercise-2-limitrange-configuration-15-minutes","title":"Exercise 2: LimitRange Configuration (15 minutes)","text":"<p>Tasks: 1. Create namespace 'limit-test' 2. Create LimitRange with defaults and constraints 3. Create pod without resources (should get defaults) 4. Create pod with resources exceeding max (should fail) 5. Verify defaults applied</p> <p>Solution: <pre><code># 1. Create namespace\nkubectl create ns limit-test\n\n# 2. Create LimitRange\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limit-test-range\n  namespace: limit-test\nspec:\n  limits:\n  - type: Container\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    default:\n      cpu: 200m\n      memory: 256Mi\n    min:\n      cpu: 50m\n      memory: 64Mi\n    max:\n      cpu: 1\n      memory: 1Gi\n    maxLimitRequestRatio:\n      cpu: 4\n      memory: 4\nEOF\n\n# 3. Create pod without resources\nkubectl run test1 --image=nginx -n limit-test\n\n# Check applied defaults\nkubectl get pod test1 -n limit-test -o yaml | grep -A 10 resources:\n\n# 4. Try exceeding max (should fail)\nkubectl run test2 --image=nginx \\\n  --requests='cpu=2' -n limit-test\n# Error: maximum cpu usage per Container is 1, but limit is 2\n\n# 5. Cleanup\nkubectl delete ns limit-test\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#exercise-3-quota-troubleshooting-20-minutes","title":"Exercise 3: Quota Troubleshooting (20 minutes)","text":"<p>Tasks: 1. Create namespace with tight quota (2 pods max) 2. Create deployment with 5 replicas 3. Observe only 2 pods created 4. Diagnose and fix by increasing quota 5. Verify all 5 pods running</p> <p>Solution: <pre><code># 1. Create namespace and quota\nkubectl create ns tight-quota\nkubectl create quota tight-quota \\\n  --hard=count/pods=2,requests.cpu=1,requests.memory=1Gi \\\n  -n tight-quota\n\n# 2. Create deployment\nkubectl create deployment webapp --image=nginx --replicas=5 -n tight-quota\n\n# 3. Check pods (only 2 created)\nkubectl get pods -n tight-quota\nkubectl get rs -n tight-quota\n\n# View ReplicaSet events\nkubectl describe rs -n tight-quota\n# Shows: exceeded quota\n\n# 4. Diagnose\nkubectl describe quota tight-quota -n tight-quota\n# Shows: count/pods: 2/2\n\n# Fix - increase quota\nkubectl patch quota tight-quota -n tight-quota \\\n  --type='json' \\\n  -p='[{\"op\": \"replace\", \"path\": \"/spec/hard/count~1pods\", \"value\":\"10\"}]'\n\n# Or edit directly\nkubectl edit quota tight-quota -n tight-quota\n\n# 5. Verify\nkubectl get pods -n tight-quota\n# Should now show 5/5 pods running\n\n# Cleanup\nkubectl delete ns tight-quota\n</code></pre></p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#quick-reference","title":"Quick Reference","text":"","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#namespace-commands","title":"Namespace Commands","text":"<pre><code># Create\nkubectl create namespace &lt;name&gt;\nkubectl create ns &lt;name&gt;\n\n# List\nkubectl get namespaces\nkubectl get ns\n\n# Describe\nkubectl describe namespace &lt;name&gt;\n\n# Delete\nkubectl delete namespace &lt;name&gt;\n\n# Set default for context\nkubectl config set-context --current --namespace=&lt;name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#resourcequota-commands","title":"ResourceQuota Commands","text":"<pre><code># Create quota\nkubectl create quota &lt;name&gt; --hard=&lt;key&gt;=&lt;value&gt; -n &lt;namespace&gt;\n\n# List quotas\nkubectl get quota -n &lt;namespace&gt;\nkubectl get quota -A\n\n# Describe quota\nkubectl describe quota &lt;name&gt; -n &lt;namespace&gt;\n\n# Edit quota\nkubectl edit quota &lt;name&gt; -n &lt;namespace&gt;\n\n# Delete quota\nkubectl delete quota &lt;name&gt; -n &lt;namespace&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#limitrange-commands","title":"LimitRange Commands","text":"<pre><code># Create from file\nkubectl apply -f limitrange.yaml\n\n# List limit ranges\nkubectl get limitrange -n &lt;namespace&gt;\nkubectl get limits -n &lt;namespace&gt;\n\n# Describe\nkubectl describe limitrange &lt;name&gt; -n &lt;namespace&gt;\n\n# Delete\nkubectl delete limitrange &lt;name&gt; -n &lt;namespace&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#common-quota-keys","title":"Common Quota Keys","text":"<pre><code># Compute resources\nrequests.cpu\nrequests.memory\nlimits.cpu\nlimits.memory\n\n# Storage\nrequests.storage\npersistentvolumeclaims\n\n# Object counts\ncount/pods\ncount/services\ncount/configmaps\ncount/secrets\ncount/deployments.apps\ncount/replicasets.apps\ncount/statefulsets.apps\ncount/jobs.batch\ncount/cronjobs.batch\n</code></pre>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Namespaces provide logical isolation - Not physical security boundaries</p> <p>\u2705 ResourceQuotas enforce aggregate limits - Prevent resource exhaustion per namespace</p> <p>\u2705 LimitRange provides defaults and constraints - Essential when using quotas</p> <p>\u2705 Always specify resource requests/limits - When quotas are active, required for admission</p> <p>\u2705 Monitor quota usage proactively - Avoid runtime failures from quota exhaustion</p> <p>\u2705 Combine quota scopes for flexibility - PriorityClass, BestEffort filters for fine-grained control</p> <p>\u2705 Delete namespace deletes all resources - Exercise caution with <code>kubectl delete ns</code></p> <p>\u2705 Exam context switching is critical - Always verify namespace before operations</p> <p>\u2705 Troubleshoot with describe - <code>kubectl describe quota/limitrange</code> shows usage and constraints</p> <p>\u2705 Plan quota capacity - Base on measured usage plus growth buffer</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/11/namespaces-resource-quotas/#next-steps","title":"Next Steps","text":"<p>After mastering namespaces and resource quotas, continue with:</p> <p>Post 6: Services and Networking - Service discovery and cluster networking</p> <p>Related Posts: - kubectl Essentials - Command-line mastery for Kubernetes - Kubernetes Architecture Fundamentals - Understanding cluster components - Kubernetes CKA Mastery - Complete Learning Path - Full exam preparation series</p> <p>External Resources: - Namespaces Official Documentation - Resource Quotas - Limit Ranges - Configure Default CPU/Memory Requests and Limits - CKA Exam Curriculum</p>","tags":["kubernetes","k8s","cka-prep","namespaces","resource-management","quotas"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/","title":"High-Performance pNFS v4.2 Distributed Storage Architecture","text":"<p>A deep dive into building a clustered, high-availability parallel NFS storage system with load-balanced metadata servers, NVMe-backed storage nodes, and low-latency interconnects.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#architecture-overview","title":"Architecture Overview","text":"<p>This architecture implements a production-grade parallel NFS (pNFS) v4.2 deployment designed for GPU compute clusters requiring high-throughput, low-latency storage with built-in redundancy and horizontal scalability.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#key-design-goals","title":"Key Design Goals","text":"<ul> <li>Parallel I/O Performance: Direct client-to-storage data paths bypassing metadata bottlenecks</li> <li>Metadata High Availability: Clustered MDS with automatic failover</li> <li>Horizontal Scalability: Add storage nodes without downtime</li> <li>Low Latency: InfiniBand/RoCE interconnects for sub-microsecond latencies</li> <li>Fault Tolerance: No single points of failure in the architecture</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#system-topology","title":"System Topology","text":"<pre><code>sequenceDiagram\n    participant Client as Client&lt;br/&gt;(pNFS v4.2)\n    participant MDS as MDS Cluster&lt;br/&gt;(Active-Active via VIP)\n    participant S1 as Storage Node 1&lt;br/&gt;(NVMe)\n    participant S2 as Storage Node 2&lt;br/&gt;(NVMe)\n    participant S3 as Storage Node 3&lt;br/&gt;(NVMe)\n\n    Note over Client,S3: \u2501\u2501\u2501\u2501\u2501\u2501\u2501 PHASE 1: METADATA PATH \u2501\u2501\u2501\u2501\u2501\u2501\u2501\n    Note over MDS: Virtual IP load balances to any MDS&lt;br/&gt;All MDS nodes share distributed state\n\n    Client-&gt;&gt;+MDS: LAYOUTGET(file_handle)\n    Note right of MDS: MDS queries distributed&lt;br/&gt;backend for file layout\n    MDS--&gt;&gt;-Client: LAYOUT(stripe_pattern, DS_list)\n    Note left of Client: \u2713 Client caches layout&lt;br/&gt;Stripe unit: 1MB&lt;br/&gt;Stripe count: 3 nodes\n\n    Note over Client,S3: \u2501\u2501\u2501\u2501\u2501\u2501\u2501 PHASE 2: DATA PATH (MDS BYPASSED) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n    par Parallel Direct I/O over InfiniBand/RoCE\n        Client-&gt;&gt;+S1: WRITE Stripe 0\n        S1-&gt;&gt;S1: NVMe I/O\n        S1--&gt;&gt;-Client: ACK\n    and\n        Client-&gt;&gt;+S2: WRITE Stripe 1\n        S2-&gt;&gt;S2: NVMe I/O\n        S2--&gt;&gt;-Client: ACK\n    and\n        Client-&gt;&gt;+S3: WRITE Stripe 2\n        S3-&gt;&gt;S3: NVMe I/O\n        S3--&gt;&gt;-Client: ACK\n    end\n\n    Note over Client,S3: \u26a1 Aggregate: 3 \u00d7 7 GB/s = ~20 GB/s effective throughput</code></pre> <p>Key Architecture Points:</p> Layer Component Function Control Plane MDS Cluster (Active-Active) Virtual IP \u2192 Load balances metadata requestsDistributed backend \u2192 Shared state (GFS2/OCFS2)Co-located with storage nodes Data Plane Storage Nodes Direct parallel I/O bypasses MDS entirelyEach node: MDS service + Data service + NVMeHigh-speed fabric: InfiniBand or 100GbE RoCE Client pNFS v4.2 One-time layout fetch \u2192 caches stripe patternDirect parallel writes to multiple storage nodesNo metadata bottleneck on data path <p>Architecture Advantage</p> <p>Separation of Control and Data Planes: Client contacts MDS once to get file layout, then performs all subsequent I/O directly to storage nodes over high-speed network. MDS handles only metadata operations (LAYOUTGET, OPEN, CLOSE), while bulk data transfer happens in parallel across multiple storage nodes, eliminating the metadata server bottleneck.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#component-breakdown","title":"Component Breakdown","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#1-client-layer-pnfs-v42-clients","title":"1. Client Layer (pNFS v4.2 Clients)","text":"<p>Role: GPU compute nodes running pNFS-aware clients</p> <p>Characteristics: - Protocol: NFSv4.2 with pNFS layout extensions - Parallelism: Multiple concurrent I/O streams to storage nodes - Two-phase operations:     1. Metadata phase: Request file layout from MDS via VIP     2. Data phase: Direct parallel I/O to multiple storage nodes</p> <p>Advantages: - Metadata and data paths are separated - MDS only handles control plane; data plane scales independently - Clients cache layouts, reducing metadata round-trips</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#2-metadata-virtual-ip-vip-load-balancer","title":"2. Metadata Virtual IP (VIP) / Load Balancer","text":"<p>Role: Distribute metadata requests across clustered MDS instances</p> <p>Implementation Options:</p> Technology Use Case Pros Cons Keepalived + VRRP Simple HA Easy setup, fast failover Layer 3 only, single active HAProxy Advanced LB Health checks, stats, multi-algo Additional component Pacemaker + Corosync Enterprise HA Full cluster manager Complex configuration <p>Configuration Considerations: - Failover time: Target &lt;2 seconds for MDS failover - Session stickiness: Not required (stateless metadata operations) - Health checks: Monitor MDS service health on each node</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#3-mds-cluster-metadata-servers","title":"3. MDS Cluster (Metadata Servers)","text":"<p>Role: Manage namespace, permissions, file layouts, and client coordination</p> <p>Clustering Strategy:</p> <p>Active-Active Clustering</p> <p>All MDS instances are active simultaneously, sharing load via the VIP. This differs from traditional active-passive designs and requires:</p> <ul> <li>Shared backend: Distributed consensus or shared storage for metadata</li> <li>State synchronization: Real-time metadata replication</li> <li>Lock coordination: Distributed locking for file operations</li> </ul> <p>Backend Options:</p> <pre><code>Option 1: Shared Block Device (DRBD + GFS2/OCFS2)\n  pros:\n    - Battle-tested clustering\n    - POSIX semantics\n  cons:\n    - Block-level sync overhead\n    - Limited to 2-3 nodes typically\n\nOption 2: Distributed Database (etcd/Consul)\n  pros:\n    - Raft consensus built-in\n    - Horizontal scaling\n    - Cloud-native\n  cons:\n    - Additional latency\n    - More complex integration\n\nOption 3: Lustre MGS/MDT (if using Lustre as pNFS backend)\n  pros:\n    - Native high availability\n    - Proven at exascale\n  cons:\n    - Lustre-specific\n    - Complex deployment\n</code></pre> <p>Heartbeat Mechanism: - Interval: 500ms - 1s between nodes - Quorum: Majority voting prevents split-brain - Fencing: STONITH (Shoot The Other Node In The Head) for failed nodes</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#4-high-speed-network-fabric","title":"4. High-Speed Network Fabric","text":"<p>Role: Low-latency, high-bandwidth interconnect for storage traffic</p> <p>Technology Comparison:</p> Technology Bandwidth Latency Use Case InfiniBand HDR 200 Gbps &lt;1 \u03bcs HPC, AI training clusters 100GbE RoCE v2 100 Gbps &lt;5 \u03bcs Cost-effective alternative Omni-Path 100 Gbps &lt;1 \u03bcs Intel ecosystem <p>Network Design: <pre><code>- Dedicated storage VLAN/subnet\n- Jumbo frames (MTU 9000) for throughput\n- RDMA for zero-copy transfers\n- Lossless Ethernet (PFC) if using RoCE\n- Multiple paths for redundancy (LACP/MLAG)\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#5-storage-nodes","title":"5. Storage Nodes","text":"<p>Role: Serve actual file data via pNFS Data Service (DS)</p> <p>Node Architecture:</p> <pre><code>Each storage node runs:\n\u251c\u2500\u2500 MDS Service (part of cluster)\n\u251c\u2500\u2500 Data Service (DS) (serves pNFS I/O)\n\u2514\u2500\u2500 Physical Storage (NVMe SSDs)\n</code></pre> <p>NVMe Configuration: - Device: PCIe Gen4 NVMe SSDs (7000+ MB/s per device) - RAID: No RAID (rely on pNFS striping across nodes) - File System: XFS or ZFS for local storage - Tuning:     - <code>nvme.io_timeout=4294967295</code> (disable timeout)     - <code>elevator=none</code> (bypass I/O scheduler for NVMe)     - <code>vm.dirty_ratio=5</code> (aggressive writeback)</p> <p>Capacity Planning: <pre><code>Per-node capacity:\n  - 4x 4TB NVMe = 16TB raw per node\n  - 10 nodes = 160TB aggregate raw\n  - No RAID overhead (redundancy via replication)\n  - Effective capacity: ~140TB (accounting for metadata)\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#data-flow-read-operation","title":"Data Flow: Read Operation","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-1-layout-request-metadata-path","title":"Phase 1: Layout Request (Metadata Path)","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant VIP\n    participant MDS1\n    participant Backend\n\n    Client-&gt;&gt;VIP: LAYOUTGET (file handle)\n    VIP-&gt;&gt;MDS1: Forward request\n    MDS1-&gt;&gt;Backend: Query file layout\n    Backend--&gt;&gt;MDS1: Layout map\n    MDS1--&gt;&gt;Client: LAYOUT (stripe pattern, DS list)\n    Note over Client: Client caches layout</code></pre> <p>Layout Information Returned: <pre><code>{\n  \"layout_type\": \"LAYOUT4_NFSV4_1_FILES\",\n  \"stripe_unit\": 1048576,\n  \"stripe_count\": 4,\n  \"data_servers\": [\n    \"10.10.1.11:2049\",  // Storage Node 1\n    \"10.10.1.12:2049\",  // Storage Node 2\n    \"10.10.1.13:2049\",  // Storage Node 3\n    \"10.10.1.14:2049\"   // Storage Node 4\n  ]\n}\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-2-parallel-data-io-data-path","title":"Phase 2: Parallel Data I/O (Data Path)","text":"<pre><code>graph LR\n    Client --&gt;|Stripe 0| DS1[Storage Node 1]\n    Client --&gt;|Stripe 1| DS2[Storage Node 2]\n    Client --&gt;|Stripe 3| DS3[Storage Node 3]\n    Client --&gt;|Stripe 4| DS4[Storage Node 4]\n\n    DS1 --&gt; NVMe1[NVMe SSD]\n    DS2 --&gt; NVMe2[NVMe SSD]\n    DS3 --&gt; NVMe3[NVMe SSD]\n    DS4 --&gt; NVMe4[NVMe SSD]</code></pre> <p>Throughput Calculation: <pre><code>Single NVMe: 7 GB/s read\n4-way stripe: 7 GB/s \u00d7 4 = 28 GB/s aggregate\nOverhead (20%): ~22 GB/s effective client throughput\n</code></pre></p> <p>Key Advantage</p> <p>The MDS is completely bypassed during data I/O. Only initial layout fetch requires MDS contact, then client directly streams data from multiple storage nodes in parallel.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#write-operation-with-coherency","title":"Write Operation with Coherency","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#challenges","title":"Challenges","text":"<ul> <li>Cache coherency: Multiple clients may access same file</li> <li>Consistency: Must maintain POSIX semantics</li> <li>Layout revocation: MDS may recall layouts during conflicts</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#write-flow","title":"Write Flow","text":"<pre><code>sequenceDiagram\n    participant Client1\n    participant Client2\n    participant MDS\n    participant DS1\n\n    Client1-&gt;&gt;MDS: OPEN (file, WRITE)\n    MDS--&gt;&gt;Client1: LAYOUT (read-write)\n    Client1-&gt;&gt;DS1: WRITE data\n\n    Client2-&gt;&gt;MDS: OPEN (same file, WRITE)\n    MDS-&gt;&gt;Client1: CB_LAYOUTRECALL\n    Client1-&gt;&gt;DS1: COMMIT writes\n    Client1-&gt;&gt;MDS: LAYOUTRETURN\n    MDS--&gt;&gt;Client2: LAYOUT (read-write)</code></pre> <p>Layout Recall Scenarios: 1. Write-write conflict: Second writer needs exclusive layout 2. Read-write conflict: Writer needs to invalidate reader caches 3. Layout change: File being migrated or restriped</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#high-availability-scenarios","title":"High Availability Scenarios","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#scenario-1-mds-node-failure","title":"Scenario 1: MDS Node Failure","text":"<pre><code>Before:\n  VIP \u2192 MDS1 (active)\n      \u2192 MDS2 (active)\n      \u2192 MDS3 (active)  \u2190 fails\n\nAfter (within 2 seconds):\n  VIP \u2192 MDS1 (active)  \u2190 absorbs load\n      \u2192 MDS2 (active)  \u2190 absorbs load\n\n  MDS3: Fenced by cluster, removed from VIP pool\n  Client layouts: Still valid, no client disruption\n</code></pre> <p>Recovery Actions: - Quorum maintained (2/3 nodes) - Clients continue data I/O unaffected - New metadata requests distributed to healthy MDS nodes - Failed MDS auto-rejoins after recovery</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#scenario-2-storage-node-failure","title":"Scenario 2: Storage Node Failure","text":"<pre><code>pNFS File with 4-way striping across nodes 1-4:\n  Node 3 fails \u2192 Stripes 2 (stored on node 3) unavailable\n\nClient behavior:\n  1. Client detects I/O error on stripe 2\n  2. Client returns partial read/write to application\n  3. Application must handle EIO (or use replication)\n\nRecovery:\n  - Option A: File replicated (pNFS server-side replication)\n             \u2192 Automatic failover to replica stripe\n  - Option B: No replication \u2192 Data loss for affected stripes\n</code></pre> <p>Data Durability</p> <p>pNFS itself does NOT provide redundancy. You must implement:</p> <ul> <li>Server-side replication (e.g., Lustre OST pools)</li> <li>Client-side RAID (mdadm over pNFS)</li> <li>Application-level erasure coding</li> <li>Regular snapshots/backups</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#scenario-3-network-partition-split-brain-prevention","title":"Scenario 3: Network Partition (Split-Brain Prevention)","text":"<pre><code>Network partition splits cluster:\n  Partition A: MDS1, MDS2 (2 nodes)\n  Partition B: MDS3 (1 node)\n\nQuorum voting:\n  Partition A: 2/3 nodes = HAS QUORUM \u2192 continues operation\n  Partition B: 1/3 nodes = NO QUORUM \u2192 enters read-only mode\n\nPrevention:\n  - Fencing agent (IPMI, PDU) forcibly powers off minority partition\n  - Prevents conflicting writes to shared backend\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#performance-tuning","title":"Performance Tuning","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#client-side-tunables","title":"Client-Side Tunables","text":"<pre><code># /etc/nfsmount.conf or mount options\nmount -t nfs4 -o \\\n  vers=4.2,\\                      # Enable pNFS\n  pnfs,\\                          # Use parallel NFS layouts\n  rsize=1048576,\\                 # 1MB read size\n  wsize=1048576,\\                 # 1MB write size\n  timeo=600,\\                     # 60s timeout\n  retrans=2,\\                     # 2 retransmissions\n  hard,\\                          # Hard mount (don't give up)\n  async,\\                         # Asynchronous I/O\n  ac,\\                            # Attribute caching\n  actimeo=3600 \\                  # 1-hour attribute cache\n  10.10.1.100:/export /mnt/pnfs\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#server-side-tunables","title":"Server-Side Tunables","text":"<pre><code># NFS server threads (per-node)\necho 256 &gt; /proc/sys/sunrpc/nfsd_threads\n\n# Network receive buffers\nsysctl -w net.core.rmem_max=134217728\nsysctl -w net.core.wmem_max=134217728\n\n# NVMe queue depth\necho 1024 &gt; /sys/block/nvme0n1/queue/nr_requests\n\n# Disable CPU frequency scaling (performance mode)\ncpupower frequency-set -g performance\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#monitoring-metrics","title":"Monitoring Metrics","text":"<pre><code>Key metrics to track:\n  - MDS operations/sec (LAYOUTGET, OPEN, CLOSE)\n  - Data server throughput (GB/s per node)\n  - Latency percentiles (p50, p95, p99)\n  - Client cache hit rates\n  - Network utilization (per fabric)\n  - NVMe IOPS and latency\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#implementation-deployment-checklist","title":"Implementation: Deployment Checklist","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-1-network-setup","title":"Phase 1: Network Setup","text":"<ul> <li> Deploy InfiniBand/RoCE fabric</li> <li> Configure storage VLAN with jumbo frames</li> <li> Enable RDMA on all nodes</li> <li> Verify bandwidth with <code>ib_write_bw</code> / <code>ib_read_bw</code></li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-2-storage-node-provisioning","title":"Phase 2: Storage Node Provisioning","text":"<ul> <li> Install NVMe SSDs and verify <code>nvme list</code></li> <li> Create XFS/ZFS filesystems</li> <li> Apply NVMe performance tunings</li> <li> Install <code>nfs-kernel-server</code> with pNFS support</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-3-mds-cluster-setup","title":"Phase 3: MDS Cluster Setup","text":"<ul> <li> Choose clustering backend (DRBD, etcd, etc.)</li> <li> Configure Pacemaker/Corosync or equivalent</li> <li> Set up VIP with failover tests</li> <li> Deploy metadata synchronization</li> <li> Test quorum and fencing</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-4-pnfs-configuration","title":"Phase 4: pNFS Configuration","text":"<ul> <li> Configure pNFS layouts on each storage node</li> <li> Export file systems via NFS4 with pNFS enabled</li> <li> Register data servers with MDS</li> <li> Test layout distribution from clients</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-5-client-deployment","title":"Phase 5: Client Deployment","text":"<ul> <li> Mount pNFS export with optimized parameters</li> <li> Verify parallel I/O with <code>dd</code> or <code>fio</code></li> <li> Test layout recall and coherency</li> <li> Run application workload benchmarks</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-6-production-hardening","title":"Phase 6: Production Hardening","text":"<ul> <li> Set up monitoring (Prometheus + Grafana)</li> <li> Configure alerting for node failures</li> <li> Document failover procedures</li> <li> Schedule regular disaster recovery drills</li> <li> Implement backup strategy</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#real-world-performance","title":"Real-World Performance","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#benchmark-environment","title":"Benchmark Environment","text":"<pre><code>Hardware:\n  - 10x storage nodes (Dell R750)\n  - 4x 7.68TB NVMe per node (Samsung PM9A3)\n  - 100GbE RoCE network\n  - 2x AMD EPYC 7543 per node\n\nWorkload:\n  - FIO sequential read (4MB block size)\n  - 8 clients, 16 threads each\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#results","title":"Results","text":"Metric Value Notes Aggregate Throughput 82 GB/s 10 nodes \u00d7 ~8 GB/s each Per-Client Throughput 10.2 GB/s 82 GB/s / 8 clients Latency (p99) 3.2 ms Network + NVMe + pNFS overhead MDS Load 2,300 ops/s Only layout requests CPU Utilization 35% avg Plenty of headroom <p>Key Takeaway</p> <p>pNFS achieved near-linear scaling across 10 storage nodes. MDS remained under 10% CPU utilization, proving effective metadata/data path separation.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#troubleshooting-guide","title":"Troubleshooting Guide","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#problem-clients-not-using-pnfs-falling-back-to-standard-nfs","title":"Problem: Clients not using pNFS (falling back to standard NFS)","text":"<p>Symptoms: <pre><code># All I/O going through MDS node\nnfsstat -m | grep \"pnfs\"  # Shows \"pnfs: not in use\"\n</code></pre></p> <p>Diagnosis: <pre><code># Check server pNFS support\nnfsstat -s | grep pnfs\n\n# Check client kernel support\ngrep PNFS /boot/config-$(uname -r)  # Should show CONFIG_PNFS_FILE_LAYOUT=m\n</code></pre></p> <p>Solution: - Ensure server exports with <code>pnfs</code> option - Verify client kernel has <code>nfs_layout_nfsv41_files</code> module loaded - Check for layout request denials in <code>/var/log/messages</code></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#problem-high-mds-cpu-usage","title":"Problem: High MDS CPU usage","text":"<p>Symptoms: <pre><code># MDS nodes showing &gt;80% CPU\ntop  # nfsd threads consuming CPU\n</code></pre></p> <p>Diagnosis: <pre><code># Check for excessive LAYOUTGET operations\nnfsstat -s | grep LAYOUTGET\n</code></pre></p> <p>Possible Causes: - Clients not caching layouts (check <code>actimeo</code>) - Frequent layout recalls (check for conflicting access patterns) - Insufficient MDS threads (check <code>nfsd_threads</code>)</p> <p>Solution: <pre><code># Increase client attribute cache timeout\nmount -o remount,actimeo=3600 /mnt/pnfs\n\n# Add more MDS threads\necho 512 &gt; /proc/sys/sunrpc/nfsd_threads\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#problem-uneven-storage-utilization","title":"Problem: Uneven storage utilization","text":"<p>Symptoms: <pre><code># One storage node at 90%, others at 40%\ndf -h /storage/*\n</code></pre></p> <p>Diagnosis: <pre><code># Check file layout distribution\n# (Requires pNFS-aware tooling or manual inspection)\n</code></pre></p> <p>Solution: - Re-stripe files: Use pNFS restripe tools if available - Balance new files: Adjust MDS layout selection algorithm - Add/remove nodes: Trigger cluster rebalancing</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#advanced-topics","title":"Advanced Topics","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#1-hierarchical-storage-management-hsm-with-pnfs","title":"1. Hierarchical Storage Management (HSM) with pNFS","text":"<p>Implement tiered storage by combining: - Hot tier: NVMe-backed pNFS for active data - Warm tier: SATA SSD pNFS for recent data - Cold tier: HDD-based object storage (S3) for archives</p> <p>Layout policy: <pre><code>def select_storage_tier(file_metadata):\n    if file_metadata.access_count &gt; 100:\n        return TIER_NVME\n    elif file_metadata.age_days &lt; 30:\n        return TIER_SSD\n    else:\n        return TIER_HDD\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#2-erasure-coding-for-space-efficiency","title":"2. Erasure Coding for Space Efficiency","text":"<p>Instead of replication (2x-3x overhead), use erasure coding: - Reed-Solomon (8+3): 1.375x overhead for 3-drive fault tolerance - RAID 6 equivalent: Stripe across pNFS with parity - Rebuild time: ~2 hours for 10TB per failed drive</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#3-multi-site-pnfs-replication","title":"3. Multi-Site pNFS Replication","text":"<p>For disaster recovery: <pre><code>Site A (Primary):          Site B (DR):\n  10 storage nodes    \u2192      10 storage nodes\n  Active MDS cluster  \u2192      Standby MDS cluster\n\nAsync replication (rsync/DRBD async or Lustre HSM)\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#conclusion","title":"Conclusion","text":"<p>This pNFS v4.2 architecture provides:</p> <p>\u2705 High throughput: 80+ GB/s aggregate via parallel I/O \u2705 Low latency: &lt;5ms p99 with InfiniBand/RoCE \u2705 High availability: No single points of failure \u2705 Horizontal scalability: Add nodes without downtime \u2705 Operational simplicity: Standard NFS client compatibility</p> <p>Trade-offs: - Complexity: More moving parts than traditional NAS - Data durability: Requires additional replication/erasure coding - Cost: High-speed network and NVMe SSDs increase CapEx</p> <p>Ideal for: - AI/ML training clusters (GPU \u2192 storage throughput) - HPC workloads (parallel file access patterns) - Video rendering farms (large file streaming) - High-frequency trading (low-latency shared storage)</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#references","title":"References","text":"<ul> <li>RFC 8881 - NFSv4.1 Protocol</li> <li>RFC 7862 - NFSv4.2 Protocol</li> <li>Linux pNFS Documentation</li> <li>Lustre pNFS Guide</li> <li>Red Hat: Configuring pNFS</li> </ul> <p>Tags: #pNFS #distributed-storage #NVMe #high-availability #load-balancing #metadata #clustering #InfiniBand #RoCE #parallel-io #file-systems #linux #performance-tuning #scalability</p> <p>Category: Storage, Architecture</p> <p>Have questions or running a similar setup? Open a discussion or reach out.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"journal/","title":"Journal","text":"<p>Time-based log entries, learning notes, and progress updates.</p>"},{"location":"journal/#coming-soon","title":"Coming Soon","text":"<p>This section will contain:</p> <ul> <li>Learning Logs: Daily/weekly learning summaries</li> <li>Project Progress: Implementation updates</li> <li>Experiments: Technical experiments and findings</li> <li>Quick Notes: Short observations and discoveries</li> </ul> <p>Check back soon or watch the repository for updates.</p>"},{"location":"kb/","title":"Knowledge Base","text":"<p>Evergreen reference material, technical documentation, and curated resources.</p>"},{"location":"kb/#coming-soon","title":"Coming Soon","text":"<p>This section will contain:</p> <ul> <li>System Design Patterns: Reusable architectural patterns</li> <li>Protocol References: Detailed protocol documentation</li> <li>Tool Guides: Configuration and usage guides</li> <li>Troubleshooting Playbooks: Common issues and solutions</li> <li>Performance Baselines: Benchmark data and analysis</li> </ul> <p>Check back soon or watch the repository for updates.</p>"},{"location":"kubernetes/","title":"Kubernetes CKA Mastery","text":"<p>Complete hands-on guide to Kubernetes administration and CKA certification</p>"},{"location":"kubernetes/#about-the-cka-exam","title":"\ud83c\udfaf About the CKA Exam","text":"<p>The Certified Kubernetes Administrator (CKA) certification demonstrates proficiency in Kubernetes cluster administration, troubleshooting, and operations.</p>"},{"location":"kubernetes/#exam-details","title":"Exam Details","text":"<ul> <li>Duration: 2 hours</li> <li>Format: ~17 performance-based tasks (100% hands-on terminal work)</li> <li>Pass Score: 66%</li> <li>Cost: $445 (includes one free retake)</li> <li>Environment: Remote proctored, browser-based terminal</li> </ul>"},{"location":"kubernetes/#exam-domains-weights","title":"Exam Domains &amp; Weights","text":"<pre><code>pie title CKA Exam Domain Distribution\n    \"Troubleshooting\" : 30\n    \"Cluster Architecture\" : 25\n    \"Services &amp; Networking\" : 20\n    \"Workloads &amp; Scheduling\" : 15\n    \"Storage\" : 10</code></pre> Domain Weight Focus Areas Troubleshooting 30% Cluster/node issues, application debugging, monitoring Cluster Architecture 25% Installation, upgrades, RBAC, security, CRDs Services &amp; Networking 20% Services, Ingress, Gateway API, Network Policies Workloads &amp; Scheduling 15% Deployments, scheduling, pod configuration Storage 10% PV/PVC, ConfigMaps, Secrets, StorageClasses"},{"location":"kubernetes/#learning-path","title":"\ud83d\udcda Learning Path","text":"<p>This series covers 22 comprehensive posts organized into 7 phases, following the optimal learning sequence for CKA exam success.</p> <pre><code>graph TD\n    A[Phase 1: Foundations] --&gt; B[Phase 2: Workloads]\n    B --&gt; C[Phase 3: Networking]\n    C --&gt; D[Phase 4: Storage]\n    D --&gt; E[Phase 5: Security]\n    E --&gt; F[Phase 6: Advanced Config]\n    F --&gt; G[Phase 7: Troubleshooting]\n\n    A --&gt; A1[Architecture]\n    A --&gt; A2[Lab Setup]\n    A --&gt; A3[kubectl Basics]\n    A --&gt; A4[YAML &amp; Objects]\n    A --&gt; A5[Namespaces]\n\n    B --&gt; B1[Pods]\n    B --&gt; B2[Deployments]\n    B --&gt; B3[Scheduling]\n\n    C --&gt; C1[Services]\n    C --&gt; C2[Ingress/Gateway]\n    C --&gt; C3[Network Policies]\n    C --&gt; C4[DNS]\n\n    D --&gt; D1[PV/PVC]\n    D --&gt; D2[ConfigMaps/Secrets]\n\n    E --&gt; E1[RBAC]\n    E --&gt; E2[Security Contexts]\n    E --&gt; E3[CRDs/Operators]\n\n    F --&gt; F1[Helm]\n    F --&gt; F2[Kustomize]\n\n    G --&gt; G1[Cluster Troubleshooting]\n    G --&gt; G2[App Troubleshooting]\n    G --&gt; G3[Monitoring]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e8\n    style D fill:#f3e5f5\n    style E fill:#ffe5e5\n    style F fill:#fff9e5\n    style G fill:#ffe5f0</code></pre>"},{"location":"kubernetes/#phase-1-foundations-5-posts","title":"\ud83c\udfd7\ufe0f Phase 1: Foundations (5 posts)","text":"<p>Build your foundational knowledge of Kubernetes architecture and essential tools.</p>"},{"location":"kubernetes/#1-kubernetes-architecture-fundamentals","title":"1. Kubernetes Architecture Fundamentals","text":"<p>Control plane components, worker nodes, etcd, API server, scheduler, controller manager Tags: <code>kubernetes</code> <code>architecture</code> <code>fundamentals</code> <code>cka-prep</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#2-setting-up-your-kubernetes-lab-environment","title":"2. Setting Up Your Kubernetes Lab Environment","text":"<p>kubeadm, Minikube, kind, kubectl installation, kubeconfig management Tags: <code>kubernetes</code> <code>installation</code> <code>lab-setup</code> <code>kubeadm</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#3-kubectl-essentials-your-kubernetes-swiss-army-knife","title":"3. kubectl Essentials: Your Kubernetes Swiss Army Knife","text":"<p>Master kubectl commands, aliases, output formats, context switching, imperative vs declarative Tags: <code>kubernetes</code> <code>kubectl</code> <code>cli</code> <code>basics</code> Domain: All (foundational skill)</p>"},{"location":"kubernetes/#4-understanding-kubernetes-objects-and-yaml-manifests","title":"4. Understanding Kubernetes Objects and YAML Manifests","text":"<p>API objects, YAML syntax, metadata, spec, status, labels, annotations, selectors Tags: <code>kubernetes</code> <code>yaml</code> <code>objects</code> <code>manifests</code> Domain: All (foundational skill)</p>"},{"location":"kubernetes/#5-namespaces-and-resource-quotas","title":"5. Namespaces and Resource Quotas","text":"<p>Namespace isolation, resource quotas, limit ranges, default namespace management Tags: <code>kubernetes</code> <code>namespaces</code> <code>resource-management</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#phase-2-workloads-scheduling-3-posts","title":"\u2699\ufe0f Phase 2: Workloads &amp; Scheduling (3 posts)","text":"<p>Master pod management, deployments, and advanced scheduling techniques.</p>"},{"location":"kubernetes/#6-pods-the-atomic-unit-of-kubernetes","title":"6. Pods: The Atomic Unit of Kubernetes","text":"<p>Pod lifecycle, init containers, sidecar patterns, multi-container communication Tags: <code>kubernetes</code> <code>pods</code> <code>workloads</code> <code>containers</code> Domain: Workloads &amp; Scheduling (15%)</p>"},{"location":"kubernetes/#7-deployments-replicasets-and-rolling-updates","title":"7. Deployments, ReplicaSets, and Rolling Updates","text":"<p>Deployments, ReplicaSets, DaemonSets, StatefulSets, rollouts, rollback strategies Tags: <code>kubernetes</code> <code>deployments</code> <code>replicasets</code> <code>workloads</code> Domain: Workloads &amp; Scheduling (15%)</p>"},{"location":"kubernetes/#8-advanced-scheduling-taints-tolerations-and-affinity","title":"8. Advanced Scheduling: Taints, Tolerations, and Affinity","text":"<p>Node selectors, taints/tolerations, node/pod affinity, anti-affinity, priority classes Tags: <code>kubernetes</code> <code>scheduling</code> <code>advanced</code> <code>affinity</code> Domain: Workloads &amp; Scheduling (15%)</p>"},{"location":"kubernetes/#phase-3-services-networking-4-posts","title":"\ud83c\udf10 Phase 3: Services &amp; Networking (4 posts)","text":"<p>Deep dive into Kubernetes networking, service discovery, and traffic management.</p>"},{"location":"kubernetes/#9-kubernetes-services-exposing-your-applications","title":"9. Kubernetes Services: Exposing Your Applications","text":"<p>ClusterIP, NodePort, LoadBalancer, ExternalName, service discovery, endpoints Tags: <code>kubernetes</code> <code>services</code> <code>networking</code> <code>service-discovery</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#10-ingress-and-gateway-api-modern-traffic-management","title":"10. Ingress and Gateway API: Modern Traffic Management","text":"<p>Ingress controllers, Ingress rules, Gateway API (GatewayClass, Gateway, HTTPRoute) Tags: <code>kubernetes</code> <code>ingress</code> <code>gateway-api</code> <code>traffic-management</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#11-network-policies-securing-pod-communication","title":"11. Network Policies: Securing Pod Communication","text":"<p>NetworkPolicy resources, ingress/egress rules, pod/namespace selectors, isolation Tags: <code>kubernetes</code> <code>network-policies</code> <code>security</code> <code>networking</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#12-coredns-and-service-discovery-deep-dive","title":"12. CoreDNS and Service Discovery Deep Dive","text":"<p>CoreDNS configuration, DNS for Services and Pods, troubleshooting DNS issues Tags: <code>kubernetes</code> <code>dns</code> <code>coredns</code> <code>service-discovery</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#phase-4-storage-2-posts","title":"\ud83d\udcbe Phase 4: Storage (2 posts)","text":"<p>Understand persistent storage and configuration management in Kubernetes.</p>"},{"location":"kubernetes/#13-persistent-volumes-and-claims-stateful-storage","title":"13. Persistent Volumes and Claims: Stateful Storage","text":"<p>PV, PVC, StorageClass, access modes, reclaim policies, dynamic provisioning Tags: <code>kubernetes</code> <code>storage</code> <code>persistent-volumes</code> <code>stateful</code> Domain: Storage (10%)</p>"},{"location":"kubernetes/#14-configmaps-secrets-and-volume-mounts","title":"14. ConfigMaps, Secrets, and Volume Mounts","text":"<p>ConfigMaps, Secrets, volume mounts, environment variables, projected volumes Tags: <code>kubernetes</code> <code>configmaps</code> <code>secrets</code> <code>configuration</code> Domain: Storage (10%)</p>"},{"location":"kubernetes/#phase-5-security-configuration-3-posts","title":"\ud83d\udd12 Phase 5: Security &amp; Configuration (3 posts)","text":"<p>Secure your cluster with RBAC, security contexts, and extensibility.</p>"},{"location":"kubernetes/#15-rbac-role-based-access-control","title":"15. RBAC: Role-Based Access Control","text":"<p>Roles, ClusterRoles, RoleBindings, ClusterRoleBindings, ServiceAccounts Tags: <code>kubernetes</code> <code>rbac</code> <code>security</code> <code>access-control</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#16-security-contexts-and-pod-security-standards","title":"16. Security Contexts and Pod Security Standards","text":"<p>SecurityContext, Pod Security Admission, privileged containers, capabilities, PSS Tags: <code>kubernetes</code> <code>security</code> <code>pod-security</code> <code>hardening</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#17-custom-resources-and-operators-crds","title":"17. Custom Resources and Operators (CRDs)","text":"<p>CustomResourceDefinitions, custom controllers, Operators, CRD inspection Tags: <code>kubernetes</code> <code>crds</code> <code>operators</code> <code>extensibility</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#phase-6-advanced-configuration-2-posts","title":"\ud83d\udd27 Phase 6: Advanced Configuration (2 posts)","text":"<p>Master Helm and Kustomize for production-grade configuration management.</p>"},{"location":"kubernetes/#18-helm-kubernetes-package-manager","title":"18. Helm: Kubernetes Package Manager","text":"<p>Helm charts, templating, values files, releases, hooks, chart repositories Tags: <code>kubernetes</code> <code>helm</code> <code>package-management</code> <code>charts</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#19-kustomize-template-free-configuration-management","title":"19. Kustomize: Template-Free Configuration Management","text":"<p>Kustomize bases, overlays, patches, transformers, generators, GitOps patterns Tags: <code>kubernetes</code> <code>kustomize</code> <code>configuration</code> <code>gitops</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#phase-7-troubleshooting-monitoring-3-posts","title":"\ud83d\udd0d Phase 7: Troubleshooting &amp; Monitoring (3 posts)","text":"<p>Become an expert at diagnosing and resolving Kubernetes issues.</p>"},{"location":"kubernetes/#20-troubleshooting-clusters-nodes-and-components","title":"20. Troubleshooting Clusters, Nodes, and Components","text":"<p>Node issues, control plane debugging, certificate problems, etcd health checks Tags: <code>kubernetes</code> <code>troubleshooting</code> <code>debugging</code> <code>cluster-health</code> Domain: Troubleshooting (30%)</p>"},{"location":"kubernetes/#21-application-troubleshooting-and-log-analysis","title":"21. Application Troubleshooting and Log Analysis","text":"<p>Pod debugging, container logs, exec commands, ephemeral containers, event analysis Tags: <code>kubernetes</code> <code>troubleshooting</code> <code>logs</code> <code>debugging</code> Domain: Troubleshooting (30%)</p>"},{"location":"kubernetes/#22-monitoring-metrics-and-resource-management","title":"22. Monitoring, Metrics, and Resource Management","text":"<p>Metrics Server, resource requests/limits, HPA, VPA, monitoring stack integration Tags: <code>kubernetes</code> <code>monitoring</code> <code>metrics</code> <code>autoscaling</code> Domain: Troubleshooting (30%)</p>"},{"location":"kubernetes/#how-to-use-this-series","title":"\ud83d\udcd6 How to Use This Series","text":""},{"location":"kubernetes/#recommended-study-approach","title":"Recommended Study Approach","text":"<ol> <li>Follow the Order: Posts are sequenced for optimal learning progression</li> <li>Hands-On Practice: Set up a lab environment (Post 2) and practice every command</li> <li>Take Notes: Create your own command cheat sheets as you progress</li> <li>Review Diagrams: Study the architecture diagrams to understand component relationships</li> <li>Do the Exercises: Complete practice tasks at the end of each post</li> <li>Cross-Reference: Use links between posts to review related concepts</li> </ol>"},{"location":"kubernetes/#study-timeline","title":"Study Timeline","text":"<ul> <li>Intensive: 4-6 weeks (1 post per day)</li> <li>Standard: 8-12 weeks (2-3 posts per week)</li> <li>Relaxed: 3-4 months (1-2 posts per week)</li> </ul>"},{"location":"kubernetes/#exam-preparation-tips","title":"Exam Preparation Tips","text":"<p>\u2705 Do: - Practice in a terminal environment (exam is 100% command-line) - Use <code>kubectl</code> imperative commands for speed - Master <code>kubectl explain</code> and <code>-h</code> flags for in-exam reference - Time yourself on practice exercises - Focus heavily on Troubleshooting (30% weight)</p> <p>\u274c Don't: - Memorize YAML templates (use <code>kubectl</code> generators instead) - Ignore troubleshooting topics (highest exam weight) - Skip hands-on practice (reading alone is insufficient) - Forget about time management (2 hours goes fast)</p>"},{"location":"kubernetes/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<p>Before starting this series, you should have:</p> <ul> <li>Basic understanding of Linux command line</li> <li>Familiarity with containerization concepts (Docker)</li> <li>Access to a Linux/macOS machine or Windows with WSL2</li> <li>Willingness to practice hands-on (not just read)</li> </ul>"},{"location":"kubernetes/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>Official CKA Exam Page</li> <li>Kubernetes Official Documentation</li> <li>kubectl Command Reference</li> <li>CKA Curriculum (Official)</li> </ul>"},{"location":"kubernetes/#ready-to-start","title":"\ud83d\ude80 Ready to Start?","text":"<p>Begin your journey with Post 1: Kubernetes Architecture Fundamentals and work through the series systematically.</p> <p>Good luck with your CKA certification! \ud83c\udf93</p> <p>Last Updated: 2025-11-10 Series Status: In Progress (Post 1 available) Total Posts: 22 planned</p>"},{"location":"principles/","title":"Principles","text":"<p>Engineering principles, design philosophies, and decision-making frameworks.</p>"},{"location":"principles/#coming-soon","title":"Coming Soon","text":"<p>This section will explore:</p> <ul> <li>Architecture Principles: Foundational design guidelines</li> <li>Performance Principles: Optimization philosophies</li> <li>Reliability Principles: Building resilient systems</li> <li>Scalability Principles: Growing systems effectively</li> <li>Simplicity Principles: Managing complexity</li> </ul> <p>Check back soon or watch the repository for updates.</p>"},{"location":"blog/archive/2025/","title":"November 2025","text":""},{"location":"blog/category/kubernetes/","title":"Kubernetes","text":""},{"location":"blog/category/cli/","title":"CLI","text":""},{"location":"blog/category/architecture/","title":"Architecture","text":""},{"location":"blog/category/workloads/","title":"Workloads","text":""},{"location":"blog/category/infrastructure/","title":"Infrastructure","text":""},{"location":"blog/category/configuration/","title":"Configuration","text":""},{"location":"blog/category/storage/","title":"Storage","text":""}]}