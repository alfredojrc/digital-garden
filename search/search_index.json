{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Digital Garden","text":"<p>A living knowledge base for distributed systems, storage architecture, and infrastructure engineering</p>"},{"location":"#latest-posts","title":"\ud83d\udcdd Latest Posts","text":"<p>\u27a1\ufe0f View All Posts</p>"},{"location":"#high-performance-pnfs-v42-distributed-storage-architecture","title":"High-Performance pNFS v4.2 Distributed Storage Architecture","text":"<p>Storage \u00b7 Architecture \u00b7 12 min read</p> <p>A deep dive into building a clustered, high-availability parallel NFS storage system with load-balanced metadata servers, NVMe-backed storage nodes, and low-latency interconnects. Features production-grade architecture with InfiniBand/RoCE fabrics, active-active MDS clustering, and parallel data paths achieving 28 GB/s aggregate throughput.</p> <p>Topics: pNFS v4.2, distributed storage, NVMe, InfiniBand, RoCE, high availability, load balancing, metadata clustering</p> <p>Read more \u2192</p>"},{"location":"#browse-by-theme","title":"\ud83d\uddc2\ufe0f Browse by Theme","text":"<ul> <li> <p> AI &amp; Automation</p> <p>Agents, MCP servers, tool orchestration, LLM workflows</p> <p>Explore AI \u2192</p> </li> <li> <p> Cloud Infrastructure</p> <p>GCP, Azure, AWS, multi-cloud architectures</p> <p>Explore Cloud \u2192</p> </li> <li> <p> Infrastructure as Code</p> <p>Terraform, Ansible, GitOps, automation</p> <p>Explore IaC \u2192</p> </li> <li> <p> On-Premise Systems</p> <p>Bare metal, datacenter, networking, storage</p> <p>Explore On-Prem \u2192</p> </li> <li> <p> Cybersecurity</p> <p>Security architecture, hardening, compliance</p> <p>Explore Security \u2192</p> </li> <li> <p> Storage &amp; Networking</p> <p>File systems, protocols, high-performance I/O</p> <p>Explore Storage \u2192</p> </li> </ul>"},{"location":"#additional-resources","title":"\ud83e\udded Additional Resources","text":"<ul> <li>Knowledge Base: Curated reference material and evergreen documentation</li> <li>Principles: Engineering principles and design patterns</li> <li>Journal: Progress logs and learning notes</li> <li>Tags: Browse all content by tag</li> </ul> <p>This garden grows continuously \u00b7 Follow on GitHub \u00b7 RSS Feed</p>"},{"location":"blog/","title":"Blog","text":"<p>Technical deep-dives into distributed systems, storage architecture, and infrastructure engineering.</p>"},{"location":"blog/#navigate","title":"Navigate","text":"<ul> <li>Browse by Tags for topic-based exploration</li> <li>View the Archive for chronological browsing</li> <li>Filter by Categories below</li> </ul> <p>All posts include estimated read times and are optimized for both technical depth and practical application.</p>"},{"location":"blog/tags/","title":"Tag Index","text":"<p>Browse all posts by tag. Tags provide fine-grained topic classification across multiple dimensions.</p>"},{"location":"blog/tags/#tag:infiniband","title":"InfiniBand","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:nvme","title":"NVMe","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:roce","title":"RoCE","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:architecture","title":"architecture","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:cka-prep","title":"cka-prep","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:clustering","title":"clustering","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:control-plane","title":"control-plane","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:distributed-storage","title":"distributed-storage","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:high-availability","title":"high-availability","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:k8s","title":"k8s","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:kubectl","title":"kubectl","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:kubernetes","title":"kubernetes","text":"<ul> <li>            Kubernetes Architecture Fundamentals          </li> </ul>"},{"location":"blog/tags/#tag:load-balancing","title":"load-balancing","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:metadata","title":"metadata","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:pnfs","title":"pNFS","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag:parallel-io","title":"parallel-io","text":"<ul> <li>            High-Performance pNFS v4.2 Distributed Storage Architecture          </li> </ul>"},{"location":"blog/tags/#tag-descriptions","title":"Tag Descriptions","text":"<ul> <li>pNFS: Parallel NFS protocol and implementations</li> <li>distributed-storage: Multi-node storage architectures</li> <li>NVMe: Non-Volatile Memory Express technologies</li> <li>high-availability: HA clustering and failover systems</li> <li>load-balancing: Traffic distribution and request routing</li> <li>metadata: Metadata server design and optimization</li> <li>clustering: Cluster management and coordination</li> <li>InfiniBand: InfiniBand networking and RDMA</li> <li>RoCE: RDMA over Converged Ethernet</li> <li>parallel-io: Parallel I/O patterns and optimization</li> <li>file-systems: File system design and internals</li> <li>linux: Linux kernel and system programming</li> <li>performance-tuning: System optimization techniques</li> <li>scalability: Scaling strategies and patterns</li> </ul>"},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/","title":"Kubernetes Architecture Fundamentals","text":"<p>Deep dive into Kubernetes cluster architecture, control plane components, and the distributed systems design that powers container orchestration at scale.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#overview","title":"Overview","text":"<p>Kubernetes is a distributed system designed to manage containerized applications across a cluster of machines. Understanding its architecture is foundational for the CKA exam and real-world cluster administration.</p> <p>CKA Exam Domain: Cluster Architecture, Installation &amp; Configuration (25%)</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#cluster-architecture","title":"Cluster Architecture","text":"<p>A Kubernetes cluster consists of two types of nodes:</p> <ol> <li>Control Plane Nodes: Run the core components that manage the cluster</li> <li>Worker Nodes: Run application workloads (pods)</li> </ol> <pre><code>graph TB\n    subgraph \"Control Plane Nodes\"\n        API[API Server&lt;br/&gt;:6443]\n        SCHED[Scheduler]\n        CM[Controller&lt;br/&gt;Manager]\n        CCM[Cloud Controller&lt;br/&gt;Manager]\n        ETCD[(etcd&lt;br/&gt;:2379-2380)]\n    end\n\n    subgraph \"Worker Node 1\"\n        KUB1[kubelet]\n        KPXY1[kube-proxy]\n        POD1[Pods]\n        CRI1[Container&lt;br/&gt;Runtime]\n    end\n\n    subgraph \"Worker Node 2\"\n        KUB2[kubelet]\n        KPXY2[kube-proxy]\n        POD2[Pods]\n        CRI2[Container&lt;br/&gt;Runtime]\n    end\n\n    subgraph \"Worker Node 3\"\n        KUB3[kubelet]\n        KPXY3[kube-proxy]\n        POD3[Pods]\n        CRI3[Container&lt;br/&gt;Runtime]\n    end\n\n    API --&gt;|watches| ETCD\n    API --&gt; SCHED\n    API --&gt; CM\n    API --&gt; CCM\n\n    KUB1 --&gt;|API calls| API\n    KUB2 --&gt;|API calls| API\n    KUB3 --&gt;|API calls| API\n\n    KUB1 --&gt; CRI1 --&gt; POD1\n    KUB2 --&gt; CRI2 --&gt; POD2\n    KUB3 --&gt; CRI3 --&gt; POD3\n\n    KPXY1 -.-&gt;|iptables/IPVS| POD1\n    KPXY2 -.-&gt;|iptables/IPVS| POD2\n    KPXY3 -.-&gt;|iptables/IPVS| POD3\n\n    style API fill:#e1f5ff\n    style ETCD fill:#ffe5e5\n    style SCHED fill:#fff4e1\n    style CM fill:#e8f5e8</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#control-plane-components","title":"Control Plane Components","text":"<p>The control plane makes global decisions about the cluster and detects/responds to cluster events.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#1-api-server-kube-apiserver","title":"1. API Server (kube-apiserver)","text":"<p>Purpose: Front-end for the Kubernetes control plane. All communication goes through the API server.</p> <p>Key Responsibilities: - Exposes the Kubernetes API (REST interface) - Validates and configures API objects (pods, services, replication controllers) - Serves as the only component that directly interacts with etcd - Handles authentication, authorization, and admission control</p> <p>Default Port: <code>6443</code> (HTTPS)</p> <pre><code># Check API server status\nkubectl get --raw='/healthz?verbose'\n\n# View API server configuration\nkubectl -n kube-system get pod kube-apiserver-&lt;node-name&gt; -o yaml\n\n# Check API server logs\nkubectl -n kube-system logs kube-apiserver-&lt;node-name&gt;\n</code></pre> <p>API Request Flow:</p> <pre><code>sequenceDiagram\n    participant U as User/kubectl\n    participant API as API Server\n    participant AUTH as Authentication\n    participant AUTHZ as Authorization\n    participant ADM as Admission&lt;br/&gt;Controllers\n    participant ETCD as etcd\n\n    U-&gt;&gt;+API: HTTP Request&lt;br/&gt;(create Pod)\n\n    API-&gt;&gt;+AUTH: Authenticate\n    Note right of AUTH: Verify user identity&lt;br/&gt;(certs, tokens, SA)\n    AUTH--&gt;&gt;-API: User ID\n\n    API-&gt;&gt;+AUTHZ: Authorize\n    Note right of AUTHZ: Check RBAC rules&lt;br/&gt;(can user create pod?)\n    AUTHZ--&gt;&gt;-API: Authorized\n\n    API-&gt;&gt;+ADM: Admission Control\n    Note right of ADM: Validate &amp; Mutate&lt;br/&gt;(defaults, quotas, PSP)\n    ADM--&gt;&gt;-API: Admitted\n\n    API-&gt;&gt;+ETCD: Write to etcd\n    ETCD--&gt;&gt;-API: Persisted\n\n    API--&gt;&gt;-U: 201 Created\n\n    Note over API,ETCD: Object now exists in desired state</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#2-etcd","title":"2. etcd","text":"<p>Purpose: Consistent, highly-available key-value store used as Kubernetes' backing store for all cluster data.</p> <p>Key Characteristics: - Distributed consensus using Raft algorithm - Stores all cluster state and configuration - Only the API server writes to etcd - Supports watch operations for real-time updates</p> <p>Default Ports: - <code>2379</code> - Client requests - <code>2380</code> - Server-to-server communication</p> <pre><code># Check etcd cluster health\nETCDCTL_API=3 etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  endpoint health\n\n# List etcd members\nETCDCTL_API=3 etcdctl member list\n\n# Get all keys (see what's stored)\nETCDCTL_API=3 etcdctl get / --prefix --keys-only\n\n# Backup etcd\nETCDCTL_API=3 etcdctl snapshot save snapshot.db\n</code></pre> <p>High Availability: For production clusters, run etcd with at least 3 nodes (odd number for quorum).</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#3-scheduler-kube-scheduler","title":"3. Scheduler (kube-scheduler)","text":"<p>Purpose: Watches for newly created pods with no assigned node and selects a node for them to run on.</p> <p>Scheduling Algorithm: 1. Filtering: Find nodes that satisfy pod requirements (feasible nodes)    - Resource requests (CPU, memory)    - Node selectors    - Taints and tolerations    - Affinity/anti-affinity rules</p> <ol> <li>Scoring: Rank feasible nodes</li> <li>Spread pods across nodes for availability</li> <li>Prefer nodes with available resources</li> <li> <p>Consider pod priorities</p> </li> <li> <p>Binding: Assign pod to highest-scoring node</p> </li> </ol> <pre><code>graph TD\n    START([New Pod Created]) --&gt; FILTER[Filtering Phase]\n\n    FILTER --&gt; CHECK1{Resource&lt;br/&gt;Requirements?}\n    CHECK1 --&gt;|Fail| UNSCHEDULABLE[Pod Unschedulable]\n    CHECK1 --&gt;|Pass| CHECK2{Node&lt;br/&gt;Selectors?}\n\n    CHECK2 --&gt;|Fail| UNSCHEDULABLE\n    CHECK2 --&gt;|Pass| CHECK3{Taints/&lt;br/&gt;Tolerations?}\n\n    CHECK3 --&gt;|Fail| UNSCHEDULABLE\n    CHECK3 --&gt;|Pass| CHECK4{Affinity&lt;br/&gt;Rules?}\n\n    CHECK4 --&gt;|Fail| UNSCHEDULABLE\n    CHECK4 --&gt;|Pass| FEASIBLE[Feasible Nodes]\n\n    FEASIBLE --&gt; SCORE[Scoring Phase]\n    SCORE --&gt; RANK[Rank Nodes&lt;br/&gt;by Score]\n    RANK --&gt; BIND[Bind Pod to&lt;br/&gt;Top Node]\n    BIND --&gt; SUCCESS([Pod Scheduled])\n\n    style START fill:#e1f5ff\n    style SUCCESS fill:#e8f5e8\n    style UNSCHEDULABLE fill:#ffe5e5</code></pre> <pre><code># View scheduler configuration\nkubectl -n kube-system get pod kube-scheduler-&lt;node-name&gt; -o yaml\n\n# Check scheduler logs\nkubectl -n kube-system logs kube-scheduler-&lt;node-name&gt;\n\n# View events (scheduling decisions)\nkubectl get events --sort-by='.lastTimestamp'\n\n# See why a pod is not scheduled\nkubectl describe pod &lt;pod-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#4-controller-manager-kube-controller-manager","title":"4. Controller Manager (kube-controller-manager)","text":"<p>Purpose: Runs controller processes that regulate the state of the cluster.</p> <p>Key Controllers: - Node Controller: Monitors node health, marks nodes as unreachable - Replication Controller: Maintains correct number of pod replicas - Endpoints Controller: Populates Endpoints objects (joins Services &amp; Pods) - Service Account &amp; Token Controllers: Create default accounts and API access tokens</p> <p>Control Loop Pattern:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Watch: Controller starts\n    Watch --&gt; Compare: Detect change\n    Compare --&gt; DesiredState: Check desired state\n    DesiredState --&gt; CurrentState: Check current state\n    CurrentState --&gt; Match: States match?\n\n    Match --&gt; Watch: Yes, no action\n    Match --&gt; Reconcile: No, take action\n\n    Reconcile --&gt; CreateResources: Create missing resources\n    Reconcile --&gt; UpdateResources: Update existing resources\n    Reconcile --&gt; DeleteResources: Delete extra resources\n\n    CreateResources --&gt; Watch\n    UpdateResources --&gt; Watch\n    DeleteResources --&gt; Watch</code></pre> <pre><code># View controller manager logs\nkubectl -n kube-system logs kube-controller-manager-&lt;node-name&gt;\n\n# Check which controllers are enabled\nkubectl -n kube-system get pod kube-controller-manager-&lt;node-name&gt; -o yaml | grep enable\n\n# Watch controller actions in events\nkubectl get events --watch\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#5-cloud-controller-manager-cloud-controller-manager","title":"5. Cloud Controller Manager (cloud-controller-manager)","text":"<p>Purpose: Embeds cloud-specific control logic. Allows cloud providers to integrate with Kubernetes.</p> <p>Key Controllers: - Node Controller: Check cloud provider to determine if a deleted node has been removed - Route Controller: Set up routes in the cloud infrastructure - Service Controller: Create, update, delete cloud load balancers</p> <p>Note: Only relevant when running Kubernetes on cloud platforms (AWS, GCP, Azure).</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#worker-node-components","title":"Worker Node Components","text":"<p>Worker nodes run application workloads and maintain running pods.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#1-kubelet","title":"1. kubelet","text":"<p>Purpose: Agent that runs on each node, ensures containers are running in pods.</p> <p>Key Responsibilities: - Registers node with API server - Watches for pod assignments to its node - Ensures containers described in PodSpec are running and healthy - Reports node and pod status back to API server - Performs container health checks (liveness, readiness probes)</p> <p>Default Port: <code>10250</code></p> <pre><code># Check kubelet status (on node directly)\nsystemctl status kubelet\n\n# View kubelet configuration\ncat /var/lib/kubelet/config.yaml\n\n# Check kubelet logs\njournalctl -u kubelet -f\n\n# View node status from control plane\nkubectl get nodes\nkubectl describe node &lt;node-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#2-kube-proxy","title":"2. kube-proxy","text":"<p>Purpose: Network proxy that maintains network rules on nodes, enabling communication to pods.</p> <p>Key Responsibilities: - Implements Service abstraction - Maintains iptables/IPVS rules for service IPs - Forwards traffic to correct backend pods - Performs load balancing across pod replicas</p> <p>Modes: - iptables (default): Uses Linux iptables for packet filtering - IPVS: Uses Linux IPVS for better performance at scale - userspace: Legacy mode (rarely used)</p> <pre><code># Check kube-proxy mode\nkubectl -n kube-system logs kube-proxy-&lt;pod-name&gt; | grep \"proxy mode\"\n\n# View kube-proxy configuration\nkubectl -n kube-system get cm kube-proxy -o yaml\n\n# Check iptables rules (on node)\niptables-save | grep -i kube\n\n# View IPVS rules (if using IPVS mode)\nipvsadm -Ln\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#3-container-runtime","title":"3. Container Runtime","text":"<p>Purpose: Software responsible for running containers.</p> <p>Supported Runtimes (via Container Runtime Interface - CRI): - containerd: Lightweight, industry-standard (default for most distributions) - CRI-O: Lightweight alternative specifically for Kubernetes - Docker Engine: Via cri-dockerd shim (removed as default in Kubernetes 1.24)</p> <pre><code># Check container runtime\nkubectl get nodes -o wide\n\n# On node: check containerd\nsystemctl status containerd\ncrictl ps\n\n# List images\ncrictl images\n\n# Pull image\ncrictl pull nginx:latest\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#component-communication","title":"Component Communication","text":"<p>All components communicate through the API server. No direct component-to-component communication exists.</p> <pre><code>graph LR\n    subgraph \"Control Plane\"\n        API[API Server]\n        SCHED[Scheduler]\n        CM[Controller&lt;br/&gt;Manager]\n        ETCD[(etcd)]\n    end\n\n    subgraph \"Worker Nodes\"\n        KUB1[kubelet]\n        KUB2[kubelet]\n        KUB3[kubelet]\n    end\n\n    CLIENT[kubectl/Users]\n\n    CLIENT --&gt;|REST API| API\n    SCHED --&gt;|watch/update| API\n    CM --&gt;|watch/update| API\n    API &lt;--&gt;|read/write| ETCD\n    KUB1 --&gt;|watch/update| API\n    KUB2 --&gt;|watch/update| API\n    KUB3 --&gt;|watch/update| API\n\n    style API fill:#e1f5ff\n    style ETCD fill:#ffe5e5</code></pre> <p>Communication Patterns:</p> <ol> <li>kubectl \u2192 API Server: Users interact with cluster via kubectl</li> <li>API Server \u2194 etcd: All state stored in etcd</li> <li>Scheduler \u2192 API Server: Watches for unscheduled pods, updates bindings</li> <li>Controller Manager \u2192 API Server: Watches resources, reconciles state</li> <li>kubelet \u2192 API Server: Reports node/pod status, watches for assigned pods</li> </ol>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#kubernetes-object-model","title":"Kubernetes Object Model","text":"<p>Kubernetes manages objects that represent the desired state of your cluster.</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#object-anatomy","title":"Object Anatomy","text":"<pre><code>apiVersion: v1              # API version\nkind: Pod                   # Object type\nmetadata:                   # Object metadata\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx\n    tier: frontend\n  annotations:\n    description: \"Example pod\"\nspec:                       # Desired state\n  containers:\n  - name: nginx\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\nstatus:                     # Current state (managed by system)\n  phase: Running\n  conditions: [...]\n</code></pre> <p>Key Fields: - apiVersion: API group and version (<code>v1</code>, <code>apps/v1</code>, <code>networking.k8s.io/v1</code>) - kind: Object type (Pod, Deployment, Service) - metadata: Identifying information (name, namespace, labels) - spec: Desired state defined by user - status: Current state observed by system (read-only)</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#common-exam-tasks","title":"Common Exam Tasks","text":"","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#scenario-1-check-cluster-component-health","title":"Scenario 1: Check Cluster Component Health","text":"<pre><code># Check all control plane components\nkubectl get componentstatuses\n\n# Check system pods\nkubectl -n kube-system get pods\n\n# Verify API server\nkubectl get --raw='/healthz?verbose'\n\n# Check etcd health\nkubectl -n kube-system exec etcd-&lt;node&gt; -- etcdctl endpoint health\n\n# View node status\nkubectl get nodes\nkubectl describe node &lt;node-name&gt;\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#scenario-2-troubleshoot-kubelet-issues","title":"Scenario 2: Troubleshoot kubelet Issues","text":"<pre><code># On worker node:\nsystemctl status kubelet\njournalctl -u kubelet -f\n\n# Check kubelet config\ncat /var/lib/kubelet/config.yaml\n\n# Restart kubelet\nsystemctl restart kubelet\n\n# From control plane:\nkubectl describe node &lt;node-name&gt;\nkubectl get events --field-selector involvedObject.kind=Node\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#scenario-3-backup-and-restore-etcd","title":"Scenario 3: Backup and Restore etcd","text":"<pre><code># Backup\nETCDCTL_API=3 etcdctl snapshot save /backup/snapshot.db \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key\n\n# Verify backup\nETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db\n\n# Restore (advanced - exam may require)\nETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\\n  --data-dir=/var/lib/etcd-restore\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#practice-exercises","title":"Practice Exercises","text":"<ol> <li>Inspect Cluster Architecture</li> <li>List all control plane pods</li> <li>Check which nodes are running control plane components</li> <li> <p>Identify the API server endpoint and port</p> </li> <li> <p>Component Analysis</p> </li> <li>View logs from each control plane component</li> <li>Check resource usage of control plane pods</li> <li> <p>Identify which container runtime each node is using</p> </li> <li> <p>Troubleshooting Simulation</p> </li> <li>Simulate kubelet failure (stop service) and observe effects</li> <li>Check events to see scheduling decisions</li> <li>Examine etcd data to see how objects are stored</li> </ol>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Kubernetes is a distributed system with control plane and worker nodes</p> <p>\u2705 API server is the central hub - all communication flows through it</p> <p>\u2705 etcd stores all cluster state - critical for backups and disaster recovery</p> <p>\u2705 Scheduler assigns pods to nodes using filtering and scoring</p> <p>\u2705 Controllers maintain desired state through continuous reconciliation loops</p> <p>\u2705 kubelet is the node agent ensuring containers run as specified</p> <p>\u2705 kube-proxy handles networking enabling Service abstraction</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#quick-reference-commands","title":"Quick Reference Commands","text":"<pre><code># Cluster info\nkubectl cluster-info\nkubectl version\nkubectl api-resources\nkubectl api-versions\n\n# Component health\nkubectl get componentstatuses\nkubectl -n kube-system get pods\nkubectl get nodes\n\n# Component logs\nkubectl -n kube-system logs &lt;component-pod&gt;\njournalctl -u kubelet (on node)\njournalctl -u containerd (on node)\n\n# etcd operations\nETCDCTL_API=3 etcdctl endpoint health\nETCDCTL_API=3 etcdctl snapshot save &lt;file&gt;\nETCDCTL_API=3 etcdctl member list\n\n# Node inspection\nkubectl describe node &lt;node-name&gt;\nkubectl get nodes -o wide\nkubectl top nodes (requires metrics-server)\n</code></pre>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/11/kubernetes-architecture-fundamentals/#next-steps","title":"Next Steps","text":"<p>Continue to Post 2: Setting Up Your Kubernetes Lab Environment to build your hands-on learning environment for practicing CKA exam scenarios.</p> <p>Related Posts: - Kubernetes CKA Mastery - Complete Learning Path - Post 3: kubectl Essentials (coming soon) - Post 15: RBAC and Security (coming soon)</p> <p>External Resources: - Kubernetes Components (Official Docs) - Kubernetes Architecture Diagram (CNCF) - CKA Exam Curriculum</p>","tags":["kubernetes","k8s","cka-prep","architecture","control-plane","kubectl"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/","title":"High-Performance pNFS v4.2 Distributed Storage Architecture","text":"<p>A deep dive into building a clustered, high-availability parallel NFS storage system with load-balanced metadata servers, NVMe-backed storage nodes, and low-latency interconnects.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#architecture-overview","title":"Architecture Overview","text":"<p>This architecture implements a production-grade parallel NFS (pNFS) v4.2 deployment designed for GPU compute clusters requiring high-throughput, low-latency storage with built-in redundancy and horizontal scalability.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#key-design-goals","title":"Key Design Goals","text":"<ul> <li>Parallel I/O Performance: Direct client-to-storage data paths bypassing metadata bottlenecks</li> <li>Metadata High Availability: Clustered MDS with automatic failover</li> <li>Horizontal Scalability: Add storage nodes without downtime</li> <li>Low Latency: InfiniBand/RoCE interconnects for sub-microsecond latencies</li> <li>Fault Tolerance: No single points of failure in the architecture</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#system-topology","title":"System Topology","text":"<pre><code>sequenceDiagram\n    participant Client as Client&lt;br/&gt;(pNFS v4.2)\n    participant MDS as MDS Cluster&lt;br/&gt;(Active-Active via VIP)\n    participant S1 as Storage Node 1&lt;br/&gt;(NVMe)\n    participant S2 as Storage Node 2&lt;br/&gt;(NVMe)\n    participant S3 as Storage Node 3&lt;br/&gt;(NVMe)\n\n    Note over Client,S3: \u2501\u2501\u2501\u2501\u2501\u2501\u2501 PHASE 1: METADATA PATH \u2501\u2501\u2501\u2501\u2501\u2501\u2501\n    Note over MDS: Virtual IP load balances to any MDS&lt;br/&gt;All MDS nodes share distributed state\n\n    Client-&gt;&gt;+MDS: LAYOUTGET(file_handle)\n    Note right of MDS: MDS queries distributed&lt;br/&gt;backend for file layout\n    MDS--&gt;&gt;-Client: LAYOUT(stripe_pattern, DS_list)\n    Note left of Client: \u2713 Client caches layout&lt;br/&gt;Stripe unit: 1MB&lt;br/&gt;Stripe count: 3 nodes\n\n    Note over Client,S3: \u2501\u2501\u2501\u2501\u2501\u2501\u2501 PHASE 2: DATA PATH (MDS BYPASSED) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n    par Parallel Direct I/O over InfiniBand/RoCE\n        Client-&gt;&gt;+S1: WRITE Stripe 0\n        S1-&gt;&gt;S1: NVMe I/O\n        S1--&gt;&gt;-Client: ACK\n    and\n        Client-&gt;&gt;+S2: WRITE Stripe 1\n        S2-&gt;&gt;S2: NVMe I/O\n        S2--&gt;&gt;-Client: ACK\n    and\n        Client-&gt;&gt;+S3: WRITE Stripe 2\n        S3-&gt;&gt;S3: NVMe I/O\n        S3--&gt;&gt;-Client: ACK\n    end\n\n    Note over Client,S3: \u26a1 Aggregate: 3 \u00d7 7 GB/s = ~20 GB/s effective throughput</code></pre> <p>Key Architecture Points:</p> Layer Component Function Control Plane MDS Cluster (Active-Active) Virtual IP \u2192 Load balances metadata requestsDistributed backend \u2192 Shared state (GFS2/OCFS2)Co-located with storage nodes Data Plane Storage Nodes Direct parallel I/O bypasses MDS entirelyEach node: MDS service + Data service + NVMeHigh-speed fabric: InfiniBand or 100GbE RoCE Client pNFS v4.2 One-time layout fetch \u2192 caches stripe patternDirect parallel writes to multiple storage nodesNo metadata bottleneck on data path <p>Architecture Advantage</p> <p>Separation of Control and Data Planes: Client contacts MDS once to get file layout, then performs all subsequent I/O directly to storage nodes over high-speed network. MDS handles only metadata operations (LAYOUTGET, OPEN, CLOSE), while bulk data transfer happens in parallel across multiple storage nodes, eliminating the metadata server bottleneck.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#component-breakdown","title":"Component Breakdown","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#1-client-layer-pnfs-v42-clients","title":"1. Client Layer (pNFS v4.2 Clients)","text":"<p>Role: GPU compute nodes running pNFS-aware clients</p> <p>Characteristics: - Protocol: NFSv4.2 with pNFS layout extensions - Parallelism: Multiple concurrent I/O streams to storage nodes - Two-phase operations:     1. Metadata phase: Request file layout from MDS via VIP     2. Data phase: Direct parallel I/O to multiple storage nodes</p> <p>Advantages: - Metadata and data paths are separated - MDS only handles control plane; data plane scales independently - Clients cache layouts, reducing metadata round-trips</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#2-metadata-virtual-ip-vip-load-balancer","title":"2. Metadata Virtual IP (VIP) / Load Balancer","text":"<p>Role: Distribute metadata requests across clustered MDS instances</p> <p>Implementation Options:</p> Technology Use Case Pros Cons Keepalived + VRRP Simple HA Easy setup, fast failover Layer 3 only, single active HAProxy Advanced LB Health checks, stats, multi-algo Additional component Pacemaker + Corosync Enterprise HA Full cluster manager Complex configuration <p>Configuration Considerations: - Failover time: Target &lt;2 seconds for MDS failover - Session stickiness: Not required (stateless metadata operations) - Health checks: Monitor MDS service health on each node</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#3-mds-cluster-metadata-servers","title":"3. MDS Cluster (Metadata Servers)","text":"<p>Role: Manage namespace, permissions, file layouts, and client coordination</p> <p>Clustering Strategy:</p> <p>Active-Active Clustering</p> <p>All MDS instances are active simultaneously, sharing load via the VIP. This differs from traditional active-passive designs and requires:</p> <ul> <li>Shared backend: Distributed consensus or shared storage for metadata</li> <li>State synchronization: Real-time metadata replication</li> <li>Lock coordination: Distributed locking for file operations</li> </ul> <p>Backend Options:</p> <pre><code>Option 1: Shared Block Device (DRBD + GFS2/OCFS2)\n  pros:\n    - Battle-tested clustering\n    - POSIX semantics\n  cons:\n    - Block-level sync overhead\n    - Limited to 2-3 nodes typically\n\nOption 2: Distributed Database (etcd/Consul)\n  pros:\n    - Raft consensus built-in\n    - Horizontal scaling\n    - Cloud-native\n  cons:\n    - Additional latency\n    - More complex integration\n\nOption 3: Lustre MGS/MDT (if using Lustre as pNFS backend)\n  pros:\n    - Native high availability\n    - Proven at exascale\n  cons:\n    - Lustre-specific\n    - Complex deployment\n</code></pre> <p>Heartbeat Mechanism: - Interval: 500ms - 1s between nodes - Quorum: Majority voting prevents split-brain - Fencing: STONITH (Shoot The Other Node In The Head) for failed nodes</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#4-high-speed-network-fabric","title":"4. High-Speed Network Fabric","text":"<p>Role: Low-latency, high-bandwidth interconnect for storage traffic</p> <p>Technology Comparison:</p> Technology Bandwidth Latency Use Case InfiniBand HDR 200 Gbps &lt;1 \u03bcs HPC, AI training clusters 100GbE RoCE v2 100 Gbps &lt;5 \u03bcs Cost-effective alternative Omni-Path 100 Gbps &lt;1 \u03bcs Intel ecosystem <p>Network Design: <pre><code>- Dedicated storage VLAN/subnet\n- Jumbo frames (MTU 9000) for throughput\n- RDMA for zero-copy transfers\n- Lossless Ethernet (PFC) if using RoCE\n- Multiple paths for redundancy (LACP/MLAG)\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#5-storage-nodes","title":"5. Storage Nodes","text":"<p>Role: Serve actual file data via pNFS Data Service (DS)</p> <p>Node Architecture:</p> <pre><code>Each storage node runs:\n\u251c\u2500\u2500 MDS Service (part of cluster)\n\u251c\u2500\u2500 Data Service (DS) (serves pNFS I/O)\n\u2514\u2500\u2500 Physical Storage (NVMe SSDs)\n</code></pre> <p>NVMe Configuration: - Device: PCIe Gen4 NVMe SSDs (7000+ MB/s per device) - RAID: No RAID (rely on pNFS striping across nodes) - File System: XFS or ZFS for local storage - Tuning:     - <code>nvme.io_timeout=4294967295</code> (disable timeout)     - <code>elevator=none</code> (bypass I/O scheduler for NVMe)     - <code>vm.dirty_ratio=5</code> (aggressive writeback)</p> <p>Capacity Planning: <pre><code>Per-node capacity:\n  - 4x 4TB NVMe = 16TB raw per node\n  - 10 nodes = 160TB aggregate raw\n  - No RAID overhead (redundancy via replication)\n  - Effective capacity: ~140TB (accounting for metadata)\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#data-flow-read-operation","title":"Data Flow: Read Operation","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-1-layout-request-metadata-path","title":"Phase 1: Layout Request (Metadata Path)","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant VIP\n    participant MDS1\n    participant Backend\n\n    Client-&gt;&gt;VIP: LAYOUTGET (file handle)\n    VIP-&gt;&gt;MDS1: Forward request\n    MDS1-&gt;&gt;Backend: Query file layout\n    Backend--&gt;&gt;MDS1: Layout map\n    MDS1--&gt;&gt;Client: LAYOUT (stripe pattern, DS list)\n    Note over Client: Client caches layout</code></pre> <p>Layout Information Returned: <pre><code>{\n  \"layout_type\": \"LAYOUT4_NFSV4_1_FILES\",\n  \"stripe_unit\": 1048576,\n  \"stripe_count\": 4,\n  \"data_servers\": [\n    \"10.10.1.11:2049\",  // Storage Node 1\n    \"10.10.1.12:2049\",  // Storage Node 2\n    \"10.10.1.13:2049\",  // Storage Node 3\n    \"10.10.1.14:2049\"   // Storage Node 4\n  ]\n}\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-2-parallel-data-io-data-path","title":"Phase 2: Parallel Data I/O (Data Path)","text":"<pre><code>graph LR\n    Client --&gt;|Stripe 0| DS1[Storage Node 1]\n    Client --&gt;|Stripe 1| DS2[Storage Node 2]\n    Client --&gt;|Stripe 3| DS3[Storage Node 3]\n    Client --&gt;|Stripe 4| DS4[Storage Node 4]\n\n    DS1 --&gt; NVMe1[NVMe SSD]\n    DS2 --&gt; NVMe2[NVMe SSD]\n    DS3 --&gt; NVMe3[NVMe SSD]\n    DS4 --&gt; NVMe4[NVMe SSD]</code></pre> <p>Throughput Calculation: <pre><code>Single NVMe: 7 GB/s read\n4-way stripe: 7 GB/s \u00d7 4 = 28 GB/s aggregate\nOverhead (20%): ~22 GB/s effective client throughput\n</code></pre></p> <p>Key Advantage</p> <p>The MDS is completely bypassed during data I/O. Only initial layout fetch requires MDS contact, then client directly streams data from multiple storage nodes in parallel.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#write-operation-with-coherency","title":"Write Operation with Coherency","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#challenges","title":"Challenges","text":"<ul> <li>Cache coherency: Multiple clients may access same file</li> <li>Consistency: Must maintain POSIX semantics</li> <li>Layout revocation: MDS may recall layouts during conflicts</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#write-flow","title":"Write Flow","text":"<pre><code>sequenceDiagram\n    participant Client1\n    participant Client2\n    participant MDS\n    participant DS1\n\n    Client1-&gt;&gt;MDS: OPEN (file, WRITE)\n    MDS--&gt;&gt;Client1: LAYOUT (read-write)\n    Client1-&gt;&gt;DS1: WRITE data\n\n    Client2-&gt;&gt;MDS: OPEN (same file, WRITE)\n    MDS-&gt;&gt;Client1: CB_LAYOUTRECALL\n    Client1-&gt;&gt;DS1: COMMIT writes\n    Client1-&gt;&gt;MDS: LAYOUTRETURN\n    MDS--&gt;&gt;Client2: LAYOUT (read-write)</code></pre> <p>Layout Recall Scenarios: 1. Write-write conflict: Second writer needs exclusive layout 2. Read-write conflict: Writer needs to invalidate reader caches 3. Layout change: File being migrated or restriped</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#high-availability-scenarios","title":"High Availability Scenarios","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#scenario-1-mds-node-failure","title":"Scenario 1: MDS Node Failure","text":"<pre><code>Before:\n  VIP \u2192 MDS1 (active)\n      \u2192 MDS2 (active)\n      \u2192 MDS3 (active)  \u2190 fails\n\nAfter (within 2 seconds):\n  VIP \u2192 MDS1 (active)  \u2190 absorbs load\n      \u2192 MDS2 (active)  \u2190 absorbs load\n\n  MDS3: Fenced by cluster, removed from VIP pool\n  Client layouts: Still valid, no client disruption\n</code></pre> <p>Recovery Actions: - Quorum maintained (2/3 nodes) - Clients continue data I/O unaffected - New metadata requests distributed to healthy MDS nodes - Failed MDS auto-rejoins after recovery</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#scenario-2-storage-node-failure","title":"Scenario 2: Storage Node Failure","text":"<pre><code>pNFS File with 4-way striping across nodes 1-4:\n  Node 3 fails \u2192 Stripes 2 (stored on node 3) unavailable\n\nClient behavior:\n  1. Client detects I/O error on stripe 2\n  2. Client returns partial read/write to application\n  3. Application must handle EIO (or use replication)\n\nRecovery:\n  - Option A: File replicated (pNFS server-side replication)\n             \u2192 Automatic failover to replica stripe\n  - Option B: No replication \u2192 Data loss for affected stripes\n</code></pre> <p>Data Durability</p> <p>pNFS itself does NOT provide redundancy. You must implement:</p> <ul> <li>Server-side replication (e.g., Lustre OST pools)</li> <li>Client-side RAID (mdadm over pNFS)</li> <li>Application-level erasure coding</li> <li>Regular snapshots/backups</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#scenario-3-network-partition-split-brain-prevention","title":"Scenario 3: Network Partition (Split-Brain Prevention)","text":"<pre><code>Network partition splits cluster:\n  Partition A: MDS1, MDS2 (2 nodes)\n  Partition B: MDS3 (1 node)\n\nQuorum voting:\n  Partition A: 2/3 nodes = HAS QUORUM \u2192 continues operation\n  Partition B: 1/3 nodes = NO QUORUM \u2192 enters read-only mode\n\nPrevention:\n  - Fencing agent (IPMI, PDU) forcibly powers off minority partition\n  - Prevents conflicting writes to shared backend\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#performance-tuning","title":"Performance Tuning","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#client-side-tunables","title":"Client-Side Tunables","text":"<pre><code># /etc/nfsmount.conf or mount options\nmount -t nfs4 -o \\\n  vers=4.2,\\                      # Enable pNFS\n  pnfs,\\                          # Use parallel NFS layouts\n  rsize=1048576,\\                 # 1MB read size\n  wsize=1048576,\\                 # 1MB write size\n  timeo=600,\\                     # 60s timeout\n  retrans=2,\\                     # 2 retransmissions\n  hard,\\                          # Hard mount (don't give up)\n  async,\\                         # Asynchronous I/O\n  ac,\\                            # Attribute caching\n  actimeo=3600 \\                  # 1-hour attribute cache\n  10.10.1.100:/export /mnt/pnfs\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#server-side-tunables","title":"Server-Side Tunables","text":"<pre><code># NFS server threads (per-node)\necho 256 &gt; /proc/sys/sunrpc/nfsd_threads\n\n# Network receive buffers\nsysctl -w net.core.rmem_max=134217728\nsysctl -w net.core.wmem_max=134217728\n\n# NVMe queue depth\necho 1024 &gt; /sys/block/nvme0n1/queue/nr_requests\n\n# Disable CPU frequency scaling (performance mode)\ncpupower frequency-set -g performance\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#monitoring-metrics","title":"Monitoring Metrics","text":"<pre><code>Key metrics to track:\n  - MDS operations/sec (LAYOUTGET, OPEN, CLOSE)\n  - Data server throughput (GB/s per node)\n  - Latency percentiles (p50, p95, p99)\n  - Client cache hit rates\n  - Network utilization (per fabric)\n  - NVMe IOPS and latency\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#implementation-deployment-checklist","title":"Implementation: Deployment Checklist","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-1-network-setup","title":"Phase 1: Network Setup","text":"<ul> <li> Deploy InfiniBand/RoCE fabric</li> <li> Configure storage VLAN with jumbo frames</li> <li> Enable RDMA on all nodes</li> <li> Verify bandwidth with <code>ib_write_bw</code> / <code>ib_read_bw</code></li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-2-storage-node-provisioning","title":"Phase 2: Storage Node Provisioning","text":"<ul> <li> Install NVMe SSDs and verify <code>nvme list</code></li> <li> Create XFS/ZFS filesystems</li> <li> Apply NVMe performance tunings</li> <li> Install <code>nfs-kernel-server</code> with pNFS support</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-3-mds-cluster-setup","title":"Phase 3: MDS Cluster Setup","text":"<ul> <li> Choose clustering backend (DRBD, etcd, etc.)</li> <li> Configure Pacemaker/Corosync or equivalent</li> <li> Set up VIP with failover tests</li> <li> Deploy metadata synchronization</li> <li> Test quorum and fencing</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-4-pnfs-configuration","title":"Phase 4: pNFS Configuration","text":"<ul> <li> Configure pNFS layouts on each storage node</li> <li> Export file systems via NFS4 with pNFS enabled</li> <li> Register data servers with MDS</li> <li> Test layout distribution from clients</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-5-client-deployment","title":"Phase 5: Client Deployment","text":"<ul> <li> Mount pNFS export with optimized parameters</li> <li> Verify parallel I/O with <code>dd</code> or <code>fio</code></li> <li> Test layout recall and coherency</li> <li> Run application workload benchmarks</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#phase-6-production-hardening","title":"Phase 6: Production Hardening","text":"<ul> <li> Set up monitoring (Prometheus + Grafana)</li> <li> Configure alerting for node failures</li> <li> Document failover procedures</li> <li> Schedule regular disaster recovery drills</li> <li> Implement backup strategy</li> </ul>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#real-world-performance","title":"Real-World Performance","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#benchmark-environment","title":"Benchmark Environment","text":"<pre><code>Hardware:\n  - 10x storage nodes (Dell R750)\n  - 4x 7.68TB NVMe per node (Samsung PM9A3)\n  - 100GbE RoCE network\n  - 2x AMD EPYC 7543 per node\n\nWorkload:\n  - FIO sequential read (4MB block size)\n  - 8 clients, 16 threads each\n</code></pre>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#results","title":"Results","text":"Metric Value Notes Aggregate Throughput 82 GB/s 10 nodes \u00d7 ~8 GB/s each Per-Client Throughput 10.2 GB/s 82 GB/s / 8 clients Latency (p99) 3.2 ms Network + NVMe + pNFS overhead MDS Load 2,300 ops/s Only layout requests CPU Utilization 35% avg Plenty of headroom <p>Key Takeaway</p> <p>pNFS achieved near-linear scaling across 10 storage nodes. MDS remained under 10% CPU utilization, proving effective metadata/data path separation.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#troubleshooting-guide","title":"Troubleshooting Guide","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#problem-clients-not-using-pnfs-falling-back-to-standard-nfs","title":"Problem: Clients not using pNFS (falling back to standard NFS)","text":"<p>Symptoms: <pre><code># All I/O going through MDS node\nnfsstat -m | grep \"pnfs\"  # Shows \"pnfs: not in use\"\n</code></pre></p> <p>Diagnosis: <pre><code># Check server pNFS support\nnfsstat -s | grep pnfs\n\n# Check client kernel support\ngrep PNFS /boot/config-$(uname -r)  # Should show CONFIG_PNFS_FILE_LAYOUT=m\n</code></pre></p> <p>Solution: - Ensure server exports with <code>pnfs</code> option - Verify client kernel has <code>nfs_layout_nfsv41_files</code> module loaded - Check for layout request denials in <code>/var/log/messages</code></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#problem-high-mds-cpu-usage","title":"Problem: High MDS CPU usage","text":"<p>Symptoms: <pre><code># MDS nodes showing &gt;80% CPU\ntop  # nfsd threads consuming CPU\n</code></pre></p> <p>Diagnosis: <pre><code># Check for excessive LAYOUTGET operations\nnfsstat -s | grep LAYOUTGET\n</code></pre></p> <p>Possible Causes: - Clients not caching layouts (check <code>actimeo</code>) - Frequent layout recalls (check for conflicting access patterns) - Insufficient MDS threads (check <code>nfsd_threads</code>)</p> <p>Solution: <pre><code># Increase client attribute cache timeout\nmount -o remount,actimeo=3600 /mnt/pnfs\n\n# Add more MDS threads\necho 512 &gt; /proc/sys/sunrpc/nfsd_threads\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#problem-uneven-storage-utilization","title":"Problem: Uneven storage utilization","text":"<p>Symptoms: <pre><code># One storage node at 90%, others at 40%\ndf -h /storage/*\n</code></pre></p> <p>Diagnosis: <pre><code># Check file layout distribution\n# (Requires pNFS-aware tooling or manual inspection)\n</code></pre></p> <p>Solution: - Re-stripe files: Use pNFS restripe tools if available - Balance new files: Adjust MDS layout selection algorithm - Add/remove nodes: Trigger cluster rebalancing</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#advanced-topics","title":"Advanced Topics","text":"","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#1-hierarchical-storage-management-hsm-with-pnfs","title":"1. Hierarchical Storage Management (HSM) with pNFS","text":"<p>Implement tiered storage by combining: - Hot tier: NVMe-backed pNFS for active data - Warm tier: SATA SSD pNFS for recent data - Cold tier: HDD-based object storage (S3) for archives</p> <p>Layout policy: <pre><code>def select_storage_tier(file_metadata):\n    if file_metadata.access_count &gt; 100:\n        return TIER_NVME\n    elif file_metadata.age_days &lt; 30:\n        return TIER_SSD\n    else:\n        return TIER_HDD\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#2-erasure-coding-for-space-efficiency","title":"2. Erasure Coding for Space Efficiency","text":"<p>Instead of replication (2x-3x overhead), use erasure coding: - Reed-Solomon (8+3): 1.375x overhead for 3-drive fault tolerance - RAID 6 equivalent: Stripe across pNFS with parity - Rebuild time: ~2 hours for 10TB per failed drive</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#3-multi-site-pnfs-replication","title":"3. Multi-Site pNFS Replication","text":"<p>For disaster recovery: <pre><code>Site A (Primary):          Site B (DR):\n  10 storage nodes    \u2192      10 storage nodes\n  Active MDS cluster  \u2192      Standby MDS cluster\n\nAsync replication (rsync/DRBD async or Lustre HSM)\n</code></pre></p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#conclusion","title":"Conclusion","text":"<p>This pNFS v4.2 architecture provides:</p> <p>\u2705 High throughput: 80+ GB/s aggregate via parallel I/O \u2705 Low latency: &lt;5ms p99 with InfiniBand/RoCE \u2705 High availability: No single points of failure \u2705 Horizontal scalability: Add nodes without downtime \u2705 Operational simplicity: Standard NFS client compatibility</p> <p>Trade-offs: - Complexity: More moving parts than traditional NAS - Data durability: Requires additional replication/erasure coding - Cost: High-speed network and NVMe SSDs increase CapEx</p> <p>Ideal for: - AI/ML training clusters (GPU \u2192 storage throughput) - HPC workloads (parallel file access patterns) - Video rendering farms (large file streaming) - High-frequency trading (low-latency shared storage)</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"blog/2025/11/10/pnfs-distributed-storage-architecture/#references","title":"References","text":"<ul> <li>RFC 8881 - NFSv4.1 Protocol</li> <li>RFC 7862 - NFSv4.2 Protocol</li> <li>Linux pNFS Documentation</li> <li>Lustre pNFS Guide</li> <li>Red Hat: Configuring pNFS</li> </ul> <p>Tags: #pNFS #distributed-storage #NVMe #high-availability #load-balancing #metadata #clustering #InfiniBand #RoCE #parallel-io #file-systems #linux #performance-tuning #scalability</p> <p>Category: Storage, Architecture</p> <p>Have questions or running a similar setup? Open a discussion or reach out.</p>","tags":["pNFS","distributed-storage","NVMe","high-availability","load-balancing","metadata","clustering","InfiniBand","RoCE","parallel-io"]},{"location":"journal/","title":"Journal","text":"<p>Time-based log entries, learning notes, and progress updates.</p>"},{"location":"journal/#coming-soon","title":"Coming Soon","text":"<p>This section will contain:</p> <ul> <li>Learning Logs: Daily/weekly learning summaries</li> <li>Project Progress: Implementation updates</li> <li>Experiments: Technical experiments and findings</li> <li>Quick Notes: Short observations and discoveries</li> </ul> <p>Check back soon or watch the repository for updates.</p>"},{"location":"kb/","title":"Knowledge Base","text":"<p>Evergreen reference material, technical documentation, and curated resources.</p>"},{"location":"kb/#coming-soon","title":"Coming Soon","text":"<p>This section will contain:</p> <ul> <li>System Design Patterns: Reusable architectural patterns</li> <li>Protocol References: Detailed protocol documentation</li> <li>Tool Guides: Configuration and usage guides</li> <li>Troubleshooting Playbooks: Common issues and solutions</li> <li>Performance Baselines: Benchmark data and analysis</li> </ul> <p>Check back soon or watch the repository for updates.</p>"},{"location":"kubernetes/","title":"Kubernetes CKA Mastery","text":"<p>Complete hands-on guide to Kubernetes administration and CKA certification</p>"},{"location":"kubernetes/#about-the-cka-exam","title":"\ud83c\udfaf About the CKA Exam","text":"<p>The Certified Kubernetes Administrator (CKA) certification demonstrates proficiency in Kubernetes cluster administration, troubleshooting, and operations.</p>"},{"location":"kubernetes/#exam-details","title":"Exam Details","text":"<ul> <li>Duration: 2 hours</li> <li>Format: ~17 performance-based tasks (100% hands-on terminal work)</li> <li>Pass Score: 66%</li> <li>Cost: $445 (includes one free retake)</li> <li>Environment: Remote proctored, browser-based terminal</li> </ul>"},{"location":"kubernetes/#exam-domains-weights","title":"Exam Domains &amp; Weights","text":"<pre><code>pie title CKA Exam Domain Distribution\n    \"Troubleshooting\" : 30\n    \"Cluster Architecture\" : 25\n    \"Services &amp; Networking\" : 20\n    \"Workloads &amp; Scheduling\" : 15\n    \"Storage\" : 10</code></pre> Domain Weight Focus Areas Troubleshooting 30% Cluster/node issues, application debugging, monitoring Cluster Architecture 25% Installation, upgrades, RBAC, security, CRDs Services &amp; Networking 20% Services, Ingress, Gateway API, Network Policies Workloads &amp; Scheduling 15% Deployments, scheduling, pod configuration Storage 10% PV/PVC, ConfigMaps, Secrets, StorageClasses"},{"location":"kubernetes/#learning-path","title":"\ud83d\udcda Learning Path","text":"<p>This series covers 22 comprehensive posts organized into 7 phases, following the optimal learning sequence for CKA exam success.</p> <pre><code>graph TD\n    A[Phase 1: Foundations] --&gt; B[Phase 2: Workloads]\n    B --&gt; C[Phase 3: Networking]\n    C --&gt; D[Phase 4: Storage]\n    D --&gt; E[Phase 5: Security]\n    E --&gt; F[Phase 6: Advanced Config]\n    F --&gt; G[Phase 7: Troubleshooting]\n\n    A --&gt; A1[Architecture]\n    A --&gt; A2[Lab Setup]\n    A --&gt; A3[kubectl Basics]\n    A --&gt; A4[YAML &amp; Objects]\n    A --&gt; A5[Namespaces]\n\n    B --&gt; B1[Pods]\n    B --&gt; B2[Deployments]\n    B --&gt; B3[Scheduling]\n\n    C --&gt; C1[Services]\n    C --&gt; C2[Ingress/Gateway]\n    C --&gt; C3[Network Policies]\n    C --&gt; C4[DNS]\n\n    D --&gt; D1[PV/PVC]\n    D --&gt; D2[ConfigMaps/Secrets]\n\n    E --&gt; E1[RBAC]\n    E --&gt; E2[Security Contexts]\n    E --&gt; E3[CRDs/Operators]\n\n    F --&gt; F1[Helm]\n    F --&gt; F2[Kustomize]\n\n    G --&gt; G1[Cluster Troubleshooting]\n    G --&gt; G2[App Troubleshooting]\n    G --&gt; G3[Monitoring]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e8\n    style D fill:#f3e5f5\n    style E fill:#ffe5e5\n    style F fill:#fff9e5\n    style G fill:#ffe5f0</code></pre>"},{"location":"kubernetes/#phase-1-foundations-5-posts","title":"\ud83c\udfd7\ufe0f Phase 1: Foundations (5 posts)","text":"<p>Build your foundational knowledge of Kubernetes architecture and essential tools.</p>"},{"location":"kubernetes/#1-kubernetes-architecture-fundamentals","title":"1. Kubernetes Architecture Fundamentals","text":"<p>Control plane components, worker nodes, etcd, API server, scheduler, controller manager Tags: <code>kubernetes</code> <code>architecture</code> <code>fundamentals</code> <code>cka-prep</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#2-setting-up-your-kubernetes-lab-environment","title":"2. Setting Up Your Kubernetes Lab Environment","text":"<p>kubeadm, Minikube, kind, kubectl installation, kubeconfig management Tags: <code>kubernetes</code> <code>installation</code> <code>lab-setup</code> <code>kubeadm</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#3-kubectl-essentials-your-kubernetes-swiss-army-knife","title":"3. kubectl Essentials: Your Kubernetes Swiss Army Knife","text":"<p>Master kubectl commands, aliases, output formats, context switching, imperative vs declarative Tags: <code>kubernetes</code> <code>kubectl</code> <code>cli</code> <code>basics</code> Domain: All (foundational skill)</p>"},{"location":"kubernetes/#4-understanding-kubernetes-objects-and-yaml-manifests","title":"4. Understanding Kubernetes Objects and YAML Manifests","text":"<p>API objects, YAML syntax, metadata, spec, status, labels, annotations, selectors Tags: <code>kubernetes</code> <code>yaml</code> <code>objects</code> <code>manifests</code> Domain: All (foundational skill)</p>"},{"location":"kubernetes/#5-namespaces-and-resource-quotas","title":"5. Namespaces and Resource Quotas","text":"<p>Namespace isolation, resource quotas, limit ranges, default namespace management Tags: <code>kubernetes</code> <code>namespaces</code> <code>resource-management</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#phase-2-workloads-scheduling-3-posts","title":"\u2699\ufe0f Phase 2: Workloads &amp; Scheduling (3 posts)","text":"<p>Master pod management, deployments, and advanced scheduling techniques.</p>"},{"location":"kubernetes/#6-pods-the-atomic-unit-of-kubernetes","title":"6. Pods: The Atomic Unit of Kubernetes","text":"<p>Pod lifecycle, init containers, sidecar patterns, multi-container communication Tags: <code>kubernetes</code> <code>pods</code> <code>workloads</code> <code>containers</code> Domain: Workloads &amp; Scheduling (15%)</p>"},{"location":"kubernetes/#7-deployments-replicasets-and-rolling-updates","title":"7. Deployments, ReplicaSets, and Rolling Updates","text":"<p>Deployments, ReplicaSets, DaemonSets, StatefulSets, rollouts, rollback strategies Tags: <code>kubernetes</code> <code>deployments</code> <code>replicasets</code> <code>workloads</code> Domain: Workloads &amp; Scheduling (15%)</p>"},{"location":"kubernetes/#8-advanced-scheduling-taints-tolerations-and-affinity","title":"8. Advanced Scheduling: Taints, Tolerations, and Affinity","text":"<p>Node selectors, taints/tolerations, node/pod affinity, anti-affinity, priority classes Tags: <code>kubernetes</code> <code>scheduling</code> <code>advanced</code> <code>affinity</code> Domain: Workloads &amp; Scheduling (15%)</p>"},{"location":"kubernetes/#phase-3-services-networking-4-posts","title":"\ud83c\udf10 Phase 3: Services &amp; Networking (4 posts)","text":"<p>Deep dive into Kubernetes networking, service discovery, and traffic management.</p>"},{"location":"kubernetes/#9-kubernetes-services-exposing-your-applications","title":"9. Kubernetes Services: Exposing Your Applications","text":"<p>ClusterIP, NodePort, LoadBalancer, ExternalName, service discovery, endpoints Tags: <code>kubernetes</code> <code>services</code> <code>networking</code> <code>service-discovery</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#10-ingress-and-gateway-api-modern-traffic-management","title":"10. Ingress and Gateway API: Modern Traffic Management","text":"<p>Ingress controllers, Ingress rules, Gateway API (GatewayClass, Gateway, HTTPRoute) Tags: <code>kubernetes</code> <code>ingress</code> <code>gateway-api</code> <code>traffic-management</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#11-network-policies-securing-pod-communication","title":"11. Network Policies: Securing Pod Communication","text":"<p>NetworkPolicy resources, ingress/egress rules, pod/namespace selectors, isolation Tags: <code>kubernetes</code> <code>network-policies</code> <code>security</code> <code>networking</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#12-coredns-and-service-discovery-deep-dive","title":"12. CoreDNS and Service Discovery Deep Dive","text":"<p>CoreDNS configuration, DNS for Services and Pods, troubleshooting DNS issues Tags: <code>kubernetes</code> <code>dns</code> <code>coredns</code> <code>service-discovery</code> Domain: Services &amp; Networking (20%)</p>"},{"location":"kubernetes/#phase-4-storage-2-posts","title":"\ud83d\udcbe Phase 4: Storage (2 posts)","text":"<p>Understand persistent storage and configuration management in Kubernetes.</p>"},{"location":"kubernetes/#13-persistent-volumes-and-claims-stateful-storage","title":"13. Persistent Volumes and Claims: Stateful Storage","text":"<p>PV, PVC, StorageClass, access modes, reclaim policies, dynamic provisioning Tags: <code>kubernetes</code> <code>storage</code> <code>persistent-volumes</code> <code>stateful</code> Domain: Storage (10%)</p>"},{"location":"kubernetes/#14-configmaps-secrets-and-volume-mounts","title":"14. ConfigMaps, Secrets, and Volume Mounts","text":"<p>ConfigMaps, Secrets, volume mounts, environment variables, projected volumes Tags: <code>kubernetes</code> <code>configmaps</code> <code>secrets</code> <code>configuration</code> Domain: Storage (10%)</p>"},{"location":"kubernetes/#phase-5-security-configuration-3-posts","title":"\ud83d\udd12 Phase 5: Security &amp; Configuration (3 posts)","text":"<p>Secure your cluster with RBAC, security contexts, and extensibility.</p>"},{"location":"kubernetes/#15-rbac-role-based-access-control","title":"15. RBAC: Role-Based Access Control","text":"<p>Roles, ClusterRoles, RoleBindings, ClusterRoleBindings, ServiceAccounts Tags: <code>kubernetes</code> <code>rbac</code> <code>security</code> <code>access-control</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#16-security-contexts-and-pod-security-standards","title":"16. Security Contexts and Pod Security Standards","text":"<p>SecurityContext, Pod Security Admission, privileged containers, capabilities, PSS Tags: <code>kubernetes</code> <code>security</code> <code>pod-security</code> <code>hardening</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#17-custom-resources-and-operators-crds","title":"17. Custom Resources and Operators (CRDs)","text":"<p>CustomResourceDefinitions, custom controllers, Operators, CRD inspection Tags: <code>kubernetes</code> <code>crds</code> <code>operators</code> <code>extensibility</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#phase-6-advanced-configuration-2-posts","title":"\ud83d\udd27 Phase 6: Advanced Configuration (2 posts)","text":"<p>Master Helm and Kustomize for production-grade configuration management.</p>"},{"location":"kubernetes/#18-helm-kubernetes-package-manager","title":"18. Helm: Kubernetes Package Manager","text":"<p>Helm charts, templating, values files, releases, hooks, chart repositories Tags: <code>kubernetes</code> <code>helm</code> <code>package-management</code> <code>charts</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#19-kustomize-template-free-configuration-management","title":"19. Kustomize: Template-Free Configuration Management","text":"<p>Kustomize bases, overlays, patches, transformers, generators, GitOps patterns Tags: <code>kubernetes</code> <code>kustomize</code> <code>configuration</code> <code>gitops</code> Domain: Cluster Architecture (25%)</p>"},{"location":"kubernetes/#phase-7-troubleshooting-monitoring-3-posts","title":"\ud83d\udd0d Phase 7: Troubleshooting &amp; Monitoring (3 posts)","text":"<p>Become an expert at diagnosing and resolving Kubernetes issues.</p>"},{"location":"kubernetes/#20-troubleshooting-clusters-nodes-and-components","title":"20. Troubleshooting Clusters, Nodes, and Components","text":"<p>Node issues, control plane debugging, certificate problems, etcd health checks Tags: <code>kubernetes</code> <code>troubleshooting</code> <code>debugging</code> <code>cluster-health</code> Domain: Troubleshooting (30%)</p>"},{"location":"kubernetes/#21-application-troubleshooting-and-log-analysis","title":"21. Application Troubleshooting and Log Analysis","text":"<p>Pod debugging, container logs, exec commands, ephemeral containers, event analysis Tags: <code>kubernetes</code> <code>troubleshooting</code> <code>logs</code> <code>debugging</code> Domain: Troubleshooting (30%)</p>"},{"location":"kubernetes/#22-monitoring-metrics-and-resource-management","title":"22. Monitoring, Metrics, and Resource Management","text":"<p>Metrics Server, resource requests/limits, HPA, VPA, monitoring stack integration Tags: <code>kubernetes</code> <code>monitoring</code> <code>metrics</code> <code>autoscaling</code> Domain: Troubleshooting (30%)</p>"},{"location":"kubernetes/#how-to-use-this-series","title":"\ud83d\udcd6 How to Use This Series","text":""},{"location":"kubernetes/#recommended-study-approach","title":"Recommended Study Approach","text":"<ol> <li>Follow the Order: Posts are sequenced for optimal learning progression</li> <li>Hands-On Practice: Set up a lab environment (Post 2) and practice every command</li> <li>Take Notes: Create your own command cheat sheets as you progress</li> <li>Review Diagrams: Study the architecture diagrams to understand component relationships</li> <li>Do the Exercises: Complete practice tasks at the end of each post</li> <li>Cross-Reference: Use links between posts to review related concepts</li> </ol>"},{"location":"kubernetes/#study-timeline","title":"Study Timeline","text":"<ul> <li>Intensive: 4-6 weeks (1 post per day)</li> <li>Standard: 8-12 weeks (2-3 posts per week)</li> <li>Relaxed: 3-4 months (1-2 posts per week)</li> </ul>"},{"location":"kubernetes/#exam-preparation-tips","title":"Exam Preparation Tips","text":"<p>\u2705 Do: - Practice in a terminal environment (exam is 100% command-line) - Use <code>kubectl</code> imperative commands for speed - Master <code>kubectl explain</code> and <code>-h</code> flags for in-exam reference - Time yourself on practice exercises - Focus heavily on Troubleshooting (30% weight)</p> <p>\u274c Don't: - Memorize YAML templates (use <code>kubectl</code> generators instead) - Ignore troubleshooting topics (highest exam weight) - Skip hands-on practice (reading alone is insufficient) - Forget about time management (2 hours goes fast)</p>"},{"location":"kubernetes/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<p>Before starting this series, you should have:</p> <ul> <li>Basic understanding of Linux command line</li> <li>Familiarity with containerization concepts (Docker)</li> <li>Access to a Linux/macOS machine or Windows with WSL2</li> <li>Willingness to practice hands-on (not just read)</li> </ul>"},{"location":"kubernetes/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>Official CKA Exam Page</li> <li>Kubernetes Official Documentation</li> <li>kubectl Command Reference</li> <li>CKA Curriculum (Official)</li> </ul>"},{"location":"kubernetes/#ready-to-start","title":"\ud83d\ude80 Ready to Start?","text":"<p>Begin your journey with Post 1: Kubernetes Architecture Fundamentals and work through the series systematically.</p> <p>Good luck with your CKA certification! \ud83c\udf93</p> <p>Last Updated: 2025-11-10 Series Status: In Progress (Post 1 available) Total Posts: 22 planned</p>"},{"location":"principles/","title":"Principles","text":"<p>Engineering principles, design philosophies, and decision-making frameworks.</p>"},{"location":"principles/#coming-soon","title":"Coming Soon","text":"<p>This section will explore:</p> <ul> <li>Architecture Principles: Foundational design guidelines</li> <li>Performance Principles: Optimization philosophies</li> <li>Reliability Principles: Building resilient systems</li> <li>Scalability Principles: Growing systems effectively</li> <li>Simplicity Principles: Managing complexity</li> </ul> <p>Check back soon or watch the repository for updates.</p>"},{"location":"blog/archive/2025/","title":"November 2025","text":""},{"location":"blog/category/kubernetes/","title":"Kubernetes","text":""},{"location":"blog/category/architecture/","title":"Architecture","text":""},{"location":"blog/category/storage/","title":"Storage","text":""}]}